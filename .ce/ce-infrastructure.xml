This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where line numbers have been added.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: .ce/PRPs/executed/system/PRP-0-CONTEXT-ENGINEERING.md, .serena/memories/README.md, .serena/memories/code-style-conventions.md, .serena/memories/suggested-commands.md, .serena/memories/task-completion-checklist.md, .serena/memories/testing-standards.md, .serena/memories/tool-usage-syntropy.md, .serena/memories/use-syntropy-tools-not-bash.md, .serena/memories/codebase-structure.md, .serena/memories/cwe78-prp22-newline-escape-issue.md, .serena/memories/l4-validation-usage.md, .serena/memories/linear-issue-creation-pattern.md, .serena/memories/linear-issue-tracking-integration.md, .serena/memories/linear-mcp-integration-example.md, .serena/memories/linear-mcp-integration.md, .serena/memories/project-overview.md, .serena/memories/PRP-15-remediation-workflow-implementation.md, .serena/memories/prp-2-implementation-patterns.md, .serena/memories/prp-backlog-system.md, .serena/memories/prp-structure-initialized.md, .serena/memories/serena-implementation-verification-pattern.md, .serena/memories/serena-mcp-tool-restrictions.md, .serena/memories/syntropy-status-hook-pattern.md, .serena/memories/system-model-specification.md, .serena/memories/tool-config-optimization-completed.md, .claude/commands/batch-exe-prp.md, .claude/commands/batch-gen-prp.md, .claude/commands/denoise.md, .claude/commands/execute-prp.md, .claude/commands/generate-prp.md, .claude/commands/peer-review.md, .claude/commands/sync-with-syntropy.md, .claude/commands/syntropy-health.md, .claude/commands/tools-misuse-scan.md, .claude/commands/update-context.md, .claude/commands/vacuum.md, tools/ce/*.py, tools/pyproject.toml, tools/bootstrap.sh, .claude/settings.local.json, CLAUDE.md
- Files matching these patterns are excluded: examples/patterns/example-simple-feature.md, examples/patterns/git-message-rules.md, examples/l4-validation-example.md, examples/syntropy-status-hook-system.md, tools/tests/**, .git/**, .tmp/**, __pycache__/**, *.pyc
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Line numbers have been added to the beginning of each line
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.ce/
  PRPs/
    executed/
      system/
        PRP-0-CONTEXT-ENGINEERING.md
.claude/
  commands/
    batch-exe-prp.md
    batch-gen-prp.md
    denoise.md
    execute-prp.md
    generate-prp.md
    peer-review.md
    sync-with-syntropy.md
    syntropy-health.md
    tools-misuse-scan.md
    update-context.md
    vacuum.md
  settings.local.json
tools/
  ce/
    __init__.py
    __main__.py
    blend.py
    blueprint_parser.py
    cli_handlers.py
    code_analyzer.py
    context.py
    core.py
    drift_analyzer.py
    drift.py
    exceptions.py
    execute.py
    generate.py
    init_project.py
    linear_mcp_resilience.py
    linear_utils.py
    logging_config.py
    markdown_lint.py
    mcp_adapter.py
    mcp_utils.py
    mermaid_validator.py
    metrics.py
    pattern_detectors.py
    pattern_extractor.py
    pipeline.py
    profiling.py
    prp_analyzer.py
    prp.py
    repomix_unpack.py
    resilience.py
    shell_utils.py
    update_context.py
    vacuum.py
    validate_permissions.py
    validate.py
    validation_loop.py
  bootstrap.sh
  pyproject.toml
CLAUDE.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/commands/update-context.md">
  1: # Update Context
  2: 
  3: Sync Context Engineering (CE) and Serena knowledge systems with actual codebase implementation state.
  4: 
  5: ## Usage
  6: ```bash
  7: /update-context [--prp <prp-file>]
  8: ```
  9: 
 10: ## Parameters
 11: - `--prp <prp-file>`: Optional. Target specific PRP file for sync (path relative to project root)
 12: 
 13: ## What it does
 14: 
 15: Universal system hygiene command that maintains bidirectional sync between knowledge systems and codebase:
 16: 
 17: 1. **Scans PRPs**
 18:    - Universal mode: All PRPs in `PRPs/feature-requests/` and `PRPs/executed/`
 19:    - Targeted mode: Single PRP specified with `--prp` flag
 20: 
 21: 2. **Updates YAML Headers**
 22:    - Sets `context_sync.ce_updated` flag (based on implementation verification)
 23:    - Sets `context_sync.serena_updated` flag (if Serena MCP available)
 24:    - Adds `last_sync` timestamp
 25:    - Updates `updated_by` attribution
 26: 
 27: 3. **Verifies Implementations**
 28:    - Extracts expected functions/classes from PRP content
 29:    - Cross-references with actual codebase via Serena MCP
 30:    - Marks `ce_updated=true` only if ALL expected implementations found
 31: 
 32: 4. **Manages PRP Status**
 33:    - Auto-transitions PRPs from `status: new` â†’ `status: executed` when verified
 34:    - Moves files from `PRPs/feature-requests/` â†’ `PRPs/executed/` atomically
 35:    - Identifies deprecated PRPs for archival
 36: 
 37: 5. **Detects Pattern Drift**
 38:    - **Code Violations**: Scans codebase for violations of documented patterns (examples/)
 39:    - **Missing Examples**: Identifies critical PRPs without corresponding pattern documentation
 40:    - Generates structured drift report with solution proposals
 41:    - Saves report to `.ce/drift-report.md`
 42: 
 43: 6. **Reports Results**
 44:    - Summary statistics (PRPs scanned/updated/moved)
 45:    - Drift score and violation count
 46:    - Clear logging of all operations
 47: 
 48: ## Examples
 49: 
 50: ```bash
 51: # Sync all PRPs with codebase
 52: /update-context
 53: 
 54: # Sync specific PRP only
 55: /update-context --prp PRPs/executed/PRP-13-production-hardening.md
 56: 
 57: # Typical workflow
 58: # 1. Implement feature
 59: # 2. Run /update-context to verify and sync
 60: # 3. Review drift report if generated
 61: # 4. Fix violations or create missing examples
 62: ```
 63: 
 64: ## When to Use
 65: 
 66: **Run /update-context after:**
 67: - Completing PRP implementation
 68: - Significant codebase refactoring
 69: - Adding new examples/ patterns
 70: - Weekly system hygiene (prevent drift accumulation)
 71: 
 72: **Run with --prp flag when:**
 73: - Testing single PRP verification
 74: - Debugging context sync issues
 75: - Quick spot-check after small change
 76: 
 77: ## Drift Detection
 78: 
 79: When drift is detected, generates `.ce/drift-report.md` with:
 80: 
 81: **Part 1: Code Violating Documented Patterns**
 82: - Error handling violations (bare except, missing troubleshooting)
 83: - Naming convention violations (versioned suffixes)
 84: - KISS violations (overcomplicated implementations)
 85: 
 86: **Part 2: Missing Pattern Documentation**
 87: - Critical PRPs (complexity â‰¥ medium) without examples/
 88: - Suggested example paths
 89: - Rationale for documentation need
 90: 
 91: **Each violation includes:**
 92: - File location
 93: - Specific issue
 94: - Pattern reference
 95: - Proposed solution
 96: 
 97: ## Graceful Degradation
 98: 
 99: - **Serena MCP unavailable**: Continues with warnings, sets `serena_updated=false`
100: - **examples/ missing**: Skips drift detection with info log
101: - **Invalid YAML**: Skips file with warning, continues with others
102: - **Permission errors**: Raises error with troubleshooting guidance (no silent failures)
103: 
104: ## YAML Header Updates
105: 
106: Example before:
107: ```yaml
108: status: new
109: context_sync:
110:   ce_updated: false
111:   serena_updated: false
112: ```
113: 
114: Example after:
115: ```yaml
116: status: executed
117: context_sync:
118:   ce_updated: true
119:   serena_updated: true
120:   last_sync: 2025-10-14T17:00:00Z
121: updated_by: update-context-command
122: ```
123: 
124: ## Success Criteria
125: 
126: âœ… All PRPs scanned successfully
127: âœ… YAML headers updated accurately
128: âœ… Status transitions executed correctly
129: âœ… Files moved atomically (no data loss)
130: âœ… Drift detection identifies real violations
131: âœ… No false positives in pattern checks
132: 
133: ## Related Commands
134: 
135: - `/generate-prp` - Create new PRP blueprint
136: - `/execute-prp` - Implement PRP feature
137: - `/peer-review` - Review PRP or execution
138: - `/validate-prp-system` - Comprehensive system validation
139: 
140: **Goal:** Prevent documentation rot through automated sync verification and drift detection.
</file>

<file path="tools/ce/__init__.py">
1: """Context Engineering CLI Tools.
2: 
3: Minimal, efficient tooling for Context Engineering framework operations.
4: """
5: 
6: __version__ = "0.1.0"
</file>

<file path="tools/ce/blueprint_parser.py">
  1: """PRP blueprint parsing functions.
  2: 
  3: Extracts implementation phases from PRP IMPLEMENTATION BLUEPRINT markdown sections.
  4: Parses structured blueprint data into executable phase dictionaries.
  5: """
  6: 
  7: import re
  8: from typing import Dict, Any, List, Optional
  9: from pathlib import Path
 10: 
 11: from .exceptions import BlueprintParseError
 12: 
 13: 
 14: def parse_blueprint(prp_path: str) -> List[Dict[str, Any]]:
 15:     """Parse PRP IMPLEMENTATION BLUEPRINT into executable phases.
 16: 
 17:     Args:
 18:         prp_path: Path to PRP markdown file
 19: 
 20:     Returns:
 21:         [
 22:             {
 23:                 "phase_number": 1,
 24:                 "phase_name": "Core Logic Implementation",
 25:                 "goal": "Implement main authentication flow",
 26:                 "approach": "Class-based with async methods",
 27:                 "hours": 4.0,
 28:                 "files_to_modify": [
 29:                     {"path": "src/auth.py", "description": "Add auth logic"}
 30:                 ],
 31:                 "files_to_create": [
 32:                     {"path": "src/models/user.py", "description": "User model"}
 33:                 ],
 34:                 "functions": [
 35:                     {
 36:                         "signature": "def authenticate(username: str) -> User:",
 37:                         "docstring": "Authenticate user with credentials",
 38:                         "full_code": "<complete function body if provided>"
 39:                     }
 40:                 ],
 41:                 "validation_command": "pytest tests/test_auth.py -v",
 42:                 "checkpoint_command": "git add src/ && git commit -m 'feat: auth'"
 43:             },
 44:             # ... more phases
 45:         ]
 46: 
 47:     Raises:
 48:         FileNotFoundError: If PRP file doesn't exist
 49:         BlueprintParseError: If blueprint section missing or malformed
 50: 
 51:     Process:
 52:         1. Read PRP file
 53:         2. Extract ## ðŸ”§ Implementation Blueprint section
 54:         3. Split by ### Phase N: pattern
 55:         4. For each phase:
 56:            a. Extract phase number, name, hours from heading
 57:            b. Extract **Goal**: text
 58:            c. Extract **Approach**: text
 59:            d. Parse **Files to Modify**: list
 60:            e. Parse **Files to Create**: list
 61:            f. Extract **Key Functions**: code blocks
 62:            g. Extract **Validation Command**: command
 63:            h. Extract **Checkpoint**: git command
 64:         5. Validate required fields present
 65:     """
 66:     # Check file exists
 67:     path = Path(prp_path)
 68:     if not path.exists():
 69:         raise FileNotFoundError(
 70:             f"PRP file not found: {prp_path}\n"
 71:             f"ðŸ”§ Troubleshooting: Verify file path is correct"
 72:         )
 73: 
 74:     # Read file
 75:     content = path.read_text()
 76: 
 77:     # Extract IMPLEMENTATION BLUEPRINT section
 78:     # Note: (?=\n## [^#]) ensures we stop at ## headers (not ###)
 79:     blueprint_match = re.search(
 80:         r"##\s+ðŸ”§\s+Implementation\s+Blueprint\s*\n(.*?)(?=\n## [^#]|\Z)",
 81:         content,
 82:         re.DOTALL | re.IGNORECASE
 83:     )
 84: 
 85:     if not blueprint_match:
 86:         raise BlueprintParseError(
 87:             prp_path,
 88:             "Missing '## ðŸ”§ Implementation Blueprint' section\n"
 89:             "ðŸ”§ Troubleshooting:\n"
 90:             "   - Ensure PRP file contains Implementation Blueprint section\n"
 91:             "   - Check section header format (must include ðŸ”§ emoji)\n"
 92:             "   - Reference: examples/system-prps/ for correct format"
 93:         )
 94: 
 95:     blueprint_text = blueprint_match.group(1)
 96: 
 97:     # Split by phase headings: ### Phase N: Name (X hours)
 98:     phase_pattern = r"###\s+Phase\s+(\d+):\s+([^\(]+)\(([^)]+)\)"
 99:     phase_splits = list(re.finditer(phase_pattern, blueprint_text))
100: 
101:     if not phase_splits:
102:         raise BlueprintParseError(
103:             prp_path,
104:             "No phases found (expected '### Phase N: Name (X hours)' format)\n"
105:             "ðŸ”§ Troubleshooting:\n"
106:             "   - Add phase sections: ### Phase 1: Name (X hours)\n"
107:             "   - Ensure phases are numbered sequentially\n"
108:             "   - Reference: examples/system-prps/example-simple-feature.md"
109:         )
110: 
111:     phases = []
112: 
113:     for i, match in enumerate(phase_splits):
114:         phase_number = int(match.group(1))
115:         phase_name = match.group(2).strip()
116:         hours_str = match.group(3).strip()
117: 
118:         # Parse hours (handle "X hours", "X.Y hours", etc.)
119:         hours_match = re.search(r"(\d+(?:\.\d+)?)", hours_str)
120:         hours = float(hours_match.group(1)) if hours_match else 0.0
121: 
122:         # Extract phase content (from this phase to next phase or end)
123:         start = match.end()
124:         end = phase_splits[i + 1].start() if i + 1 < len(phase_splits) else len(blueprint_text)
125:         phase_text = blueprint_text[start:end]
126: 
127:         # Parse phase content
128:         phase_data = {
129:             "phase_number": phase_number,
130:             "phase_name": phase_name,
131:             "hours": hours,
132:             "goal": extract_field(phase_text, r"\*\*Goal\*\*:\s*(.+?)(?=\n\n|\*\*|$)", prp_path),
133:             "approach": extract_field(phase_text, r"\*\*Approach\*\*:\s*(.+?)(?=\n\n|\*\*|$)", prp_path),
134:             "files_to_modify": parse_file_list(phase_text, "Files to Modify"),
135:             "files_to_create": parse_file_list(phase_text, "Files to Create"),
136:             "functions": extract_function_signatures(phase_text),
137:             "validation_command": extract_field(
138:                 phase_text,
139:                 r"\*\*Validation\s+Command\*\*:\s*`([^`]+)`",
140:                 prp_path,
141:                 required=False
142:             ),
143:             "checkpoint_command": extract_field(
144:                 phase_text,
145:                 r"\*\*Checkpoint\*\*:\s*`([^`]+)`",
146:                 prp_path,
147:                 required=False
148:             )
149:         }
150: 
151:         phases.append(phase_data)
152: 
153:     return phases
154: 
155: 
156: def extract_field(
157:     text: str,
158:     pattern: str,
159:     prp_path: str,
160:     required: bool = True
161: ) -> Optional[str]:
162:     """Extract a field from phase text using regex.
163: 
164:     Args:
165:         text: Phase text to search
166:         pattern: Regex pattern with one capture group
167:         prp_path: PRP path for error messages
168:         required: Whether field is required
169: 
170:     Returns:
171:         Extracted value or None if not found and not required
172: 
173:     Raises:
174:         BlueprintParseError: If required field not found
175:     """
176:     match = re.search(pattern, text, re.DOTALL)
177:     if not match:
178:         if required:
179:             raise BlueprintParseError(
180:                 prp_path,
181:                 f"Required field not found using pattern: {pattern}\n"
182:                 f"ðŸ”§ Troubleshooting:\n"
183:                 f"   - Check field name spelling (capitalization matters)\n"
184:                 f"   - Verify the field uses ** ** formatting: **Field Name**: value\n"
185:                 f"   - Common patterns: **Goal**, **Approach**, **Validation Command**, **Checkpoint**"
186:             )
187:         return None
188: 
189:     return match.group(1).strip()
190: 
191: 
192: def parse_file_list(section_text: str, marker: str) -> List[Dict[str, str]]:
193:     """Parse **Files to Modify**: or **Files to Create**: section.
194: 
195:     Args:
196:         section_text: Phase section text
197:         marker: "Files to Modify" or "Files to Create"
198: 
199:     Returns:
200:         [
201:             {"path": "src/auth.py", "description": "Add auth logic"},
202:             {"path": "tests/test_auth.py", "description": "Add tests"}
203:         ]
204: 
205:     Pattern:
206:         **Files to Modify**:
207:         - `path/to/file.py` - Description
208:     """
209:     result = []
210: 
211:     # Find the marker section
212:     marker_pattern = rf"\*\*{re.escape(marker)}\*\*:\s*\n((?:- `[^`]+` - [^\n]+\n?)*)"
213:     match = re.search(marker_pattern, section_text, re.MULTILINE)
214: 
215:     if not match:
216:         return []
217: 
218:     list_content = match.group(1)
219: 
220:     # Parse each list item: - `path/to/file.py` - Description
221:     item_pattern = r"- `([^`]+)` - (.+)"
222:     for item_match in re.finditer(item_pattern, list_content):
223:         result.append({
224:             "path": item_match.group(1).strip(),
225:             "description": item_match.group(2).strip()
226:         })
227: 
228:     return result
229: 
230: 
231: def extract_function_signatures(phase_text: str) -> List[Dict[str, str]]:
232:     """Extract function signatures from **Key Functions**: code blocks.
233: 
234:     Args:
235:         phase_text: Phase section text
236: 
237:     Returns:
238:         [
239:             {
240:                 "signature": "def authenticate(username: str) -> User:",
241:                 "docstring": "Authenticate user with credentials",
242:                 "full_code": "<complete function body if provided>"
243:             }
244:         ]
245:     """
246:     result = []
247: 
248:     # Find **Key Functions**: section followed by ```python code block
249:     func_pattern = r"\*\*Key\s+Functions\*\*:.*?```python\s*\n(.*?)```"
250:     matches = re.finditer(func_pattern, phase_text, re.DOTALL | re.IGNORECASE)
251: 
252:     for match in matches:
253:         code_block = match.group(1).strip()
254: 
255:         # Split by function definitions
256:         func_defs = re.split(r'\n(?=def |async def |class )', code_block)
257: 
258:         for func_def in func_defs:
259:             func_def = func_def.strip()
260:             if not func_def:
261:                 continue
262: 
263:             # Extract signature (first line)
264:             lines = func_def.split('\n')
265:             signature = lines[0].strip()
266: 
267:             # Extract docstring if present
268:             docstring = None
269:             docstring_match = re.search(r'"""(.*?)"""', func_def, re.DOTALL)
270:             if docstring_match:
271:                 docstring = docstring_match.group(1).strip()
272: 
273:             result.append({
274:                 "signature": signature,
275:                 "docstring": docstring or "",
276:                 "full_code": func_def
277:             })
278: 
279:     return result
280: 
281: 
282: def extract_phase_metadata(phase_text: str) -> Dict[str, Any]:
283:     """Extract metadata from phase heading.
284: 
285:     Args:
286:         phase_text: Full phase section text
287: 
288:     Returns:
289:         {
290:             "phase_number": 1,
291:             "phase_name": "Core Logic Implementation",
292:             "hours": 4.0
293:         }
294: 
295:     Pattern: ### Phase 1: Core Logic Implementation (4 hours)
296:     """
297:     # This function is superseded by the inline parsing in parse_blueprint()
298:     # Kept for backwards compatibility and as a utility function
299:     pattern = r"###\s+Phase\s+(\d+):\s+([^\(]+)\(([^)]+)\)"
300:     match = re.search(pattern, phase_text)
301: 
302:     if not match:
303:         return {
304:             "phase_number": 0,
305:             "phase_name": "Unknown",
306:             "hours": 0.0
307:         }
308: 
309:     hours_match = re.search(r"(\d+(?:\.\d+)?)", match.group(3))
310:     hours = float(hours_match.group(1)) if hours_match else 0.0
311: 
312:     return {
313:         "phase_number": int(match.group(1)),
314:         "phase_name": match.group(2).strip(),
315:         "hours": hours
316:     }
</file>

<file path="tools/ce/code_analyzer.py">
  1: """Shared code analysis module for pattern detection across languages.
  2: 
  3: This module provides unified code analysis functions used by both pattern_extractor
  4: and drift_analyzer to eliminate duplication and maintain a single source of truth
  5: for pattern detection logic.
  6: """
  7: 
  8: import ast
  9: import re
 10: from typing import Dict, List
 11: 
 12: 
 13: def analyze_code_patterns(code: str, language: str) -> Dict[str, List[str]]:
 14:     """Analyze code and extract semantic patterns.
 15: 
 16:     Args:
 17:         code: Source code string
 18:         language: Programming language (python, typescript, javascript, etc.)
 19: 
 20:     Returns:
 21:         Dict mapping pattern categories to detected patterns:
 22:         {
 23:             "code_structure": ["async/await", "class-based", ...],
 24:             "error_handling": ["try-except", "early-return", ...],
 25:             "naming_conventions": ["snake_case", "camelCase", ...],
 26:             "data_flow": ["props", "state", ...],
 27:             "test_patterns": ["pytest", "jest", ...],
 28:             "import_patterns": ["relative", "absolute"]
 29:         }
 30:     """
 31:     if language.lower() in ("python", "py"):
 32:         return _analyze_python(code)
 33:     elif language.lower() in ("typescript", "ts", "javascript", "js"):
 34:         return _analyze_typescript(code)
 35:     else:
 36:         return _analyze_generic(code)
 37: 
 38: 
 39: def _analyze_python(code: str) -> Dict[str, List[str]]:
 40:     """Analyze Python code using AST for accurate pattern detection.
 41: 
 42:     Falls back to regex-based analysis if AST parsing fails.
 43:     Refactored to reduce nesting depth from 7 to 4 levels.
 44:     """
 45:     from .pattern_detectors import (
 46:         process_class_node,
 47:         process_function_node,
 48:         process_try_node,
 49:         process_if_node,
 50:         process_import_node
 51:     )
 52: 
 53:     patterns = {
 54:         "code_structure": [],
 55:         "error_handling": [],
 56:         "naming_conventions": [],
 57:         "data_flow": [],
 58:         "test_patterns": [],
 59:         "import_patterns": []
 60:     }
 61: 
 62:     try:
 63:         tree = ast.parse(code)
 64:     except SyntaxError:
 65:         # Fallback to regex if AST parsing fails
 66:         return _analyze_generic(code)
 67: 
 68:     # Code structure analysis using extracted pattern detectors
 69:     for node in ast.walk(tree):
 70:         # Async patterns
 71:         if isinstance(node, (ast.AsyncFunctionDef, ast.AsyncFor, ast.AsyncWith, ast.Await)):
 72:             patterns["code_structure"].append("async/await")
 73: 
 74:         # Class-based (delegated to reduce nesting)
 75:         elif isinstance(node, ast.ClassDef):
 76:             process_class_node(node, patterns)
 77: 
 78:         # Function-based (delegated to reduce nesting)
 79:         elif isinstance(node, ast.FunctionDef):
 80:             process_function_node(node, patterns)
 81: 
 82:         # Error handling
 83:         elif isinstance(node, ast.Try):
 84:             process_try_node(node, patterns)
 85: 
 86:         elif isinstance(node, ast.If):
 87:             process_if_node(node, patterns)
 88: 
 89:         # Import patterns
 90:         elif isinstance(node, ast.ImportFrom):
 91:             process_import_node(node, patterns)
 92: 
 93:     return patterns
 94: 
 95: 
 96: def _analyze_typescript(code: str) -> Dict[str, List[str]]:
 97:     """Analyze TypeScript/JavaScript code using regex patterns."""
 98:     patterns = {
 99:         "code_structure": [],
100:         "error_handling": [],
101:         "naming_conventions": [],
102:         "data_flow": [],
103:         "test_patterns": [],
104:         "import_patterns": []
105:     }
106: 
107:     # Code structure
108:     if re.search(r"\basync\s+", code) or re.search(r"\bawait\s+", code):
109:         patterns["code_structure"].append("async/await")
110:     if re.search(r"\.then\(", code):
111:         patterns["code_structure"].append("promises")
112:     if re.search(r"\bclass\s+\w+", code):
113:         patterns["code_structure"].append("class-based")
114:     if re.search(r"=>\s*{", code) or re.search(r"\bfunction\s+\w+", code):
115:         patterns["code_structure"].append("functional")
116: 
117:     # Error handling
118:     if re.search(r"\btry\s*{", code):
119:         patterns["error_handling"].append("try-catch")
120:     if re.search(r"\bif\s*\(.*?\)\s*return", code):
121:         patterns["error_handling"].append("early-return")
122: 
123:     # Naming conventions
124:     func_names = re.findall(r"function\s+(\w+)", code)
125:     var_names = re.findall(r"(?:const|let|var)\s+(\w+)", code)
126:     class_names = re.findall(r"class\s+(\w+)", code)
127: 
128:     for name in func_names + var_names + class_names:
129:         if "_" in name:
130:             patterns["naming_conventions"].append("snake_case")
131:         elif name[0].islower() and any(c.isupper() for c in name[1:]):
132:             patterns["naming_conventions"].append("camelCase")
133:         elif name[0].isupper():
134:             patterns["naming_conventions"].append("PascalCase")
135: 
136:     # Test patterns
137:     if re.search(r"\bdescribe\(", code) or re.search(r"\bit\(", code):
138:         patterns["test_patterns"].append("jest")
139:     if re.search(r"\btest\(", code):
140:         patterns["test_patterns"].append("jest")
141: 
142:     # Import patterns
143:     if re.search(r"import\s+.*?\s+from\s+['\"]\.{1,2}/", code):
144:         patterns["import_patterns"].append("relative")
145:     if re.search(r"import\s+.*?\s+from\s+['\"][^./]", code):
146:         patterns["import_patterns"].append("absolute")
147: 
148:     return patterns
149: 
150: 
151: def _analyze_generic(code: str) -> Dict[str, List[str]]:
152:     """Fallback regex-based pattern detection for any language."""
153:     patterns = {
154:         "code_structure": [],
155:         "error_handling": [],
156:         "naming_conventions": [],
157:         "data_flow": [],
158:         "test_patterns": [],
159:         "import_patterns": []
160:     }
161: 
162:     # Basic structure detection
163:     if "async" in code.lower() or "await" in code.lower():
164:         patterns["code_structure"].append("async/await")
165:     if "class " in code.lower():
166:         patterns["code_structure"].append("class-based")
167:     if "function" in code.lower() or "def " in code:
168:         patterns["code_structure"].append("functional")
169: 
170:     # Error handling
171:     if "try" in code.lower():
172:         patterns["error_handling"].append("try-catch")
173:     if re.search(r"\breturn\b.*?(?:if|when)", code, re.IGNORECASE):
174:         patterns["error_handling"].append("early-return")
175: 
176:     # Naming patterns (simple heuristic)
177:     if "_" in code:
178:         patterns["naming_conventions"].append("snake_case")
179:     if re.search(r"[a-z][A-Z]", code):
180:         patterns["naming_conventions"].append("camelCase")
181: 
182:     return patterns
183: 
184: 
185: def determine_language(file_extension: str) -> str:
186:     """Map file extension to language identifier.
187: 
188:     Args:
189:         file_extension: File extension including dot (e.g., ".py", ".ts")
190: 
191:     Returns:
192:         Language identifier string
193:     """
194:     lang_map = {
195:         ".py": "python",
196:         ".ts": "typescript",
197:         ".tsx": "typescript",
198:         ".js": "javascript",
199:         ".jsx": "javascript",
200:         ".go": "go",
201:         ".rs": "rust",
202:         ".java": "java",
203:         ".c": "c",
204:         ".cpp": "cpp",
205:         ".h": "c",
206:         ".hpp": "cpp"
207:     }
208:     return lang_map.get(file_extension.lower(), "unknown")
209: 
210: 
211: def count_code_symbols(code: str, language: str) -> int:
212:     """Estimate symbol count (functions, classes, methods) in code.
213: 
214:     Args:
215:         code: Source code string
216:         language: Programming language
217: 
218:     Returns:
219:         Estimated count of code symbols
220:     """
221:     if language.lower() in ("python", "py"):
222:         try:
223:             tree = ast.parse(code)
224:             return sum(1 for node in ast.walk(tree)
225:                       if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)))
226:         except SyntaxError:
227:             pass
228: 
229:     # Fallback: regex-based counting
230:     func_count = len(re.findall(r"\b(def|function|class)\s+\w+", code))
231:     return func_count
</file>

<file path="tools/ce/context.py">
  1: """Context management: sync, health checks, pruning."""
  2: 
  3: from typing import Dict, Any, List, Optional
  4: from .core import run_cmd, git_status, git_diff, count_git_files, count_git_diff_lines
  5: from .validate import validate_level_1, validate_level_2
  6: from .exceptions import ContextDriftError
  7: import logging
  8: 
  9: logger = logging.getLogger(__name__)
 10: 
 11: 
 12: def sync() -> Dict[str, Any]:
 13:     """Sync context with codebase changes.
 14: 
 15:     Detects git diff and reports files that need reindexing.
 16: 
 17:     Returns:
 18:         Dict with: reindexed_count (int), files (List[str]), drift_score (float)
 19: 
 20:     Note: Real git diff detection - no mocked sync.
 21:     """
 22:     try:
 23:         changed_files = git_diff(since="HEAD~5", name_only=True)
 24:     except Exception as e:
 25:         raise RuntimeError(
 26:             f"Failed to get changed files: {str(e)}\n"
 27:             f"ðŸ”§ Troubleshooting: Ensure you're in a git repository with commits"
 28:         ) from e
 29: 
 30:     # Calculate drift score (percentage of files changed)
 31:     # Get total tracked files
 32:     total_files = count_git_files()
 33:     drift_score = len(changed_files) / max(total_files, 1)  # Prevent division by zero
 34: 
 35:     return {
 36:         "reindexed_count": len(changed_files),
 37:         "files": changed_files,
 38:         "drift_score": drift_score,
 39:         "drift_level": (
 40:             "LOW" if drift_score < 0.15 else
 41:             "MEDIUM" if drift_score < 0.30 else
 42:             "HIGH"
 43:         )
 44:     }
 45: 
 46: 
 47: def health() -> Dict[str, Any]:
 48:     """Comprehensive context health check.
 49: 
 50:     Returns:
 51:         Dict with: compilation (bool), git_clean (bool), tests_passing (bool),
 52:                    drift_score (float), recommendations (List[str])
 53: 
 54:     Note: Real validation - no fake health scores.
 55:     """
 56:     recommendations = []
 57: 
 58:     # Check compilation (Level 1)
 59:     try:
 60:         l1_result = validate_level_1()
 61:         compilation_ok = l1_result["success"]
 62:         if not compilation_ok:
 63:             recommendations.append("Fix compilation errors with: ce validate --level 1")
 64:     except Exception as e:
 65:         compilation_ok = False
 66:         recommendations.append(f"Cannot run validation: {str(e)}")
 67: 
 68:     # Check git state
 69:     try:
 70:         git_state = git_status()
 71:         git_clean = git_state["clean"]
 72:         if not git_clean:
 73:             staged = len(git_state["staged"])
 74:             unstaged = len(git_state["unstaged"])
 75:             untracked = len(git_state["untracked"])
 76:             recommendations.append(
 77:                 f"Uncommitted changes: {staged} staged, {unstaged} unstaged, "
 78:                 f"{untracked} untracked"
 79:             )
 80:     except Exception as e:
 81:         git_clean = False
 82:         recommendations.append(f"Git check failed: {str(e)}")
 83: 
 84:     # Check tests (Level 2) - but don't block on failure
 85:     try:
 86:         l2_result = validate_level_2()
 87:         tests_passing = l2_result["success"]
 88:         if not tests_passing:
 89:             recommendations.append("Tests failing - fix with: ce validate --level 2")
 90:     except Exception as e:
 91:         tests_passing = False
 92:         recommendations.append("Cannot run tests - may need npm install")
 93: 
 94:     # Check context drift
 95:     try:
 96:         sync_result = sync()
 97:         drift_score = sync_result["drift_score"] * 100  # Convert to percentage (0-100)
 98:         drift_level = sync_result["drift_level"]
 99: 
100:         if drift_level == "HIGH":
101:             recommendations.append(
102:                 f"High context drift ({drift_score:.2f}%) - run: ce context sync"
103:             )
104:     except Exception:
105:         drift_score = 0.0
106:         drift_level = "UNKNOWN"
107: 
108:     # Overall health
109:     overall_healthy = compilation_ok and git_clean and tests_passing
110: 
111:     return {
112:         "healthy": overall_healthy,
113:         "compilation": compilation_ok,
114:         "git_clean": git_clean,
115:         "tests_passing": tests_passing,
116:         "drift_score": drift_score,  # Now returns percentage (0-100)
117:         "drift_level": drift_level,
118:         "recommendations": recommendations
119:     }
120: 
121: 
122: def prune(age_days: int = 7, dry_run: bool = False) -> Dict[str, Any]:
123:     """Prune old context memories (placeholder for Serena integration).
124: 
125:     Args:
126:         age_days: Delete memories older than this many days
127:         dry_run: If True, only report what would be deleted
128: 
129:     Returns:
130:         Dict with: deleted_count (int), files_deleted (List[str])
131: 
132:     Note: This is a placeholder. Full implementation requires Serena MCP integration.
133:     """
134:     # FIXME: Placeholder implementation - needs Serena MCP integration
135:     # This would normally:
136:     # 1. Query Serena for memory files
137:     # 2. Check age and access patterns
138:     # 3. Delete or compress based on priority
139: 
140:     return {
141:         "deleted_count": 0,
142:         "files_deleted": [],
143:         "dry_run": dry_run,
144:         "message": "Pruning requires Serena MCP integration (not yet implemented)"
145:     }
146: 
147: 
148: # ============================================================================
149: # Pre-Generation Sync Functions (Step 2.5)
150: # ============================================================================
151: 
152: def verify_git_clean() -> Dict[str, Any]:
153:     """Verify git working tree is clean.
154: 
155:     Returns:
156:         {
157:             "clean": True,
158:             "uncommitted_files": [],
159:             "untracked_files": []
160:         }
161: 
162:     Raises:
163:         RuntimeError: If uncommitted changes detected
164: 
165:     Process:
166:         1. Run: git status --porcelain
167:         2. Parse output for uncommitted/untracked files
168:         3. If any found: raise RuntimeError with file list
169:         4. Return clean status
170:     """
171:     try:
172:         status = git_status()
173:     except Exception as e:
174:         raise RuntimeError(
175:             f"Failed to check git status: {str(e)}\n"
176:             f"ðŸ”§ Troubleshooting: Ensure you're in a git repository"
177:         ) from e
178: 
179:     uncommitted = status["staged"] + status["unstaged"]
180:     untracked = status["untracked"]
181: 
182:     if uncommitted or untracked:
183:         file_list = "\n".join(
184:             [f"  - {f} (uncommitted)" for f in uncommitted] +
185:             [f"  - {f} (untracked)" for f in untracked]
186:         )
187:         raise RuntimeError(
188:             f"Working tree has uncommitted changes:\n{file_list}\n"
189:             f"ðŸ”§ Troubleshooting: Commit or stash changes before PRP generation"
190:         )
191: 
192:     return {
193:         "clean": True,
194:         "uncommitted_files": [],
195:         "untracked_files": []
196:     }
197: 
198: 
199: def check_drift_threshold(drift_score: float, force: bool = False) -> None:
200:     """Check drift score against thresholds and abort if needed.
201: 
202:     Args:
203:         drift_score: Drift percentage (0-100)
204:         force: Skip abort (for debugging)
205: 
206:     Raises:
207:         ContextDriftError: If drift > 30% and not force
208: 
209:     Thresholds:
210:         - 0-10%: INFO log, continue
211:         - 10-30%: WARNING log, continue
212:         - 30%+: ERROR log, abort (unless force=True)
213:     """
214:     if drift_score <= 10:
215:         logger.info(f"Context healthy: {drift_score:.2f}% drift")
216:     elif drift_score <= 30:
217:         logger.warning(f"Moderate drift: {drift_score:.2f}% - consider running ce context sync")
218:     else:
219:         # High drift - abort unless forced
220:         if not force:
221:             troubleshooting = (
222:                 "- Review recent commits: git log -5 --oneline\n"
223:                 "- Run: ce context sync to update indexes\n"
224:                 "- Check drift report: ce context health --verbose\n"
225:                 "- Consider: ce context prune to remove stale entries\n"
226:                 "- If confident, use --force to skip this check (not recommended)"
227:             )
228:             raise ContextDriftError(
229:                 drift_score=drift_score,
230:                 threshold=30.0,
231:                 troubleshooting=troubleshooting
232:             )
233:         else:
234:             logger.warning(f"High drift {drift_score:.2f}% - FORCED to continue (dangerous!)")
235: 
236: 
237: def pre_generation_sync(
238:     prp_id: Optional[str] = None,
239:     force: bool = False
240: ) -> Dict[str, Any]:
241:     """Execute Step 2.5: Pre-generation context sync and health check.
242: 
243:     Args:
244:         prp_id: Optional PRP ID for logging
245:         force: Skip drift abort (dangerous - for debugging only)
246: 
247:     Returns:
248:         {
249:             "success": True,
250:             "sync_completed": True,
251:             "drift_score": 8.2,  # 0-100%
252:             "git_clean": True,
253:             "abort_triggered": False,
254:             "warnings": []
255:         }
256: 
257:     Raises:
258:         ContextDriftError: If drift > 30% and force=False
259:         RuntimeError: If sync fails or git state dirty
260: 
261:     Process:
262:         1. Verify git clean state
263:         2. Run context sync
264:         3. Run health check
265:         4. Check drift threshold
266:         5. Return health report
267:     """
268:     warnings = []
269:     prp_log = f" (PRP-{prp_id})" if prp_id else ""
270: 
271:     logger.info(f"Starting pre-generation sync{prp_log}")
272: 
273:     # Step 1: Verify git clean state
274:     try:
275:         git_check = verify_git_clean()
276:         logger.info("âœ“ Git working tree clean")
277:     except RuntimeError as e:
278:         logger.error(f"Git state check failed: {e}")
279:         raise
280: 
281:     # Step 2: Run context sync
282:     try:
283:         sync_result = sync()
284:         logger.info(f"âœ“ Context sync completed: {sync_result['reindexed_count']} files reindexed")
285:     except Exception as e:
286:         raise RuntimeError(
287:             f"Context sync failed: {str(e)}\n"
288:             f"ðŸ”§ Troubleshooting: Check git configuration and ensure repository has commits"
289:         ) from e
290: 
291:     # Step 3: Run health check
292:     try:
293:         health_result = health()
294:         drift_score = health_result["drift_score"]  # Already percentage (0-100)
295:         logger.info(f"âœ“ Health check completed: {drift_score:.2f}% drift")
296:     except Exception as e:
297:         raise RuntimeError(
298:             f"Health check failed: {str(e)}\n"
299:             f"ðŸ”§ Troubleshooting: Ensure validation tools are available"
300:         ) from e
301: 
302:     # Step 4: Check drift threshold
303:     try:
304:         check_drift_threshold(drift_score, force=force)
305:     except ContextDriftError:
306:         logger.error(f"Pre-generation sync aborted due to high drift{prp_log}")
307:         raise
308: 
309:     # Step 5: Return health report
310:     result = {
311:         "success": True,
312:         "sync_completed": True,
313:         "drift_score": drift_score,
314:         "git_clean": git_check["clean"],
315:         "abort_triggered": False,
316:         "warnings": warnings
317:     }
318: 
319:     logger.info(f"Pre-generation sync successful{prp_log}")
320:     return result
321: 
322: 
323: # ============================================================================
324: # Post-Execution Sync Functions (Step 6.5)
325: # ============================================================================
326: 
327: def post_execution_sync(
328:     prp_id: str,
329:     skip_cleanup: bool = False
330: ) -> Dict[str, Any]:
331:     """Execute Step 6.5: Post-execution cleanup and context sync.
332: 
333:     Args:
334:         prp_id: PRP identifier
335:         skip_cleanup: Skip cleanup protocol (for testing)
336: 
337:     Returns:
338:         {
339:             "success": True,
340:             "cleanup_completed": True,
341:             "sync_completed": True,
342:             "final_checkpoint": "checkpoint-PRP-003-final-20251012-160000",
343:             "drift_score": 5.1,  # After sync
344:             "memories_archived": 2,
345:             "memories_deleted": 3,
346:             "checkpoints_deleted": 2
347:         }
348: 
349:     Raises:
350:         RuntimeError: If cleanup or sync fails
351: 
352:     Process:
353:         1. Execute cleanup protocol (unless skip_cleanup)
354:         2. Run context sync
355:         3. Run health check
356:         4. Create final checkpoint
357:         5. Remove active PRP session
358:         6. Return cleanup + sync summary
359: 
360:     Integration Points:
361:         - cleanup_prp(prp_id): From PRP-2
362:         - context_sync(): Existing context.py function
363:         - context_health(): Existing context.py function
364:         - create_checkpoint(phase="final"): From PRP-2
365:     """
366:     from .prp import cleanup_prp, create_checkpoint, get_active_prp, end_prp
367:     from datetime import datetime, timezone
368: 
369:     logger.info(f"Starting post-execution sync (PRP-{prp_id})")
370: 
371:     result = {
372:         "success": True,
373:         "cleanup_completed": False,
374:         "sync_completed": False,
375:         "final_checkpoint": None,
376:         "drift_score": 0.0,
377:         "memories_archived": 0,
378:         "memories_deleted": 0,
379:         "checkpoints_deleted": 0
380:     }
381: 
382:     # Step 1: Execute cleanup protocol (unless skip_cleanup)
383:     if not skip_cleanup:
384:         try:
385:             cleanup_result = cleanup_prp(prp_id)
386:             result["cleanup_completed"] = True
387:             result["memories_archived"] = len(cleanup_result["memories_archived"])
388:             result["memories_deleted"] = len(cleanup_result["memories_deleted"])
389:             result["checkpoints_deleted"] = cleanup_result["checkpoints_deleted"]
390:             logger.info(f"âœ“ Cleanup completed: {result['checkpoints_deleted']} checkpoints deleted")
391:         except Exception as e:
392:             raise RuntimeError(
393:                 f"Cleanup protocol failed: {str(e)}\n"
394:                 f"ðŸ”§ Troubleshooting: Review cleanup errors and retry manually"
395:             ) from e
396:     else:
397:         logger.info("Skipping cleanup protocol (skip_cleanup=True)")
398:         result["cleanup_completed"] = True
399: 
400:     # Step 2: Run context sync
401:     try:
402:         sync_result = sync()
403:         result["sync_completed"] = True
404:         logger.info(f"âœ“ Context sync completed: {sync_result['reindexed_count']} files reindexed")
405:     except Exception as e:
406:         raise RuntimeError(
407:             f"Context sync failed: {str(e)}\n"
408:             f"ðŸ”§ Troubleshooting: Check git configuration and repository state"
409:         ) from e
410: 
411:     # Step 3: Run health check
412:     try:
413:         health_result = health()
414:         drift_score = health_result["drift_score"]  # Already percentage (0-100)
415:         result["drift_score"] = drift_score
416:         logger.info(f"âœ“ Health check completed: {drift_score:.2f}% drift")
417: 
418:         # Warn if drift still high after sync
419:         if drift_score > 10:
420:             logger.warning(f"Drift still elevated after sync: {drift_score:.2f}%")
421:     except Exception as e:
422:         logger.warning(f"Health check failed: {e}")
423: 
424:     # Step 4: Create final checkpoint (if active PRP session exists)
425:     active = get_active_prp()
426:     if active and active["prp_id"] == prp_id:
427:         try:
428:             checkpoint_result = create_checkpoint(
429:                 phase="final",
430:                 message=f"PRP-{prp_id} complete with context sync"
431:             )
432:             result["final_checkpoint"] = checkpoint_result["tag_name"]
433:             logger.info(f"âœ“ Final checkpoint created: {checkpoint_result['tag_name']}")
434:         except RuntimeError as e:
435:             # Don't fail if checkpoint creation fails (may already be committed)
436:             logger.warning(f"Could not create final checkpoint: {e}")
437: 
438:         # Step 5: Remove active PRP session
439:         try:
440:             end_prp(prp_id)
441:             logger.info(f"âœ“ Active PRP session ended")
442:         except Exception as e:
443:             logger.warning(f"Could not end PRP session: {e}")
444:     else:
445:         logger.info("No active PRP session to end")
446: 
447:     logger.info(f"Post-execution sync completed (PRP-{prp_id})")
448:     return result
449: 
450: 
451: def sync_serena_context() -> Dict[str, Any]:
452:     """Sync Serena MCP context with current codebase.
453: 
454:     Returns:
455:         {
456:             "success": True,
457:             "files_indexed": 127,
458:             "symbols_updated": 453,
459:             "memories_refreshed": 5
460:         }
461: 
462:     Process:
463:         1. Trigger Serena re-index (if available)
464:         2. Update relevant memories with new patterns
465:         3. Refresh codebase structure knowledge
466:         4. Return sync summary
467: 
468:     Note: This is a placeholder. Full implementation requires Serena MCP integration.
469:     """
470:     # FIXME: Placeholder implementation - needs Serena MCP integration
471:     logger.warning("Serena MCP sync not implemented - skipping")
472: 
473:     return {
474:         "success": True,
475:         "files_indexed": 0,
476:         "symbols_updated": 0,
477:         "memories_refreshed": 0,
478:         "message": "Serena sync requires MCP integration (not yet implemented)"
479:     }
480: 
481: 
482: def prune_stale_memories(age_days: int = 30) -> Dict[str, Any]:
483:     """Prune stale Serena memories older than age_days.
484: 
485:     Args:
486:         age_days: Delete memories older than this (default: 30 days)
487: 
488:     Returns:
489:         {
490:             "success": True,
491:             "memories_pruned": 12,
492:             "space_freed_kb": 45.2
493:         }
494: 
495:     Process:
496:         1. List all Serena memories
497:         2. Filter by age (creation timestamp)
498:         3. Exclude essential memories (never delete):
499:            - project-patterns
500:            - code-style-conventions
501:            - testing-standards
502:         4. Delete stale memories via Serena MCP
503:         5. Return pruning summary
504: 
505:     Note: This is a placeholder. Full implementation requires Serena MCP integration.
506:     """
507:     # FIXME: Placeholder implementation - needs Serena MCP integration
508:     logger.warning(f"Serena memory pruning not implemented - would prune memories older than {age_days} days")
509: 
510:     return {
511:         "success": True,
512:         "memories_pruned": 0,
513:         "space_freed_kb": 0.0,
514:         "message": "Memory pruning requires Serena MCP integration (not yet implemented)"
515:     }
516: 
517: 
518: # ============================================================================
519: # Drift Detection & Reporting Functions
520: # ============================================================================
521: 
522: def calculate_drift_score() -> float:
523:     """Calculate context drift score (0-100%).
524: 
525:     Returns:
526:         Drift percentage (0 = perfect sync, 100 = completely stale)
527: 
528:     Calculation:
529:         drift = (
530:             file_changes_score * 0.4 +
531:             memory_staleness_score * 0.3 +
532:             dependency_changes_score * 0.2 +
533:             uncommitted_changes_score * 0.1
534:         )
535: 
536:     Components:
537:         - file_changes_score: % of tracked files modified since last sync
538:         - memory_staleness_score: Age of oldest Serena memory (normalized)
539:         - dependency_changes_score: pyproject.toml/package.json changes
540:         - uncommitted_changes_score: Penalty for dirty git state
541:     """
542:     # Component 1: File changes (40% weight)
543:     try:
544:         changed_files = git_diff(since="HEAD~5", name_only=True)
545:         try:
546:             total_files = count_git_files()
547:             file_changes_score = (len(changed_files) / max(total_files, 1)) * 100
548:         except RuntimeError:
549:             file_changes_score = 0
550:     except Exception as e:
551:         logger.warning(
552:             f"Failed to calculate file changes score: {e}\n"
553:             f"ðŸ”§ Troubleshooting: Ensure git is available and repository has commits"
554:         )
555:         file_changes_score = 0
556: 
557:     # Component 2: Memory staleness (30% weight)
558:     # FIXME: Placeholder - needs Serena MCP integration
559:     memory_staleness_score = 0  # Would check age of memories
560: 
561:     # Component 3: Dependency changes (20% weight)
562:     dependency_changes_score = 0
563:     try:
564:         # Check if pyproject.toml changed recently
565:         deps_lines = count_git_diff_lines(
566:             ref="HEAD~5",
567:             files=["pyproject.toml", "package.json"]
568:         )
569:         # Normalize: >10 lines of changes = 100% score
570:         dependency_changes_score = min((deps_lines / 10.0) * 100, 100)
571:     except Exception as e:
572:         logger.warning(
573:             f"Failed to check dependency changes: {e}\n"
574:             f"ðŸ”§ Troubleshooting: Ensure git is available"
575:         )
576: 
577:     # Component 4: Uncommitted changes (10% weight)
578:     uncommitted_changes_score = 0
579:     try:
580:         status = git_status()
581:         uncommitted = len(status["staged"]) + len(status["unstaged"])
582:         untracked = len(status["untracked"])
583:         # Normalize: >5 files = 100% score
584:         uncommitted_changes_score = min(((uncommitted + untracked) / 5.0) * 100, 100)
585:     except Exception as e:
586:         logger.warning(
587:             f"Failed to check uncommitted changes: {e}\n"
588:             f"ðŸ”§ Troubleshooting: Ensure git is available and you're in a repository"
589:         )
590: 
591:     # Weighted sum
592:     drift = (
593:         file_changes_score * 0.4 +
594:         memory_staleness_score * 0.3 +
595:         dependency_changes_score * 0.2 +
596:         uncommitted_changes_score * 0.1
597:     )
598: 
599:     return drift
600: 
601: 
602: def context_health_verbose() -> Dict[str, Any]:
603:     """Detailed context health report with breakdown.
604: 
605:     Returns:
606:         {
607:             "drift_score": 23.4,
608:             "threshold": "warn",  # healthy | warn | critical
609:             "components": {
610:                 "file_changes": {"score": 18.2, "details": "12/127 files modified"},
611:                 "memory_staleness": {"score": 5.1, "details": "Oldest: 8 days"},
612:                 "dependency_changes": {"score": 0, "details": "No changes"},
613:                 "uncommitted_changes": {"score": 0.1, "details": "1 untracked file"}
614:             },
615:             "recommendations": [
616:                 "Run: ce context sync to refresh indexes",
617:                 "Consider: ce context prune to remove stale memories"
618:             ]
619:         }
620:     """
621:     components = {}
622:     recommendations = []
623: 
624:     # File changes component
625:     try:
626:         changed_files = git_diff(since="HEAD~5", name_only=True)
627:         try:
628:             total_files = count_git_files()
629:             file_score = (len(changed_files) / max(total_files, 1)) * 100
630:             components["file_changes"] = {
631:                 "score": file_score,
632:                 "details": f"{len(changed_files)}/{total_files} files modified"
633:             }
634:             if file_score > 15:
635:                 recommendations.append("Run: ce context sync to refresh indexes")
636:         except RuntimeError:
637:             components["file_changes"] = {"score": 0, "details": "Error: could not count files"}
638:     except Exception as e:
639:         logger.warning(
640:             f"Failed to calculate file changes component: {e}\n"
641:             f"ðŸ”§ Troubleshooting: Ensure git is available"
642:         )
643:         components["file_changes"] = {"score": 0, "details": f"Error: {e}"}
644: 
645:     # Memory staleness component (placeholder)
646:     components["memory_staleness"] = {
647:         "score": 0,
648:         "details": "Serena MCP not available"
649:     }
650: 
651:     # Dependency changes component
652:     try:
653:         deps_lines = count_git_diff_lines(
654:             ref="HEAD~5",
655:             files=["pyproject.toml", "package.json"]
656:         )
657:         deps_score = min((deps_lines / 10.0) * 100, 100)
658:         components["dependency_changes"] = {
659:             "score": deps_score,
660:             "details": f"{deps_lines} lines changed" if deps_lines > 0 else "No changes"
661:         }
662:         if deps_score > 10:
663:             recommendations.append("Dependencies changed - run: uv sync")
664:     except Exception as e:
665:         logger.warning(
666:             f"Failed to check dependency changes component: {e}\n"
667:             f"ðŸ”§ Troubleshooting: Ensure git is available"
668:         )
669:         components["dependency_changes"] = {"score": 0, "details": "No changes"}
670: 
671:     # Uncommitted changes component
672:     try:
673:         status = git_status()
674:         uncommitted = len(status["staged"]) + len(status["unstaged"])
675:         untracked = len(status["untracked"])
676:         uncommitted_score = min(((uncommitted + untracked) / 5.0) * 100, 100)
677:         components["uncommitted_changes"] = {
678:             "score": uncommitted_score,
679:             "details": f"{uncommitted} uncommitted, {untracked} untracked"
680:         }
681:         if uncommitted + untracked > 0:
682:             recommendations.append("Commit or stash changes before PRP operations")
683:     except Exception as e:
684:         logger.warning(
685:             f"Failed to check uncommitted changes component: {e}\n"
686:             f"ðŸ”§ Troubleshooting: Ensure git is available and you're in a repository"
687:         )
688:         components["uncommitted_changes"] = {"score": 0, "details": "0 uncommitted"}
689: 
690:     # Calculate overall drift
691:     drift_score = calculate_drift_score()
692: 
693:     # Determine threshold
694:     if drift_score <= 10:
695:         threshold = "healthy"
696:     elif drift_score <= 30:
697:         threshold = "warn"
698:     else:
699:         threshold = "critical"
700: 
701:     return {
702:         "drift_score": drift_score,
703:         "threshold": threshold,
704:         "components": components,
705:         "recommendations": recommendations
706:     }
707: 
708: 
709: def drift_report_markdown() -> str:
710:     """Generate markdown drift report for logging.
711: 
712:     Returns:
713:         Markdown-formatted drift report
714: 
715:     Format:
716:         ## Context Health Report
717: 
718:         **Drift Score**: 23.4% (âš ï¸ WARNING)
719: 
720:         ### Components
721:         - File Changes: 18.2% (12/127 files modified)
722:         - Memory Staleness: 5.1% (Oldest: 8 days)
723:         - Dependency Changes: 0% (No changes)
724:         - Uncommitted Changes: 0.1% (1 untracked file)
725: 
726:         ### Recommendations
727:         1. Run: ce context sync to refresh indexes
728:         2. Consider: ce context prune to remove stale memories
729:     """
730:     report = context_health_verbose()
731: 
732:     # Status emoji
733:     if report["threshold"] == "healthy":
734:         status_emoji = "âœ…"
735:         status_text = "HEALTHY"
736:     elif report["threshold"] == "warn":
737:         status_emoji = "âš ï¸"
738:         status_text = "WARNING"
739:     else:
740:         status_emoji = "âŒ"
741:         status_text = "CRITICAL"
742: 
743:     # Build markdown
744:     md = f"## Context Health Report\n\n"
745:     md += f"**Drift Score**: {report['drift_score']:.2f}% ({status_emoji} {status_text})\n\n"
746: 
747:     # Components
748:     md += "### Components\n"
749:     for name, comp in report["components"].items():
750:         display_name = name.replace("_", " ").title()
751:         md += f"- {display_name}: {comp['score']:.2f}% ({comp['details']})\n"
752: 
753:     # Recommendations
754:     if report["recommendations"]:
755:         md += "\n### Recommendations\n"
756:         for i, rec in enumerate(report["recommendations"], 1):
757:             md += f"{i}. {rec}\n"
758: 
759:     return md
760: 
761: 
762: # ============================================================================
763: # Auto-Sync Mode Configuration
764: # ============================================================================
765: 
766: def enable_auto_sync() -> Dict[str, Any]:
767:     """Enable auto-sync mode in .ce/config.
768: 
769:     Returns:
770:         {
771:             "success": True,
772:             "mode": "enabled",
773:             "config_path": ".ce/config"
774:         }
775: 
776:     Process:
777:         1. Create .ce/config if not exists
778:         2. Set auto_sync: true in config
779:         3. Log: "Auto-sync enabled - Steps 2.5 and 6.5 will run automatically"
780:     """
781:     from pathlib import Path
782:     import json
783: 
784:     config_dir = Path(".ce")
785:     config_file = config_dir / "config"
786: 
787:     # Create directory if needed
788:     config_dir.mkdir(exist_ok=True)
789: 
790:     # Read existing config or create new
791:     if config_file.exists():
792:         try:
793:             config = json.loads(config_file.read_text())
794:         except (json.JSONDecodeError, OSError):
795:             config = {}
796:     else:
797:         config = {}
798: 
799:     # Set auto_sync
800:     config["auto_sync"] = True
801: 
802:     # Write config
803:     config_file.write_text(json.dumps(config, indent=2))
804: 
805:     logger.info("Auto-sync enabled - Steps 2.5 and 6.5 will run automatically")
806: 
807:     return {
808:         "success": True,
809:         "mode": "enabled",
810:         "config_path": str(config_file)
811:     }
812: 
813: 
814: def disable_auto_sync() -> Dict[str, Any]:
815:     """Disable auto-sync mode in .ce/config.
816: 
817:     Returns:
818:         {
819:             "success": True,
820:             "mode": "disabled",
821:             "config_path": ".ce/config"
822:         }
823:     """
824:     from pathlib import Path
825:     import json
826: 
827:     config_dir = Path(".ce")
828:     config_file = config_dir / "config"
829: 
830:     # Read existing config
831:     if config_file.exists():
832:         try:
833:             config = json.loads(config_file.read_text())
834:         except (json.JSONDecodeError, OSError):
835:             config = {}
836:     else:
837:         config = {}
838: 
839:     # Set auto_sync to false
840:     config["auto_sync"] = False
841: 
842:     # Write config
843:     config_dir.mkdir(exist_ok=True)
844:     config_file.write_text(json.dumps(config, indent=2))
845: 
846:     logger.info("Auto-sync disabled - Manual sync required")
847: 
848:     return {
849:         "success": True,
850:         "mode": "disabled",
851:         "config_path": str(config_file)
852:     }
853: 
854: 
855: def is_auto_sync_enabled() -> bool:
856:     """Check if auto-sync mode is enabled.
857: 
858:     Returns:
859:         True if enabled, False otherwise
860: 
861:     Process:
862:         1. Read .ce/config
863:         2. Return config.get("auto_sync", False)
864:     """
865:     from pathlib import Path
866:     import json
867: 
868:     config_file = Path(".ce/config")
869: 
870:     if not config_file.exists():
871:         return False
872: 
873:     try:
874:         config = json.loads(config_file.read_text())
875:         return config.get("auto_sync", False)
876:     except (json.JSONDecodeError, OSError):
877:         return False
878: 
879: 
880: def get_auto_sync_status() -> Dict[str, Any]:
881:     """Get auto-sync mode status.
882: 
883:     Returns:
884:         {
885:             "enabled": True,
886:             "config_path": ".ce/config",
887:             "message": "Auto-sync is enabled"
888:         }
889:     """
890:     from pathlib import Path
891: 
892:     enabled = is_auto_sync_enabled()
893:     config_file = Path(".ce/config")
894: 
895:     message = (
896:         "Auto-sync is enabled - Steps 2.5 and 6.5 run automatically"
897:         if enabled
898:         else "Auto-sync is disabled - Manual sync required"
899:     )
900: 
901:     return {
902:         "enabled": enabled,
903:         "config_path": str(config_file),
904:         "message": message
905:     }
</file>

<file path="tools/ce/core.py">
  1: """Core operations: file, git, and shell utilities."""
  2: 
  3: import subprocess
  4: import time
  5: import shlex
  6: from pathlib import Path
  7: from typing import Dict, List, Any, Optional, Union
  8: 
  9: 
 10: def run_cmd(
 11:     cmd: Union[str, List[str]],
 12:     cwd: Optional[str] = None,
 13:     timeout: int = 60,
 14:     capture_output: bool = True
 15: ) -> Dict[str, Any]:
 16:     """Execute shell command with timeout and error handling.
 17: 
 18:     Args:
 19:         cmd: Shell command (str will be safely split) or list of args
 20:         cwd: Working directory (default: current)
 21:         timeout: Command timeout in seconds
 22:         capture_output: Whether to capture stdout/stderr
 23: 
 24:     Returns:
 25:         Dict with: success (bool), stdout (str), stderr (str),
 26:                    exit_code (int), duration (float)
 27: 
 28:     Raises:
 29:         ValueError: If command is empty
 30:         subprocess.TimeoutExpired: If command exceeds timeout
 31: 
 32:     Security: Uses shell=False to prevent command injection (CWE-78).
 33:               String commands are safely parsed with shlex.split().
 34:     
 35:     Note: No fishy fallbacks - exceptions are thrown to troubleshoot quickly.
 36:     """
 37:     start = time.time()
 38: 
 39:     # Convert string to safe list
 40:     if isinstance(cmd, str):
 41:         cmd_list = shlex.split(cmd)  # Safe parsing with proper escaping
 42:     else:
 43:         cmd_list = cmd
 44: 
 45:     # Handle empty command
 46:     if not cmd_list:
 47:         raise ValueError(
 48:             "Empty command provided\n"
 49:             "ðŸ”§ Troubleshooting: Provide a valid command string or list"
 50:         )
 51: 
 52:     try:
 53:         result = subprocess.run(
 54:             cmd_list,  # âœ… List format
 55:             shell=False,  # âœ… SAFE - no shell interpretation (CWE-78 fix)
 56:             cwd=cwd,
 57:             timeout=timeout,
 58:             capture_output=capture_output,
 59:             text=True
 60:         )
 61: 
 62:         duration = time.time() - start
 63: 
 64:         return {
 65:             "success": result.returncode == 0,
 66:             "stdout": result.stdout if capture_output else "",
 67:             "stderr": result.stderr if capture_output else "",
 68:             "exit_code": result.returncode,
 69:             "duration": duration
 70:         }
 71: 
 72:     except subprocess.TimeoutExpired as e:
 73:         duration = time.time() - start
 74:         raise TimeoutError(
 75:             f"Command timed out after {timeout}s: {' '.join(cmd_list)}\n"
 76:             f"ðŸ”§ Troubleshooting: Increase timeout or check for hanging process"
 77:         ) from e
 78: 
 79:     except Exception as e:
 80:         duration = time.time() - start
 81:         raise RuntimeError(
 82:             f"Command failed: {' '.join(cmd_list)}\n"
 83:             f"Error: {str(e)}\n"
 84:             f"ðŸ”§ Troubleshooting: Check command syntax and permissions"
 85:         ) from e
 86: 
 87: 
 88: def count_git_files() -> int:
 89:     """Count total tracked files in git repository.
 90: 
 91:     Replaces shell pattern: git ls-files | wc -l
 92: 
 93:     Returns:
 94:         Number of tracked files
 95: 
 96:     Raises:
 97:         RuntimeError: If not in git repository
 98: 
 99:     Security: Uses subprocess.run with shell=False (CWE-78 safe).
100:     """
101:     try:
102:         result = subprocess.run(
103:             ["git", "ls-files"],
104:             capture_output=True,
105:             text=True,
106:             shell=False,  # âœ… SAFE
107:             timeout=30
108:         )
109: 
110:         if result.returncode != 0:
111:             raise RuntimeError(
112:                 "Failed to list git files\n"
113:                 "ðŸ”§ Troubleshooting: Ensure you're in a git repository"
114:             )
115: 
116:         files = result.stdout.strip().split('\n') if result.stdout.strip() else []
117:         return len(files)
118: 
119:     except subprocess.TimeoutExpired:
120:         raise RuntimeError(
121:             "Git ls-files timed out\n"
122:             "ðŸ”§ Troubleshooting: Repository may be too large"
123:         )
124: 
125: 
126: def count_git_diff_lines(
127:     ref: str = "HEAD~5",
128:     files: Optional[List[str]] = None
129: ) -> int:
130:     """Count lines changed in git diff.
131: 
132:     Replaces shell pattern: git diff HEAD~5 -- file1 file2 | wc -l
133: 
134:     Args:
135:         ref: Git reference to diff against (default: HEAD~5)
136:         files: Optional list of files to diff
137: 
138:     Returns:
139:         Number of changed lines
140: 
141:     Security: Uses subprocess.run with shell=False (CWE-78 safe).
142:     Note: Returns 0 on error (graceful degradation for health checks).
143:     """
144:     cmd = ["git", "diff", ref]
145:     if files:
146:         cmd.extend(["--"] + files)
147: 
148:     try:
149:         result = subprocess.run(
150:             cmd,
151:             capture_output=True,
152:             text=True,
153:             shell=False,  # âœ… SAFE
154:             timeout=30
155:         )
156: 
157:         if result.returncode != 0:
158:             return 0
159: 
160:         return len(result.stdout.split('\n')) if result.stdout else 0
161: 
162:     except subprocess.TimeoutExpired:
163:         return 0
164: 
165: 
166: def read_file(path: str, encoding: str = "utf-8") -> str:
167:     """Read file with validation."""
168:     file_path = Path(path)
169:     if not file_path.exists():
170:         raise FileNotFoundError(f"File not found: {path}\nðŸ”§ Troubleshooting: Check path spelling")
171:     if not file_path.is_file():
172:         raise ValueError(f"Path is not a file: {path}\nðŸ”§ Troubleshooting: Use different method")
173:     return file_path.read_text(encoding=encoding)
174: 
175: 
176: def write_file(path: str, content: str, encoding: str = "utf-8", create_dirs: bool = True) -> None:
177:     """Write file with security validation."""
178:     file_path = Path(path)
179:     sensitive_patterns = [("API_KEY", "API keys"), ("SECRET", "Secrets"), ("PASSWORD", "Passwords")]
180:     for pattern, msg in sensitive_patterns:
181:         if pattern in content.upper():
182:             raise ValueError(f"Sensitive data: {msg}\nðŸ”§ Use environment variables")
183:     if create_dirs:
184:         file_path.parent.mkdir(parents=True, exist_ok=True)
185:     file_path.write_text(content, encoding=encoding)
186: 
187: 
188: def git_status() -> Dict[str, Any]:
189:     """Get git repository status."""
190:     check_result = run_cmd("git rev-parse --git-dir", capture_output=True)
191:     if not check_result["success"]:
192:         raise RuntimeError("Not in git repository")
193:     result = run_cmd("git status --porcelain", capture_output=True)
194:     if not result["success"]:
195:         raise RuntimeError(f"Git status failed: {result['stderr']}")
196:     
197:     staged, unstaged, untracked = [], [], []
198:     lines = result["stdout"].strip().split("\n") if result["stdout"].strip() else []
199:     
200:     for line in lines:
201:         if not line:
202:             continue
203:         status, filepath = line[:2], line[3:]
204:         if status[0] != " " and status[0] != "?":
205:             staged.append(filepath)
206:         if status[1] != " " and status[1] != "?":
207:             unstaged.append(filepath)
208:         if status == "??":
209:             untracked.append(filepath)
210:     
211:     return {"clean": len(staged) == 0 and len(unstaged) == 0 and len(untracked) == 0,
212:             "staged": staged, "unstaged": unstaged, "untracked": untracked}
213: 
214: 
215: def git_diff(since: str = "HEAD~5", name_only: bool = True) -> List[str]:
216:     """Get changed files since specified ref."""
217:     flag = "--name-only" if name_only else "--stat"
218:     result = run_cmd(f"git diff {flag} {since}", capture_output=True)
219:     if not result["success"]:
220:         raise RuntimeError(f"Git diff failed: {result['stderr']}")
221:     return [f.strip() for f in result["stdout"].strip().split("\n") if f.strip()]
222: 
223: 
224: def git_checkpoint(message: str = "Context Engineering checkpoint") -> str:
225:     """Create git tag checkpoint for recovery."""
226:     import datetime
227:     timestamp = int(datetime.datetime.now().timestamp())
228:     checkpoint_id = f"checkpoint-{timestamp}"
229:     result = run_cmd(["git", "tag", "-a", checkpoint_id, "-m", message], capture_output=True)
230:     if not result["success"]:
231:         raise RuntimeError(f"Failed to create checkpoint: {result['stderr']}")
232:     return checkpoint_id
233: 
234: 
235: def run_py(code: Optional[str] = None, file: Optional[str] = None, args: str = "", auto: Optional[str] = None) -> Dict[str, Any]:
236:     """Execute Python code using uv with strict LOC limits."""
237:     if auto is not None:
238:         if code is not None or file is not None:
239:             raise ValueError("Cannot use 'auto' with 'code' or 'file'")
240:         file = auto if "/" in auto or auto.endswith(".py") else None
241:         code = auto if file is None else None
242: 
243:     if code is None and file is None:
244:         raise ValueError("Either 'code', 'file', or 'auto' must be provided")
245:     if code is not None and file is not None:
246:         raise ValueError("Cannot provide both 'code' and 'file'")
247: 
248:     if code is not None:
249:         lines = [line for line in code.split('\n') if line.strip()]
250:         if len(lines) > 3:
251:             raise ValueError(f"Ad-hoc code exceeds 3 LOC limit")
252:         cmd = ["uv", "run", "python", "-c", code]
253:         if args:
254:             cmd.extend(args.split())
255:         return run_cmd(cmd, timeout=120)
256: 
257:     if file is not None:
258:         file_path = Path(file)
259:         if not any(part == "tmp" for part in file_path.parts):
260:             raise ValueError(f"File must be in tmp/ folder")
261:         if not file_path.exists():
262:             raise FileNotFoundError(f"Python file not found: {file}")
263:         cmd = ["uv", "run", "python", file]
264:         if args:
265:             cmd.extend(args.split())
266:         return run_cmd(cmd, timeout=300)
</file>

<file path="tools/ce/drift_analyzer.py">
  1: """Drift analysis engine for L4 validation.
  2: 
  3: Calculates semantic drift between expected patterns (from PRP EXAMPLES)
  4: and actual implementation code. Uses shared code_analyzer module for
  5: pattern detection.
  6: """
  7: 
  8: import time
  9: from typing import Dict, List, Any
 10: from pathlib import Path
 11: 
 12: from .code_analyzer import analyze_code_patterns, determine_language, count_code_symbols
 13: 
 14: 
 15: def analyze_implementation(
 16:     prp_path: str,
 17:     implementation_paths: List[str]
 18: ) -> Dict[str, Any]:
 19:     """Analyze implementation code structure using Serena MCP.
 20: 
 21:     Args:
 22:         prp_path: Path to PRP file (for extracting expected patterns)
 23:         implementation_paths: Paths to implementation files to analyze
 24: 
 25:     Returns:
 26:         {
 27:             "detected_patterns": {
 28:                 "code_structure": ["async/await", "class-based"],
 29:                 "error_handling": ["try-except"],
 30:                 "naming_conventions": ["snake_case"],
 31:                 ...
 32:             },
 33:             "files_analyzed": ["src/validate.py", "src/core.py"],
 34:             "symbol_count": 42,
 35:             "analysis_duration": 2.5,
 36:             "serena_available": False
 37:         }
 38: 
 39:     Uses (if Serena MCP available):
 40:         - serena.get_symbols_overview(file) for structure
 41:         - serena.find_symbol(name) for detailed analysis
 42:         - serena.read_file(file) for pattern matching
 43: 
 44:     Fallback (if Serena unavailable):
 45:         - Python ast module for Python files
 46:         - Regex-based pattern detection for other languages
 47:         - Log warning: "Serena MCP unavailable - using fallback analysis (reduced accuracy)"
 48: 
 49:     Raises:
 50:         RuntimeError: If neither Serena nor fallback analysis succeeds
 51:     """
 52:     import time
 53:     start_time = time.time()
 54: 
 55:     all_patterns = {
 56:         "code_structure": [],
 57:         "error_handling": [],
 58:         "naming_conventions": [],
 59:         "data_flow": [],
 60:         "test_patterns": [],
 61:         "import_patterns": []
 62:     }
 63: 
 64:     files_analyzed = []
 65:     symbol_count = 0
 66: 
 67:     # MVP: Serena MCP integration deferred - use fallback analysis
 68:     # TODO: Future enhancement - integrate Serena MCP for semantic analysis
 69:     serena_available = False
 70: 
 71:     for impl_path in implementation_paths:
 72:         impl_path_obj = Path(impl_path)
 73:         if not impl_path_obj.exists():
 74:             continue
 75: 
 76:         files_analyzed.append(impl_path)
 77: 
 78:         # Determine language from file extension
 79:         extension = impl_path_obj.suffix.lower()
 80:         language = determine_language(extension)
 81: 
 82:         code = impl_path_obj.read_text()
 83: 
 84:         # Analyze patterns using shared code analyzer
 85:         patterns = analyze_code_patterns(code, language)
 86: 
 87:         # Count symbols
 88:         symbol_count += count_code_symbols(code, language)
 89: 
 90:         # Merge patterns
 91:         for category, values in patterns.items():
 92:             if category in all_patterns:
 93:                 all_patterns[category].extend(values)
 94: 
 95:     # Deduplicate
 96:     for category in all_patterns:
 97:         all_patterns[category] = list(set(all_patterns[category]))
 98: 
 99:     duration = time.time() - start_time
100: 
101:     if not files_analyzed:
102:         raise RuntimeError(
103:             f"No implementation files found at {implementation_paths}\n"
104:             f"ðŸ”§ Troubleshooting: Verify file paths exist and are readable"
105:         )
106: 
107:     return {
108:         "detected_patterns": all_patterns,
109:         "files_analyzed": files_analyzed,
110:         "symbol_count": symbol_count,
111:         "analysis_duration": round(duration, 2),
112:         "serena_available": serena_available
113:     }
114: 
115: 
116: def calculate_drift_score(
117:     expected_patterns: Dict[str, Any],
118:     detected_patterns: Dict[str, Any]
119: ) -> Dict[str, Any]:
120:     """Calculate drift score between expected and detected patterns.
121: 
122:     Scoring methodology:
123:     - Each category (code_structure, error_handling, etc.) weighted equally
124:     - Within category: count mismatches / total expected patterns
125:     - Overall drift = average across all categories * 100
126: 
127:     Args:
128:         expected_patterns: From extract_patterns_from_prp()
129:         detected_patterns: From analyze_implementation()
130: 
131:     Returns:
132:         {
133:             "drift_score": 23.5,  # 0-100%, lower is better
134:             "category_scores": {
135:                 "code_structure": 10.0,
136:                 "error_handling": 0.0,
137:                 "naming_conventions": 50.0,
138:                 ...
139:             },
140:             "mismatches": [
141:                 {
142:                     "category": "naming_conventions",
143:                     "expected": "snake_case",
144:                     "detected": "camelCase",
145:                     "severity": "medium",
146:                     "affected_symbols": ["processData", "handleError"]
147:                 }
148:             ],
149:             "threshold_action": "auto_fix"  # auto_accept | auto_fix | escalate
150:         }
151:     """
152:     category_scores = {}
153:     mismatches = []
154: 
155:     # Categories to compare (exclude raw_examples)
156:     categories = [
157:         "code_structure",
158:         "error_handling",
159:         "naming_conventions",
160:         "data_flow",
161:         "test_patterns",
162:         "import_patterns"
163:     ]
164: 
165:     for category in categories:
166:         expected = expected_patterns.get(category, [])
167:         detected = detected_patterns.get(category, [])
168: 
169:         if not expected:
170:             # No expectations for this category - skip
171:             continue
172: 
173:         # Calculate mismatches
174:         missing_patterns = set(expected) - set(detected)
175:         unexpected_patterns = set(detected) - set(expected)
176: 
177:         # Mismatch score = (missing + unexpected) / (expected + detected)
178:         # This penalizes both missing expected patterns and unexpected patterns
179:         total_expected = len(expected)
180:         mismatch_count = len(missing_patterns)
181: 
182:         if total_expected > 0:
183:             category_score = (mismatch_count / total_expected) * 100
184:         else:
185:             category_score = 0.0
186: 
187:         category_scores[category] = round(category_score, 1)
188: 
189:         # Record mismatches
190:         for missing in missing_patterns:
191:             mismatches.append({
192:                 "category": category,
193:                 "expected": missing,
194:                 "detected": list(unexpected_patterns) if unexpected_patterns else None,
195:                 "severity": _determine_severity(category, missing),
196:                 "affected_symbols": []  # MVP: Symbol tracking deferred
197:             })
198: 
199:     # Calculate overall drift score (average of category scores)
200:     if category_scores:
201:         drift_score = sum(category_scores.values()) / len(category_scores)
202:     else:
203:         drift_score = 0.0
204: 
205:     drift_score = round(drift_score, 1)
206: 
207:     # Determine threshold action
208:     if drift_score < 10.0:
209:         threshold_action = "auto_accept"
210:     elif drift_score < 30.0:
211:         threshold_action = "auto_fix"
212:     else:
213:         threshold_action = "escalate"
214: 
215:     return {
216:         "drift_score": drift_score,
217:         "category_scores": category_scores,
218:         "mismatches": mismatches,
219:         "threshold_action": threshold_action
220:     }
221: 
222: 
223: def get_auto_fix_suggestions(mismatches: List[Dict]) -> List[str]:
224:     """Generate fix suggestions for 10-30% drift (MVP: display only, no auto-apply).
225: 
226:     Future enhancement: Apply fixes automatically using Serena edit operations.
227: 
228:     Args:
229:         mismatches: List of mismatch dicts from calculate_drift_score()
230: 
231:     Returns:
232:         List of actionable fix suggestions (e.g., "Rename processData â†’ process_data")
233:     """
234:     suggestions = []
235: 
236:     for mismatch in mismatches:
237:         category = mismatch["category"]
238:         expected = mismatch["expected"]
239:         detected = mismatch.get("detected", [])
240: 
241:         if category == "naming_conventions":
242:             # Suggest naming convention fixes
243:             if expected == "snake_case" and detected:
244:                 suggestions.append(
245:                     f"âš ï¸  Convert naming from {detected} to snake_case convention"
246:                 )
247:             elif expected == "camelCase" and "snake_case" in (detected or []):
248:                 suggestions.append(
249:                     f"âš ï¸  Convert naming from snake_case to camelCase convention"
250:                 )
251:             elif expected == "PascalCase" and detected:
252:                 suggestions.append(
253:                     f"âš ï¸  Convert class names to PascalCase convention"
254:                 )
255: 
256:         elif category == "error_handling":
257:             if expected == "try-except" and not detected:
258:                 suggestions.append(
259:                     f"âš ï¸  Add try-except error handling blocks"
260:                 )
261:             elif expected == "early-return" and not detected:
262:                 suggestions.append(
263:                     f"âš ï¸  Add guard clauses with early returns"
264:                 )
265: 
266:         elif category == "code_structure":
267:             if expected == "async/await" and detected:
268:                 if "callbacks" in (detected or []):
269:                     suggestions.append(
270:                         f"âš ï¸  Convert callback-based code to async/await pattern"
271:                     )
272:             elif expected == "class-based" and "functional" in (detected or []):
273:                 suggestions.append(
274:                     f"âš ï¸  Consider refactoring to class-based structure"
275:                 )
276: 
277:         elif category == "test_patterns":
278:             if expected == "pytest" and not detected:
279:                 suggestions.append(
280:                     f"âš ï¸  Add pytest-style test functions (test_* naming)"
281:                 )
282: 
283:     if not suggestions:
284:         suggestions.append("â„¹ï¸  Review patterns and consider manual alignment")
285: 
286:     return suggestions
287: 
288: 
289: def _determine_severity(category: str, pattern: str) -> str:
290:     """Determine severity of missing pattern."""
291:     # High severity: security/correctness patterns
292:     high_severity_patterns = {
293:         "error_handling": ["try-except", "try-catch"],
294:         "code_structure": ["async/await"]  # if expected but missing, may cause issues
295:     }
296: 
297:     # Medium severity: consistency/maintainability
298:     medium_severity_patterns = {
299:         "naming_conventions": ["snake_case", "camelCase", "PascalCase"],
300:         "test_patterns": ["pytest", "jest"]
301:     }
302: 
303:     # Low severity: style preferences
304:     low_severity_patterns = {
305:         "import_patterns": ["relative", "absolute"],
306:         "data_flow": ["props", "state"]
307:     }
308: 
309:     if category in high_severity_patterns and pattern in high_severity_patterns[category]:
310:         return "high"
311:     elif category in medium_severity_patterns and pattern in medium_severity_patterns[category]:
312:         return "medium"
313:     else:
314:         return "low"
</file>

<file path="tools/ce/drift.py">
  1: """Drift history tracking and analysis.
  2: 
  3: Provides tools for querying and analyzing architectural drift decisions
  4: across PRPs, creating an audit trail for pattern conformance validation.
  5: """
  6: 
  7: import yaml
  8: import re
  9: import logging
 10: from glob import glob
 11: from pathlib import Path
 12: from typing import Dict, List, Any, Optional
 13: from datetime import datetime, timezone
 14: 
 15: logger = logging.getLogger(__name__)
 16: 
 17: 
 18: def parse_drift_justification(prp_path: str) -> Optional[Dict[str, Any]]:
 19:     """Extract DRIFT_JUSTIFICATION from PRP YAML header.
 20: 
 21:     Args:
 22:         prp_path: Path to PRP markdown file
 23: 
 24:     Returns:
 25:         {
 26:             "prp_id": "PRP-001",
 27:             "prp_name": "Level 4 Pattern Conformance",
 28:             "drift_decision": {
 29:                 "score": 45.2,
 30:                 "action": "accepted",
 31:                 "justification": "...",
 32:                 "timestamp": "2025-10-12T15:30:00Z",
 33:                 "category_breakdown": {...},
 34:                 "reviewer": "human"
 35:             }
 36:         }
 37:         Returns None if no drift_decision found
 38: 
 39:     Raises:
 40:         FileNotFoundError: If PRP file doesn't exist
 41:         ValueError: If YAML header malformed
 42:     """
 43:     path = Path(prp_path)
 44:     if not path.exists():
 45:         raise FileNotFoundError(
 46:             f"PRP file not found: {prp_path}\n"
 47:             f"ðŸ”§ Troubleshooting: Check PRP path and ensure file exists"
 48:         )
 49: 
 50:     try:
 51:         with open(path, 'r') as f:
 52:             content = f.read()
 53: 
 54:         # Extract YAML header (between --- markers)
 55:         yaml_match = re.match(r'^---\n(.*?)\n---', content, re.DOTALL)
 56:         if not yaml_match:
 57:             raise ValueError(
 58:                 f"No YAML header found in {prp_path}\n"
 59:                 f"ðŸ”§ Troubleshooting: Ensure PRP has YAML front matter between --- markers"
 60:             )
 61: 
 62:         yaml_content = yaml_match.group(1)
 63:         header = yaml.safe_load(yaml_content)
 64: 
 65:         # Check if drift_decision exists
 66:         if "drift_decision" not in header:
 67:             return None
 68: 
 69:         return {
 70:             "prp_id": header.get("prp_id", "UNKNOWN"),
 71:             "prp_name": header.get("name", "Unknown PRP"),
 72:             "drift_decision": header["drift_decision"]
 73:         }
 74: 
 75:     except Exception as e:
 76:         raise ValueError(
 77:             f"Failed to parse PRP YAML: {str(e)}\n"
 78:             f"ðŸ”§ Troubleshooting: Verify YAML syntax in {prp_path}"
 79:         ) from e
 80: 
 81: 
 82: def get_drift_history(
 83:     last_n: Optional[int] = None,
 84:     prp_id: Optional[str] = None,
 85:     action_filter: Optional[str] = None
 86: ) -> List[Dict[str, Any]]:
 87:     """Query drift decision history across all PRPs.
 88: 
 89:     Args:
 90:         last_n: Return only last N decisions (by timestamp)
 91:         prp_id: Filter by specific PRP ID
 92:         action_filter: Filter by action (accepted, rejected, examples_updated)
 93: 
 94:     Returns:
 95:         List of drift decisions sorted by timestamp (newest first)
 96: 
 97:     Example:
 98:         >>> history = get_drift_history(last_n=3)
 99:         >>> history[0]["drift_decision"]["score"]
100:         45.2
101:     """
102:     prp_dirs = ["PRPs/executed", "PRPs/feature-requests"]
103:     all_decisions = []
104: 
105:     for prp_dir in prp_dirs:
106:         dir_path = Path(prp_dir)
107:         if not dir_path.exists():
108:             continue
109: 
110:         # Find all PRP markdown files
111:         for prp_file in dir_path.glob("PRP-*.md"):
112:             try:
113:                 decision = parse_drift_justification(str(prp_file))
114:                 if decision:
115:                     all_decisions.append(decision)
116:             except Exception as e:
117:                 logger.warning(f"Skipping {prp_file}: {e}")
118: 
119:     # Apply filters
120:     if prp_id:
121:         all_decisions = [d for d in all_decisions if d["prp_id"] == prp_id]
122: 
123:     if action_filter:
124:         all_decisions = [
125:             d for d in all_decisions
126:             if d["drift_decision"]["action"] == action_filter
127:         ]
128: 
129:     # Sort by timestamp (newest first)
130:     all_decisions.sort(
131:         key=lambda d: d["drift_decision"].get("timestamp", ""),
132:         reverse=True
133:     )
134: 
135:     # Apply limit
136:     if last_n:
137:         all_decisions = all_decisions[:last_n]
138: 
139:     return all_decisions
140: 
141: 
142: def drift_summary() -> Dict[str, Any]:
143:     """Generate aggregate statistics for all drift decisions.
144: 
145:     Returns:
146:         {
147:             "total_prps": 15,
148:             "prps_with_drift": 8,
149:             "decisions": {
150:                 "accepted": 5,
151:                 "rejected": 2,
152:                 "examples_updated": 1
153:             },
154:             "avg_drift_score": 23.7,
155:             "score_distribution": {
156:                 "low": 3,      # 0-10%
157:                 "medium": 4,   # 10-30%
158:                 "high": 1      # 30%+
159:             },
160:             "category_breakdown": {
161:                 "code_structure": {"avg": 25.0, "count": 8},
162:                 "error_handling": {"avg": 15.0, "count": 8},
163:                 "naming_conventions": {"avg": 30.0, "count": 8}
164:             },
165:             "reviewer_breakdown": {
166:                 "human": 6,
167:                 "auto_accept": 2,
168:                 "auto_fix": 0
169:             }
170:         }
171:     """
172:     history = get_drift_history()
173: 
174:     if not history:
175:         return {
176:             "total_prps": 0,
177:             "prps_with_drift": 0,
178:             "decisions": {},
179:             "avg_drift_score": 0.0,
180:             "score_distribution": {},
181:             "category_breakdown": {},
182:             "reviewer_breakdown": {}
183:         }
184: 
185:     # Count decisions by action
186:     decisions = {}
187:     for h in history:
188:         action = h["drift_decision"]["action"]
189:         decisions[action] = decisions.get(action, 0) + 1
190: 
191:     # Calculate average drift score
192:     scores = [h["drift_decision"]["score"] for h in history]
193:     avg_drift = sum(scores) / len(scores)
194: 
195:     # Score distribution
196:     score_dist = {"low": 0, "medium": 0, "high": 0}
197:     for score in scores:
198:         if score <= 10:
199:             score_dist["low"] += 1
200:         elif score <= 30:
201:             score_dist["medium"] += 1
202:         else:
203:             score_dist["high"] += 1
204: 
205:     # Category breakdown
206:     categories = {}
207:     for h in history:
208:         breakdown = h["drift_decision"].get("category_breakdown", {})
209:         for cat, score in breakdown.items():
210:             if cat not in categories:
211:                 categories[cat] = {"total": 0, "count": 0}
212:             categories[cat]["total"] += score
213:             categories[cat]["count"] += 1
214: 
215:     category_breakdown = {
216:         cat: {
217:             "avg": data["total"] / data["count"],
218:             "count": data["count"]
219:         }
220:         for cat, data in categories.items()
221:     }
222: 
223:     # Reviewer breakdown
224:     reviewers = {}
225:     for h in history:
226:         reviewer = h["drift_decision"].get("reviewer", "unknown")
227:         reviewers[reviewer] = reviewers.get(reviewer, 0) + 1
228: 
229:     return {
230:         "total_prps": len(history),
231:         "prps_with_drift": len(history),
232:         "decisions": decisions,
233:         "avg_drift_score": round(avg_drift, 2),
234:         "score_distribution": score_dist,
235:         "category_breakdown": category_breakdown,
236:         "reviewer_breakdown": reviewers
237:     }
238: 
239: 
240: def show_drift_decision(prp_id: str) -> Dict[str, Any]:
241:     """Display detailed drift decision for specific PRP.
242: 
243:     Args:
244:         prp_id: PRP identifier (e.g., "PRP-001")
245: 
246:     Returns:
247:         Full drift decision with metadata
248: 
249:     Raises:
250:         ValueError: If PRP not found or has no drift decision
251:     """
252:     history = get_drift_history(prp_id=prp_id)
253: 
254:     if not history:
255:         raise ValueError(
256:             f"No drift decision found for {prp_id}\n"
257:             f"ðŸ”§ Troubleshooting: Verify PRP ID and check if drift decision exists in YAML header"
258:         )
259: 
260:     return history[0]
261: 
262: 
263: def compare_drift_decisions(prp_id_1: str, prp_id_2: str) -> Dict[str, Any]:
264:     """Compare drift decisions between two PRPs.
265: 
266:     Args:
267:         prp_id_1: First PRP ID
268:         prp_id_2: Second PRP ID
269: 
270:     Returns:
271:         {
272:             "prp_1": {...},
273:             "prp_2": {...},
274:             "comparison": {
275:                 "score_diff": 12.5,
276:                 "same_action": True,
277:                 "common_categories": ["code_structure", "naming_conventions"],
278:                 "divergent_categories": ["error_handling"]
279:             }
280:         }
281: 
282:     Raises:
283:         ValueError: If either PRP not found or missing drift decision
284:     """
285:     decision_1 = show_drift_decision(prp_id_1)
286:     decision_2 = show_drift_decision(prp_id_2)
287: 
288:     # Calculate comparison
289:     score_diff = abs(
290:         decision_1["drift_decision"]["score"] -
291:         decision_2["drift_decision"]["score"]
292:     )
293: 
294:     same_action = (
295:         decision_1["drift_decision"]["action"] ==
296:         decision_2["drift_decision"]["action"]
297:     )
298: 
299:     # Category comparison
300:     cat_1 = set(decision_1["drift_decision"].get("category_breakdown", {}).keys())
301:     cat_2 = set(decision_2["drift_decision"].get("category_breakdown", {}).keys())
302: 
303:     common_categories = list(cat_1 & cat_2)
304:     divergent_categories = list(cat_1 ^ cat_2)
305: 
306:     return {
307:         "prp_1": decision_1,
308:         "prp_2": decision_2,
309:         "comparison": {
310:             "score_diff": round(score_diff, 2),
311:             "same_action": same_action,
312:             "common_categories": common_categories,
313:             "divergent_categories": divergent_categories
314:         }
315:     }
</file>

<file path="tools/ce/exceptions.py">
 1: """Custom exceptions for PRP execution orchestration."""
 2: 
 3: 
 4: class EscalationRequired(Exception):
 5:     """Raised when automatic self-healing fails and human intervention is needed.
 6: 
 7:     Attributes:
 8:         reason: Escalation trigger reason (persistent_error, ambiguous, architecture, dependencies, security)
 9:         error: Parsed error dict that triggered escalation
10:         troubleshooting: Multi-line troubleshooting guidance for user
11:     """
12: 
13:     def __init__(
14:         self,
15:         reason: str,
16:         error: dict,
17:         troubleshooting: str
18:     ):
19:         self.reason = reason
20:         self.error = error
21:         self.troubleshooting = troubleshooting
22: 
23:         # Format error message
24:         error_type = error.get("type", "unknown")
25:         error_msg = error.get("message", "No message")
26:         error_loc = f"{error.get('file', 'unknown')}:{error.get('line', '?')}"
27: 
28:         message = (
29:             f"Escalation required ({reason})\n"
30:             f"Error type: {error_type}\n"
31:             f"Location: {error_loc}\n"
32:             f"Message: {error_msg}\n\n"
33:             f"ðŸ”§ Troubleshooting:\n{troubleshooting}"
34:         )
35: 
36:         super().__init__(message)
37: 
38: 
39: class BlueprintParseError(ValueError):
40:     """Raised when PRP blueprint parsing fails."""
41: 
42:     def __init__(self, prp_path: str, issue: str):
43:         message = (
44:             f"Failed to parse PRP blueprint: {prp_path}\n"
45:             f"Issue: {issue}\n"
46:             f"ðŸ”§ Troubleshooting: Ensure PRP has well-formed IMPLEMENTATION BLUEPRINT section"
47:         )
48:         super().__init__(message)
49: 
50: 
51: class ValidationError(RuntimeError):
52:     """Raised when validation fails after max attempts."""
53: 
54:     def __init__(self, level: str, error_details: dict):
55:         self.level = level
56:         self.error_details = error_details
57: 
58:         message = (
59:             f"Validation failed at Level {level}\n"
60:             f"Attempts: {error_details.get('attempts', 0)}\n"
61:             f"Last error: {error_details.get('last_error', 'Unknown')}\n"
62:             f"ðŸ”§ Troubleshooting: Review validation output for specific errors"
63:         )
64:         super().__init__(message)
65: 
66: 
67: class ContextDriftError(RuntimeError):
68:     """Raised when context drift exceeds acceptable threshold.
69: 
70:     Attributes:
71:         drift_score: Drift percentage (0-100)
72:         threshold: Threshold that was exceeded
73:         troubleshooting: Multi-line troubleshooting guidance
74:     """
75: 
76:     def __init__(self, drift_score: float, threshold: float, troubleshooting: str):
77:         self.drift_score = drift_score
78:         self.threshold = threshold
79:         self.troubleshooting = troubleshooting
80: 
81:         message = (
82:             f"Context drift too high: {drift_score:.1f}% (threshold: {threshold:.1f}%)\n"
83:             f"ðŸ”§ Troubleshooting:\n{troubleshooting}"
84:         )
85:         super().__init__(message)
</file>

<file path="tools/ce/execute.py">
  1: """PRP execution orchestration with phase-by-phase implementation and self-healing.
  2: 
  3: Testing Strategy:
  4:     This module achieves 54% line coverage (263/487 statements), focusing on comprehensive
  5:     testing of core utility functions rather than integration orchestration.
  6: 
  7:     Coverage Breakdown:
  8:         âœ… Core Utilities (100% coverage):
  9:            - parse_validation_error(): 7 tests covering all error types
 10:              (ImportError, AssertionError, SyntaxError, TypeError, NameError)
 11:            - apply_self_healing_fix(): 4 tests with real file operations
 12:            - check_escalation_triggers(): 7 tests for all 5 trigger conditions
 13:            - _add_import_statement(): 2 tests for import positioning
 14:            - escalate_to_human(): 2 tests for exception raising
 15: 
 16:         âš ï¸  Integration Orchestration (0% coverage):
 17:            - run_validation_loop() (lines 727-902): 176 lines
 18:            - execute_prp() (lines 359-497): 139 lines
 19: 
 20:            Rationale: These functions require complex mocking (10+ patches per test)
 21:            due to dynamic imports, state management across retry loops, and multiple
 22:            external dependencies. Better suited for E2E testing with real validation
 23:            scenarios rather than unit tests.
 24: 
 25:     Quality Assurance:
 26:         - All tests follow "Real Functionality Testing" policy (no hardcoded success)
 27:         - Self-healing tests use real file operations (tempfile, not mocks)
 28:         - Error parsing tests use realistic error output samples
 29:         - Escalation trigger tests verify all 5 escalation conditions
 30:         - 33/33 tests passing with pytest
 31: 
 32:     Future Testing:
 33:         - Integration tests for run_validation_loop() with real test projects
 34:         - E2E tests for execute_prp() with full PRP execution scenarios
 35:         - Performance tests for validation retry loops
 36: """
 37: 
 38: import re
 39: from typing import Dict, Any, List, Optional
 40: 
 41: from .exceptions import EscalationRequired
 42: from .blueprint_parser import parse_blueprint
 43: from .validation_loop import (
 44:     run_validation_loop,
 45:     calculate_confidence_score,
 46:     parse_validation_error,
 47:     check_escalation_triggers,
 48:     apply_self_healing_fix,
 49:     escalate_to_human
 50: )
 51: 
 52: 
 53: # ============================================================================
 54: # Phase 1: Blueprint parsing moved to blueprint_parser.py (imported above)
 55: # ============================================================================
 56: 
 57: 
 58: # ============================================================================
 59: # Phase 2: Execution Orchestration Functions
 60: # ============================================================================
 61: 
 62: def execute_prp(
 63:     prp_id: str,
 64:     start_phase: Optional[int] = None,
 65:     end_phase: Optional[int] = None,
 66:     skip_validation: bool = False,
 67:     dry_run: bool = False
 68: ) -> Dict[str, Any]:
 69:     """Main execution function - orchestrates PRP implementation.
 70: 
 71:     Args:
 72:         prp_id: PRP identifier (e.g., "PRP-4")
 73:         start_phase: Optional phase to start from (None = Phase 1)
 74:         end_phase: Optional phase to end at (None = all phases)
 75:         skip_validation: Skip validation loops (dangerous - for debugging only)
 76:         dry_run: Parse blueprint and return phases without execution
 77: 
 78:     Returns:
 79:         {
 80:             "success": True,
 81:             "prp_id": "PRP-4",
 82:             "phases_completed": 3,
 83:             "validation_results": {
 84:                 "L1": {"passed": True, "attempts": 1},
 85:                 "L2": {"passed": True, "attempts": 2},
 86:                 "L3": {"passed": True, "attempts": 1},
 87:                 "L4": {"passed": True, "attempts": 1}
 88:             },
 89:             "checkpoints_created": ["checkpoint-PRP-4-phase1", ...],
 90:             "confidence_score": "10/10",
 91:             "execution_time": "45m 23s"
 92:         }
 93: 
 94:     Raises:
 95:         RuntimeError: If execution fails after escalation
 96:         FileNotFoundError: If PRP file not found
 97: 
 98:     Process:
 99:         1. Initialize PRP context: ce prp start <prp_id>
100:         2. Parse blueprint: parse_blueprint(prp_path)
101:         3. Filter phases: start_phase to end_phase
102:         4. Handle dry-run: If dry_run=True, return parsed blueprint without execution
103:         5. For each phase:
104:            a. Update phase in state: update_prp_phase(phase_name)
105:            b. Execute phase: execute_phase(phase)
106:            c. Run validation loop: run_validation_loop(phase) (unless skip_validation)
107:            d. Create checkpoint: create_checkpoint(phase)
108:            e. Update validation attempts in state
109:         6. Calculate confidence score
110:         7. End PRP context: ce prp end <prp_id>
111:         8. Return execution summary
112:     """
113:     import time
114:     from .prp import start_prp, end_prp, update_prp_phase, create_checkpoint
115: 
116:     start_time = time.time()
117: 
118:     # Find PRP file
119:     prp_path = _find_prp_file(prp_id)
120: 
121:     # Parse blueprint
122:     phases = parse_blueprint(prp_path)
123: 
124:     # Filter phases
125:     if start_phase:
126:         phases = [p for p in phases if p["phase_number"] >= start_phase]
127:     if end_phase:
128:         phases = [p for p in phases if p["phase_number"] <= end_phase]
129: 
130:     if not phases:
131:         raise RuntimeError(
132:             f"No phases to execute (start={start_phase}, end={end_phase})\n"
133:             f"ðŸ”§ Troubleshooting: Check phase numbers in PRP"
134:         )
135: 
136:     # Dry run - return parsed blueprint
137:     if dry_run:
138:         return {
139:             "success": True,
140:             "dry_run": True,
141:             "prp_id": prp_id,
142:             "phases": phases,
143:             "total_phases": len(phases)
144:         }
145: 
146:     # Initialize PRP context
147:     prp_name = phases[0]["phase_name"] if phases else prp_id
148:     start_result = start_prp(prp_id, prp_name)
149: 
150:     # Track execution state
151:     phases_completed = 0
152:     checkpoints_created = []
153:     validation_results = {}
154: 
155:     try:
156:         # Execute each phase
157:         for phase in phases:
158:             phase_num = phase["phase_number"]
159:             phase_name = phase["phase_name"]
160: 
161:             print(f"\n{'='*80}")
162:             print(f"Phase {phase_num}: {phase_name}")
163:             print(f"Goal: {phase['goal']}")
164:             print(f"{'='*80}\n")
165: 
166:             # Update phase in state
167:             update_prp_phase(f"phase{phase_num}")
168: 
169:             # Execute phase
170:             exec_result = execute_phase(phase)
171:             if not exec_result["success"]:
172:                 raise RuntimeError(
173:                     f"Phase {phase_num} execution failed: {exec_result.get('error', 'Unknown error')}\n"
174:                     f"ðŸ”§ Troubleshooting: Check phase implementation logic"
175:                 )
176: 
177:             # Run validation loop (unless skipped)
178:             if not skip_validation and phase.get("validation_command"):
179:                 val_result = run_validation_loop(phase, prp_path)
180:                 validation_results[f"Phase{phase_num}"] = val_result
181: 
182:                 if not val_result["success"]:
183:                     raise RuntimeError(
184:                         f"Phase {phase_num} validation failed after {val_result.get('attempts', 0)} attempts\n"
185:                         f"ðŸ”§ Troubleshooting: Review validation errors"
186:                     )
187: 
188:             # Create checkpoint
189:             checkpoint_result = create_checkpoint(
190:                 f"phase{phase_num}",
191:                 f"Phase {phase_num} complete: {phase_name}"
192:             )
193:             checkpoints_created.append(checkpoint_result["tag_name"])
194: 
195:             phases_completed += 1
196:             print(f"\nâœ… Phase {phase_num} complete\n")
197: 
198:         # Calculate confidence score
199:         confidence_score = calculate_confidence_score(validation_results)
200: 
201:         # Calculate execution time
202:         duration_seconds = time.time() - start_time
203:         hours = int(duration_seconds // 3600)
204:         minutes = int((duration_seconds % 3600) // 60)
205:         seconds = int(duration_seconds % 60)
206: 
207:         if hours > 0:
208:             execution_time = f"{hours}h {minutes}m {seconds}s"
209:         elif minutes > 0:
210:             execution_time = f"{minutes}m {seconds}s"
211:         else:
212:             execution_time = f"{seconds}s"
213: 
214:         # Step 6.5: Post-execution sync (if auto-sync enabled)
215:         from .context import is_auto_sync_enabled, post_execution_sync
216:         if is_auto_sync_enabled():
217:             try:
218:                 print(f"\n{'='*80}")
219:                 print("Running post-execution sync...")
220:                 print(f"{'='*80}\n")
221:                 sync_result = post_execution_sync(prp_id, skip_cleanup=False)
222:                 print(f"âœ… Post-sync complete: drift={sync_result['drift_score']:.1f}%")
223:                 print(f"   Cleanup: {sync_result['cleanup_completed']}")
224:                 print(f"   Memories archived: {sync_result['memories_archived']}")
225:                 print(f"   Final checkpoint: {sync_result.get('final_checkpoint', 'N/A')}")
226:             except Exception as e:
227:                 # Non-blocking - log warning but allow execution to complete
228:                 print(f"âš ï¸  Post-execution sync failed: {e}")
229:                 print(f"ðŸ”§ Troubleshooting: Run 'ce context post-sync {prp_id}' manually")
230: 
231:         # End PRP context
232:         end_result = end_prp(prp_id)
233: 
234:         return {
235:             "success": True,
236:             "prp_id": prp_id,
237:             "phases_completed": phases_completed,
238:             "validation_results": validation_results,
239:             "checkpoints_created": checkpoints_created,
240:             "confidence_score": confidence_score,
241:             "execution_time": execution_time
242:         }
243: 
244:     except Exception as e:
245:         # On error, still try to end PRP context
246:         try:
247:             end_prp(prp_id)
248:         except Exception as cleanup_error:
249:             import logging
250:             logger = logging.getLogger(__name__)
251:             logger.warning(f"Failed to end PRP context during cleanup: {cleanup_error}")
252:         raise
253: 
254: 
255: def execute_phase(phase: Dict[str, Any]) -> Dict[str, Any]:
256:     """Execute a single blueprint phase using Serena MCP for file operations.
257: 
258:     Args:
259:         phase: Parsed phase dict from parse_blueprint()
260: 
261:     Returns:
262:         {
263:             "success": True,
264:             "files_modified": ["src/auth.py"],
265:             "files_created": ["src/models/user.py"],
266:             "functions_added": ["authenticate", "validate_token"],
267:             "duration": "12m 34s",
268:             "error": "Error message if success=False"
269:         }
270: 
271:     Process:
272:         1. Create files listed in files_to_create using Serena MCP (or fallback)
273:         2. Modify files listed in files_to_modify using Serena MCP (or fallback)
274:         3. Implement functions from function signatures
275:         4. Log progress to console (shows method used: mcp vs filesystem)
276:         5. Return execution summary
277: 
278:     Implementation Strategy:
279:         - Use Serena MCP when available for symbol-aware code insertion
280:         - Graceful fallback to filesystem operations when MCP unavailable
281:         - Use function signatures as implementation guides
282:         - Follow approach description for implementation style
283:         - Reference goal for context
284: 
285:     MCP Integration (PRP-9):
286:         - mcp_adapter.py provides abstraction layer
287:         - File creation: create_file_with_mcp() tries MCP, falls back to filesystem
288:         - Code insertion: insert_code_with_mcp() uses symbol-aware MCP or naive append
289:         - Console output shows method used (mcp/mcp_symbol_aware/filesystem_append)
290:     """
291:     import time
292: 
293:     start_time = time.time()
294: 
295:     files_created = []
296:     files_modified = []
297:     functions_added = []
298: 
299:     try:
300:         # Create new files
301:         for file_entry in phase.get("files_to_create", []):
302:             filepath = file_entry["path"]
303:             description = file_entry["description"]
304:             print(f"  ðŸ“ Create: {filepath} - {description}")
305: 
306:             # Generate initial file content based on description and functions
307:             content = _generate_file_content(filepath, description, phase)
308: 
309:             # Create file using Serena MCP or fallback to filesystem
310:             from .mcp_adapter import create_file_with_mcp
311:             result = create_file_with_mcp(filepath, content)
312: 
313:             if not result["success"]:
314:                 raise RuntimeError(
315:                     f"Failed to create {filepath}: {result.get('error')}\n"
316:                     f"ðŸ”§ Troubleshooting:\n"
317:                     f"  1. Verify parent directory exists and is writable\n"
318:                     f"  2. Check file path doesn't contain invalid characters\n"
319:                     f"  3. Ensure Serena MCP is available (fallback may fail)\n"
320:                     f"  4. Review phase files_to_create list for accuracy"
321:                 )
322: 
323:             files_created.append(filepath)
324:             print(f"    âœ“ Created via {result['method']}: {filepath}")
325: 
326:         # Modify existing files
327:         for file_entry in phase.get("files_to_modify", []):
328:             filepath = file_entry["path"]
329:             description = file_entry["description"]
330:             print(f"  âœï¸  Modify: {filepath} - {description}")
331: 
332:             # Add functions to existing file
333:             _add_functions_to_file(filepath, phase.get("functions", []), phase)
334: 
335:             files_modified.append(filepath)
336: 
337:         # Track implemented functions
338:         for func_entry in phase.get("functions", []):
339:             signature = func_entry["signature"]
340:             func_name_match = re.search(r'(?:def|class)\s+(\w+)', signature)
341:             if func_name_match:
342:                 func_name = func_name_match.group(1)
343:                 print(f"  ðŸ”§ Implement: {func_name}")
344:                 functions_added.append(func_name)
345: 
346:         duration = time.time() - start_time
347: 
348:         return {
349:             "success": True,
350:             "files_created": files_created,
351:             "files_modified": files_modified,
352:             "functions_added": functions_added,
353:             "duration": f"{duration:.2f}s"
354:         }
355: 
356:     except Exception as e:
357:         duration = time.time() - start_time
358:         raise RuntimeError(
359:             f"Phase execution failed after {duration:.2f}s\n"
360:             f"Error: {str(e)}\n"
361:             f"Files created: {files_created}\n"
362:             f"Files modified: {files_modified}\n"
363:             f"ðŸ”§ Troubleshooting:\n"
364:             f"  1. Check if file paths are valid\n"
365:             f"  2. Verify Serena MCP is available\n"
366:             f"  3. Review function signatures for syntax errors\n"
367:             f"  4. Check phase goal and approach for clarity"
368:         ) from e
369: 
370: 
371: def _generate_file_content(filepath: str, description: str, phase: Dict[str, Any]) -> str:
372:     """Generate initial content for a new file based on context.
373: 
374:     Args:
375:         filepath: Path to file being created
376:         description: File description from phase
377:         phase: Phase context with goal, approach, functions
378: 
379:     Returns:
380:         Generated file content with module docstring and function stubs
381:     """
382:     lines = []
383: 
384:     # Add module docstring
385:     lines.append(f'"""{description}."""')
386:     lines.append("")
387: 
388:     # Add relevant functions for this file
389:     for func_entry in phase.get("functions", []):
390:         full_code = func_entry.get("full_code", "")
391:         if full_code:
392:             lines.append(full_code)
393:             lines.append("")
394:             lines.append("")
395: 
396:     # If no functions, add placeholder comment
397:     if not phase.get("functions"):
398:         lines.append(f"# {phase['goal']}")
399:         lines.append(f"# Approach: {phase['approach']}")
400: 
401:     return "\n".join(lines)
402: 
403: 
404: def _add_functions_to_file(filepath: str, functions: List[Dict[str, str]], phase: Dict[str, Any]) -> None:
405:     """Add functions to an existing file using Serena MCP.
406: 
407:     Args:
408:         filepath: Path to file to modify
409:         functions: List of function dicts with signature, docstring, full_code
410:         phase: Phase context
411: 
412:     Raises:
413:         RuntimeError: If file modification fails
414:     """
415:     if not functions:
416:         return
417: 
418:     # Use Serena MCP for symbol-aware insertion or fallback to filesystem
419:     from .mcp_adapter import insert_code_with_mcp
420: 
421:     try:
422:         # Insert each function using symbol-aware insertion
423:         for func_entry in functions:
424:             full_code = func_entry.get("full_code", "")
425:             if full_code:
426:                 result = insert_code_with_mcp(
427:                     filepath=filepath,
428:                     code=full_code,
429:                     mode="after_last_symbol"  # Insert after last function/class
430:                 )
431: 
432:                 if not result["success"]:
433:                     raise RuntimeError(
434:                         f"Failed to insert code: {result.get('error')}\n"
435:                         f"ðŸ”§ Troubleshooting:\n"
436:                         f"  1. Verify file exists and is writable: {filepath}\n"
437:                         f"  2. Check function code is syntactically valid\n"
438:                         f"  3. Ensure Serena MCP is available for symbol-aware insertion\n"
439:                         f"  4. Review phase functions list for correctness"
440:                     )
441: 
442:                 method = result["method"]
443:                 if method == "mcp_symbol_aware":
444:                     print(f"    âœ“ Inserted via MCP (after {result.get('symbol')})")
445:                 else:
446:                     print(f"    âœ“ Inserted via {method}")
447: 
448:     except Exception as e:
449:         raise RuntimeError(
450:             f"Failed to add functions to {filepath}\n"
451:             f"Error: {str(e)}\n"
452:             f"ðŸ”§ Troubleshooting:\n"
453:             f"  1. Check file exists and is writable\n"
454:             f"  2. Verify function code is syntactically valid\n"
455:             f"  3. Review phase functions list"
456:         ) from e
457: 
458: 
459: def _find_prp_file(prp_id: str) -> str:
460:     """Find PRP file path from PRP ID.
461: 
462:     Args:
463:         prp_id: PRP identifier (e.g., "PRP-4")
464: 
465:     Returns:
466:         Absolute path to PRP file
467: 
468:     Raises:
469:         FileNotFoundError: If PRP file not found
470: 
471:     Search strategy:
472:         1. Check PRPs/feature-requests/PRP-{id}-*.md
473:         2. Check PRPs/executed/PRP-{id}-*.md
474:         3. Check PRPs/PRP-{id}-*.md
475:     """
476:     from pathlib import Path
477: 
478:     # Get project root (assuming we're in tools/ce/)
479:     project_root = Path(__file__).parent.parent.parent
480: 
481:     # Search locations
482:     search_paths = [
483:         project_root / "PRPs" / "feature-requests",
484:         project_root / "PRPs" / "executed",
485:         project_root / "PRPs"
486:     ]
487: 
488:     # Extract numeric ID (e.g., "PRP-4" -> "4")
489:     numeric_id = prp_id.replace("PRP-", "").replace("prp-", "")
490: 
491:     for search_dir in search_paths:
492:         if not search_dir.exists():
493:             continue
494: 
495:         # Look for PRP-{id}-*.md or PRP{id}-*.md
496:         patterns = [
497:             f"PRP-{numeric_id}-*.md",
498:             f"PRP{numeric_id}-*.md",
499:             f"prp-{numeric_id}-*.md"
500:         ]
501: 
502:         for pattern in patterns:
503:             matches = list(search_dir.glob(pattern))
504:             if matches:
505:                 return str(matches[0].absolute())
506: 
507:     raise FileNotFoundError(
508:         f"PRP file not found: {prp_id}\n"
509:         f"ðŸ”§ Troubleshooting: Check PRPs/feature-requests/ or PRPs/executed/"
510:     )
</file>

<file path="tools/ce/generate.py">
   1: """PRP generation from INITIAL.md.
   2: 
   3: This module automates PRP (Product Requirements Prompt) generation by:
   4: 1. Parsing INITIAL.md structure (FEATURE, EXAMPLES, DOCUMENTATION, OTHER CONSIDERATIONS)
   5: 2. Orchestrating MCP tools (Serena, Context7, Sequential Thinking) for research
   6: 3. Synthesizing comprehensive PRP with all 6 sections
   7: 
   8: Usage:
   9:     from ce.generate import generate_prp
  10:     result = generate_prp("feature-requests/auth/INITIAL.md")
  11: """
  12: 
  13: import re
  14: import logging
  15: from pathlib import Path
  16: from typing import Dict, List, Any, Optional
  17: 
  18: logger = logging.getLogger(__name__)
  19: 
  20: # Section markers for INITIAL.md parsing
  21: SECTION_MARKERS = {
  22:     "feature": r"^##\s*FEATURE\s*$",
  23:     "planning": r"^##\s*PLANNING\s+CONTEXT\s*$",
  24:     "examples": r"^##\s*EXAMPLES\s*$",
  25:     "documentation": r"^##\s*DOCUMENTATION\s*$",
  26:     "other": r"^##\s*OTHER\s+CONSIDERATIONS\s*$"
  27: }
  28: 
  29: 
  30: def parse_initial_md(filepath: str) -> Dict[str, Any]:
  31:     """Parse INITIAL.md into structured sections.
  32: 
  33:     Args:
  34:         filepath: Path to INITIAL.md file
  35: 
  36:     Returns:
  37:         {
  38:             "feature_name": "User Authentication System",
  39:             "feature": "Build user auth with JWT tokens...",
  40:             "examples": [
  41:                 {"type": "inline", "language": "python", "code": "..."},
  42:                 {"type": "file_ref", "file": "src/auth.py", "lines": "42-67"}
  43:             ],
  44:             "documentation": [
  45:                 {"title": "JWT Guide", "url": "https://...", "type": "link"},
  46:                 {"title": "pytest", "url": "", "type": "library"}
  47:             ],
  48:             "other_considerations": "Security: Hash passwords with bcrypt...",
  49:             "raw_content": "<full file content>"
  50:         }
  51: 
  52:     Raises:
  53:         FileNotFoundError: If INITIAL.md doesn't exist
  54:         ValueError: If required sections missing (FEATURE, EXAMPLES)
  55: 
  56:     Process:
  57:         1. Read file content
  58:         2. Extract feature name from first heading
  59:         3. Split content by section markers (## FEATURE, ## EXAMPLES, etc.)
  60:         4. Parse EXAMPLES for code block references
  61:         5. Parse DOCUMENTATION for URL links
  62:         6. Validate FEATURE and EXAMPLES present (minimum required)
  63:     """
  64:     # Check file exists
  65:     file_path = Path(filepath)
  66:     if not file_path.exists():
  67:         raise FileNotFoundError(
  68:             f"INITIAL.md not found: {filepath}\n"
  69:             f"ðŸ”§ Troubleshooting: Verify file path is correct and file exists"
  70:         )
  71: 
  72:     # Read content
  73:     content = file_path.read_text(encoding="utf-8")
  74: 
  75:     # Extract feature name from first heading (# Feature: <name>)
  76:     feature_name_match = re.search(r"^#\s+Feature:\s+(.+)$", content, re.MULTILINE)
  77:     if not feature_name_match:
  78:         raise ValueError(
  79:             f"Feature name not found in {filepath}\n"
  80:             f"ðŸ”§ Troubleshooting: First line must be '# Feature: <name>'"
  81:         )
  82:     feature_name = feature_name_match.group(1).strip()
  83: 
  84:     # Split content by section markers
  85:     sections = {}
  86:     lines = content.split("\n")
  87:     current_section = None
  88:     section_content = []
  89: 
  90:     for line in lines:
  91:         # Check if line is a section marker
  92:         matched_section = None
  93:         for section_key, pattern in SECTION_MARKERS.items():
  94:             if re.match(pattern, line.strip()):
  95:                 # Save previous section if exists
  96:                 if current_section and section_content:
  97:                     sections[current_section] = "\n".join(section_content).strip()
  98:                     section_content = []
  99:                 current_section = section_key
 100:                 matched_section = section_key
 101:                 break
 102: 
 103:         # If not a section marker and we're in a section, accumulate content
 104:         if not matched_section and current_section:
 105:             section_content.append(line)
 106: 
 107:     # Save last section
 108:     if current_section and section_content:
 109:         sections[current_section] = "\n".join(section_content).strip()
 110: 
 111:     # Validate required sections
 112:     if "feature" not in sections:
 113:         raise ValueError(
 114:             f"Required FEATURE section missing in {filepath}\n"
 115:             f"ðŸ”§ Troubleshooting: Add '## FEATURE' section with feature description"
 116:         )
 117:     if "examples" not in sections:
 118:         raise ValueError(
 119:             f"Required EXAMPLES section missing in {filepath}\n"
 120:             f"ðŸ”§ Troubleshooting: Add '## EXAMPLES' section with code examples"
 121:         )
 122: 
 123:     # Parse subsections
 124:     return {
 125:         "feature_name": feature_name,
 126:         "feature": sections.get("feature", ""),
 127:         "planning_context": sections.get("planning", ""),
 128:         "examples": extract_code_examples(sections.get("examples", "")),
 129:         "documentation": extract_documentation_links(sections.get("documentation", "")),
 130:         "other_considerations": sections.get("other", ""),
 131:         "raw_content": content
 132:     }
 133: 
 134: 
 135: def extract_code_examples(examples_text: str) -> List[Dict[str, Any]]:
 136:     """Extract code examples from EXAMPLES section.
 137: 
 138:     Patterns supported:
 139:         - Inline code blocks with language tags
 140:         - File references (e.g., "See src/auth.py:42-67")
 141:         - Natural language descriptions
 142: 
 143:     Returns:
 144:         [
 145:             {"type": "inline", "language": "python", "code": "..."},
 146:             {"type": "file_ref", "file": "src/auth.py", "lines": "42-67"},
 147:             {"type": "description", "text": "Uses async/await pattern"}
 148:         ]
 149:     """
 150:     if not examples_text:
 151:         return []
 152: 
 153:     examples = []
 154: 
 155:     # Pattern 1: Inline code blocks with language tag
 156:     # Matches: ```python\ncode\n```
 157:     code_block_pattern = r"```(\w+)\n(.*?)```"
 158:     for match in re.finditer(code_block_pattern, examples_text, re.DOTALL):
 159:         language = match.group(1)
 160:         code = match.group(2).strip()
 161:         examples.append({
 162:             "type": "inline",
 163:             "language": language,
 164:             "code": code
 165:         })
 166: 
 167:     # Pattern 2: File references
 168:     # Matches: "See src/auth.py:42-67", "src/auth.py lines 42-67", etc.
 169:     file_ref_pattern = r"(?:See\s+)?([a-zA-Z0-9_/.-]+\.py)(?::|\s+lines?\s+)(\d+-\d+)"
 170:     for match in re.finditer(file_ref_pattern, examples_text):
 171:         file_path = match.group(1)
 172:         line_range = match.group(2)
 173:         examples.append({
 174:             "type": "file_ref",
 175:             "file": file_path,
 176:             "lines": line_range
 177:         })
 178: 
 179:     # Pattern 3: Natural language descriptions (paragraphs without code/file refs)
 180:     # Extract paragraphs not containing code blocks or file references
 181:     # Remove code blocks and file references from text
 182:     text_without_code = re.sub(code_block_pattern, "", examples_text, flags=re.DOTALL)
 183:     text_without_refs = re.sub(file_ref_pattern, "", text_without_code)
 184: 
 185:     # Split into paragraphs and filter non-empty
 186:     paragraphs = [p.strip() for p in text_without_refs.split("\n\n") if p.strip()]
 187:     for paragraph in paragraphs:
 188:         # Skip very short paragraphs (likely headers or fragments)
 189:         if len(paragraph) > 20:
 190:             examples.append({
 191:                 "type": "description",
 192:                 "text": paragraph
 193:             })
 194: 
 195:     return examples
 196: 
 197: 
 198: def extract_documentation_links(docs_text: str) -> List[Dict[str, str]]:
 199:     """Extract documentation URLs from DOCUMENTATION section.
 200: 
 201:     Patterns supported:
 202:         - Markdown links: [Title](url)
 203:         - Plain URLs: https://...
 204:         - Library names: "FastAPI", "pytest"
 205: 
 206:     Returns:
 207:         [
 208:             {"title": "FastAPI Docs", "url": "https://...", "type": "link"},
 209:             {"title": "pytest", "url": "", "type": "library"}
 210:         ]
 211:     """
 212:     if not docs_text:
 213:         return []
 214: 
 215:     doc_links = []
 216: 
 217:     # Pattern 1: Markdown links [Title](url)
 218:     markdown_link_pattern = r"\[([^\]]+)\]\(([^\)]+)\)"
 219:     for match in re.finditer(markdown_link_pattern, docs_text):
 220:         title = match.group(1).strip()
 221:         url = match.group(2).strip()
 222:         doc_links.append({
 223:             "title": title,
 224:             "url": url,
 225:             "type": "link"
 226:         })
 227: 
 228:     # Pattern 2: Plain URLs (https://... or http://...)
 229:     plain_url_pattern = r"(https?://[^\s\)]+)"
 230:     # Only extract URLs not already captured by markdown links
 231:     text_without_markdown = re.sub(markdown_link_pattern, "", docs_text)
 232:     for match in re.finditer(plain_url_pattern, text_without_markdown):
 233:         url = match.group(1).strip()
 234:         # Use domain as title
 235:         domain = url.split("/")[2]
 236:         doc_links.append({
 237:             "title": domain,
 238:             "url": url,
 239:             "type": "link"
 240:         })
 241: 
 242:     # Pattern 3: Library names (words in quotes or standalone)
 243:     # Matches: "FastAPI", "pytest", FastAPI, pytest
 244:     # This is heuristic - captures quoted words or capitalized words likely to be library names
 245:     library_pattern = r"[\"']([A-Za-z0-9_-]+)[\"']|(?:^|\s)([A-Z][a-zA-Z0-9_-]+)(?:\s|$)"
 246:     text_without_urls = re.sub(plain_url_pattern, "", text_without_markdown)
 247:     for match in re.finditer(library_pattern, text_without_urls):
 248:         library_name = match.group(1) or match.group(2)
 249:         if library_name:
 250:             # Only add if not already in doc_links
 251:             if not any(lib["title"] == library_name for lib in doc_links):
 252:                 doc_links.append({
 253:                     "title": library_name,
 254:                     "url": "",
 255:                     "type": "library"
 256:                 })
 257: 
 258:     return doc_links
 259: 
 260: 
 261: # =============================================================================
 262: # Phase 2: Serena Research Orchestration
 263: # =============================================================================
 264: 
 265: 
 266: def research_codebase(
 267:     feature_name: str,
 268:     examples: List[Dict[str, Any]],
 269:     initial_context: str
 270: ) -> Dict[str, Any]:
 271:     """Orchestrate codebase research using Serena MCP.
 272: 
 273:     Args:
 274:         feature_name: Target feature name (e.g., "User Authentication")
 275:         examples: Parsed EXAMPLES from INITIAL.md
 276:         initial_context: FEATURE section text for context
 277: 
 278:     Returns:
 279:         {
 280:             "related_files": ["src/auth.py", "src/models/user.py"],
 281:             "patterns": [
 282:                 {"pattern": "async/await", "locations": ["src/auth.py:42"]},
 283:                 {"pattern": "JWT validation", "locations": ["src/auth.py:67"]}
 284:             ],
 285:             "similar_implementations": [
 286:                 {
 287:                     "file": "src/oauth.py",
 288:                     "symbol": "OAuthHandler/authenticate",
 289:                     "code": "...",
 290:                     "relevance": "Similar authentication flow"
 291:                 }
 292:             ],
 293:             "test_patterns": [
 294:                 {"file": "tests/test_auth.py", "pattern": "pytest fixtures"}
 295:             ],
 296:             "architecture": {
 297:                 "layer": "authentication",
 298:                 "dependencies": ["jwt", "bcrypt"],
 299:                 "conventions": ["snake_case", "async functions"]
 300:             }
 301:         }
 302: 
 303:     Raises:
 304:         RuntimeError: If Serena MCP unavailable (non-blocking - log warning, return empty results)
 305: 
 306:     Process:
 307:         1. Extract keywords from feature_name (e.g., "authentication", "JWT")
 308:         2. Search for patterns: mcp__serena__search_for_pattern(keywords)
 309:         3. Discover symbols: mcp__serena__find_symbol(related_classes)
 310:         4. Get detailed code: mcp__serena__find_symbol(include_body=True)
 311:         5. Find references: mcp__serena__find_referencing_symbols(key_functions)
 312:         6. Infer architecture: Analyze file structure and imports
 313:         7. Detect test patterns: Look for pytest/unittest in tests/
 314:     """
 315:     logger.info(f"Starting codebase research for: {feature_name}")
 316: 
 317:     # Initialize result structure
 318:     result = {
 319:         "related_files": [],
 320:         "patterns": [],
 321:         "similar_implementations": [],
 322:         "test_patterns": [],
 323:         "architecture": {
 324:             "layer": "",
 325:             "dependencies": [],
 326:             "conventions": []
 327:         },
 328:         "serena_available": False
 329:     }
 330: 
 331:     try:
 332:         # Check if Serena MCP is available by attempting import
 333:         # In production, this would be: from mcp import serena
 334:         # For now, we'll gracefully handle unavailability
 335:         logger.info("Serena MCP research would execute here (graceful degradation)")
 336: 
 337:         # Extract keywords from feature name
 338:         keywords = _extract_keywords(feature_name)
 339:         logger.info(f"Extracted keywords: {keywords}")
 340: 
 341:         # Search for similar patterns
 342:         patterns = search_similar_patterns(keywords)
 343:         result["patterns"] = patterns
 344: 
 345:         # Infer test patterns
 346:         test_patterns = infer_test_patterns({})
 347:         result["test_patterns"] = test_patterns
 348: 
 349:         result["serena_available"] = False  # Will be True when MCP integrated
 350: 
 351:     except Exception as e:
 352:         logger.warning(f"Serena MCP unavailable or error during research: {e}")
 353:         logger.warning("Continuing with reduced research functionality")
 354: 
 355:     return result
 356: 
 357: 
 358: def search_similar_patterns(keywords: List[str], path: str = ".") -> List[Dict[str, Any]]:
 359:     """Search for similar code patterns using keywords.
 360: 
 361:     Uses: mcp__serena__search_for_pattern
 362: 
 363:     Args:
 364:         keywords: Search terms (e.g., ["authenticate", "JWT", "token"])
 365:         path: Search scope (default: entire project)
 366: 
 367:     Returns:
 368:         [
 369:             {"file": "src/auth.py", "line": 42, "snippet": "..."},
 370:             {"file": "src/oauth.py", "line": 67, "snippet": "..."}
 371:         ]
 372:     """
 373:     logger.info(f"Searching for patterns with keywords: {keywords}")
 374: 
 375:     # Graceful degradation when Serena unavailable
 376:     patterns = []
 377: 
 378:     try:
 379:         # This would use: mcp__serena__search_for_pattern(pattern="|".join(keywords))
 380:         # For now, return empty (will be populated when MCP integrated)
 381:         logger.info("Pattern search would execute via Serena MCP")
 382:     except Exception as e:
 383:         logger.warning(f"Pattern search unavailable: {e}")
 384: 
 385:     return patterns
 386: 
 387: 
 388: def analyze_symbol_structure(symbol_name: str, file_path: str) -> Dict[str, Any]:
 389:     """Get detailed symbol information.
 390: 
 391:     Uses: mcp__serena__find_symbol, mcp__serena__get_symbols_overview
 392: 
 393:     Args:
 394:         symbol_name: Class/function name
 395:         file_path: File containing symbol
 396: 
 397:     Returns:
 398:         {
 399:             "name": "AuthHandler",
 400:             "type": "class",
 401:             "methods": ["authenticate", "validate_token", "refresh"],
 402:             "code": "<full class body>",
 403:             "references": 5
 404:         }
 405:     """
 406:     logger.info(f"Analyzing symbol: {symbol_name} in {file_path}")
 407: 
 408:     # Graceful degradation
 409:     result = {
 410:         "name": symbol_name,
 411:         "type": "unknown",
 412:         "methods": [],
 413:         "code": "",
 414:         "references": 0
 415:     }
 416: 
 417:     try:
 418:         # Would use: mcp__serena__find_symbol(name_path=symbol_name, relative_path=file_path)
 419:         logger.info("Symbol analysis would execute via Serena MCP")
 420:     except Exception as e:
 421:         logger.warning(f"Symbol analysis unavailable: {e}")
 422: 
 423:     return result
 424: 
 425: 
 426: def infer_test_patterns(project_structure: Dict[str, Any]) -> List[Dict[str, str]]:
 427:     """Detect test framework and patterns.
 428: 
 429:     Process:
 430:         1. Look for pytest.ini, setup.cfg, pyproject.toml
 431:         2. Search for test files (test_*.py, *_test.py)
 432:         3. Analyze test imports (pytest, unittest, nose)
 433:         4. Extract test command from pyproject.toml or tox.ini
 434: 
 435:     Returns:
 436:         [
 437:             {
 438:                 "framework": "pytest",
 439:                 "test_command": "pytest tests/ -v",
 440:                 "patterns": ["fixtures", "parametrize", "async tests"],
 441:                 "coverage_required": True
 442:             }
 443:         ]
 444:     """
 445:     logger.info("Inferring test patterns from project structure")
 446: 
 447:     # Check for pytest.ini, pyproject.toml
 448:     test_patterns = []
 449: 
 450:     # Default pytest pattern (most Python projects)
 451:     default_pattern = {
 452:         "framework": "pytest",
 453:         "test_command": "uv run pytest tests/ -v",
 454:         "patterns": ["fixtures", "parametrize"],
 455:         "coverage_required": True
 456:     }
 457:     test_patterns.append(default_pattern)
 458: 
 459:     return test_patterns
 460: 
 461: 
 462: def _extract_keywords(text: str) -> List[str]:
 463:     """Extract keywords from feature name or description.
 464: 
 465:     Args:
 466:         text: Feature name or description
 467: 
 468:     Returns:
 469:         List of keywords (lowercase, deduplicated)
 470:     """
 471:     # Simple keyword extraction - split by spaces, lowercase, remove common words
 472:     stop_words = {"a", "an", "the", "and", "or", "but", "with", "for", "to", "of", "in", "on"}
 473:     words = re.findall(r'\b\w+\b', text.lower())
 474:     keywords = [w for w in words if w not in stop_words and len(w) > 2]
 475:     return list(set(keywords))  # Deduplicate
 476: 
 477: 
 478: # =============================================================================
 479: # Phase 3: Context7 Integration
 480: # =============================================================================
 481: 
 482: 
 483: def fetch_documentation(
 484:     documentation_links: List[Dict[str, str]],
 485:     feature_context: str,
 486:     serena_research: Dict[str, Any]
 487: ) -> Dict[str, Any]:
 488:     """Fetch documentation using Context7 MCP.
 489: 
 490:     Args:
 491:         documentation_links: Parsed from INITIAL.md DOCUMENTATION section
 492:             [{"title": "FastAPI", "url": "", "type": "library"}, ...]
 493:         feature_context: FEATURE section text for topic extraction
 494:         serena_research: Results from research_codebase() for additional context
 495: 
 496:     Returns:
 497:         {
 498:             "library_docs": [
 499:                 {
 500:                     "library_name": "FastAPI",
 501:                     "library_id": "/tiangolo/fastapi",
 502:                     "topics": ["routing", "security", "dependencies"],
 503:                     "content": "<fetched markdown docs>",
 504:                     "tokens_used": 5000
 505:                 }
 506:             ],
 507:             "external_links": [
 508:                 {
 509:                     "title": "JWT Best Practices",
 510:                     "url": "https://jwt.io/introduction",
 511:                     "content": "<fetched content via WebFetch>",
 512:                     "relevant_sections": ["token structure", "security"]
 513:                 }
 514:             ],
 515:             "context7_available": False,
 516:             "sequential_thinking_available": False
 517:         }
 518: 
 519:     Raises:
 520:         RuntimeError: If Context7 MCP unavailable (non-blocking - log warning, return empty)
 521: 
 522:     Process:
 523:         1. Extract topics from feature_context using Sequential Thinking MCP
 524:         2. Resolve library names to Context7 IDs: mcp__context7__resolve-library-id
 525:         3. Fetch docs: mcp__context7__get-library-docs(library_id, topics)
 526:         4. Fetch external links: WebFetch tool for URLs
 527:         5. Synthesize relevance scores
 528:     """
 529:     logger.info("Starting documentation fetch with Context7 and Sequential Thinking")
 530: 
 531:     # Initialize result structure
 532:     result = {
 533:         "library_docs": [],
 534:         "external_links": [],
 535:         "context7_available": False,
 536:         "sequential_thinking_available": False
 537:     }
 538: 
 539:     try:
 540:         # Extract topics from feature context using Sequential Thinking
 541:         topics = extract_topics_from_feature(feature_context, serena_research)
 542:         logger.info(f"Extracted topics: {topics}")
 543: 
 544:         # Resolve library IDs and fetch docs
 545:         libraries = [doc for doc in documentation_links if doc["type"] == "library"]
 546:         for lib in libraries:
 547:             lib_result = resolve_and_fetch_library_docs(
 548:                 lib["title"],
 549:                 topics,
 550:                 feature_context
 551:             )
 552:             if lib_result:
 553:                 result["library_docs"].append(lib_result)
 554: 
 555:         # Fetch external link content
 556:         external_links = [doc for doc in documentation_links if doc["type"] == "link"]
 557:         for link in external_links:
 558:             link_result = fetch_external_link(link["url"], link["title"], topics)
 559:             if link_result:
 560:                 result["external_links"].append(link_result)
 561: 
 562:         result["context7_available"] = False  # Will be True when MCP integrated
 563:         result["sequential_thinking_available"] = False
 564: 
 565:     except Exception as e:
 566:         logger.warning(f"Context7/Sequential Thinking MCP unavailable: {e}")
 567:         logger.warning("Continuing with reduced documentation functionality")
 568: 
 569:     return result
 570: 
 571: 
 572: def extract_topics_from_feature(
 573:     feature_text: str,
 574:     serena_research: Dict[str, Any]
 575: ) -> List[str]:
 576:     """Extract documentation topics using Sequential Thinking MCP.
 577: 
 578:     Uses: mcp__syntropy__thinking__sequentialthinking
 579: 
 580:     Args:
 581:         feature_text: FEATURE section from INITIAL.md
 582:         serena_research: Codebase research results for additional context
 583: 
 584:     Returns:
 585:         List of topics (e.g., ["routing", "security", "async", "testing"])
 586: 
 587:     Process:
 588:         1. Call Sequential Thinking MCP with feature analysis prompt
 589:         2. Extract topics from reasoning chain
 590:         3. Deduplicate and filter to 3-5 most relevant topics
 591:         4. Fall back to heuristic if MCP unavailable
 592:     """
 593:     logger.info("Extracting topics from feature text using Sequential Thinking")
 594: 
 595:     # Check if sequential thinking is enabled
 596:     import os
 597:     use_thinking = os.environ.get('CE_USE_SEQUENTIAL_THINKING', 'true').lower() == 'true'
 598: 
 599:     if not use_thinking:
 600:         logger.info("Sequential thinking disabled via --no-thinking flag")
 601:         return _extract_topics_heuristic(feature_text)
 602: 
 603:     try:
 604:         from .mcp_utils import call_syntropy_mcp
 605: 
 606:         prompt = f"""Analyze this feature description and identify 3-5 key technical topics that would need documentation:
 607: 
 608: Feature: {feature_text}
 609: 
 610: Codebase Context:
 611: - Related patterns: {len(serena_research.get('patterns', []))}
 612: - Test framework: {serena_research.get('test_patterns', [{}])[0].get('framework', 'unknown') if serena_research.get('test_patterns') else 'unknown'}
 613: 
 614: Think step-by-step about:
 615: 1. What technical areas does this feature touch? (e.g., authentication, async, database)
 616: 2. What documentation would help implement this? (e.g., library guides, API docs)
 617: 3. What are the 3-5 most critical topics to focus documentation on?
 618: 
 619: Return final answer as: TOPICS: topic1, topic2, topic3"""
 620: 
 621:         result = call_syntropy_mcp(
 622:             "thinking",
 623:             "sequentialthinking",
 624:             {
 625:                 "thought": prompt,
 626:                 "thoughtNumber": 1,
 627:                 "totalThoughts": 5,
 628:                 "nextThoughtNeeded": True
 629:             }
 630:         )
 631: 
 632:         # Log reasoning chain
 633:         _log_thinking_chain(result, "Topic Extraction")
 634: 
 635:         # Extract topics from result
 636:         topics = _extract_topics_from_thinking_result(result)
 637: 
 638:         if topics:
 639:             logger.info(f"Extracted topics (sequential thinking): {topics}")
 640:             return topics
 641: 
 642:     except Exception as e:
 643:         logger.warning(f"Sequential thinking unavailable: {e}")
 644:         logger.warning("Falling back to heuristic topic extraction")
 645: 
 646:     # Graceful degradation - heuristic approach
 647:     return _extract_topics_heuristic(feature_text)
 648: 
 649: 
 650: def _extract_topics_from_thinking_result(result: Dict[str, Any]) -> List[str]:
 651:     """Parse topics from sequential thinking result.
 652: 
 653:     Args:
 654:         result: MCP tool result
 655: 
 656:     Returns:
 657:         List of topics extracted from thinking chain
 658:     """
 659:     # Extract content from MCP result
 660:     content = ""
 661:     if isinstance(result, dict) and "content" in result:
 662:         if isinstance(result["content"], list):
 663:             for item in result["content"]:
 664:                 if isinstance(item, dict) and "text" in item:
 665:                     content += item["text"] + " "
 666:         elif isinstance(result["content"], str):
 667:             content = result["content"]
 668: 
 669:     # Look for TOPICS: pattern in result
 670:     topics_match = re.search(r"TOPICS:\s*(.+)", content, re.IGNORECASE)
 671:     if topics_match:
 672:         topics_str = topics_match.group(1)
 673:         # Split by comma and clean
 674:         topics = [t.strip() for t in topics_str.split(",")]
 675:         return topics[:5]  # Limit to 5
 676: 
 677:     return []
 678: 
 679: 
 680: def _log_thinking_chain(result: Dict[str, Any], context: str) -> None:
 681:     """Log sequential thinking reasoning chain.
 682: 
 683:     Args:
 684:         result: MCP result with thinking chain
 685:         context: Context label (e.g., "Topic Extraction")
 686:     """
 687:     logger.info(f"ðŸ§  Sequential Thinking Chain - {context}")
 688: 
 689:     # Extract content
 690:     content = ""
 691:     if isinstance(result, dict) and "content" in result:
 692:         if isinstance(result["content"], list):
 693:             for item in result["content"]:
 694:                 if isinstance(item, dict) and "text" in item:
 695:                     content += item["text"] + "\n"
 696: 
 697:     # Log each thought
 698:     thoughts = re.finditer(r"Thought (\d+):\s*(.+?)(?=Thought \d+:|\Z)", content, re.DOTALL)
 699:     for thought in thoughts:
 700:         thought_num = thought.group(1)
 701:         thought_text = thought.group(2).strip()[:200]  # First 200 chars
 702:         logger.info(f"  Thought {thought_num}: {thought_text}...")
 703: 
 704:     logger.info(f"ðŸ§  End of thinking chain - {context}")
 705: 
 706: 
 707: def _extract_topics_heuristic(feature_text: str) -> List[str]:
 708:     """Heuristic-based topic extraction (fallback).
 709: 
 710:     Args:
 711:         feature_text: Feature description text
 712: 
 713:     Returns:
 714:         List of topics based on keyword matching
 715:     """
 716:     technical_terms = []
 717: 
 718:     # Common technical patterns to look for
 719:     patterns = {
 720:         "authentication": ["auth", "login", "jwt", "oauth", "token"],
 721:         "database": ["database", "sql", "nosql", "query", "model"],
 722:         "api": ["api", "rest", "graphql", "endpoint", "route"],
 723:         "async": ["async", "await", "concurrent", "parallel"],
 724:         "testing": ["test", "pytest", "unittest", "mock"],
 725:         "security": ["security", "encrypt", "hash", "bcrypt", "secure"],
 726:         "validation": ["validate", "validation", "schema", "verify"],
 727:     }
 728: 
 729:     feature_lower = feature_text.lower()
 730:     for topic, keywords in patterns.items():
 731:         if any(kw in feature_lower for kw in keywords):
 732:             technical_terms.append(topic)
 733: 
 734:     # Limit to 3-5 topics
 735:     topics = technical_terms[:5] if technical_terms else ["general"]
 736: 
 737:     logger.info(f"Extracted topics (heuristic): {topics}")
 738:     return topics
 739: 
 740: 
 741: def resolve_and_fetch_library_docs(
 742:     library_name: str,
 743:     topics: List[str],
 744:     feature_context: str,
 745:     max_tokens: int = 5000
 746: ) -> Dict[str, Any]:
 747:     """Resolve library ID and fetch documentation.
 748: 
 749:     Uses: mcp__context7__resolve-library-id, mcp__context7__get-library-docs
 750: 
 751:     Args:
 752:         library_name: Library to fetch (e.g., "FastAPI", "pytest")
 753:         topics: Topics to focus documentation (e.g., ["routing", "security"])
 754:         feature_context: Feature description for relevance filtering
 755:         max_tokens: Maximum tokens to retrieve
 756: 
 757:     Returns:
 758:         {
 759:             "library_name": "FastAPI",
 760:             "library_id": "/tiangolo/fastapi",
 761:             "topics": ["routing", "security"],
 762:             "content": "<markdown docs>",
 763:             "tokens_used": 4500
 764:         }
 765:         None if library not found or fetch fails
 766: 
 767:     Process:
 768:         1. resolve-library-id(library_name) â†’ library_id
 769:         2. get-library-docs(library_id, topics, max_tokens)
 770:         3. Return structured result
 771:     """
 772:     logger.info(f"Resolving and fetching docs for library: {library_name}")
 773: 
 774:     # Graceful degradation
 775:     try:
 776:         # Would use: mcp__context7__resolve-library-id(libraryName=library_name)
 777:         # Would use: mcp__context7__get-library-docs(context7CompatibleLibraryID=library_id, topic=topics, tokens=max_tokens)
 778:         logger.info(f"Context7 fetch would execute for {library_name}")
 779:         return None  # Return None when MCP unavailable
 780:     except Exception as e:
 781:         logger.warning(f"Failed to fetch docs for {library_name}: {e}")
 782:         return None
 783: 
 784: 
 785: def fetch_external_link(
 786:     url: str,
 787:     title: str,
 788:     topics: List[str]
 789: ) -> Dict[str, Any]:
 790:     """Fetch external documentation link using WebFetch.
 791: 
 792:     Uses: WebFetch tool
 793: 
 794:     Args:
 795:         url: URL to fetch
 796:         title: Link title from INITIAL.md
 797:         topics: Topics for relevance filtering
 798: 
 799:     Returns:
 800:         {
 801:             "title": "JWT Best Practices",
 802:             "url": "https://jwt.io/introduction",
 803:             "content": "<fetched markdown>",
 804:             "relevant_sections": ["token structure", "security"]
 805:         }
 806:         None if fetch fails
 807: 
 808:     Process:
 809:         1. WebFetch(url, prompt=f"Extract content relevant to: {topics}")
 810:         2. Parse and structure response
 811:         3. Identify relevant sections
 812:     """
 813:     logger.info(f"Fetching external link: {url}")
 814: 
 815:     # Graceful degradation
 816:     try:
 817:         # Would use: WebFetch(url=url, prompt=f"Extract documentation relevant to: {', '.join(topics)}")
 818:         logger.info(f"WebFetch would execute for {url}")
 819:         return None  # Return None for now
 820:     except Exception as e:
 821:         logger.warning(f"Failed to fetch {url}: {e}")
 822:         return None
 823: 
 824: 
 825: # =============================================================================
 826: # Phase 4: Template Engine
 827: # =============================================================================
 828: 
 829: 
 830: def generate_prp(
 831:     initial_md_path: str,
 832:     output_dir: str = "PRPs/feature-requests",
 833:     join_prp: Optional[str] = None
 834: ) -> str:
 835:     """Generate complete PRP from INITIAL.md.
 836: 
 837:     Main orchestration function that coordinates all phases.
 838: 
 839:     Args:
 840:         initial_md_path: Path to INITIAL.md file
 841:         output_dir: Directory for output PRP file
 842:         join_prp: Optional PRP to join (number, ID like 'PRP-12', or file path)
 843:                   If provided, updates existing PRP's Linear issue instead of creating new
 844: 
 845:     Returns:
 846:         Path to generated PRP file
 847: 
 848:     Raises:
 849:         FileNotFoundError: If INITIAL.md doesn't exist
 850:         ValueError: If INITIAL.md invalid or join_prp invalid
 851:         RuntimeError: If PRP generation or Linear integration fails
 852: 
 853:     Process:
 854:         1. Parse INITIAL.md â†’ structured data
 855:         2. Research codebase â†’ Serena findings
 856:         3. Fetch documentation â†’ Context7 + WebFetch
 857:         4. Synthesize sections (TLDR, Implementation, Validation Gates, etc.)
 858:         5. Get next PRP ID
 859:         6. Write PRP file with YAML header
 860:         7. Create/update Linear issue with defaults
 861:         8. Update PRP YAML with issue ID
 862:         9. Check completeness
 863:     """
 864:     logger.info(f"Starting PRP generation from: {initial_md_path}")
 865: 
 866:     # Step 2.5: Pre-generation sync (if auto-sync enabled)
 867:     from .context import is_auto_sync_enabled, pre_generation_sync
 868:     if is_auto_sync_enabled():
 869:         try:
 870:             logger.info("Auto-sync enabled - running pre-generation sync...")
 871:             sync_result = pre_generation_sync(force=False)
 872:             logger.info(f"Pre-sync complete: drift={sync_result['drift_score']:.1f}%")
 873:         except Exception as e:
 874:             logger.error(f"Pre-generation sync failed: {e}")
 875:             raise RuntimeError(
 876:                 f"Generation aborted due to sync failure\n"
 877:                 f"Error: {e}\n"
 878:                 f"ðŸ”§ Troubleshooting: Run 'ce context health' to diagnose issues"
 879:             ) from e
 880: 
 881:     # Phase 1: Parse INITIAL.md
 882:     parsed_data = parse_initial_md(initial_md_path)
 883:     logger.info(f"Parsed feature: {parsed_data['feature_name']}")
 884: 
 885:     # Phase 2: Research codebase
 886:     serena_research = research_codebase(
 887:         parsed_data["feature_name"],
 888:         parsed_data["examples"],
 889:         parsed_data["feature"]
 890:     )
 891:     logger.info(f"Codebase research complete: {len(serena_research['patterns'])} patterns found")
 892: 
 893:     # Phase 3: Fetch documentation
 894:     documentation = fetch_documentation(
 895:         parsed_data["documentation"],
 896:         parsed_data["feature"],
 897:         serena_research
 898:     )
 899:     logger.info(f"Documentation fetched: {len(documentation['library_docs'])} libraries")
 900: 
 901:     # Phase 4: Synthesize PRP sections
 902:     prp_content = synthesize_prp_content(parsed_data, serena_research, documentation)
 903: 
 904:     # Get next PRP ID
 905:     prp_id = get_next_prp_id(output_dir)
 906:     logger.info(f"Assigned PRP ID: {prp_id}")
 907: 
 908:     # Write PRP file
 909:     output_path = Path(output_dir) / f"{prp_id}-{_slugify(parsed_data['feature_name'])}.md"
 910:     output_path.parent.mkdir(parents=True, exist_ok=True)
 911: 
 912:     with open(output_path, "w", encoding="utf-8") as f:
 913:         f.write(prp_content)
 914: 
 915:     logger.info(f"PRP generated: {output_path}")
 916: 
 917:     # Step 7: Create or update Linear issue
 918:     try:
 919:         from .linear_utils import create_issue_with_defaults
 920:         from .linear_mcp_resilience import (
 921:             create_issue_resilient,
 922:             update_issue_resilient,
 923:             get_linear_mcp_status
 924:         )
 925: 
 926:         issue_identifier = None
 927: 
 928:         if join_prp:
 929:             # Join existing PRP's issue
 930:             logger.info(f"Joining PRP: {join_prp}")
 931:             target_prp_path = _resolve_prp_path(join_prp)
 932:             target_issue_id = _extract_issue_from_prp(target_prp_path)
 933: 
 934:             if not target_issue_id:
 935:                 logger.warning(f"Target PRP has no Linear issue: {target_prp_path}")
 936:                 logger.warning("Creating new issue instead")
 937:             else:
 938:                 # Update existing issue with resilience + auth recovery
 939:                 logger.info(f"Updating Linear issue: {target_issue_id}")
 940:                 result = _update_linear_issue_with_resilience(
 941:                     target_issue_id,
 942:                     prp_id,
 943:                     parsed_data['feature_name'],
 944:                     str(output_path)
 945:                 )
 946: 
 947:                 if result["success"]:
 948:                     issue_identifier = target_issue_id
 949:                     logger.info(f"Updated issue {target_issue_id} with {prp_id}")
 950:                 else:
 951:                     logger.warning(f"Failed to update issue: {result['error']}")
 952:                     logger.info("Will create new issue instead")
 953: 
 954:         if not issue_identifier:
 955:             # Create new issue with resilience + auth recovery
 956:             logger.info("Creating new Linear issue with resilience")
 957:             result = create_issue_resilient(
 958:                 title=f"{prp_id}: {parsed_data['feature_name']}",
 959:                 description=_generate_issue_description(prp_id, parsed_data, str(output_path)),
 960:                 state="todo"
 961:             )
 962: 
 963:             if result["success"]:
 964:                 issue_data = result.get("result", {})
 965:                 # Extract identifier from result (when actual MCP integration added)
 966:                 issue_identifier = issue_data.get("id") or issue_data.get("identifier") or f"{prp_id}-created"
 967:                 logger.info(f"Created Linear issue: {issue_identifier}")
 968:             else:
 969:                 logger.error(f"Linear issue creation failed: {result['error']}")
 970:                 logger.warning("Circuit breaker state:", get_linear_mcp_status())
 971:                 logger.warning("Continuing without Linear integration")
 972: 
 973:         # Update PRP YAML with issue ID
 974:         if issue_identifier:
 975:             _update_prp_yaml_with_issue(str(output_path), issue_identifier)
 976:             logger.info(f"Updated PRP YAML with issue: {issue_identifier}")
 977: 
 978:     except ImportError as e:
 979:         logger.warning(f"Linear resilience utils not available: {e}")
 980:         logger.warning("Skipping issue creation")
 981:     except Exception as e:
 982:         logger.error(f"Linear issue creation failed: {e}")
 983:         logger.warning("Continuing without Linear integration")
 984: 
 985:     # Check completeness
 986:     completeness = check_prp_completeness(str(output_path))
 987:     if not completeness["complete"]:
 988:         logger.warning(f"PRP incomplete: {completeness['missing_sections']}")
 989:     else:
 990:         logger.info("PRP completeness check: PASSED")
 991: 
 992:     return str(output_path)
 993: 
 994: 
 995: def synthesize_prp_content(
 996:     parsed_data: Dict[str, Any],
 997:     serena_research: Dict[str, Any],
 998:     documentation: Dict[str, Any]
 999: ) -> str:
1000:     """Synthesize complete PRP content from research.
1001: 
1002:     Args:
1003:         parsed_data: Parsed INITIAL.md data
1004:         serena_research: Codebase research results
1005:         documentation: Fetched documentation
1006: 
1007:     Returns:
1008:         Complete PRP markdown content with YAML header
1009: 
1010:     Process:
1011:         1. Generate YAML header with metadata
1012:         2. Synthesize TLDR section
1013:         3. Synthesize Context section
1014:         4. Synthesize Implementation Steps
1015:         5. Synthesize Validation Gates
1016:         6. Add Research Findings appendix
1017:         7. Format final markdown
1018:     """
1019:     logger.info("Synthesizing PRP content")
1020: 
1021:     # Generate sections
1022:     yaml_header = _generate_yaml_header(parsed_data)
1023:     tldr = synthesize_tldr(parsed_data, serena_research)
1024:     context = synthesize_context(parsed_data, documentation)
1025:     implementation = synthesize_implementation(parsed_data, serena_research)
1026:     validation_gates = synthesize_validation_gates(parsed_data, serena_research)
1027:     testing = synthesize_testing_strategy(parsed_data, serena_research)
1028:     rollout = synthesize_rollout_plan(parsed_data)
1029: 
1030:     # Combine sections
1031:     prp_content = f"""---
1032: {yaml_header}
1033: ---
1034: 
1035: # {parsed_data['feature_name']}
1036: 
1037: ## 1. TL;DR
1038: 
1039: {tldr}
1040: 
1041: ## 2. Context
1042: 
1043: {context}
1044: 
1045: ## 3. Implementation Steps
1046: 
1047: {implementation}
1048: 
1049: ## 4. Validation Gates
1050: 
1051: {validation_gates}
1052: 
1053: ## 5. Testing Strategy
1054: 
1055: {testing}
1056: 
1057: ## 6. Rollout Plan
1058: 
1059: {rollout}
1060: 
1061: ---
1062: 
1063: ## Research Findings
1064: 
1065: ### Serena Codebase Analysis
1066: - **Patterns Found**: {len(serena_research['patterns'])}
1067: - **Test Patterns**: {len(serena_research['test_patterns'])}
1068: - **Serena Available**: {serena_research['serena_available']}
1069: 
1070: ### Documentation Sources
1071: - **Library Docs**: {len(documentation['library_docs'])}
1072: - **External Links**: {len(documentation['external_links'])}
1073: - **Context7 Available**: {documentation['context7_available']}
1074: """
1075: 
1076:     return prp_content
1077: 
1078: 
1079: def synthesize_tldr(
1080:     parsed_data: Dict[str, Any],
1081:     serena_research: Dict[str, Any]
1082: ) -> str:
1083:     """Generate TLDR section.
1084: 
1085:     Args:
1086:         parsed_data: INITIAL.md structured data
1087:         serena_research: Codebase research findings
1088: 
1089:     Returns:
1090:         TLDR markdown text (3-5 bullet points)
1091:     """
1092:     feature = parsed_data["feature"]
1093:     examples_count = len(parsed_data["examples"])
1094: 
1095:     tldr = f"""**Objective**: {parsed_data['feature_name']}
1096: 
1097: **What**: {feature[:200]}...
1098: 
1099: **Why**: Enable functionality described in INITIAL.md with {examples_count} reference examples
1100: 
1101: **Effort**: Medium (3-5 hours estimated based on complexity)
1102: 
1103: **Dependencies**: {', '.join([doc['title'] for doc in parsed_data['documentation'][:3]])}
1104: """
1105:     return tldr
1106: 
1107: 
1108: def synthesize_context(
1109:     parsed_data: Dict[str, Any],
1110:     documentation: Dict[str, Any]
1111: ) -> str:
1112:     """Generate Context section.
1113: 
1114:     Args:
1115:         parsed_data: INITIAL.md data
1116:         documentation: Fetched documentation
1117: 
1118:     Returns:
1119:         Context markdown with background and constraints
1120:     """
1121:     feature = parsed_data["feature"]
1122:     other = parsed_data.get("other_considerations", "")
1123: 
1124:     context = f"""### Background
1125: 
1126: {feature}
1127: 
1128: ### Constraints and Considerations
1129: 
1130: {other if other else "See INITIAL.md for additional considerations"}
1131: 
1132: ### Documentation References
1133: 
1134: """
1135:     # Add documentation links
1136:     for doc in parsed_data["documentation"]:
1137:         if doc["type"] == "link":
1138:             context += f"- [{doc['title']}]({doc['url']})\n"
1139:         elif doc["type"] == "library":
1140:             context += f"- {doc['title']} (library documentation)\n"
1141: 
1142:     return context
1143: 
1144: 
1145: def _extract_planning_context(parsed_data: Dict[str, Any]) -> Dict[str, Any]:
1146:     """Extract PLANNING CONTEXT from INITIAL.md.
1147: 
1148:     Args:
1149:         parsed_data: Parsed INITIAL.md data
1150: 
1151:     Returns:
1152:         {
1153:             "complexity": "medium",
1154:             "architectural_impact": "moderate",
1155:             "risk_factors": ["..."],
1156:             "success_metrics": ["..."]
1157:         }
1158:     """
1159:     raw_content = parsed_data.get("raw_content", "")
1160: 
1161:     # Extract PLANNING CONTEXT section
1162:     planning_match = re.search(
1163:         r"##\s*PLANNING\s+CONTEXT\s*\n(.*?)(?=\n##|\Z)",
1164:         raw_content,
1165:         re.DOTALL | re.IGNORECASE
1166:     )
1167: 
1168:     if not planning_match:
1169:         return {
1170:             "complexity": "unknown",
1171:             "architectural_impact": "unknown",
1172:             "risk_factors": [],
1173:             "success_metrics": []
1174:         }
1175: 
1176:     planning_text = planning_match.group(1)
1177: 
1178:     # Extract complexity
1179:     complexity_match = re.search(
1180:         r"\*\*Complexity Assessment\*\*:\s*(\w+)",
1181:         planning_text,
1182:         re.IGNORECASE
1183:     )
1184:     complexity = complexity_match.group(1) if complexity_match else "unknown"
1185: 
1186:     # Extract architectural impact
1187:     arch_match = re.search(
1188:         r"\*\*Architectural Impact\*\*:\s*(\w+)",
1189:         planning_text,
1190:         re.IGNORECASE
1191:     )
1192:     arch_impact = arch_match.group(1) if arch_match else "unknown"
1193: 
1194:     # Extract risk factors (lines starting with - after "Risk Factors")
1195:     risk_section = re.search(
1196:         r"\*\*Risk Factors\*\*:\s*\n((?:- .+\n?)+)",
1197:         planning_text,
1198:         re.MULTILINE
1199:     )
1200:     risk_factors = []
1201:     if risk_section:
1202:         risk_lines = risk_section.group(1).strip().split("\n")
1203:         risk_factors = [line.lstrip("- ").strip() for line in risk_lines if line.strip()]
1204: 
1205:     return {
1206:         "complexity": complexity,
1207:         "architectural_impact": arch_impact,
1208:         "risk_factors": risk_factors,
1209:         "success_metrics": []  # TODO: Extract if needed
1210:     }
1211: 
1212: 
1213: def generate_implementation_phases_with_thinking(
1214:     parsed_data: Dict[str, Any],
1215:     serena_research: Dict[str, Any]
1216: ) -> str:
1217:     """Generate implementation phases using sequential thinking.
1218: 
1219:     Uses: mcp__syntropy__thinking__sequentialthinking
1220: 
1221:     Args:
1222:         parsed_data: INITIAL.md structured data
1223:         serena_research: Codebase research findings
1224: 
1225:     Returns:
1226:         Implementation phases markdown
1227: 
1228:     Process:
1229:         1. Extract planning context from INITIAL.md
1230:         2. Call sequential thinking with implementation planning prompt
1231:         3. Parse phases from reasoning chain
1232:         4. Fall back to template-based if unavailable
1233:     """
1234:     logger.info("Generating implementation phases with sequential thinking")
1235: 
1236:     # Check if sequential thinking is enabled
1237:     import os
1238:     use_thinking = os.environ.get('CE_USE_SEQUENTIAL_THINKING', 'true').lower() == 'true'
1239: 
1240:     if not use_thinking:
1241:         logger.info("Sequential thinking disabled via --no-thinking flag")
1242:         return ""  # Empty string triggers fallback in synthesize_implementation
1243: 
1244:     try:
1245:         from .mcp_utils import call_syntropy_mcp
1246: 
1247:         # Extract planning context
1248:         planning_context = _extract_planning_context(parsed_data)
1249: 
1250:         prompt = f"""Plan implementation phases for this feature:
1251: 
1252: Feature: {parsed_data['feature_name']}
1253: Description: {parsed_data['feature'][:300]}...
1254: 
1255: Planning Context:
1256: - Complexity: {planning_context.get('complexity', 'unknown')}
1257: - Architectural Impact: {planning_context.get('architectural_impact', 'unknown')}
1258: - Risk Factors: {', '.join(planning_context.get('risk_factors', ['unknown']))}
1259: 
1260: Codebase Context:
1261: - Similar patterns: {len(serena_research.get('patterns', []))}
1262: - Test framework: {serena_research.get('test_patterns', [{}])[0].get('framework', 'pytest') if serena_research.get('test_patterns') else 'pytest'}
1263: 
1264: Think step-by-step:
1265: 1. What are the logical implementation phases?
1266: 2. What dependencies exist between phases?
1267: 3. What time estimates are realistic?
1268: 4. What validation should happen at each phase?
1269: 
1270: Provide phases in format:
1271: PHASE 1: <name> (<time estimate>)
1272: - Step 1
1273: - Step 2
1274: 
1275: PHASE 2: ..."""
1276: 
1277:         result = call_syntropy_mcp(
1278:             "thinking",
1279:             "sequentialthinking",
1280:             {
1281:                 "thought": prompt,
1282:                 "thoughtNumber": 1,
1283:                 "totalThoughts": 8,
1284:                 "nextThoughtNeeded": True
1285:             }
1286:         )
1287: 
1288:         # Log reasoning chain
1289:         _log_thinking_chain(result, "Implementation Planning")
1290: 
1291:         # Extract phases from thinking result
1292:         phases = _extract_phases_from_thinking_result(result)
1293: 
1294:         if phases:
1295:             logger.info(f"Generated {len(phases.split('Phase'))-1} implementation phases")
1296:             return phases
1297: 
1298:     except Exception as e:
1299:         logger.warning(f"Sequential thinking unavailable: {e}")
1300:         logger.warning("Falling back to template-based phases")
1301: 
1302:     # Graceful degradation
1303:     return ""  # Empty string triggers fallback in synthesize_implementation
1304: 
1305: 
1306: def _extract_phases_from_thinking_result(result: Dict[str, Any]) -> str:
1307:     """Parse implementation phases from sequential thinking result.
1308: 
1309:     Args:
1310:         result: MCP tool result
1311: 
1312:     Returns:
1313:         Markdown formatted phases
1314:     """
1315:     # Extract content
1316:     content = ""
1317:     if isinstance(result, dict) and "content" in result:
1318:         if isinstance(result["content"], list):
1319:             for item in result["content"]:
1320:                 if isinstance(item, dict) and "text" in item:
1321:                     content += item["text"] + "\n"
1322:         elif isinstance(result["content"], str):
1323:             content = result["content"]
1324: 
1325:     # Extract phases (PHASE 1: ... format)
1326:     phases_text = ""
1327:     phase_matches = re.finditer(
1328:         r"PHASE (\d+):\s*([^\n]+)\n((?:- .+\n?)+)",
1329:         content,
1330:         re.MULTILINE
1331:     )
1332: 
1333:     for match in phase_matches:
1334:         phase_num = match.group(1)
1335:         phase_name = match.group(2).strip()
1336:         phase_steps = match.group(3).strip()
1337: 
1338:         phases_text += f"### Phase {phase_num}: {phase_name}\n\n"
1339:         phases_text += f"{phase_steps}\n\n"
1340: 
1341:     if phases_text:
1342:         return phases_text
1343: 
1344:     # If no phases found, return empty to trigger fallback
1345:     return ""
1346: 
1347: 
1348: def synthesize_implementation(
1349:     parsed_data: Dict[str, Any],
1350:     serena_research: Dict[str, Any]
1351: ) -> str:
1352:     """Generate Implementation Steps section.
1353: 
1354:     Tries sequential thinking first, falls back to template-based.
1355: 
1356:     Args:
1357:         parsed_data: INITIAL.md data
1358:         serena_research: Codebase patterns
1359: 
1360:     Returns:
1361:         Implementation steps markdown
1362:     """
1363:     # Try sequential thinking first
1364:     phases_with_thinking = generate_implementation_phases_with_thinking(
1365:         parsed_data,
1366:         serena_research
1367:     )
1368: 
1369:     if phases_with_thinking:
1370:         return phases_with_thinking
1371: 
1372:     # Fallback: Template-based phases (current implementation)
1373:     examples = parsed_data["examples"]
1374: 
1375:     steps = """### Phase 1: Setup and Research (30 min)
1376: 
1377: 1. Review INITIAL.md examples and requirements
1378: 2. Analyze existing codebase patterns
1379: 3. Identify integration points
1380: 
1381: ### Phase 2: Core Implementation (2-3 hours)
1382: 
1383: """
1384:     # Generate steps from examples
1385:     for i, example in enumerate(examples[:3], 1):
1386:         if example["type"] == "inline":
1387:             steps += f"{i}. Implement {example.get('language', 'code')} component\n"
1388:         elif example["type"] == "file_ref":
1389:             steps += f"{i}. Reference pattern in {example['file']}\n"
1390: 
1391:     steps += """
1392: ### Phase 3: Testing and Validation (1-2 hours)
1393: 
1394: 1. Write unit tests following project patterns
1395: 2. Write integration tests
1396: 3. Run validation gates
1397: 4. Update documentation
1398: """
1399: 
1400:     return steps
1401: 
1402: 
1403: def synthesize_validation_gates(
1404:     parsed_data: Dict[str, Any],
1405:     serena_research: Dict[str, Any]
1406: ) -> str:
1407:     """Generate Validation Gates section.
1408: 
1409:     Args:
1410:         parsed_data: INITIAL.md data with acceptance criteria
1411:         serena_research: Test patterns from codebase
1412: 
1413:     Returns:
1414:         Validation gates markdown
1415:     """
1416:     test_framework = "pytest"
1417:     if serena_research["test_patterns"]:
1418:         test_framework = serena_research["test_patterns"][0]["framework"]
1419: 
1420:     gates = f"""### Gate 1: Unit Tests Pass
1421: 
1422: **Command**: `uv run {test_framework} tests/unit/ -v`
1423: 
1424: **Success Criteria**:
1425: - All new unit tests pass
1426: - Existing tests not broken
1427: - Code coverage â‰¥ 80%
1428: 
1429: ### Gate 2: Integration Tests Pass
1430: 
1431: **Command**: `uv run {test_framework} tests/integration/ -v`
1432: 
1433: **Success Criteria**:
1434: - Integration tests verify end-to-end functionality
1435: - No regressions in existing features
1436: 
1437: ### Gate 3: Acceptance Criteria Met
1438: 
1439: **Verification**: Manual review against INITIAL.md requirements
1440: 
1441: **Success Criteria**:
1442: """
1443:     # Extract acceptance criteria from feature text
1444:     feature = parsed_data["feature"]
1445:     if "acceptance criteria" in feature.lower():
1446:         gates += "\n- Requirements from INITIAL.md validated\n"
1447:     else:
1448:         gates += "\n- All examples from INITIAL.md working\n"
1449:         gates += "- Feature behaves as described\n"
1450: 
1451:     return gates
1452: 
1453: 
1454: def synthesize_testing_strategy(
1455:     parsed_data: Dict[str, Any],
1456:     serena_research: Dict[str, Any]
1457: ) -> str:
1458:     """Generate Testing Strategy section."""
1459:     test_cmd = "uv run pytest tests/ -v"
1460:     if serena_research["test_patterns"]:
1461:         test_cmd = serena_research["test_patterns"][0]["test_command"]
1462: 
1463:     return f"""### Test Framework
1464: 
1465: {serena_research['test_patterns'][0]['framework'] if serena_research['test_patterns'] else 'pytest'}
1466: 
1467: ### Test Command
1468: 
1469: ```bash
1470: {test_cmd}
1471: ```
1472: 
1473: ### Coverage Requirements
1474: 
1475: - Unit test coverage: â‰¥ 80%
1476: - Integration tests for critical paths
1477: - Edge cases from INITIAL.md covered
1478: """
1479: 
1480: 
1481: def synthesize_rollout_plan(parsed_data: Dict[str, Any]) -> str:
1482:     """Generate Rollout Plan section."""
1483:     return """### Phase 1: Development
1484: 
1485: 1. Implement core functionality
1486: 2. Write tests
1487: 3. Pass validation gates
1488: 
1489: ### Phase 2: Review
1490: 
1491: 1. Self-review code changes
1492: 2. Peer review (optional)
1493: 3. Update documentation
1494: 
1495: ### Phase 3: Deployment
1496: 
1497: 1. Merge to main branch
1498: 2. Monitor for issues
1499: 3. Update stakeholders
1500: """
1501: 
1502: 
1503: def get_next_prp_id(prps_dir: str = "PRPs/feature-requests") -> str:
1504:     """Get next available PRP ID.
1505: 
1506:     Args:
1507:         prps_dir: Directory containing PRPs
1508: 
1509:     Returns:
1510:         Next PRP ID (e.g., "PRP-123")
1511: 
1512:     Process:
1513:         1. List all PRP-*.md files in directory
1514:         2. Extract numeric IDs
1515:         3. Return max + 1
1516:     """
1517:     prps_path = Path(prps_dir)
1518:     if not prps_path.exists():
1519:         return "PRP-1"
1520: 
1521:     # Find all PRP-*.md files
1522:     prp_files = list(prps_path.glob("PRP-*.md"))
1523:     if not prp_files:
1524:         return "PRP-1"
1525: 
1526:     # Extract numeric IDs
1527:     ids = []
1528:     for file in prp_files:
1529:         match = re.match(r"PRP-(\d+)", file.name)
1530:         if match:
1531:             ids.append(int(match.group(1)))
1532: 
1533:     # Return next ID
1534:     next_id = max(ids) + 1 if ids else 1
1535:     return f"PRP-{next_id}"
1536: 
1537: 
1538: def check_prp_completeness(prp_path: str) -> Dict[str, Any]:
1539:     """Check if PRP has all required sections.
1540: 
1541:     Args:
1542:         prp_path: Path to PRP file
1543: 
1544:     Returns:
1545:         {
1546:             "complete": True/False,
1547:             "missing_sections": [],
1548:             "warnings": []
1549:         }
1550: 
1551:     Required sections:
1552:         1. TL;DR
1553:         2. Context
1554:         3. Implementation Steps
1555:         4. Validation Gates
1556:         5. Testing Strategy
1557:         6. Rollout Plan
1558:     """
1559:     required_sections = [
1560:         "TL;DR",
1561:         "Context",
1562:         "Implementation Steps",
1563:         "Validation Gates",
1564:         "Testing Strategy",
1565:         "Rollout Plan"
1566:     ]
1567: 
1568:     content = Path(prp_path).read_text(encoding="utf-8")
1569: 
1570:     missing = []
1571:     for section in required_sections:
1572:         # Check for section header (## N. Section or ## Section)
1573:         pattern = rf"##\s+\d*\.?\s*{re.escape(section)}"
1574:         if not re.search(pattern, content, re.IGNORECASE):
1575:             missing.append(section)
1576: 
1577:     warnings = []
1578:     if len(content) < 1000:
1579:         warnings.append("PRP content seems short (< 1000 chars)")
1580: 
1581:     return {
1582:         "complete": len(missing) == 0,
1583:         "missing_sections": missing,
1584:         "warnings": warnings
1585:     }
1586: 
1587: 
1588: def _generate_yaml_header(parsed_data: Dict[str, Any]) -> str:
1589:     """Generate YAML frontmatter for PRP."""
1590:     from datetime import datetime
1591: 
1592:     now = datetime.now().isoformat()
1593: 
1594:     return f"""prp_id: TBD
1595: feature_name: {parsed_data['feature_name']}
1596: status: pending
1597: created: {now}
1598: updated: {now}
1599: complexity: medium
1600: estimated_hours: 3-5
1601: dependencies: {', '.join([doc['title'] for doc in parsed_data['documentation'][:3]])}"""
1602: 
1603: 
1604: def _slugify(text: str) -> str:
1605:     """Convert text to URL-friendly slug."""
1606:     # Lowercase and replace spaces with hyphens
1607:     slug = text.lower().replace(" ", "-")
1608:     # Remove special characters
1609:     slug = re.sub(r'[^a-z0-9-]', '', slug)
1610:     # Remove multiple hyphens
1611:     slug = re.sub(r'-+', '-', slug)
1612:     return slug.strip("-")
1613: 
1614: 
1615: # =============================================================================
1616: # Linear Integration Helpers
1617: # =============================================================================
1618: 
1619: 
1620: def _resolve_prp_path(join_prp: str) -> Path:
1621:     """Resolve join_prp reference to PRP file path.
1622: 
1623:     Args:
1624:         join_prp: PRP reference (number like "12", ID like "PRP-12", or file path)
1625: 
1626:     Returns:
1627:         Path to PRP file
1628: 
1629:     Raises:
1630:         ValueError: If join_prp invalid or PRP not found
1631:     """
1632:     # Check if it's already a valid file path
1633:     if "/" in join_prp or "\\" in join_prp:
1634:         prp_path = Path(join_prp)
1635:         if prp_path.exists():
1636:             return prp_path
1637:         raise ValueError(
1638:             f"PRP file not found: {join_prp}\n"
1639:             f"ðŸ”§ Troubleshooting: Verify file path is correct"
1640:         )
1641: 
1642:     # Parse as PRP number or ID
1643:     prp_number = None
1644:     if join_prp.startswith("PRP-"):
1645:         # Extract number from "PRP-12"
1646:         match = re.match(r"PRP-(\d+)", join_prp)
1647:         if match:
1648:             prp_number = int(match.group(1))
1649:     else:
1650:         # Try parsing as plain number "12"
1651:         try:
1652:             prp_number = int(join_prp)
1653:         except ValueError:
1654:             raise ValueError(
1655:                 f"Invalid PRP reference: {join_prp}\n"
1656:                 f"ðŸ”§ Troubleshooting: Use format '12', 'PRP-12', or file path"
1657:             )
1658: 
1659:     if not prp_number:
1660:         raise ValueError(
1661:             f"Could not parse PRP reference: {join_prp}\n"
1662:             f"ðŸ”§ Troubleshooting: Use format '12', 'PRP-12', or file path"
1663:         )
1664: 
1665:     # Search for PRP file in feature-requests/ and executed/
1666:     prp_id = f"PRP-{prp_number}"
1667:     search_dirs = ["PRPs/feature-requests", "PRPs/executed"]
1668: 
1669:     for search_dir in search_dirs:
1670:         search_path = Path(search_dir)
1671:         if search_path.exists():
1672:             # Find PRP-{number}-*.md
1673:             matches = list(search_path.glob(f"{prp_id}-*.md"))
1674:             if matches:
1675:                 return matches[0]
1676: 
1677:     raise ValueError(
1678:         f"PRP not found: {prp_id}\n"
1679:         f"ðŸ”§ Troubleshooting: Searched in {', '.join(search_dirs)}"
1680:     )
1681: 
1682: 
1683: def _extract_issue_from_prp(prp_path: Path) -> Optional[str]:
1684:     """Extract Linear issue ID from PRP YAML header.
1685: 
1686:     Args:
1687:         prp_path: Path to PRP file
1688: 
1689:     Returns:
1690:         Issue ID (e.g., "BLA-24") or None if not found
1691:     """
1692:     content = prp_path.read_text(encoding="utf-8")
1693: 
1694:     # Extract YAML frontmatter
1695:     yaml_match = re.match(r"---\n(.*?)\n---", content, re.DOTALL)
1696:     if not yaml_match:
1697:         return None
1698: 
1699:     yaml_content = yaml_match.group(1)
1700: 
1701:     # Extract issue field
1702:     issue_match = re.search(r"^issue:\s*(.+)$", yaml_content, re.MULTILINE)
1703:     if not issue_match:
1704:         return None
1705: 
1706:     issue_value = issue_match.group(1).strip()
1707: 
1708:     # Return None for null/empty values
1709:     if issue_value.lower() in ["null", "none", ""]:
1710:         return None
1711: 
1712:     return issue_value
1713: 
1714: 
1715: def _update_linear_issue_with_resilience(
1716:     issue_id: str,
1717:     prp_id: str,
1718:     feature_name: str,
1719:     prp_path: str
1720: ) -> Dict[str, Any]:
1721:     """Update existing Linear issue with new PRP info using resilience layer.
1722: 
1723:     Args:
1724:         issue_id: Linear issue identifier (e.g., "BLA-24")
1725:         prp_id: New PRP ID (e.g., "PRP-15")
1726:         feature_name: New PRP feature name
1727:         prp_path: Path to new PRP file
1728: 
1729:     Returns:
1730:         Result dict from update_issue_resilient with success/error status
1731: 
1732:     Process:
1733:         1. Generate update text for new PRP
1734:         2. Call update_issue_resilient with auth recovery
1735:         3. Return detailed result with success status
1736:     """
1737:     from .linear_mcp_resilience import update_issue_resilient
1738: 
1739:     logger.info(f"Updating Linear issue {issue_id} with {prp_id}")
1740: 
1741:     update_text = f"""
1742: 
1743: ---
1744: 
1745: ## Related: {prp_id} - {feature_name}
1746: 
1747: **PRP File**: `{prp_path}`
1748: 
1749: This PRP is related to the same feature/initiative.
1750: """
1751: 
1752:     # Call with resilience + auth recovery
1753:     result = update_issue_resilient(issue_id, update_text)
1754: 
1755:     if result["success"]:
1756:         logger.info(f"Successfully updated issue {issue_id}")
1757:     else:
1758:         logger.warning(f"Failed to update issue: {result['error']}")
1759: 
1760:     return result
1761: 
1762: 
1763: def _update_linear_issue(
1764:     issue_id: str,
1765:     prp_id: str,
1766:     feature_name: str,
1767:     prp_path: str
1768: ) -> None:
1769:     """Update existing Linear issue with new PRP info.
1770: 
1771:     DEPRECATED: Use _update_linear_issue_with_resilience instead.
1772: 
1773:     Args:
1774:         issue_id: Linear issue identifier (e.g., "BLA-24")
1775:         prp_id: New PRP ID (e.g., "PRP-15")
1776:         feature_name: New PRP feature name
1777:         prp_path: Path to new PRP file
1778: 
1779:     Raises:
1780:         RuntimeError: If update fails
1781:     """
1782:     logger.info(f"Updating Linear issue {issue_id} with {prp_id}")
1783: 
1784:     # FIXME: Placeholder - replace with actual Linear MCP call
1785:     # In full implementation, this would:
1786:     # 1. Get current issue description via mcp__linear-server__get_issue
1787:     # 2. Append new PRP section to description
1788:     # 3. Update issue via mcp__linear-server__update_issue
1789: 
1790:     update_text = f"""
1791: 
1792: ---
1793: 
1794: ## Related: {prp_id} - {feature_name}
1795: 
1796: **PRP File**: `{prp_path}`
1797: 
1798: This PRP is related to the same feature/initiative.
1799: """
1800: 
1801:     logger.info(f"Would append to issue {issue_id}: {update_text[:100]}...")
1802:     logger.warning("Linear MCP integration pending - issue not actually updated")
1803: 
1804: 
1805: def _generate_issue_description(
1806:     prp_id: str,
1807:     parsed_data: Dict[str, Any],
1808:     prp_path: str
1809: ) -> str:
1810:     """Generate Linear issue description from PRP data.
1811: 
1812:     Args:
1813:         prp_id: PRP identifier (e.g., "PRP-15")
1814:         parsed_data: Parsed INITIAL.md data
1815:         prp_path: Path to generated PRP file
1816: 
1817:     Returns:
1818:         Markdown description for Linear issue
1819:     """
1820:     feature = parsed_data["feature"]
1821:     examples_count = len(parsed_data["examples"])
1822: 
1823:     # Truncate feature description for issue
1824:     feature_summary = feature[:300] + "..." if len(feature) > 300 else feature
1825: 
1826:     description = f"""## Feature
1827: 
1828: {feature_summary}
1829: 
1830: ## PRP Details
1831: 
1832: - **PRP ID**: {prp_id}
1833: - **PRP File**: `{prp_path}`
1834: - **Examples Provided**: {examples_count}
1835: 
1836: ## Implementation
1837: 
1838: See PRP file for detailed implementation steps, validation gates, and testing strategy.
1839: 
1840: """
1841: 
1842:     # Add other considerations if present
1843:     if parsed_data.get("other_considerations"):
1844:         other = parsed_data["other_considerations"]
1845:         other_summary = other[:200] + "..." if len(other) > 200 else other
1846:         description += f"""## Considerations
1847: 
1848: {other_summary}
1849: """
1850: 
1851:     return description
1852: 
1853: 
1854: def _update_prp_yaml_with_issue(prp_path: str, issue_id: str) -> None:
1855:     """Update PRP YAML header with Linear issue ID.
1856: 
1857:     Args:
1858:         prp_path: Path to PRP file
1859:         issue_id: Linear issue identifier
1860: 
1861:     Raises:
1862:         RuntimeError: If YAML update fails
1863:     """
1864:     content = Path(prp_path).read_text(encoding="utf-8")
1865: 
1866:     # Check if YAML frontmatter exists
1867:     yaml_match = re.match(r"(---\n.*?\n---)", content, re.DOTALL)
1868:     if not yaml_match:
1869:         raise RuntimeError(
1870:             f"No YAML frontmatter found in {prp_path}\n"
1871:             f"ðŸ”§ Troubleshooting: PRP file should start with YAML frontmatter"
1872:         )
1873: 
1874:     yaml_block = yaml_match.group(1)
1875: 
1876:     # Check if issue field exists
1877:     if re.search(r"^issue:", yaml_block, re.MULTILINE):
1878:         # Update existing issue field
1879:         updated_yaml = re.sub(
1880:             r"^issue:.*$",
1881:             f"issue: {issue_id}",
1882:             yaml_block,
1883:             flags=re.MULTILINE
1884:         )
1885:     else:
1886:         # Add issue field before closing ---
1887:         updated_yaml = yaml_block.replace(
1888:             "\n---",
1889:             f"\nissue: {issue_id}\n---"
1890:         )
1891: 
1892:     # Replace YAML block in content
1893:     updated_content = content.replace(yaml_block, updated_yaml)
1894: 
1895:     # Write back to file
1896:     Path(prp_path).write_text(updated_content, encoding="utf-8")
</file>

<file path="tools/ce/linear_mcp_resilience.py">
  1: """Linear MCP resilience layer with automatic auth recovery.
  2: 
  3: Ensures Linear MCP within Syntropy calls handles authentication failures gracefully:
  4: 1. Detects auth failures (401, "Not connected", "unauthorized")
  5: 2. Attempts auth reset (rm -rf ~/.mcp-auth)
  6: 3. Retries operation with retry/backoff logic
  7: 4. Falls back gracefully if auth recovery fails
  8: 
  9: Design:
 10: - Circuit breaker prevents repeated auth attempts after threshold
 11: - Retry with exponential backoff (1s, 2s, 4s)
 12: - Detailed error messages with troubleshooting guidance
 13: - No silent failures - all auth issues surfaced
 14: """
 15: 
 16: import subprocess
 17: from typing import Callable, Any, Optional, Dict
 18: from pathlib import Path
 19: from datetime import datetime
 20: 
 21: from ce.resilience import CircuitBreaker, retry_with_backoff, CircuitBreakerOpenError
 22: from ce.logging_config import get_logger
 23: 
 24: logger = get_logger(__name__)
 25: 
 26: # Circuit breaker for Linear MCP operations
 27: linear_breaker = CircuitBreaker(
 28:     name="linear-mcp",
 29:     failure_threshold=3,
 30:     recovery_timeout=300  # 5 minutes between recovery attempts
 31: )
 32: 
 33: # Circuit breaker specifically for auth recovery
 34: auth_recovery_breaker = CircuitBreaker(
 35:     name="linear-mcp-auth-recovery",
 36:     failure_threshold=2,
 37:     recovery_timeout=600  # 10 minutes between recovery attempts
 38: )
 39: 
 40: # Auth cache to avoid repeated resets
 41: _auth_reset_cache: Dict[str, datetime] = {}
 42: AUTH_RESET_COOLDOWN = 60  # Minimum seconds between auth resets
 43: 
 44: 
 45: def _is_auth_error(error: Exception, error_msg: str = "") -> bool:
 46:     """Detect if error is authentication-related.
 47: 
 48:     Patterns:
 49:     - "Not connected" (Linear MCP disconnected)
 50:     - "401" or "unauthorized" (HTTP auth failure)
 51:     - "authentication" (generic auth failure)
 52:     - "permission denied" (auth permission issue)
 53:     """
 54:     error_text = str(error).lower() + error_msg.lower()
 55: 
 56:     auth_patterns = [
 57:         "not connected",
 58:         "401",
 59:         "unauthorized",
 60:         "authentication",
 61:         "permission denied",
 62:         "auth failed",
 63:         "invalid credentials",
 64:         "access denied"
 65:     ]
 66: 
 67:     return any(pattern in error_text for pattern in auth_patterns)
 68: 
 69: 
 70: def _can_reset_auth() -> bool:
 71:     """Check if auth reset is allowed (respects cooldown).
 72: 
 73:     Returns:
 74:         True if enough time has passed since last reset
 75:     """
 76:     last_reset = _auth_reset_cache.get("linear_mcp_last_reset")
 77:     if last_reset is None:
 78:         return True
 79: 
 80:     elapsed = (datetime.now() - last_reset).total_seconds()
 81:     return elapsed >= AUTH_RESET_COOLDOWN
 82: 
 83: 
 84: def _reset_linear_mcp_auth() -> bool:
 85:     """Reset Linear MCP auth by clearing MCP auth cache.
 86: 
 87:     Executes: rm -rf ~/.mcp-auth
 88: 
 89:     Returns:
 90:         True if reset succeeded, False otherwise
 91: 
 92:     Side Effects:
 93:         - Clears ~/.mcp-auth directory
 94:         - Updates auth reset cache timestamp
 95:     """
 96:     if not _can_reset_auth():
 97:         logger.debug("Auth reset on cooldown - skipping")
 98:         return False
 99: 
100:     try:
101:         auth_dir = Path.home() / ".mcp-auth"
102: 
103:         if not auth_dir.exists():
104:             logger.debug("Auth directory already cleared")
105:             _auth_reset_cache["linear_mcp_last_reset"] = datetime.now()
106:             return True
107: 
108:         # Use subprocess for safe deletion
109:         result = subprocess.run(
110:             ["rm", "-rf", str(auth_dir)],
111:             capture_output=True,
112:             timeout=5
113:         )
114: 
115:         if result.returncode != 0:
116:             logger.warning(f"Auth reset command failed: {result.stderr.decode()}")
117:             return False
118: 
119:         logger.info("Linear MCP auth reset successfully")
120:         _auth_reset_cache["linear_mcp_last_reset"] = datetime.now()
121:         return True
122: 
123:     except subprocess.TimeoutExpired:
124:         logger.error("Auth reset command timed out")
125:         return False
126:     except Exception as e:
127:         logger.error(f"Failed to reset Linear MCP auth: {e}")
128:         return False
129: 
130: 
131: @retry_with_backoff(
132:     max_attempts=3,
133:     base_delay=1.0,
134:     max_delay=10.0,
135:     exceptions=(RuntimeError, ConnectionError, IOError, OSError)
136: )
137: def _call_linear_mcp_with_retry(func: Callable, *args, **kwargs) -> Any:
138:     """Call Linear MCP function with retry logic.
139: 
140:     Args:
141:         func: Linear MCP function to call
142:         *args: Positional arguments
143:         **kwargs: Keyword arguments
144: 
145:     Returns:
146:         Function result
147: 
148:     Raises:
149:         RuntimeError: If all retries exhausted
150:     """
151:     try:
152:         return func(*args, **kwargs)
153:     except Exception as e:
154:         error_msg = str(e)
155: 
156:         # Check if auth error
157:         if _is_auth_error(e, error_msg):
158:             logger.warning(f"Auth error detected: {error_msg}")
159:             logger.info("Attempting auth recovery...")
160: 
161:             # Try to recover auth
162:             if _reset_linear_mcp_auth():
163:                 logger.info("Auth reset succeeded - will retry operation")
164:                 # Retry is handled by decorator
165:                 raise RuntimeError(f"Auth recovered, retrying: {error_msg}") from e
166:             else:
167:                 logger.error("Auth reset failed - operation cannot proceed")
168:                 raise RuntimeError(
169:                     f"Linear MCP auth failed and recovery failed\n"
170:                     f"Error: {error_msg}\n"
171:                     f"ðŸ”§ Troubleshooting:\n"
172:                     f"  1. Manually run: rm -rf ~/.mcp-auth\n"
173:                     f"  2. Verify Linear MCP is properly configured\n"
174:                     f"  3. Check network connectivity to Linear service\n"
175:                     f"  4. Restart Claude Code and retry"
176:                 ) from e
177: 
178:         # Non-auth error - propagate
179:         raise
180: 
181: 
182: def call_linear_mcp_resilient(
183:     func: Callable,
184:     *args,
185:     operation_name: str = "Linear MCP operation",
186:     **kwargs
187: ) -> Dict[str, Any]:
188:     """Call Linear MCP function with full resilience (retry + circuit breaker + auth recovery).
189: 
190:     Args:
191:         func: Linear MCP function to call
192:         *args: Positional arguments
193:         operation_name: Human-readable operation name for logging
194:         **kwargs: Keyword arguments
195: 
196:     Returns:
197:         {
198:             "success": True,
199:             "result": <function result>,
200:             "method": "direct_call",
201:             "attempts": 1,
202:             "error": None
203:         }
204:         OR
205:         {
206:             "success": False,
207:             "result": None,
208:             "method": "failed",
209:             "attempts": N,
210:             "error": "<error message>",
211:             "recovery_attempted": True/False
212:         }
213: 
214:     Process:
215:         1. Check circuit breaker state
216:         2. Call function with retry + auth recovery
217:         3. On auth error: attempt auth reset, retry
218:         4. On persistent failure: open circuit breaker
219:         5. Return detailed result
220: 
221:     Side Effects:
222:         - May reset ~/.mcp-auth on auth failure
223:         - Updates circuit breaker state
224:     """
225:     logger.info(f"Starting resilient Linear MCP call: {operation_name}")
226: 
227:     attempt = 0
228:     recovery_attempted = False
229: 
230:     try:
231:         # Check circuit breaker
232:         if linear_breaker.state == "open":
233:             if not linear_breaker._should_attempt_reset():
234:                 raise CircuitBreakerOpenError(
235:                     f"Circuit breaker '{linear_breaker.name}' is OPEN\n"
236:                     f"Failures: {linear_breaker.failure_count}/{linear_breaker.failure_threshold}\n"
237:                     f"ðŸ”§ Troubleshooting: Wait {linear_breaker.recovery_timeout}s or check Linear service health"
238:                 )
239:             # Attempt recovery from half-open state
240:             linear_breaker._transition_to_half_open()
241: 
242:         # Call with retry + auth recovery
243:         attempt = 1
244:         try:
245:             result = _call_linear_mcp_with_retry(func, *args, **kwargs)
246:             linear_breaker._on_success()
247: 
248:             return {
249:                 "success": True,
250:                 "result": result,
251:                 "method": "direct_call",
252:                 "attempts": attempt,
253:                 "error": None,
254:                 "recovery_attempted": False
255:             }
256: 
257:         except RuntimeError as retry_error:
258:             # Check if it's auth recovery retry
259:             if "Auth recovered" in str(retry_error):
260:                 recovery_attempted = True
261:                 attempt += 1
262:                 logger.info(f"Retrying after auth recovery (attempt {attempt})")
263:                 result = _call_linear_mcp_with_retry(func, *args, **kwargs)
264:                 linear_breaker._on_success()
265: 
266:                 return {
267:                     "success": True,
268:                     "result": result,
269:                     "method": "after_auth_recovery",
270:                     "attempts": attempt,
271:                     "error": None,
272:                     "recovery_attempted": True
273:                 }
274:             raise
275: 
276:     except CircuitBreakerOpenError as e:
277:         linear_breaker._on_failure()
278:         logger.error(f"Circuit breaker open: {e}")
279: 
280:         return {
281:             "success": False,
282:             "result": None,
283:             "method": "circuit_breaker_open",
284:             "attempts": attempt,
285:             "error": str(e),
286:             "recovery_attempted": False
287:         }
288: 
289:     except Exception as e:
290:         linear_breaker._on_failure()
291:         error_msg = str(e)
292: 
293:         logger.error(f"Linear MCP operation failed: {error_msg}")
294: 
295:         # Provide actionable error with troubleshooting
296:         is_auth = _is_auth_error(e, error_msg)
297: 
298:         return {
299:             "success": False,
300:             "result": None,
301:             "method": "auth_recovery" if is_auth else "failed",
302:             "attempts": attempt,
303:             "error": f"{error_msg}\n"
304:                     f"ðŸ”§ Troubleshooting:\n"
305:                     f"  1. Check Linear MCP connectivity\n"
306:                     f"  2. Run: rm -rf ~/.mcp-auth\n"
307:                     f"  3. Verify API credentials are valid\n"
308:                     f"  4. Check network connectivity\n",
309:             "recovery_attempted": recovery_attempted
310:         }
311: 
312: 
313: def create_issue_resilient(
314:     title: str,
315:     description: str,
316:     state: str = "todo",
317:     labels: Optional[list] = None,
318:     override_assignee: Optional[str] = None,
319:     override_project: Optional[str] = None
320: ) -> Dict[str, Any]:
321:     """Create Linear issue with resilience and auth recovery.
322: 
323:     Args:
324:         title: Issue title
325:         description: Issue description
326:         state: Issue state
327:         labels: Optional labels
328:         override_assignee: Optional assignee override
329:         override_project: Optional project override
330: 
331:     Returns:
332:         Result dict from call_linear_mcp_resilient with issue data on success
333:     """
334:     # Import here to avoid circular imports
335:     from ce.linear_utils import create_issue_with_defaults
336: 
337:     # This gets the prepared issue data (not actually calling MCP yet)
338:     issue_data = create_issue_with_defaults(
339:         title=title,
340:         description=description,
341:         state=state,
342:         labels=labels,
343:         override_assignee=override_assignee,
344:         override_project=override_project
345:     )
346: 
347:     # TODO: Replace with actual Linear MCP call
348:     # For now, return prepared data with success flag
349:     logger.warning("Linear MCP create_issue not yet integrated - returning prepared data")
350: 
351:     return {
352:         "success": True,
353:         "result": issue_data,
354:         "method": "prepared_data_only",
355:         "attempts": 1,
356:         "error": None,
357:         "recovery_attempted": False
358:     }
359: 
360: 
361: def update_issue_resilient(
362:     issue_id: str,
363:     description: str,
364:     state: Optional[str] = None
365: ) -> Dict[str, Any]:
366:     """Update Linear issue with resilience and auth recovery.
367: 
368:     Args:
369:         issue_id: Linear issue ID (e.g., "BLA-24")
370:         description: Updated description
371:         state: Optional new state
372: 
373:     Returns:
374:         Result dict from call_linear_mcp_resilient
375:     """
376:     # TODO: Replace with actual Linear MCP call
377:     logger.warning("Linear MCP update_issue not yet integrated")
378: 
379:     return {
380:         "success": False,
381:         "result": None,
382:         "method": "not_implemented",
383:         "attempts": 1,
384:         "error": "Linear MCP update_issue not yet implemented",
385:         "recovery_attempted": False
386:     }
387: 
388: 
389: def get_linear_mcp_status() -> Dict[str, Any]:
390:     """Get Linear MCP health status.
391: 
392:     Returns:
393:         {
394:             "connected": True/False,
395:             "circuit_breaker_state": "closed|open|half_open",
396:             "failure_count": N,
397:             "last_auth_reset": "ISO timestamp or null",
398:             "auth_reset_available": True/False,
399:             "diagnostics": "..."
400:         }
401:     """
402:     return {
403:         "connected": linear_breaker.state == "closed",
404:         "circuit_breaker_state": linear_breaker.state,
405:         "failure_count": linear_breaker.failure_count,
406:         "last_auth_reset": _auth_reset_cache.get("linear_mcp_last_reset"),
407:         "auth_reset_available": _can_reset_auth(),
408:         "diagnostics": f"Circuit state: {linear_breaker.state}, "
409:                       f"Failures: {linear_breaker.failure_count}/{linear_breaker.failure_threshold}"
410:     }
</file>

<file path="tools/ce/linear_utils.py">
  1: """Linear integration utilities for Context Engineering.
  2: 
  3: Provides helpers for reading Linear defaults and creating issues with
  4: project-specific configuration.
  5: """
  6: 
  7: import logging
  8: from pathlib import Path
  9: from typing import Dict, Any, Optional
 10: import yaml
 11: 
 12: logger = logging.getLogger(__name__)
 13: 
 14: 
 15: def get_linear_defaults() -> Dict[str, Any]:
 16:     """Read Linear defaults from .ce/linear-defaults.yml.
 17: 
 18:     Returns:
 19:         Dict with keys: project, assignee, team, default_labels
 20: 
 21:     Raises:
 22:         FileNotFoundError: If linear-defaults.yml not found
 23:         RuntimeError: If YAML parsing fails
 24:     """
 25:     # Find project root (go up from tools/)
 26:     project_root = Path(__file__).parent.parent.parent
 27:     config_path = project_root / ".ce" / "linear-defaults.yml"
 28: 
 29:     if not config_path.exists():
 30:         raise FileNotFoundError(
 31:             f"Linear defaults not found: {config_path}\n"
 32:             f"ðŸ”§ Troubleshooting:\n"
 33:             f"   - Create .ce/linear-defaults.yml with project/assignee config\n"
 34:             f"   - See CLAUDE.md for template"
 35:         )
 36: 
 37:     try:
 38:         with open(config_path) as f:
 39:             config = yaml.safe_load(f)
 40:     except yaml.YAMLError as e:
 41:         raise RuntimeError(
 42:             f"Failed to parse Linear defaults: {e}\n"
 43:             f"ðŸ”§ Troubleshooting: Check YAML syntax in {config_path}"
 44:         ) from e
 45: 
 46:     # Validate required fields
 47:     required_fields = ["project", "assignee", "team"]
 48:     missing = [f for f in required_fields if f not in config]
 49: 
 50:     if missing:
 51:         raise RuntimeError(
 52:             f"Missing required fields in Linear defaults: {', '.join(missing)}\n"
 53:             f"ðŸ”§ Troubleshooting: Add to {config_path}"
 54:         )
 55: 
 56:     return config
 57: 
 58: 
 59: def create_issue_with_defaults(
 60:     title: str,
 61:     description: str,
 62:     state: str = "todo",
 63:     labels: Optional[list] = None,
 64:     override_assignee: Optional[str] = None,
 65:     override_project: Optional[str] = None
 66: ) -> Dict[str, Any]:
 67:     """Create Linear issue using project defaults.
 68: 
 69:     Args:
 70:         title: Issue title
 71:         description: Issue description (markdown)
 72:         state: Issue state (todo, in_progress, done)
 73:         labels: Optional labels (merges with defaults)
 74:         override_assignee: Optional assignee override
 75:         override_project: Optional project override
 76: 
 77:     Returns:
 78:         Linear API response with issue details
 79: 
 80:     Example:
 81:         issue = create_issue_with_defaults(
 82:             title="PRP-15: New Feature",
 83:             description="Implement feature X",
 84:             state="todo"
 85:         )
 86:         print(f"Created: {issue['identifier']}")
 87:     """
 88:     defaults = get_linear_defaults()
 89: 
 90:     # Merge labels
 91:     final_labels = list(defaults.get("default_labels", []))
 92:     if labels:
 93:         final_labels.extend(labels)
 94:     # Deduplicate
 95:     final_labels = list(set(final_labels))
 96: 
 97:     # Prepare issue data
 98:     issue_data = {
 99:         "team": defaults["team"],
100:         "title": title,
101:         "description": description,
102:         "state": state,
103:         "labels": final_labels,
104:         "assignee": override_assignee or defaults["assignee"],
105:         "project": override_project or defaults["project"]
106:     }
107: 
108:     logger.info(f"Creating Linear issue with defaults: {title}")
109:     logger.debug(f"Issue data: {issue_data}")
110: 
111:     # Note: Actual MCP call would go here
112:     # For now, return the prepared data structure
113:     return issue_data
114: 
115: 
116: def get_default_assignee() -> str:
117:     """Get default assignee email from config.
118: 
119:     Returns:
120:         Assignee email address
121: 
122:     Example:
123:         assignee = get_default_assignee()
124:         # "blazej.przybyszewski@gmail.com"
125:     """
126:     defaults = get_linear_defaults()
127:     return defaults["assignee"]
128: 
129: 
130: def get_default_project() -> str:
131:     """Get default project name from config.
132: 
133:     Returns:
134:         Project name
135: 
136:     Example:
137:         project = get_default_project()
138:         # "Context Engineering"
139:     """
140:     defaults = get_linear_defaults()
141:     return defaults["project"]
</file>

<file path="tools/ce/logging_config.py">
  1: """Logging configuration module - structured logging with JSON formatter.
  2: 
  3: Provides JSON-based structured logging for production observability and
  4: human-readable text logging for development.
  5: """
  6: 
  7: import logging
  8: import json
  9: import sys
 10: from typing import Dict, Any
 11: 
 12: 
 13: class JSONFormatter(logging.Formatter):
 14:     """JSON formatter for structured logging.
 15: 
 16:     Outputs logs in JSON format for machine parsing.
 17: 
 18:     Example output:
 19:         {"timestamp": "2025-01-13T10:30:45", "level": "INFO",
 20:          "message": "prp.execution.started", "prp_id": "PRP-003"}
 21:     """
 22: 
 23:     def format(self, record: logging.LogRecord) -> str:
 24:         """Format log record as JSON.
 25: 
 26:         Args:
 27:             record: Log record to format
 28: 
 29:         Returns:
 30:             JSON string
 31: 
 32:         Note: Includes extra fields from record.extra dict if provided.
 33:         """
 34:         log_data = {
 35:             "timestamp": self.formatTime(record, self.datefmt),
 36:             "level": record.levelname,
 37:             "logger": record.name,
 38:             "message": record.getMessage(),
 39:         }
 40: 
 41:         # Add extra fields from record
 42:         if hasattr(record, "prp_id"):
 43:             log_data["prp_id"] = record.prp_id
 44:         if hasattr(record, "phase"):
 45:             log_data["phase"] = record.phase
 46:         if hasattr(record, "duration"):
 47:             log_data["duration"] = record.duration
 48:         if hasattr(record, "success"):
 49:             log_data["success"] = record.success
 50: 
 51:         # Add exception info if present
 52:         if record.exc_info:
 53:             log_data["exception"] = self.formatException(record.exc_info)
 54: 
 55:         return json.dumps(log_data)
 56: 
 57: 
 58: def setup_logging(
 59:     level: str = "INFO",
 60:     json_output: bool = False,
 61:     log_file: str = None
 62: ) -> logging.Logger:
 63:     """Setup application logging.
 64: 
 65:     Args:
 66:         level: Log level (DEBUG, INFO, WARNING, ERROR)
 67:         json_output: If True, use JSON formatter
 68:         log_file: Optional file path for file logging
 69: 
 70:     Returns:
 71:         Configured root logger
 72: 
 73:     Example:
 74:         setup_logging(level="DEBUG", json_output=True)
 75:         logger = logging.getLogger(__name__)
 76:         logger.info("prp.started", extra={"prp_id": "PRP-003"})
 77: 
 78:     Note: Call this once at application startup. All subsequent loggers
 79:     will inherit this configuration.
 80:     """
 81:     # Get root logger
 82:     logger = logging.getLogger()
 83:     logger.setLevel(getattr(logging, level.upper()))
 84: 
 85:     # Remove existing handlers
 86:     logger.handlers.clear()
 87: 
 88:     # Console handler
 89:     console_handler = logging.StreamHandler(sys.stderr)
 90:     if json_output:
 91:         console_handler.setFormatter(JSONFormatter())
 92:     else:
 93:         console_handler.setFormatter(
 94:             logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
 95:         )
 96:     logger.addHandler(console_handler)
 97: 
 98:     # File handler (optional)
 99:     if log_file:
100:         file_handler = logging.FileHandler(log_file)
101:         file_handler.setFormatter(JSONFormatter())  # Always use JSON for file logs
102:         logger.addHandler(file_handler)
103: 
104:     return logger
105: 
106: 
107: def get_logger(name: str) -> logging.Logger:
108:     """Get logger for module.
109: 
110:     Args:
111:         name: Module name (typically __name__)
112: 
113:     Returns:
114:         Logger instance configured with application settings
115: 
116:     Example:
117:         logger = get_logger(__name__)
118:         logger.info("Starting operation", extra={"prp_id": "PRP-003"})
119: 
120:     Note: Use this instead of logging.getLogger() for consistency.
121:     """
122:     return logging.getLogger(name)
</file>

<file path="tools/ce/markdown_lint.py">
  1: """Markdown linting utilities using markdownlint-cli2.
  2: 
  3: Provides markdown validation and auto-fixing capabilities.
  4: """
  5: 
  6: import subprocess
  7: from pathlib import Path
  8: from typing import Dict, Any
  9: 
 10: 
 11: def lint_markdown(auto_fix: bool = False) -> Dict[str, Any]:
 12:     """Lint markdown files using markdownlint-cli2.
 13: 
 14:     Args:
 15:         auto_fix: If True, attempt to auto-fix issues
 16: 
 17:     Returns:
 18:         Dict with success, errors, output, fixed_count
 19: 
 20:     Raises:
 21:         RuntimeError: If markdownlint-cli2 is not installed
 22: 
 23:     Note: No fishy fallbacks - exceptions thrown for troubleshooting.
 24:     """
 25:     # Check if markdownlint-cli2 is available
 26:     check_cmd = ["which", "markdownlint-cli2"]
 27:     check_result = subprocess.run(
 28:         check_cmd,
 29:         capture_output=True,
 30:         text=True
 31:     )
 32: 
 33:     if check_result.returncode != 0:
 34:         raise RuntimeError(
 35:             "markdownlint-cli2 not found\n"
 36:             "ðŸ”§ Troubleshooting: Install with 'npm install --save-dev markdownlint-cli2'"
 37:         )
 38: 
 39:     # Patterns for markdown files to lint
 40:     patterns = [
 41:         "docs/**/*.md",
 42:         "PRPs/**/*.md",
 43:         "examples/**/*.md",
 44:         "*.md"
 45:     ]
 46: 
 47:     cmd = ["markdownlint-cli2"]
 48:     if auto_fix:
 49:         cmd.append("--fix")
 50:     cmd.extend(patterns)
 51: 
 52:     # Run from project root
 53:     project_root = Path(__file__).parent.parent.parent
 54: 
 55:     result = subprocess.run(
 56:         cmd,
 57:         capture_output=True,
 58:         text=True,
 59:         cwd=project_root
 60:     )
 61: 
 62:     # Parse output
 63:     output_lines = result.stdout.strip().split("\n") if result.stdout else []
 64:     error_lines = result.stderr.strip().split("\n") if result.stderr else []
 65: 
 66:     # Count fixes if auto-fix was enabled
 67:     fixed_count = 0
 68:     if auto_fix:
 69:         for line in output_lines:
 70:             if "Fixed:" in line:
 71:                 fixed_count += 1
 72: 
 73:     return {
 74:         "success": result.returncode == 0,
 75:         "errors": [line for line in error_lines if line],
 76:         "output": [line for line in output_lines if line],
 77:         "fixed_count": fixed_count,
 78:         "exit_code": result.returncode
 79:     }
 80: 
 81: 
 82: def run_markdown_validation(auto_fix: bool = True) -> Dict[str, Any]:
 83:     """Run markdown validation with optional auto-fix.
 84: 
 85:     Args:
 86:         auto_fix: If True, attempt to auto-fix issues before reporting
 87: 
 88:     Returns:
 89:         Dict with success, message, details
 90:     """
 91:     try:
 92:         # First try to auto-fix if requested
 93:         if auto_fix:
 94:             fix_result = lint_markdown(auto_fix=True)
 95:             if fix_result["fixed_count"] > 0:
 96:                 # Re-run validation to check if all issues were fixed
 97:                 validation_result = lint_markdown(auto_fix=False)
 98:                 return {
 99:                     "success": validation_result["success"],
100:                     "message": f"Fixed {fix_result['fixed_count']} issues, validation {'passed' if validation_result['success'] else 'has remaining issues'}",
101:                     "details": {
102:                         "fixed_count": fix_result["fixed_count"],
103:                         "remaining_errors": validation_result["errors"]
104:                     }
105:                 }
106: 
107:         # Run validation without auto-fix
108:         result = lint_markdown(auto_fix=False)
109: 
110:         if result["success"]:
111:             return {
112:                 "success": True,
113:                 "message": "All markdown files validated successfully",
114:                 "details": result
115:             }
116:         else:
117:             return {
118:                 "success": False,
119:                 "message": f"Markdown validation found {len(result['errors'])} issues",
120:                 "details": result
121:             }
122: 
123:     except Exception as e:
124:         raise RuntimeError(
125:             f"Markdown validation failed: {str(e)}\n"
126:             f"ðŸ”§ Troubleshooting: Ensure markdownlint-cli2 is installed via npm"
127:         )
</file>

<file path="tools/ce/mcp_adapter.py">
  1: """MCP adapter layer for Serena file operations with graceful fallback.
  2: 
  3: This module provides abstraction for file operations, using Serena MCP when available
  4: and falling back to local filesystem operations when MCP is unavailable.
  5: 
  6: MCP Availability:
  7:     - Claude Code context: Serena MCP typically available
  8:     - Standalone CLI: Falls back to filesystem
  9:     - Test environment: Uses mcp_fake for testing
 10: 
 11: Design Decision (ADR-001):
 12:     - Optional fallback approach for MVP
 13:     - Simple try/catch detection
 14:     - Unified error handling
 15:     - Performance acceptable (<100ms overhead per MCP call)
 16: """
 17: 
 18: from typing import Dict, Any, List, Optional
 19: from pathlib import Path
 20: from ce.resilience import retry_with_backoff, CircuitBreaker, CircuitBreakerOpenError
 21: from ce.logging_config import get_logger
 22: 
 23: # Logger
 24: logger = get_logger(__name__)
 25: 
 26: # Global circuit breaker for Serena MCP operations
 27: serena_breaker = CircuitBreaker(name="serena-mcp", failure_threshold=5, recovery_timeout=60)
 28: 
 29: 
 30: def _import_serena_mcp():
 31:     """Import Serena MCP module dynamically.
 32: 
 33:     Returns:
 34:         The mcp__serena module
 35: 
 36:     Raises:
 37:         ImportError: If module cannot be imported
 38: 
 39:     Note: Helper function to avoid repeated import logic throughout module.
 40:     """
 41:     import importlib
 42:     return importlib.import_module("mcp__serena")
 43: 
 44: 
 45: def is_mcp_available() -> bool:
 46:     """Check if Serena MCP is available at runtime.
 47: 
 48:     Returns:
 49:         True if Serena MCP tools are available, False otherwise
 50: 
 51:     Detection Strategy:
 52:         1. Try importing mcp__serena tools
 53:         2. Attempt minimal read operation
 54:         3. Cache result for session (not implemented in MVP)
 55: 
 56:     Note: This is a simple detection strategy. More sophisticated
 57:     approaches (version checking, capability negotiation) deferred to future.
 58:     """
 59:     try:
 60:         # Attempt to import Serena MCP tools
 61:         serena_module = _import_serena_mcp()
 62: 
 63:         # Check if key functions exist
 64:         required_functions = [
 65:             "read_file",
 66:             "create_text_file",
 67:             "get_symbols_overview",
 68:             "insert_after_symbol"
 69:         ]
 70: 
 71:         for func_name in required_functions:
 72:             if not hasattr(serena_module, func_name):
 73:                 return False
 74: 
 75:         return True
 76: 
 77:     except (ImportError, ModuleNotFoundError, AttributeError):
 78:         return False
 79: 
 80: 
 81: @retry_with_backoff(max_attempts=3, base_delay=1.0, exceptions=(IOError, ConnectionError, TimeoutError))
 82: def _create_file_via_mcp(filepath: str, content: str):
 83:     """Create file via MCP with retry logic.
 84: 
 85:     Args:
 86:         filepath: Relative path to file
 87:         content: File content
 88: 
 89:     Raises:
 90:         Exception: If MCP call fails after retries
 91: 
 92:     Note: Internal function with retry decorator. Circuit breaker applied at call site.
 93:     """
 94:     serena = _import_serena_mcp()
 95:     serena.create_text_file(filepath, content)
 96: 
 97: 
 98: def create_file_with_mcp(filepath: str, content: str) -> Dict[str, Any]:
 99:     """Create file using Serena MCP or fallback to filesystem.
100: 
101:     Args:
102:         filepath: Relative path to file to create
103:         content: File content
104: 
105:     Returns:
106:         {
107:             "success": True,
108:             "method": "mcp" or "filesystem",
109:             "filepath": "<path>",
110:             "error": "<error message if success=False>"
111:         }
112: 
113:     Process:
114:         1. Check MCP availability
115:         2. If available, try mcp__serena__create_text_file (with retry + circuit breaker)
116:         3. On MCP failure or unavailable, fallback to filesystem
117:         4. Return result with method used
118: 
119:     Raises:
120:         RuntimeError: If both MCP and filesystem operations fail
121: 
122:     Note: Graceful fallback ensures execution continues even when MCP unavailable.
123:     """
124:     # Try MCP first if available
125:     if is_mcp_available():
126:         try:
127:             # Apply circuit breaker + retry
128:             _create_file_via_mcp(filepath, content)
129: 
130:             return {
131:                 "success": True,
132:                 "method": "mcp",
133:                 "filepath": filepath
134:             }
135: 
136:         except CircuitBreakerOpenError as e:
137:             # Circuit breaker open - fall back immediately
138:             logger.warning(
139:                 "MCP circuit breaker open, falling back to filesystem",
140:                 extra={"filepath": filepath, "error": str(e)}
141:             )
142: 
143:         except Exception as e:
144:             # Other MCP failure - log and fallback
145:             logger.warning(
146:                 "MCP file creation failed, falling back to filesystem",
147:                 extra={"filepath": filepath, "error": str(e)}
148:             )
149: 
150:     # Fallback to filesystem
151:     try:
152:         file_path = Path(filepath)
153:         file_path.parent.mkdir(parents=True, exist_ok=True)
154:         file_path.write_text(content)
155: 
156:         return {
157:             "success": True,
158:             "method": "filesystem",
159:             "filepath": filepath
160:         }
161: 
162:     except Exception as e:
163:         raise RuntimeError(
164:             f"Failed to create {filepath} (both MCP and filesystem failed)\n"
165:             f"Error: {str(e)}\n"
166:             f"ðŸ”§ Troubleshooting:\n"
167:             f"  1. Check file path is valid\n"
168:             f"  2. Verify parent directory exists or can be created\n"
169:             f"  3. Check write permissions\n"
170:             f"  4. Review file content for invalid characters"
171:         ) from e
172: 
173: 
174: @retry_with_backoff(max_attempts=3, base_delay=1.0, exceptions=(IOError, ConnectionError, TimeoutError))
175: def _insert_code_via_mcp(filepath: str, code: str, mode: str, symbol_name: str):
176:     """Insert code via MCP with retry logic.
177: 
178:     Args:
179:         filepath: Path to file
180:         code: Code to insert
181:         mode: "after" or "before"
182:         symbol_name: Symbol name path
183: 
184:     Raises:
185:         Exception: If MCP call fails after retries
186: 
187:     Note: Internal function with retry decorator.
188:     """
189:     serena = _import_serena_mcp()
190:     if mode == "after":
191:         serena.insert_after_symbol(symbol_name, filepath, code)
192:     else:
193:         serena.insert_before_symbol(symbol_name, filepath, code)
194: 
195: 
196: def insert_code_with_mcp(
197:     filepath: str,
198:     code: str,
199:     mode: str = "append"
200: ) -> Dict[str, Any]:
201:     """Insert code using Serena MCP symbol operations or fallback.
202: 
203:     Args:
204:         filepath: Path to file to modify
205:         code: Code to insert
206:         mode: Insertion mode - "append", "after_last_symbol", "before_first_symbol"
207: 
208:     Returns:
209:         {
210:             "success": True,
211:             "method": "mcp_symbol_aware" | "mcp_append" | "filesystem_append",
212:             "filepath": "<path>",
213:             "symbol": "<symbol name if symbol-aware>",
214:             "error": "<error message if success=False>"
215:         }
216: 
217:     Process:
218:         1. Check MCP availability
219:         2. If available and mode is symbol-aware:
220:            a. Get symbols overview
221:            b. Insert after/before symbol (with retry + circuit breaker)
222:         3. If MCP unavailable or append mode:
223:            a. Read file, append code, write back
224:         4. Return result with method used
225: 
226:     Raises:
227:         RuntimeError: If file modification fails
228: 
229:     Note: Symbol-aware insertion requires Serena MCP. Fallback mode is naive append.
230:     """
231:     # Try MCP symbol-aware insertion
232:     if is_mcp_available() and mode != "append":
233:         try:
234:             serena = _import_serena_mcp()
235: 
236:             # Get symbols to find insertion point
237:             symbols = serena.get_symbols_overview(filepath)
238: 
239:             if symbols and len(symbols) > 0:
240:                 if mode == "after_last_symbol":
241:                     last_symbol = symbols[-1]["name_path"]
242:                     _insert_code_via_mcp(filepath, code, "after", last_symbol)
243: 
244:                     return {
245:                         "success": True,
246:                         "method": "mcp_symbol_aware",
247:                         "filepath": filepath,
248:                         "symbol": last_symbol
249:                     }
250: 
251:                 elif mode == "before_first_symbol":
252:                     first_symbol = symbols[0]["name_path"]
253:                     _insert_code_via_mcp(filepath, code, "before", first_symbol)
254: 
255:                     return {
256:                         "success": True,
257:                         "method": "mcp_symbol_aware",
258:                         "filepath": filepath,
259:                         "symbol": first_symbol
260:                     }
261: 
262:             # No symbols found, fall through to append
263: 
264:         except CircuitBreakerOpenError as e:
265:             # Circuit breaker open - fall back immediately
266:             logger.warning(
267:                 "MCP circuit breaker open, falling back to append",
268:                 extra={"filepath": filepath, "mode": mode, "error": str(e)}
269:             )
270: 
271:         except Exception as e:
272:             # Other MCP failure - log and fallback
273:             logger.warning(
274:                 "MCP symbol insertion failed, falling back to append",
275:                 extra={"filepath": filepath, "mode": mode, "error": str(e)}
276:             )
277: 
278:     # Fallback: append to end of file
279:     try:
280:         file_path = Path(filepath)
281:         if not file_path.exists():
282:             raise RuntimeError(
283:                 f"Cannot modify file {filepath} - file does not exist\n"
284:                 f"ðŸ”§ Troubleshooting: Ensure file is created before modification"
285:             )
286: 
287:         current_content = file_path.read_text()
288:         new_content = current_content + "\n\n" + code
289:         file_path.write_text(new_content)
290: 
291:         return {
292:             "success": True,
293:             "method": "filesystem_append",
294:             "filepath": filepath
295:         }
296: 
297:     except Exception as e:
298:         raise RuntimeError(
299:             f"Failed to insert code into {filepath}\n"
300:             f"Error: {str(e)}\n"
301:             f"ðŸ”§ Troubleshooting:\n"
302:             f"  1. Check file exists and is writable\n"
303:             f"  2. Verify code is syntactically valid\n"
304:             f"  3. Check file has valid Python syntax for symbol parsing"
305:         ) from e
306: 
307: 
308: def get_mcp_status() -> Dict[str, Any]:
309:     """Get MCP availability status for diagnostics.
310: 
311:     Returns:
312:         {
313:             "available": True/False,
314:             "version": "<version if available>",
315:             "capabilities": ["read_file", "create_text_file", ...],
316:             "context": "mcp" | "standalone" | "test"
317:         }
318: 
319:     Note: Version and detailed capabilities detection deferred to future.
320:     For MVP, only availability check implemented.
321:     """
322:     available = is_mcp_available()
323: 
324:     result = {
325:         "available": available,
326:         "version": None,  # Not implemented in MVP
327:         "capabilities": [],  # Not implemented in MVP
328:         "context": "mcp" if available else "standalone"
329:     }
330: 
331:     if available:
332:         try:
333:             serena = _import_serena_mcp()
334: 
335:             # List available functions
336:             capabilities = [
337:                 name for name in dir(serena)
338:                 if not name.startswith("_") and callable(getattr(serena, name))
339:             ]
340:             result["capabilities"] = capabilities
341: 
342:         except Exception:
343:             pass
344: 
345:     return result
</file>

<file path="tools/ce/mcp_utils.py">
  1: """MCP utility functions for Syntropy tool calls.
  2: 
  3: Provides wrappers for calling Syntropy MCP tools with proper
  4: error handling and logging.
  5: """
  6: 
  7: import logging
  8: from typing import Dict, Any, Optional
  9: 
 10: logger = logging.getLogger(__name__)
 11: 
 12: 
 13: def call_syntropy_mcp(
 14:     server: str,
 15:     tool: str,
 16:     arguments: Dict[str, Any],
 17:     timeout: int = 10
 18: ) -> Dict[str, Any]:
 19:     """Call Syntropy MCP tool.
 20: 
 21:     Args:
 22:         server: Server name (e.g., "thinking", "serena", "context7")
 23:         tool: Tool name (e.g., "sequentialthinking", "find_symbol")
 24:         arguments: Tool arguments
 25:         timeout: Timeout in seconds
 26: 
 27:     Returns:
 28:         Tool result dictionary
 29: 
 30:     Raises:
 31:         RuntimeError: If MCP call fails
 32: 
 33:     Note: This is a placeholder. Actual implementation will use
 34:     Claude Code's MCP infrastructure to make real tool calls.
 35:     """
 36:     logger.info(f"Calling Syntropy MCP: {server}:{tool}")
 37:     logger.debug(f"Arguments: {arguments}")
 38: 
 39:     # FIXME: Placeholder - replace with actual MCP call
 40:     # In full implementation, this would:
 41:     # 1. Import Claude Code MCP client
 42:     # 2. Get client for server: client = get_mcp_client(f"syntropy-{server}")
 43:     # 3. Call tool: result = client.call_tool(tool, arguments, timeout=timeout)
 44:     # 4. Return result
 45: 
 46:     # For now, log and raise (graceful degradation in callers)
 47:     raise RuntimeError(
 48:         f"MCP call not yet implemented: {server}:{tool}\n"
 49:         f"ðŸ”§ Troubleshooting: Full MCP integration pending"
 50:     )
 51: 
 52: 
 53: def is_mcp_available(server: str) -> bool:
 54:     """Check if MCP server is available.
 55: 
 56:     Args:
 57:         server: Server name (e.g., "thinking", "serena")
 58: 
 59:     Returns:
 60:         True if server available, False otherwise
 61:     """
 62:     try:
 63:         # FIXME: Placeholder - replace with actual availability check
 64:         # Would ping server or check connection status
 65:         logger.debug(f"Checking MCP availability: {server}")
 66:         return False  # Return False until implemented
 67:     except Exception as e:
 68:         logger.warning(f"MCP availability check failed: {e}")
 69:         return False
 70: 
 71: 
 72: def call_sequential_thinking(
 73:     prompt: str,
 74:     thought_number: int = 1,
 75:     total_thoughts: int = 5
 76: ) -> Optional[Dict[str, Any]]:
 77:     """Call sequential thinking MCP tool.
 78: 
 79:     Convenience wrapper for mcp__syntropy__thinking__sequentialthinking
 80: 
 81:     Args:
 82:         prompt: Thinking prompt
 83:         thought_number: Current thought number
 84:         total_thoughts: Estimated total thoughts
 85: 
 86:     Returns:
 87:         Thinking result or None if unavailable
 88:     """
 89:     try:
 90:         return call_syntropy_mcp(
 91:             "thinking",
 92:             "sequentialthinking",
 93:             {
 94:                 "thought": prompt,
 95:                 "thoughtNumber": thought_number,
 96:                 "totalThoughts": total_thoughts,
 97:                 "nextThoughtNeeded": True
 98:             }
 99:         )
100:     except Exception as e:
101:         logger.warning(f"Sequential thinking unavailable: {e}")
102:         return None
</file>

<file path="tools/ce/mermaid_validator.py">
  1: """Mermaid diagram validator with auto-fix for unquoted special characters."""
  2: 
  3: import re
  4: from pathlib import Path
  5: from typing import Dict, Any, List, Tuple
  6: 
  7: 
  8: def validate_mermaid_diagrams(file_path: str, auto_fix: bool = False) -> Dict[str, Any]:
  9:     r"""Validate mermaid diagrams in markdown file.
 10: 
 11:     Args:
 12:         file_path: Path to markdown file
 13:         auto_fix: If True, auto-fix issues by renaming nodes or adding quotes
 14: 
 15:     Returns:
 16:         Dict with: success (bool), errors (List[str]), fixes_applied (List[str])
 17: 
 18:     Validation rules:
 19:     1. Node text with special chars must be quoted or use simple node IDs
 20:     2. Node IDs should be simple (A, B, C1, etc.) if text has special chars
 21:     3. Text with <>[]{}()!?/\ should be in quotes or node renamed
 22:     4. Style statements should always specify color for theme compatibility
 23: 
 24:     Auto-fix strategies:
 25:     - Strategy 1: Rename nodes with special chars (A, B, C, D1, D2, etc.)
 26:     - Strategy 2: Quote text if short and quotes not present
 27:     - Strategy 3: Check style statements have color specified
 28:     """
 29:     content = Path(file_path).read_text()
 30:     errors = []
 31:     fixes_applied = []
 32: 
 33:     # Extract all mermaid blocks
 34:     mermaid_blocks = re.findall(
 35:         r'```mermaid\n(.*?)```',
 36:         content,
 37:         re.DOTALL
 38:     )
 39: 
 40:     if not mermaid_blocks:
 41:         return {
 42:             "success": True,
 43:             "errors": [],
 44:             "fixes_applied": [],
 45:             "diagrams_checked": 0
 46:         }
 47: 
 48:     for i, block in enumerate(mermaid_blocks):
 49:         block_errors, block_fixes = _validate_mermaid_block(block, i + 1)
 50:         errors.extend(block_errors)
 51: 
 52:         if auto_fix and block_fixes:
 53:             # Apply fixes to content
 54:             fixed_block = _apply_fixes_to_block(block, block_fixes)
 55:             content = content.replace(f'```mermaid\n{block}```', f'```mermaid\n{fixed_block}```')
 56:             fixes_applied.extend([f"Diagram {i+1}: {fix}" for fix in block_fixes])
 57: 
 58:     # Write back if fixes applied
 59:     if auto_fix and fixes_applied:
 60:         Path(file_path).write_text(content)
 61: 
 62:     return {
 63:         "success": len(errors) == 0 or (auto_fix and len(fixes_applied) > 0),
 64:         "errors": errors,
 65:         "fixes_applied": fixes_applied,
 66:         "diagrams_checked": len(mermaid_blocks)
 67:     }
 68: 
 69: 
 70: def _validate_mermaid_block(block: str, diagram_num: int) -> Tuple[List[str], List[str]]:
 71:     r"""Validate single mermaid block.
 72: 
 73:     Returns:
 74:         (errors, fix_suggestions) tuple
 75:     """
 76:     errors = []
 77:     fixes = []
 78: 
 79:     # Check 1: Node definitions with special chars but no quotes
 80:     # Pattern: NodeID[Text with special chars] or NodeID{Text with special chars}
 81:     node_pattern = r'([A-Z0-9]+)[\[\{]([^\]\}]+)[\]\}]'
 82:     nodes = re.findall(node_pattern, block)
 83: 
 84:     for node_id, node_text in nodes:
 85:         if _has_unquoted_special_chars(node_text):
 86:             errors.append(
 87:                 f"Diagram {diagram_num}: Node '{node_id}' has unquoted special chars in text: '{node_text}'"
 88:             )
 89:             fixes.append(f"Rename node '{node_id}' or quote text '{node_text}'")
 90: 
 91:     # Check 2: Style statements missing color specification
 92:     style_pattern = r'style\s+([A-Z0-9]+)\s+fill:(#[0-9a-fA-F]{6}|#[0-9a-fA-F]{3})(?!.*color:)'
 93:     styles_missing_color = re.findall(style_pattern, block)
 94: 
 95:     for node_id in styles_missing_color:
 96:         errors.append(
 97:             f"Diagram {diagram_num}: Style for node '{node_id}' missing color specification"
 98:         )
 99:         fixes.append(f"Add color:#000 or color:#fff to style {node_id}")
100: 
101:     # Check 3: Line breaks in node text without <br/> tag
102:     linebreak_pattern = r'[\[\{]([^\]\}]*\n[^\]\}]*)[\]\}]'
103:     linebreaks = re.findall(linebreak_pattern, block)
104: 
105:     for text in linebreaks:
106:         if '<br/>' not in text:
107:             errors.append(
108:                 f"Diagram {diagram_num}: Multiline text without <br/> tag: '{text[:50]}...'"
109:             )
110:             fixes.append("Replace newlines with <br/> in node text")
111: 
112:     return errors, fixes
113: 
114: 
115: def _has_unquoted_special_chars(text: str) -> bool:
116:     """Check if text has special chars that need quoting.
117: 
118:     Special chars that ACTUALLY break mermaid rendering:
119:     - Parentheses: () - used for node shape syntax
120:     - Brackets: [] - used for node shape syntax
121:     - Curly braces: {} - used for node shape syntax
122:     - Pipes: | - used for subgraph syntax
123:     - Unbalanced quotes: "' - break parsing
124: 
125:     Characters that are SAFE in mermaid node text:
126:     - Colons: : - commonly used, safe
127:     - Question marks: ? - safe
128:     - Exclamation marks: ! - safe
129:     - Slashes: / \\ - safe
130:     - HTML tags: <br/>, <sub>, <sup> - explicitly allowed
131: 
132:     Note: HTML tags like <br/> are allowed unquoted in mermaid.
133:     """
134:     # If already quoted, it's fine
135:     if (text.startswith('"') and text.endswith('"')) or \
136:        (text.startswith("'") and text.endswith("'")):
137:         return False
138: 
139:     # Exclude HTML tags from special char check
140:     # HTML tags like <br/>, <sub>, <sup> are valid mermaid syntax
141:     text_without_html = re.sub(r'<[^>]+>', '', text)
142: 
143:     # Only check for truly problematic chars that break mermaid syntax
144:     # Removed: : ? ! / \\ (these are safe in mermaid node text)
145:     special_chars = r'[\[\]\{\}\(\)\|\'"]'
146:     return bool(re.search(special_chars, text_without_html))
147: 
148: 
149: def _apply_fixes_to_block(block: str, fixes: List[str]) -> str:
150:     """Apply fixes to mermaid block.
151: 
152:     Fix strategies:
153:     1. Rename nodes with special chars to simple IDs
154:     2. Add color to style statements
155:     3. Convert newlines to <br/> in node text
156:     """
157:     fixed_block = block
158: 
159:     # Fix 1: Rename problematic nodes
160:     node_pattern = r'([A-Z0-9]+)[\[\{]([^\]\}]+)[\]\}]'
161:     nodes = re.findall(node_pattern, fixed_block)
162:     node_mapping = {}  # old_id -> new_id
163:     next_id = 1
164: 
165:     for node_id, node_text in nodes:
166:         if _has_unquoted_special_chars(node_text):
167:             # Generate new simple ID
168:             new_id = f"N{next_id}"
169:             next_id += 1
170:             node_mapping[node_id] = new_id
171: 
172:             # Replace all occurrences of old node ID
173:             # Pattern: node_id at word boundary (not part of another word)
174:             fixed_block = re.sub(
175:                 rf'\b{node_id}\b',
176:                 new_id,
177:                 fixed_block
178:             )
179: 
180:     # Fix 2: Add color to style statements missing it
181:     style_pattern = r'(style\s+[A-Z0-9]+\s+fill:#[0-9a-fA-F]{3,6})(?!.*color:)'
182: 
183:     def add_color(match):
184:         style_stmt = match.group(1)
185:         # Determine text color based on background lightness
186:         fill_match = re.search(r'fill:(#[0-9a-fA-F]{3,6})', style_stmt)
187:         if fill_match:
188:             bg_color = fill_match.group(1)
189:             text_color = _determine_text_color(bg_color)
190:             return f"{style_stmt},color:{text_color}"
191:         return style_stmt
192: 
193:     fixed_block = re.sub(style_pattern, add_color, fixed_block)
194: 
195:     # Fix 3: Convert multiline text to <br/>
196:     def fix_linebreaks(match):
197:         bracket_type = match.group(1)
198:         close_bracket = ']' if bracket_type == '[' else '}'
199:         content = match.group(2)
200:         fixed_content = content.replace('\n', '<br/>')
201:         return f"{bracket_type}{fixed_content}{close_bracket}"
202: 
203:     fixed_block = re.sub(
204:         r'([\[\{])([^\]\}]*\n[^\]\}]*)([\]\}])',
205:         fix_linebreaks,
206:         fixed_block
207:     )
208: 
209:     return fixed_block
210: 
211: 
212: def _determine_text_color(bg_color: str) -> str:
213:     """Determine text color (#000 or #fff) based on background lightness.
214: 
215:     Uses relative luminance formula:
216:     L = 0.2126 * R + 0.7152 * G + 0.0722 * B
217: 
218:     Args:
219:         bg_color: Hex color (#RGB or #RRGGBB)
220: 
221:     Returns:
222:         '#000' for light backgrounds, '#fff' for dark backgrounds
223:     """
224:     # Expand shorthand hex (#RGB -> #RRGGBB)
225:     if len(bg_color) == 4:  # #RGB
226:         bg_color = f"#{bg_color[1]*2}{bg_color[2]*2}{bg_color[3]*2}"
227: 
228:     # Extract RGB components
229:     r = int(bg_color[1:3], 16) / 255.0
230:     g = int(bg_color[3:5], 16) / 255.0
231:     b = int(bg_color[5:7], 16) / 255.0
232: 
233:     # Apply sRGB gamma correction
234:     def gamma_correct(c):
235:         return c / 12.92 if c <= 0.03928 else ((c + 0.055) / 1.055) ** 2.4
236: 
237:     r = gamma_correct(r)
238:     g = gamma_correct(g)
239:     b = gamma_correct(b)
240: 
241:     # Calculate relative luminance
242:     luminance = 0.2126 * r + 0.7152 * g + 0.0722 * b
243: 
244:     # Return black for light backgrounds, white for dark
245:     return '#000' if luminance > 0.5 else '#fff'
246: 
247: 
248: def lint_all_markdown_mermaid(directory: str = ".", auto_fix: bool = False) -> Dict[str, Any]:
249:     """Lint mermaid diagrams in all markdown files.
250: 
251:     Args:
252:         directory: Root directory to search (default: current)
253:         auto_fix: Apply fixes automatically
254: 
255:     Returns:
256:         Dict with aggregated results
257:     """
258:     md_files = list(Path(directory).rglob("*.md"))
259:     all_errors = []
260:     all_fixes = []
261:     files_with_issues = []
262:     total_diagrams = 0
263: 
264:     for md_file in md_files:
265:         result = validate_mermaid_diagrams(str(md_file), auto_fix=auto_fix)
266:         total_diagrams += result["diagrams_checked"]
267: 
268:         if result["errors"]:
269:             files_with_issues.append(str(md_file))
270:             all_errors.extend([f"{md_file}: {err}" for err in result["errors"]])
271: 
272:         if result["fixes_applied"]:
273:             all_fixes.extend([f"{md_file}: {fix}" for fix in result["fixes_applied"]])
274: 
275:     return {
276:         "success": len(all_errors) == 0 or (auto_fix and len(all_fixes) > 0),
277:         "files_checked": len(md_files),
278:         "diagrams_checked": total_diagrams,
279:         "files_with_issues": len(files_with_issues),
280:         "errors": all_errors,
281:         "fixes_applied": all_fixes
282:     }
283: 
284: 
285: if __name__ == "__main__":
286:     import sys
287: 
288:     # CLI usage: python mermaid_validator.py [--fix] [path]
289:     auto_fix = "--fix" in sys.argv
290:     path = sys.argv[-1] if len(sys.argv) > 1 and not sys.argv[-1].startswith("--") else "."
291: 
292:     result = lint_all_markdown_mermaid(path, auto_fix=auto_fix)
293: 
294:     print(f"\n{'='*80}")
295:     print(f"Mermaid Diagram Validation")
296:     print(f"{'='*80}")
297:     print(f"Files checked: {result['files_checked']}")
298:     print(f"Diagrams checked: {result['diagrams_checked']}")
299:     print(f"Files with issues: {result['files_with_issues']}")
300: 
301:     if result['errors']:
302:         print(f"\n{'='*80}")
303:         print("ERRORS:")
304:         print(f"{'='*80}")
305:         for error in result['errors']:
306:             print(f"âŒ {error}")
307: 
308:     if result['fixes_applied']:
309:         print(f"\n{'='*80}")
310:         print("FIXES APPLIED:")
311:         print(f"{'='*80}")
312:         for fix in result['fixes_applied']:
313:             print(f"âœ… {fix}")
314: 
315:     print(f"\n{'='*80}")
316:     print(f"Result: {'âœ… PASS' if result['success'] else 'âŒ FAIL'}")
317:     print(f"{'='*80}\n")
318: 
319:     sys.exit(0 if result['success'] else 1)
</file>

<file path="tools/ce/metrics.py">
  1: """Metrics collection module - track performance and success rates.
  2: 
  3: Provides lightweight metrics collection for tracking PRP execution success rates,
  4: timing data, and validation results without heavy telemetry infrastructure.
  5: """
  6: 
  7: from typing import Dict, Any, List
  8: from datetime import datetime
  9: import json
 10: from pathlib import Path
 11: 
 12: 
 13: class MetricsCollector:
 14:     """Collect and persist performance metrics.
 15: 
 16:     Tracks success rates, timing data, and validation results.
 17: 
 18:     Example:
 19:         metrics = MetricsCollector()
 20:         metrics.record_prp_execution(
 21:             prp_id="PRP-003",
 22:             success=True,
 23:             duration=1200.5,
 24:             first_pass=True,
 25:             validation_level=4
 26:         )
 27:         metrics.save()
 28: 
 29:     Attributes:
 30:         metrics_file: Path to metrics JSON file
 31:         metrics: Dict containing all collected metrics
 32:     """
 33: 
 34:     def __init__(self, metrics_file: str = "metrics.json"):
 35:         """Initialize metrics collector.
 36: 
 37:         Args:
 38:             metrics_file: Path to metrics JSON file
 39: 
 40:         Note: Creates new metrics file if it doesn't exist.
 41:         """
 42:         self.metrics_file = Path(metrics_file)
 43:         self.metrics: Dict[str, Any] = self._load_metrics()
 44: 
 45:     def _load_metrics(self) -> Dict[str, Any]:
 46:         """Load existing metrics from file.
 47: 
 48:         Returns:
 49:             Dict with metrics data structure
 50: 
 51:         Note: Creates empty structure if file doesn't exist.
 52:         """
 53:         if self.metrics_file.exists():
 54:             try:
 55:                 return json.loads(self.metrics_file.read_text())
 56:             except json.JSONDecodeError:
 57:                 # Corrupted file - start fresh
 58:                 return self._empty_metrics()
 59:         return self._empty_metrics()
 60: 
 61:     def _empty_metrics(self) -> Dict[str, Any]:
 62:         """Create empty metrics structure.
 63: 
 64:         Returns:
 65:             Dict with empty metrics data structure
 66:         """
 67:         return {
 68:             "prp_executions": [],
 69:             "validation_results": [],
 70:             "performance_stats": {}
 71:         }
 72: 
 73:     def record_prp_execution(
 74:         self,
 75:         prp_id: str,
 76:         success: bool,
 77:         duration: float,
 78:         first_pass: bool,
 79:         validation_level: int
 80:     ):
 81:         """Record PRP execution metrics.
 82: 
 83:         Args:
 84:             prp_id: PRP identifier
 85:             success: Whether execution succeeded
 86:             duration: Execution time in seconds
 87:             first_pass: Whether succeeded on first pass
 88:             validation_level: Highest validation level passed (1-4)
 89: 
 90:         Note: Call save() after recording to persist metrics.
 91:         """
 92:         self.metrics["prp_executions"].append({
 93:             "prp_id": prp_id,
 94:             "timestamp": datetime.now().isoformat(),
 95:             "success": success,
 96:             "duration": duration,
 97:             "first_pass": first_pass,
 98:             "validation_level": validation_level
 99:         })
100: 
101:     def record_validation_result(
102:         self,
103:         prp_id: str,
104:         validation_level: int,
105:         passed: bool,
106:         duration: float,
107:         error_message: str = None
108:     ):
109:         """Record validation gate result.
110: 
111:         Args:
112:             prp_id: PRP identifier
113:             validation_level: Validation level (1-4)
114:             passed: Whether validation passed
115:             duration: Validation time in seconds
116:             error_message: Error message if failed
117: 
118:         Note: Call save() after recording to persist metrics.
119:         """
120:         self.metrics["validation_results"].append({
121:             "prp_id": prp_id,
122:             "timestamp": datetime.now().isoformat(),
123:             "validation_level": validation_level,
124:             "passed": passed,
125:             "duration": duration,
126:             "error_message": error_message
127:         })
128: 
129:     def calculate_success_rates(self) -> Dict[str, float]:
130:         """Calculate success rate metrics.
131: 
132:         Returns:
133:             Dict with first_pass_rate, second_pass_rate, overall_rate, total_executions
134: 
135:         Note: Returns 0.0 rates if no executions recorded.
136:         """
137:         executions = self.metrics["prp_executions"]
138:         if not executions:
139:             return {
140:                 "first_pass_rate": 0.0,
141:                 "second_pass_rate": 0.0,
142:                 "overall_rate": 0.0,
143:                 "total_executions": 0
144:             }
145: 
146:         total = len(executions)
147:         first_pass = sum(1 for e in executions if e["first_pass"])
148:         successful = sum(1 for e in executions if e["success"])
149: 
150:         return {
151:             "first_pass_rate": (first_pass / total) * 100,
152:             "second_pass_rate": (successful / total) * 100,
153:             "overall_rate": (successful / total) * 100,
154:             "total_executions": total
155:         }
156: 
157:     def calculate_validation_stats(self) -> Dict[str, Any]:
158:         """Calculate validation gate statistics.
159: 
160:         Returns:
161:             Dict with pass rates per validation level
162: 
163:         Note: Returns empty dict if no validations recorded.
164:         """
165:         validations = self.metrics["validation_results"]
166:         if not validations:
167:             return {}
168: 
169:         # Group by level
170:         by_level = {}
171:         for v in validations:
172:             level = v["validation_level"]
173:             if level not in by_level:
174:                 by_level[level] = {"total": 0, "passed": 0}
175:             by_level[level]["total"] += 1
176:             if v["passed"]:
177:                 by_level[level]["passed"] += 1
178: 
179:         # Calculate pass rates
180:         stats = {}
181:         for level, data in by_level.items():
182:             stats[f"L{level}_pass_rate"] = (data["passed"] / data["total"]) * 100
183:             stats[f"L{level}_total"] = data["total"]
184: 
185:         return stats
186: 
187:     def get_average_duration(self) -> float:
188:         """Calculate average PRP execution duration.
189: 
190:         Returns:
191:             Average duration in seconds, or 0.0 if no executions
192: 
193:         Note: Includes both successful and failed executions.
194:         """
195:         executions = self.metrics["prp_executions"]
196:         if not executions:
197:             return 0.0
198: 
199:         total_duration = sum(e["duration"] for e in executions)
200:         return total_duration / len(executions)
201: 
202:     def save(self):
203:         """Persist metrics to file.
204: 
205:         Raises:
206:             RuntimeError: If file cannot be written
207: 
208:         Note: Creates parent directory if needed.
209:         """
210:         try:
211:             self.metrics_file.parent.mkdir(parents=True, exist_ok=True)
212:             self.metrics_file.write_text(json.dumps(self.metrics, indent=2))
213:         except Exception as e:
214:             raise RuntimeError(
215:                 f"Failed to save metrics to {self.metrics_file}\n"
216:                 f"Error: {str(e)}\n"
217:                 f"ðŸ”§ Troubleshooting:\n"
218:                 f"  1. Check write permissions\n"
219:                 f"  2. Ensure parent directory exists or can be created\n"
220:                 f"  3. Verify disk space available"
221:             ) from e
222: 
223:     def get_summary(self) -> Dict[str, Any]:
224:         """Get comprehensive metrics summary.
225: 
226:         Returns:
227:             Dict with success rates, validation stats, and performance metrics
228: 
229:         Example:
230:             {
231:                 "success_rates": {"first_pass_rate": 85.0, ...},
232:                 "validation_stats": {"L1_pass_rate": 95.0, ...},
233:                 "performance": {"avg_duration": 1200.5, ...}
234:             }
235: 
236:         Note: Useful for status dashboards and reports.
237:         """
238:         return {
239:             "success_rates": self.calculate_success_rates(),
240:             "validation_stats": self.calculate_validation_stats(),
241:             "performance": {
242:                 "avg_duration": self.get_average_duration(),
243:                 "total_prps": len(self.metrics["prp_executions"]),
244:                 "total_validations": len(self.metrics["validation_results"])
245:             }
246:         }
</file>

<file path="tools/ce/pattern_detectors.py">
  1: """Pattern detection helpers for reducing nesting depth in analysis functions.
  2: 
  3: Extracted from code_analyzer.py and update_context.py to reduce nesting from 7/5 levels to 4 max.
  4: """
  5: 
  6: import ast
  7: import re
  8: from pathlib import Path
  9: from typing import Dict, List, Tuple, Set
 10: import logging
 11: 
 12: logger = logging.getLogger(__name__)
 13: 
 14: 
 15: # ============================================================================
 16: # AST Pattern Detection (from code_analyzer.py)
 17: # ============================================================================
 18: 
 19: def process_class_node(node: ast.ClassDef, patterns: Dict[str, List[str]]) -> None:
 20:     """Process class node for patterns (reduces nesting in _analyze_python).
 21: 
 22:     Args:
 23:         node: AST ClassDef node
 24:         patterns: Pattern dict to update
 25:     """
 26:     patterns["code_structure"].append("class-based")
 27: 
 28:     # Check for decorators
 29:     if node.decorator_list:
 30:         process_class_decorators(node, patterns)
 31: 
 32:     # Check naming
 33:     if node.name[0].isupper():
 34:         patterns["naming_conventions"].append("PascalCase")
 35: 
 36: 
 37: def process_class_decorators(node: ast.ClassDef, patterns: Dict[str, List[str]]) -> None:
 38:     """Process class decorators (extracted to reduce nesting).
 39: 
 40:     Args:
 41:         node: AST ClassDef node
 42:         patterns: Pattern dict to update
 43:     """
 44:     for dec in node.decorator_list:
 45:         if isinstance(dec, ast.Name) and dec.id == "dataclass":
 46:             patterns["code_structure"].append("dataclass")
 47: 
 48: 
 49: def process_function_node(node: ast.FunctionDef, patterns: Dict[str, List[str]]) -> None:
 50:     """Process function node for patterns (reduces nesting in _analyze_python).
 51: 
 52:     Args:
 53:         node: AST FunctionDef node
 54:         patterns: Pattern dict to update
 55:     """
 56:     patterns["code_structure"].append("functional")
 57: 
 58:     # Naming conventions
 59:     if "_" in node.name:
 60:         patterns["naming_conventions"].append("snake_case")
 61:     if node.name.startswith("_") and not node.name.startswith("__"):
 62:         patterns["naming_conventions"].append("_private")
 63: 
 64:     # Test patterns
 65:     if node.name.startswith("test_"):
 66:         patterns["test_patterns"].append("pytest")
 67: 
 68:     # Decorators
 69:     if node.decorator_list:
 70:         process_function_decorators(node, patterns)
 71: 
 72: 
 73: def process_function_decorators(node: ast.FunctionDef, patterns: Dict[str, List[str]]) -> None:
 74:     """Process function decorators (extracted to reduce nesting).
 75: 
 76:     Args:
 77:         node: AST FunctionDef node
 78:         patterns: Pattern dict to update
 79:     """
 80:     for dec in node.decorator_list:
 81:         if isinstance(dec, ast.Name):
 82:             if dec.id in ("staticmethod", "classmethod", "property"):
 83:                 patterns["code_structure"].append(f"decorator-{dec.id}")
 84:             elif dec.id == "pytest":
 85:                 patterns["test_patterns"].append("pytest")
 86: 
 87: 
 88: def process_try_node(node: ast.Try, patterns: Dict[str, List[str]]) -> None:
 89:     """Process try/except node for error handling patterns.
 90: 
 91:     Args:
 92:         node: AST Try node
 93:         patterns: Pattern dict to update
 94:     """
 95:     patterns["error_handling"].append("try-except")
 96:     if node.finalbody:
 97:         patterns["error_handling"].append("try-except-finally")
 98: 
 99: 
100: def process_if_node(node: ast.If, patterns: Dict[str, List[str]]) -> None:
101:     """Process if node for guard clause detection.
102: 
103:     Args:
104:         node: AST If node
105:         patterns: Pattern dict to update
106:     """
107:     # Detect guard clauses (early return)
108:     if node.body and isinstance(node.body[0], ast.Return):
109:         patterns["error_handling"].append("early-return")
110: 
111: 
112: def process_import_node(node: ast.ImportFrom, patterns: Dict[str, List[str]]) -> None:
113:     """Process import node for import patterns.
114: 
115:     Args:
116:         node: AST ImportFrom node
117:         patterns: Pattern dict to update
118:     """
119:     if node.level > 0:
120:         patterns["import_patterns"].append("relative")
121:     else:
122:         patterns["import_patterns"].append("absolute")
123: 
124: 
125: # ============================================================================
126: # Drift Detection Pattern Checking (from update_context.py)
127: # ============================================================================
128: 
129: def check_file_for_violations(
130:     py_file: Path,
131:     pattern_checks: Dict[str, List[Tuple[str, str, str]]],
132:     project_root: Path
133: ) -> Tuple[List[str], bool]:
134:     """Check single file for pattern violations (reduces nesting in verify_codebase_matches_examples).
135: 
136:     Args:
137:         py_file: Path to Python file to check
138:         pattern_checks: Dict of pattern categories to check tuples
139:         project_root: Project root path for relative path calculation
140: 
141:     Returns:
142:         Tuple of (violations list, has_violations flag)
143:     """
144:     violations = []
145:     has_violations = False
146: 
147:     try:
148:         content = py_file.read_text()
149: 
150:         # Check each pattern category
151:         for category, checks in pattern_checks.items():
152:             category_violations = check_pattern_category(
153:                 content, checks, py_file, project_root, category
154:             )
155:             if category_violations:
156:                 violations.extend(category_violations)
157:                 has_violations = True
158: 
159:     except Exception as e:
160:         logger.warning(f"Skipping {py_file.name} - read error: {e}")
161: 
162:     return violations, has_violations
163: 
164: 
165: def check_pattern_category(
166:     content: str,
167:     checks: List[Tuple[str, str, str]],
168:     py_file: Path,
169:     project_root: Path,
170:     category: str
171: ) -> List[str]:
172:     """Check file content against pattern category checks using AST.
173: 
174:     Args:
175:         content: File content string
176:         checks: List of (check_name, regex, fix_desc) tuples
177:         py_file: Path to file being checked
178:         project_root: Project root for relative paths
179:         category: Pattern category name
180: 
181:     Returns:
182:         List of violation messages
183: 
184:     Note: Uses AST parsing instead of regex to avoid false positives from
185:     comments/docstrings and to handle multiline code properly.
186:     """
187:     from .update_context import PATTERN_FILES
188: 
189:     violations = []
190: 
191:     try:
192:         tree = ast.parse(content, filename=str(py_file))
193:     except SyntaxError:
194:         # Fallback to regex for files with syntax errors
195:         logger.warning(f"Syntax error in {py_file}, using regex fallback")
196:         return _check_pattern_category_regex(content, checks, py_file, project_root, category)
197: 
198:     for check_name, regex, fix_desc in checks:
199:         # Use AST-based checks for known patterns
200:         if check_name == "missing_troubleshooting":
201:             if _check_missing_troubleshooting_ast(tree, content):
202:                 violations.append(
203:                     f"File {py_file.relative_to(project_root)} has {check_name} "
204:                     f"(violates {PATTERN_FILES.get(category, 'pattern')}): {fix_desc}"
205:                 )
206:         elif check_name == "bare_except":
207:             if _check_bare_except_ast(tree):
208:                 violations.append(
209:                     f"File {py_file.relative_to(project_root)} has {check_name} "
210:                     f"(violates {PATTERN_FILES.get(category, 'pattern')}): {fix_desc}"
211:                 )
212:         else:
213:             # Fallback to regex for other patterns
214:             matches = re.findall(regex, content, re.MULTILINE | re.DOTALL)
215:             if matches:
216:                 violations.append(
217:                     f"File {py_file.relative_to(project_root)} has {check_name} "
218:                     f"(violates {PATTERN_FILES.get(category, 'pattern')}): {fix_desc}"
219:                 )
220: 
221:     return violations
222: 
223: 
224: def _check_missing_troubleshooting_ast(tree: ast.AST, content: str) -> bool:
225:     """Check for raise statements missing ðŸ”§ troubleshooting using AST.
226: 
227:     Args:
228:         tree: Parsed AST tree
229:         content: File content (for emoji check)
230: 
231:     Returns:
232:         True if violations found, False otherwise
233:     """
234:     for node in ast.walk(tree):
235:         if isinstance(node, ast.Raise):
236:             # Get the line where raise occurs
237:             if hasattr(node, 'lineno'):
238:                 # Check if ðŸ”§ appears in the raise message
239:                 # We need to look at the actual source for multiline strings
240:                 raise_line = node.lineno
241:                 # Check 5 lines around the raise statement
242:                 lines = content.split('\n')
243:                 start = max(0, raise_line - 2)
244:                 end = min(len(lines), raise_line + 3)
245:                 context = '\n'.join(lines[start:end])
246: 
247:                 # If this is a raise with an exception instance
248:                 if node.exc and not ('ðŸ”§' in context):
249:                     return True
250: 
251:     return False
252: 
253: 
254: def _check_bare_except_ast(tree: ast.AST) -> bool:
255:     """Check for bare except clauses using AST.
256: 
257:     Args:
258:         tree: Parsed AST tree
259: 
260:     Returns:
261:         True if bare except found, False otherwise
262:     """
263:     for node in ast.walk(tree):
264:         if isinstance(node, ast.Try):
265:             for handler in node.handlers:
266:                 # Bare except has no type specified
267:                 if handler.type is None:
268:                     return True
269: 
270:     return False
271: 
272: 
273: def _check_pattern_category_regex(
274:     content: str,
275:     checks: List[Tuple[str, str, str]],
276:     py_file: Path,
277:     project_root: Path,
278:     category: str
279: ) -> List[str]:
280:     """Regex fallback for files with syntax errors.
281: 
282:     Args:
283:         content: File content string
284:         checks: List of (check_name, regex, fix_desc) tuples
285:         py_file: Path to file being checked
286:         project_root: Project root for relative paths
287:         category: Pattern category name
288: 
289:     Returns:
290:         List of violation messages
291:     """
292:     from .update_context import PATTERN_FILES
293: 
294:     violations = []
295: 
296:     for check_name, regex, fix_desc in checks:
297:         matches = re.findall(regex, content, re.MULTILINE | re.DOTALL)
298:         if matches:
299:             violations.append(
300:                 f"File {py_file.relative_to(project_root)} has {check_name} "
301:                 f"(violates {PATTERN_FILES.get(category, 'pattern')}): {fix_desc}"
302:             )
303: 
304:     return violations
305: 
306: 
307: def check_prp_for_missing_examples(
308:     prp_path: Path,
309:     project_root: Path,
310:     keywords_to_examples: Dict[str, Tuple[str, str, str]]
311: ) -> List[Dict[str, any]]:
312:     """Check single PRP for missing examples (reduces nesting in detect_missing_examples_for_prps).
313: 
314:     Args:
315:         prp_path: Path to PRP file
316:         project_root: Project root path
317:         keywords_to_examples: Mapping of keywords to example info tuples
318: 
319:     Returns:
320:         List of missing example dicts
321:     """
322:     from .update_context import read_prp_header
323: 
324:     missing_examples = []
325: 
326:     try:
327:         metadata, content = read_prp_header(prp_path)
328: 
329:         # Check complexity/risk
330:         complexity = metadata.get("complexity", "unknown")
331:         if complexity not in ["medium", "high"]:
332:             return []
333: 
334:         # Check each keyword pattern
335:         for keyword, (example_name, suggested_path, rationale) in keywords_to_examples.items():
336:             if keyword.lower() in content.lower():
337:                 example_path = project_root / suggested_path
338:                 if not example_path.exists():
339:                     missing_examples.append({
340:                         "prp_id": metadata.get("prp_id", "unknown"),
341:                         "feature_name": metadata.get("feature_name", "unknown"),
342:                         "complexity": complexity,
343:                         "missing_example": example_name,
344:                         "suggested_path": suggested_path,
345:                         "rationale": rationale
346:                     })
347: 
348:     except Exception as e:
349:         logger.warning(f"Skipping {prp_path.name} - read error: {e}")
350: 
351:     return missing_examples
</file>

<file path="tools/ce/pattern_extractor.py">
  1: """Pattern extraction from PRP EXAMPLES sections for L4 validation.
  2: 
  3: This module extracts semantic code patterns from PRP markdown files to enable
  4: architectural drift detection. Uses shared code_analyzer module for actual
  5: pattern detection logic.
  6: """
  7: 
  8: import re
  9: from typing import Dict, List, Any
 10: from pathlib import Path
 11: 
 12: from .code_analyzer import analyze_code_patterns
 13: 
 14: 
 15: def extract_patterns_from_prp(prp_path: str) -> Dict[str, Any]:
 16:     """Extract patterns from PRP's EXAMPLES section or INITIAL.md.
 17: 
 18:     Args:
 19:         prp_path: Path to PRP markdown file
 20: 
 21:     Returns:
 22:         {
 23:             "code_structure": ["async/await", "class-based", "functional"],
 24:             "error_handling": ["try-except", "early-return", "null-checks"],
 25:             "naming_conventions": ["snake_case", "camelCase", "PascalCase"],
 26:             "data_flow": ["props", "state", "context", "closure"],
 27:             "test_patterns": ["pytest", "unittest", "fixtures"],
 28:             "import_patterns": ["relative", "absolute"],
 29:             "raw_examples": [{"language": "python", "code": "..."}]
 30:         }
 31: 
 32:     Raises:
 33:         ValueError: If EXAMPLES section not found or malformed
 34:         FileNotFoundError: If PRP file doesn't exist
 35:     """
 36:     prp_path_obj = Path(prp_path)
 37:     if not prp_path_obj.exists():
 38:         raise FileNotFoundError(
 39:             f"PRP file not found: {prp_path}\n"
 40:             f"ðŸ”§ Troubleshooting:\n"
 41:             f"   - Verify file path is correct\n"
 42:             f"   - Check if file was moved or renamed\n"
 43:             f"   - Use: ls {prp_path_obj.parent} to list directory"
 44:         )
 45: 
 46:     content = prp_path_obj.read_text()
 47: 
 48:     # Extract EXAMPLES section (both standalone and embedded in PRP)
 49:     examples_match = re.search(
 50:         r"##\s+EXAMPLES\s*\n(.*?)(?=\n##|\Z)",
 51:         content,
 52:         re.DOTALL | re.IGNORECASE
 53:     )
 54: 
 55:     if not examples_match:
 56:         raise ValueError(
 57:             f"No EXAMPLES section found in {prp_path}\n"
 58:             f"ðŸ”§ Troubleshooting: Ensure PRP contains '## EXAMPLES' section "
 59:             f"with code blocks showing patterns to follow"
 60:         )
 61: 
 62:     examples_text = examples_match.group(1)
 63: 
 64:     # Extract code blocks
 65:     code_blocks = re.findall(
 66:         r"```(\w+)?\n(.*?)```",
 67:         examples_text,
 68:         re.DOTALL
 69:     )
 70: 
 71:     if not code_blocks:
 72:         raise ValueError(
 73:             f"No code blocks found in EXAMPLES section of {prp_path}\n"
 74:             f"ðŸ”§ Troubleshooting: Add code examples using ```language markers"
 75:         )
 76: 
 77:     raw_examples = []
 78:     all_patterns = {
 79:         "code_structure": [],
 80:         "error_handling": [],
 81:         "naming_conventions": [],
 82:         "data_flow": [],
 83:         "test_patterns": [],
 84:         "import_patterns": [],
 85:         "raw_examples": []
 86:     }
 87: 
 88:     for language, code in code_blocks:
 89:         language = language or "python"  # Default to Python
 90:         raw_examples.append({"language": language, "code": code.strip()})
 91: 
 92:         # Use shared code analyzer
 93:         patterns = analyze_code_patterns(code, language)
 94: 
 95:         # Merge patterns
 96:         for category, values in patterns.items():
 97:             if category in all_patterns:
 98:                 all_patterns[category].extend(values)
 99: 
100:     # Deduplicate patterns
101:     for category in all_patterns:
102:         if category != "raw_examples":
103:             all_patterns[category] = list(set(all_patterns[category]))
104: 
105:     all_patterns["raw_examples"] = raw_examples
106: 
107:     return all_patterns
108: 
109: 
110: def parse_code_structure(code: str, language: str) -> List[str]:
111:     """Identify structural patterns in code example.
112: 
113:     Detects:
114:     - async/await vs callbacks vs synchronous
115:     - class-based vs functional vs procedural
116:     - decorator usage patterns
117:     - context manager patterns
118: 
119:     Args:
120:         code: Source code string
121:         language: Programming language (python, typescript, etc.)
122: 
123:     Returns:
124:         List of detected structural patterns
125:     """
126:     # Use shared code analyzer and extract just code_structure
127:     patterns = analyze_code_patterns(code, language)
128:     return patterns.get("code_structure", [])
</file>

<file path="tools/ce/pipeline.py">
  1: """CI/CD Pipeline abstraction and validation.
  2: 
  3: Provides platform-agnostic pipeline definition and validation.
  4: """
  5: 
  6: from typing import Dict, Any, List
  7: import yaml
  8: import jsonschema
  9: 
 10: 
 11: PIPELINE_SCHEMA = {
 12:     "type": "object",
 13:     "required": ["name", "stages"],
 14:     "properties": {
 15:         "name": {"type": "string"},
 16:         "description": {"type": "string"},
 17:         "stages": {
 18:             "type": "array",
 19:             "items": {
 20:                 "type": "object",
 21:                 "required": ["name", "nodes"],
 22:                 "properties": {
 23:                     "name": {"type": "string"},
 24:                     "nodes": {
 25:                         "type": "array",
 26:                         "items": {
 27:                             "type": "object",
 28:                             "required": ["name", "command"],
 29:                             "properties": {
 30:                                 "name": {"type": "string"},
 31:                                 "command": {"type": "string"},
 32:                                 "strategy": {"type": "string", "enum": ["real", "mock"]},
 33:                                 "timeout": {"type": "integer"}
 34:                             }
 35:                         }
 36:                     },
 37:                     "parallel": {"type": "boolean"},
 38:                     "depends_on": {"type": "array", "items": {"type": "string"}}
 39:                 }
 40:             }
 41:         }
 42:     }
 43: }
 44: 
 45: 
 46: def load_abstract_pipeline(file_path: str) -> Dict[str, Any]:
 47:     """Load abstract pipeline definition from YAML file.
 48: 
 49:     Args:
 50:         file_path: Path to abstract pipeline YAML file
 51: 
 52:     Returns:
 53:         Dict containing pipeline definition
 54: 
 55:     Raises:
 56:         FileNotFoundError: If file doesn't exist
 57:         yaml.YAMLError: If YAML parse fails
 58: 
 59:     Note: No fishy fallbacks - let exceptions propagate for troubleshooting.
 60:     """
 61:     try:
 62:         with open(file_path, 'r') as f:
 63:             pipeline = yaml.safe_load(f)
 64:     except FileNotFoundError:
 65:         raise FileNotFoundError(
 66:             f"Pipeline file not found: {file_path}\n"
 67:             f"ðŸ”§ Troubleshooting: Check the file path is correct"
 68:         )
 69:     except yaml.YAMLError as e:
 70:         raise RuntimeError(
 71:             f"Failed to parse pipeline YAML: {e}\n"
 72:             f"ðŸ”§ Troubleshooting: Validate YAML syntax at the reported line"
 73:         )
 74: 
 75:     return pipeline
 76: 
 77: 
 78: def validate_pipeline(pipeline: Dict[str, Any]) -> Dict[str, Any]:
 79:     """Validate pipeline against schema.
 80: 
 81:     Args:
 82:         pipeline: Pipeline definition dict
 83: 
 84:     Returns:
 85:         Dict with: success (bool), errors (List[str])
 86: 
 87:     Example:
 88:         result = validate_pipeline(pipeline)
 89:         if not result["success"]:
 90:             raise RuntimeError(f"Invalid pipeline: {result['errors']}")
 91:     """
 92:     errors = []
 93: 
 94:     # Schema validation
 95:     try:
 96:         jsonschema.validate(instance=pipeline, schema=PIPELINE_SCHEMA)
 97:     except jsonschema.ValidationError as e:
 98:         errors.append(f"Schema validation failed: {e.message}")
 99:         errors.append(f"ðŸ”§ Troubleshooting: Check required fields: name, stages")
100:         return {"success": False, "errors": errors}
101: 
102:     # Semantic validation - check depends_on references
103:     stage_names = [s["name"] for s in pipeline["stages"]]
104:     for stage in pipeline["stages"]:
105:         if "depends_on" in stage:
106:             for dep in stage["depends_on"]:
107:                 if dep not in stage_names:
108:                     errors.append(
109:                         f"Stage '{stage['name']}' depends on unknown stage '{dep}'\n"
110:                         f"ðŸ”§ Troubleshooting: Available stages: {stage_names}"
111:                     )
112: 
113:     return {
114:         "success": len(errors) == 0,
115:         "errors": errors
116:     }
</file>

<file path="tools/ce/profiling.py">
  1: """Profiling utilities - performance analysis and caching.
  2: 
  3: Provides decorators and utilities for profiling function execution,
  4: caching results, and optimizing performance bottlenecks.
  5: """
  6: 
  7: import cProfile
  8: import pstats
  9: import io
 10: from typing import Callable, Any, Optional
 11: import functools
 12: from datetime import datetime, timedelta
 13: from ce.logging_config import get_logger
 14: 
 15: logger = get_logger(__name__)
 16: 
 17: 
 18: def profile_function(func: Callable) -> Callable:
 19:     """Decorator to profile function execution.
 20: 
 21:     Args:
 22:         func: Function to profile
 23: 
 24:     Returns:
 25:         Wrapped function that prints profile stats
 26: 
 27:     Example:
 28:         @profile_function
 29:         def slow_function():
 30:             # ... expensive operations ...
 31: 
 32:     Note: Profiles every invocation. Use selectively on suspected bottlenecks.
 33:     """
 34:     @functools.wraps(func)
 35:     def wrapper(*args, **kwargs) -> Any:
 36:         profiler = cProfile.Profile()
 37:         profiler.enable()
 38: 
 39:         result = func(*args, **kwargs)
 40: 
 41:         profiler.disable()
 42: 
 43:         # Print stats
 44:         stream = io.StringIO()
 45:         stats = pstats.Stats(profiler, stream=stream)
 46:         stats.sort_stats('cumulative')
 47:         stats.print_stats(20)  # Top 20 functions
 48: 
 49:         logger.info(f"Profile for {func.__name__}:\n{stream.getvalue()}")
 50: 
 51:         return result
 52: 
 53:     return wrapper
 54: 
 55: 
 56: def cache_result(ttl_seconds: int = 300, max_size: int = 128):
 57:     """Decorator to cache function results with TTL and size limit.
 58: 
 59:     Args:
 60:         ttl_seconds: Time-to-live in seconds (default: 300)
 61:         max_size: Maximum cache entries (default: 128)
 62: 
 63:     Returns:
 64:         Decorator function
 65: 
 66:     Example:
 67:         @cache_result(ttl_seconds=600, max_size=256)
 68:         def expensive_computation(x, y):
 69:             return complex_calculation(x, y)
 70: 
 71:     Note: Uses simple dict cache. For production, consider Redis or memcached.
 72:     """
 73:     def decorator(func: Callable) -> Callable:
 74:         cache = {}
 75:         cache_order = []  # Track insertion order for LRU
 76: 
 77:         @functools.wraps(func)
 78:         def wrapper(*args, **kwargs) -> Any:
 79:             # Create cache key (args + kwargs)
 80:             cache_key = (args, tuple(sorted(kwargs.items())))
 81: 
 82:             # Check cache
 83:             if cache_key in cache:
 84:                 result, timestamp = cache[cache_key]
 85:                 if datetime.now() - timestamp < timedelta(seconds=ttl_seconds):
 86:                     logger.debug(f"Cache hit for {func.__name__}")
 87:                     return result
 88:                 else:
 89:                     # Expired - remove
 90:                     del cache[cache_key]
 91:                     cache_order.remove(cache_key)
 92: 
 93:             # Cache miss - compute
 94:             logger.debug(f"Cache miss for {func.__name__}")
 95:             result = func(*args, **kwargs)
 96: 
 97:             # Add to cache (with LRU eviction if needed)
 98:             if len(cache) >= max_size:
 99:                 # Evict oldest entry
100:                 oldest_key = cache_order.pop(0)
101:                 del cache[oldest_key]
102: 
103:             cache[cache_key] = (result, datetime.now())
104:             cache_order.append(cache_key)
105: 
106:             return result
107: 
108:         # Add cache management methods
109:         wrapper.cache_clear = lambda: (cache.clear(), cache_order.clear())
110:         wrapper.cache_info = lambda: {
111:             "hits": sum(1 for k in cache_order if k in cache),
112:             "size": len(cache),
113:             "max_size": max_size,
114:             "ttl_seconds": ttl_seconds
115:         }
116: 
117:         return wrapper
118: 
119:     return decorator
120: 
121: 
122: def time_function(func: Callable) -> Callable:
123:     """Decorator to measure function execution time.
124: 
125:     Args:
126:         func: Function to time
127: 
128:     Returns:
129:         Wrapped function that logs execution time
130: 
131:     Example:
132:         @time_function
133:         def slow_operation():
134:             # ... expensive work ...
135: 
136:     Note: Logs timing via structured logger with duration field.
137:     """
138:     @functools.wraps(func)
139:     def wrapper(*args, **kwargs) -> Any:
140:         start_time = datetime.now()
141: 
142:         result = func(*args, **kwargs)
143: 
144:         duration = (datetime.now() - start_time).total_seconds()
145:         logger.info(
146:             f"Function {func.__name__} completed",
147:             extra={"function": func.__name__, "duration": duration}
148:         )
149: 
150:         return result
151: 
152:     return wrapper
153: 
154: 
155: def memoize(func: Callable) -> Callable:
156:     """Simple memoization decorator (no TTL, no size limit).
157: 
158:     Args:
159:         func: Function to memoize
160: 
161:     Returns:
162:         Memoized function
163: 
164:     Example:
165:         @memoize
166:         def fibonacci(n):
167:             if n < 2:
168:                 return n
169:             return fibonacci(n-1) + fibonacci(n-2)
170: 
171:     Note: Use for pure functions with deterministic output.
172:     For production with TTL/LRU, use cache_result instead.
173:     """
174:     cache = {}
175: 
176:     @functools.wraps(func)
177:     def wrapper(*args, **kwargs):
178:         cache_key = (args, tuple(sorted(kwargs.items())))
179: 
180:         if cache_key not in cache:
181:             cache[cache_key] = func(*args, **kwargs)
182: 
183:         return cache[cache_key]
184: 
185:     wrapper.cache_clear = cache.clear
186:     wrapper.cache_info = lambda: {"size": len(cache)}
187: 
188:     return wrapper
189: 
190: 
191: class PerformanceMonitor:
192:     """Monitor performance metrics across multiple function calls.
193: 
194:     Tracks timing data and call counts for performance analysis.
195: 
196:     Example:
197:         monitor = PerformanceMonitor()
198: 
199:         @monitor.track
200:         def operation1():
201:             # ... work ...
202: 
203:         @monitor.track
204:         def operation2():
205:             # ... work ...
206: 
207:         # Print summary
208:         monitor.print_summary()
209: 
210:     Attributes:
211:         stats: Dict of function stats (call_count, total_time, avg_time)
212:     """
213: 
214:     def __init__(self):
215:         """Initialize performance monitor."""
216:         self.stats = {}
217: 
218:     def track(self, func: Callable) -> Callable:
219:         """Decorator to track function performance.
220: 
221:         Args:
222:             func: Function to track
223: 
224:         Returns:
225:             Wrapped function that records performance stats
226:         """
227:         func_name = func.__name__
228: 
229:         @functools.wraps(func)
230:         def wrapper(*args, **kwargs) -> Any:
231:             start_time = datetime.now()
232: 
233:             result = func(*args, **kwargs)
234: 
235:             duration = (datetime.now() - start_time).total_seconds()
236: 
237:             # Update stats
238:             if func_name not in self.stats:
239:                 self.stats[func_name] = {
240:                     "call_count": 0,
241:                     "total_time": 0.0,
242:                     "avg_time": 0.0
243:                 }
244: 
245:             self.stats[func_name]["call_count"] += 1
246:             self.stats[func_name]["total_time"] += duration
247:             self.stats[func_name]["avg_time"] = (
248:                 self.stats[func_name]["total_time"] / self.stats[func_name]["call_count"]
249:             )
250: 
251:             return result
252: 
253:         return wrapper
254: 
255:     def get_stats(self, func_name: Optional[str] = None) -> dict:
256:         """Get performance statistics.
257: 
258:         Args:
259:             func_name: Optional function name to filter by
260: 
261:         Returns:
262:             Dict of performance stats
263:         """
264:         if func_name:
265:             return self.stats.get(func_name, {})
266:         return self.stats
267: 
268:     def print_summary(self):
269:         """Print performance summary to logger."""
270:         if not self.stats:
271:             logger.info("No performance data collected")
272:             return
273: 
274:         summary = "\nðŸ“Š Performance Summary:\n"
275:         summary += "-" * 60 + "\n"
276:         summary += f"{'Function':<30} {'Calls':<10} {'Total(s)':<12} {'Avg(s)':<10}\n"
277:         summary += "-" * 60 + "\n"
278: 
279:         for func_name, data in sorted(self.stats.items(), key=lambda x: x[1]["total_time"], reverse=True):
280:             summary += f"{func_name:<30} {data['call_count']:<10} {data['total_time']:<12.3f} {data['avg_time']:<10.3f}\n"
281: 
282:         summary += "-" * 60
283: 
284:         logger.info(summary)
285: 
286:     def reset(self):
287:         """Clear all performance statistics."""
288:         self.stats.clear()
</file>

<file path="tools/ce/prp_analyzer.py">
  1: """PRP Size Analyzer and Decomposition Recommender.
  2: 
  3: Analyzes PRP documents for size constraints and provides decomposition
  4: recommendations to prevent "PRP obesity".
  5: """
  6: 
  7: import re
  8: from dataclasses import dataclass
  9: from enum import Enum
 10: from pathlib import Path
 11: from typing import List, Optional
 12: 
 13: 
 14: class SizeCategory(Enum):
 15:     """PRP size categories based on complexity metrics."""
 16:     GREEN = "GREEN"    # Optimal size
 17:     YELLOW = "YELLOW"  # Approaching limits
 18:     RED = "RED"        # Needs decomposition
 19: 
 20: 
 21: @dataclass
 22: class PRPMetrics:
 23:     """Metrics extracted from a PRP document."""
 24:     name: str
 25:     lines: int
 26:     estimated_hours: Optional[str]
 27:     phases: int
 28:     risk_level: str
 29:     functions: int
 30:     success_criteria: int
 31:     file_path: Path
 32: 
 33: 
 34: @dataclass
 35: class PRPAnalysis:
 36:     """Analysis results for a PRP document."""
 37:     metrics: PRPMetrics
 38:     size_category: SizeCategory
 39:     score: float  # 0-100, higher = more complex
 40:     recommendations: List[str]
 41:     decomposition_suggestions: List[str]
 42: 
 43: 
 44: def extract_prp_metrics(prp_file: Path) -> PRPMetrics:
 45:     """Extract size and complexity metrics from a PRP file.
 46: 
 47:     Args:
 48:         prp_file: Path to PRP markdown file
 49: 
 50:     Returns:
 51:         PRPMetrics object with extracted data
 52: 
 53:     Raises:
 54:         FileNotFoundError: If PRP file doesn't exist
 55:         RuntimeError: If metrics extraction fails
 56:     """
 57:     if not prp_file.exists():
 58:         raise FileNotFoundError(
 59:             f"PRP file not found: {prp_file}\n"
 60:             f"ðŸ”§ Troubleshooting: Verify file path and try again"
 61:         )
 62: 
 63:     try:
 64:         content = prp_file.read_text()
 65:     except Exception as e:
 66:         raise RuntimeError(
 67:             f"Failed to read PRP file: {e}\n"
 68:             f"ðŸ”§ Troubleshooting: Check file permissions"
 69:         )
 70: 
 71:     # Extract metrics
 72:     name = prp_file.stem
 73:     lines = len(content.split('\n'))
 74: 
 75:     # Hours - try multiple patterns
 76:     hours_match = re.search(r'estimated_hours:\s*([0-9]+(?:-[0-9]+)?)', content)
 77:     if not hours_match:
 78:         hours_match = re.search(r'Effort.*?([0-9]+-?[0-9]*)\s*hour', content, re.IGNORECASE)
 79:     hours = hours_match.group(1) if hours_match else None
 80: 
 81:     # Phases
 82:     phases = len(re.findall(r'^### Phase [0-9]+', content, re.MULTILINE))
 83: 
 84:     # Risk
 85:     risk_match = re.search(r'\*\*Risk\*\*:\s*(LOW|MEDIUM|HIGH)', content)
 86:     risk = risk_match.group(1) if risk_match else 'UNKNOWN'
 87: 
 88:     # Functions (code examples in PRP)
 89:     functions = len(re.findall(r'def \w+\(', content))
 90: 
 91:     # Success criteria
 92:     criteria = len(re.findall(r'- \[[ x]\]', content))
 93: 
 94:     return PRPMetrics(
 95:         name=name,
 96:         lines=lines,
 97:         estimated_hours=hours,
 98:         phases=phases,
 99:         risk_level=risk,
100:         functions=functions,
101:         success_criteria=criteria,
102:         file_path=prp_file
103:     )
104: 
105: 
106: def calculate_complexity_score(metrics: PRPMetrics) -> float:
107:     """Calculate complexity score (0-100) for a PRP.
108: 
109:     Score formula weights multiple factors:
110:     - Lines: 40% weight (normalized to 1500 lines max)
111:     - Functions: 25% weight (normalized to 40 functions max)
112:     - Criteria: 20% weight (normalized to 50 criteria max)
113:     - Phases: 10% weight (normalized to 15 phases max)
114:     - Risk: 5% weight (LOW=0, MEDIUM=50, HIGH=100)
115: 
116:     Args:
117:         metrics: PRPMetrics object
118: 
119:     Returns:
120:         Complexity score from 0-100
121:     """
122:     # Normalize each metric to 0-100 scale
123:     line_score = min(100, (metrics.lines / 1500) * 100)
124:     function_score = min(100, (metrics.functions / 40) * 100)
125:     criteria_score = min(100, (metrics.success_criteria / 50) * 100)
126:     phase_score = min(100, (metrics.phases / 15) * 100)
127: 
128:     # Risk mapping
129:     risk_map = {'LOW': 0, 'MEDIUM': 50, 'HIGH': 100, 'UNKNOWN': 25}
130:     risk_score = risk_map.get(metrics.risk_level, 25)
131: 
132:     # Weighted average
133:     score = (
134:         line_score * 0.40 +
135:         function_score * 0.25 +
136:         criteria_score * 0.20 +
137:         phase_score * 0.10 +
138:         risk_score * 0.05
139:     )
140: 
141:     return round(score, 2)
142: 
143: 
144: def categorize_prp_size(score: float, metrics: PRPMetrics) -> SizeCategory:
145:     """Determine size category based on complexity score and metrics.
146: 
147:     Thresholds derived from historical PRP analysis:
148:     - GREEN: score < 50, lines < 700, risk LOW-MEDIUM
149:     - YELLOW: score 50-70, lines 700-1000, risk MEDIUM
150:     - RED: score > 70, lines > 1000, risk HIGH
151: 
152:     Args:
153:         score: Complexity score (0-100)
154:         metrics: PRPMetrics object
155: 
156:     Returns:
157:         SizeCategory enum value
158:     """
159:     # Hard constraints for RED
160:     if metrics.lines > 1000 or metrics.risk_level == 'HIGH' or score > 70:
161:         return SizeCategory.RED
162: 
163:     # YELLOW thresholds
164:     if (metrics.lines > 700 or
165:         metrics.functions > 20 or
166:         metrics.success_criteria > 30 or
167:         score > 50):
168:         return SizeCategory.YELLOW
169: 
170:     # GREEN - optimal size
171:     return SizeCategory.GREEN
172: 
173: 
174: def generate_recommendations(metrics: PRPMetrics, score: float, category: SizeCategory) -> List[str]:
175:     """Generate actionable recommendations based on PRP analysis.
176: 
177:     Args:
178:         metrics: PRPMetrics object
179:         score: Complexity score
180:         category: Size category
181: 
182:     Returns:
183:         List of recommendation strings
184:     """
185:     recs = []
186: 
187:     if category == SizeCategory.GREEN:
188:         recs.append("âœ… PRP size is optimal - good job!")
189:         if score > 40:
190:             recs.append("Monitor: Approaching YELLOW threshold, avoid scope creep")
191:         return recs
192: 
193:     if category == SizeCategory.YELLOW:
194:         recs.append("âš ï¸ PRP approaching size limits - consider scope reduction")
195: 
196:         if metrics.lines > 700:
197:             recs.append(f"Lines ({metrics.lines}) approaching RED threshold (1000)")
198: 
199:         if metrics.functions > 20:
200:             recs.append(f"Functions ({metrics.functions}) indicate high implementation complexity")
201: 
202:         if metrics.success_criteria > 30:
203:             recs.append(f"Success criteria ({metrics.success_criteria}) suggest multiple features")
204: 
205:         recs.append("Recommendation: Review if PRP can be split into sub-PRPs")
206:         return recs
207: 
208:     # RED category
209:     recs.append("ðŸš¨ PRP TOO LARGE - decomposition strongly recommended")
210: 
211:     if metrics.lines > 1000:
212:         recs.append(f"Lines ({metrics.lines}) exceed RED threshold - split into sub-PRPs")
213: 
214:     if metrics.risk_level == 'HIGH':
215:         recs.append("HIGH risk rating - isolate risky components into separate PRPs")
216: 
217:     if metrics.functions > 25:
218:         recs.append(f"Functions ({metrics.functions}) indicate multiple features - create sub-PRPs")
219: 
220:     if metrics.phases > 5:
221:         recs.append(f"Phases ({metrics.phases}) could be independent PRPs")
222: 
223:     recs.append("ACTION REQUIRED: Decompose before execution")
224: 
225:     return recs
226: 
227: 
228: def suggest_decomposition(metrics: PRPMetrics) -> List[str]:
229:     """Generate decomposition strategy suggestions.
230: 
231:     Args:
232:         metrics: PRPMetrics object
233: 
234:     Returns:
235:         List of decomposition suggestion strings
236:     """
237:     suggestions = []
238: 
239:     if metrics.phases >= 5:
240:         suggestions.append(
241:             f"Phase-based decomposition: Create {metrics.phases} sub-PRPs "
242:             f"(PRP-X.1 through PRP-X.{metrics.phases})"
243:         )
244:         suggestions.append("Group related phases if some are interdependent")
245: 
246:     if metrics.functions > 20:
247:         suggestions.append(
248:             "Feature-based decomposition: Split by functional area "
249:             "(e.g., parser, validator, executor)"
250:         )
251: 
252:     if metrics.risk_level == 'HIGH':
253:         suggestions.append(
254:             "Risk-based decomposition: Isolate HIGH-risk components "
255:             "into separate PRPs for focused attention"
256:         )
257: 
258:     if metrics.success_criteria > 30:
259:         suggestions.append(
260:             "Criteria-based decomposition: Group related success criteria "
261:             "into logical sub-features"
262:         )
263: 
264:     if not suggestions:
265:         suggestions.append("No decomposition needed - PRP size is manageable")
266: 
267:     return suggestions
268: 
269: 
270: def analyze_prp(prp_file: Path) -> PRPAnalysis:
271:     """Comprehensive PRP size analysis.
272: 
273:     Args:
274:         prp_file: Path to PRP markdown file
275: 
276:     Returns:
277:         PRPAnalysis object with full analysis results
278: 
279:     Raises:
280:         FileNotFoundError: If PRP file doesn't exist
281:         RuntimeError: If analysis fails
282:     """
283:     try:
284:         metrics = extract_prp_metrics(prp_file)
285:         score = calculate_complexity_score(metrics)
286:         category = categorize_prp_size(score, metrics)
287:         recommendations = generate_recommendations(metrics, score, category)
288:         decomposition = suggest_decomposition(metrics)
289: 
290:         return PRPAnalysis(
291:             metrics=metrics,
292:             size_category=category,
293:             score=score,
294:             recommendations=recommendations,
295:             decomposition_suggestions=decomposition
296:         )
297:     except Exception as e:
298:         raise RuntimeError(
299:             f"PRP analysis failed: {e}\n"
300:             f"ðŸ”§ Troubleshooting: Verify PRP file format and try again"
301:         )
302: 
303: 
304: def format_analysis_report(analysis: PRPAnalysis, json_output: bool = False) -> str:
305:     """Format analysis results as human-readable report or JSON.
306: 
307:     Args:
308:         analysis: PRPAnalysis object
309:         json_output: If True, return JSON string
310: 
311:     Returns:
312:         Formatted report string
313:     """
314:     if json_output:
315:         import json
316:         data = {
317:             'name': analysis.metrics.name,
318:             'size_category': analysis.size_category.value,
319:             'complexity_score': analysis.score,
320:             'metrics': {
321:                 'lines': analysis.metrics.lines,
322:                 'hours': analysis.metrics.estimated_hours,
323:                 'phases': analysis.metrics.phases,
324:                 'risk': analysis.metrics.risk_level,
325:                 'functions': analysis.metrics.functions,
326:                 'criteria': analysis.metrics.success_criteria
327:             },
328:             'recommendations': analysis.recommendations,
329:             'decomposition_suggestions': analysis.decomposition_suggestions
330:         }
331:         return json.dumps(data, indent=2)
332: 
333:     # Human-readable format
334:     m = analysis.metrics
335:     lines = [
336:         f"\n{'='*80}",
337:         f"PRP Size Analysis: {m.name}",
338:         f"{'='*80}",
339:         f"\nMetrics:",
340:         f"  Lines:            {m.lines}",
341:         f"  Estimated Hours:  {m.estimated_hours or 'N/A'}",
342:         f"  Phases:           {m.phases}",
343:         f"  Risk Level:       {m.risk_level}",
344:         f"  Functions:        {m.functions}",
345:         f"  Success Criteria: {m.success_criteria}",
346:         f"\nComplexity Score: {analysis.score}/100",
347:         f"Size Category:    {analysis.size_category.value}",
348:         f"\nRecommendations:",
349:     ]
350: 
351:     for rec in analysis.recommendations:
352:         lines.append(f"  â€¢ {rec}")
353: 
354:     lines.append("\nDecomposition Suggestions:")
355:     for sug in analysis.decomposition_suggestions:
356:         lines.append(f"  â€¢ {sug}")
357: 
358:     lines.append(f"\n{'='*80}\n")
359: 
360:     return '\n'.join(lines)
</file>

<file path="tools/ce/prp.py">
  1: """PRP YAML validation and state management module."""
  2: from typing import Dict, Any, List, Optional
  3: import yaml
  4: import re
  5: import json
  6: import logging
  7: from pathlib import Path
  8: from datetime import datetime, timezone
  9: 
 10: # Required fields schema
 11: REQUIRED_FIELDS = [
 12:     "name", "description", "prp_id", "status", "priority",
 13:     "confidence", "effort_hours", "risk", "dependencies",
 14:     "parent_prp", "context_memories", "meeting_evidence",
 15:     "context_sync", "version", "created_date", "last_updated"
 16: ]
 17: 
 18: # Valid enum values
 19: VALID_STATUS = ["ready", "in_progress", "executed", "validated", "archived"]
 20: VALID_PRIORITY = ["HIGH", "MEDIUM", "LOW"]
 21: VALID_RISK = ["LOW", "MEDIUM", "HIGH"]
 22: VALID_PHASES = ["planning", "implementation", "testing", "validation", "complete"]
 23: 
 24: # State file paths
 25: STATE_DIR = Path(".ce")
 26: STATE_FILE = STATE_DIR / "active_prp_session"
 27: 
 28: # Configure logging
 29: logger = logging.getLogger(__name__)
 30: 
 31: 
 32: def validate_prp_yaml(file_path: str) -> Dict[str, Any]:
 33:     """Validate PRP YAML header against schema.
 34: 
 35:     Args:
 36:         file_path: Path to PRP markdown file
 37: 
 38:     Returns:
 39:         Dict with: success (bool), errors (list), warnings (list), header (dict)
 40: 
 41:     Raises:
 42:         FileNotFoundError: If file doesn't exist
 43:         yaml.YAMLError: If YAML parse fails
 44:     """
 45:     errors = []
 46:     warnings = []
 47: 
 48:     # Check file exists
 49:     path = Path(file_path)
 50:     if not path.exists():
 51:         raise FileNotFoundError(
 52:             f"PRP file not found: {file_path}\n"
 53:             f"ðŸ”§ Troubleshooting: Verify file path is correct"
 54:         )
 55: 
 56:     # Read file
 57:     content = path.read_text()
 58: 
 59:     # Check YAML delimiters
 60:     if not content.startswith("---\n"):
 61:         errors.append("Missing YAML front matter: file must start with '---'")
 62:         return {"success": False, "errors": errors, "warnings": warnings, "header": None}
 63: 
 64:     # Extract YAML header
 65:     parts = content.split("---", 2)
 66:     if len(parts) < 3:
 67:         errors.append("Missing closing '---' delimiter for YAML header")
 68:         return {"success": False, "errors": errors, "warnings": warnings, "header": None}
 69: 
 70:     yaml_content = parts[1].strip()
 71: 
 72:     # Parse YAML
 73:     try:
 74:         header = yaml.safe_load(yaml_content)
 75:     except yaml.YAMLError as e:
 76:         errors.append(f"YAML parse error: {str(e)}")
 77:         return {"success": False, "errors": errors, "warnings": warnings, "header": None}
 78: 
 79:     # Validate schema
 80:     return validate_schema(header, errors, warnings)
 81: 
 82: 
 83: def validate_schema(header: Dict[str, Any], errors: List[str], warnings: List[str]) -> Dict[str, Any]:
 84:     """Validate YAML header against schema."""
 85: 
 86:     # Check required fields
 87:     missing_fields = [f for f in REQUIRED_FIELDS if f not in header]
 88:     if missing_fields:
 89:         errors.append(f"Missing required fields: {', '.join(missing_fields)}")
 90: 
 91:     # Validate PRP ID format
 92:     if "prp_id" in header:
 93:         error = validate_prp_id_format(header["prp_id"])
 94:         if error:
 95:             errors.append(error)
 96: 
 97:     # Validate date formats
 98:     for date_field in ["created_date", "last_updated"]:
 99:         if date_field in header:
100:             error = validate_date_format(header[date_field], date_field)
101:             if error:
102:                 errors.append(error)
103: 
104:     # Validate status enum
105:     if "status" in header and header["status"] not in VALID_STATUS:
106:         errors.append(
107:             f"Invalid status: '{header['status']}' (must be one of: {', '.join(VALID_STATUS)})"
108:         )
109: 
110:     # Validate priority enum
111:     if "priority" in header and header["priority"] not in VALID_PRIORITY:
112:         errors.append(
113:             f"Invalid priority: '{header['priority']}' (must be one of: {', '.join(VALID_PRIORITY)})"
114:         )
115: 
116:     # Validate risk enum
117:     if "risk" in header and header["risk"] not in VALID_RISK:
118:         errors.append(
119:             f"Invalid risk: '{header['risk']}' (must be one of: {', '.join(VALID_RISK)})"
120:         )
121: 
122:     # Validate confidence format (X/10)
123:     if "confidence" in header:
124:         conf_str = str(header["confidence"])
125:         if not re.match(r'^\d{1,2}/10$', conf_str):
126:             errors.append(f"Invalid confidence format: '{conf_str}' (expected: X/10 where X is 1-10)")
127: 
128:     # Validate effort_hours is numeric
129:     if "effort_hours" in header:
130:         try:
131:             float(header["effort_hours"])
132:         except (ValueError, TypeError):
133:             errors.append(f"Invalid effort_hours: '{header['effort_hours']}' (must be numeric)")
134: 
135:     # Validate dependencies is list
136:     if "dependencies" in header and not isinstance(header["dependencies"], list):
137:         errors.append(f"Invalid dependencies: must be a list, got {type(header['dependencies']).__name__}")
138: 
139:     # Validate context_memories is list
140:     if "context_memories" in header and not isinstance(header["context_memories"], list):
141:         errors.append(f"Invalid context_memories: must be a list, got {type(header['context_memories']).__name__}")
142: 
143:     # Warnings for optional fields
144:     if not header.get("task_id"):
145:         warnings.append("Optional field 'task_id' is empty (consider linking to issue tracker)")
146: 
147:     success = len(errors) == 0
148:     return {
149:         "success": success,
150:         "errors": errors,
151:         "warnings": warnings,
152:         "header": header
153:     }
154: 
155: 
156: def validate_prp_id_format(prp_id: str) -> Optional[str]:
157:     """Validate PRP ID format (PRP-X.Y or PRP-X.Y.Z).
158: 
159:     Returns:
160:         Error message if invalid, None if valid
161:     """
162:     # Pattern: PRP-X.Y or PRP-X.Y.Z (no leading zeros)
163:     pattern = r'^PRP-([1-9]\d*)(\.(0|[1-9]\d*))?(\.(0|[1-9]\d*))?$'
164:     if not re.match(pattern, prp_id):
165:         return f"Invalid PRP ID format: '{prp_id}' (expected: PRP-X.Y or PRP-X.Y.Z, no leading zeros)"
166:     return None
167: 
168: 
169: def validate_date_format(date_str: str, field_name: str) -> Optional[str]:
170:     """Validate ISO 8601 date format.
171: 
172:     Returns:
173:         Error message if invalid, None if valid
174:     """
175:     pattern = r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z$'
176:     if not re.match(pattern, date_str):
177:         return f"Invalid date format for '{field_name}': '{date_str}' (expected: YYYY-MM-DDTHH:MM:SSZ)"
178:     return None
179: 
180: 
181: def format_validation_result(result: Dict[str, Any]) -> str:
182:     """Format validation result for human-readable output."""
183:     if result["success"]:
184:         output = "âœ… YAML validation passed\n\n"
185:         output += f"PRP ID: {result['header']['prp_id']}\n"
186:         output += f"Name: {result['header']['name']}\n"
187:         output += f"Status: {result['header']['status']}\n"
188:         output += f"Effort: {result['header']['effort_hours']}h\n"
189: 
190:         if result["warnings"]:
191:             output += "\nâš ï¸  Warnings:\n"
192:             for warning in result["warnings"]:
193:                 output += f"  - {warning}\n"
194:     else:
195:         output = "âŒ YAML validation failed\n\n"
196:         output += "Errors:\n"
197:         for error in result["errors"]:
198:             output += f"  âŒ {error}\n"
199: 
200:         if result["warnings"]:
201:             output += "\nWarnings:\n"
202:             for warning in result["warnings"]:
203:                 output += f"  âš ï¸  {warning}\n"
204: 
205:         output += "\nðŸ”§ Troubleshooting: Review docs/prp-yaml-schema.md for schema reference"
206: 
207:     return output
208: 
209: 
210: # ============================================================================
211: # PRP State Management Functions
212: # ============================================================================
213: 
214: def _write_state(state: Dict[str, Any]) -> None:
215:     """Write state to file using atomic write pattern."""
216:     STATE_DIR.mkdir(exist_ok=True)
217:     temp_file = STATE_FILE.with_suffix(".tmp")
218:     temp_file.write_text(json.dumps(state, indent=2))
219:     temp_file.replace(STATE_FILE)
220: 
221: 
222: def start_prp(prp_id: str, prp_name: Optional[str] = None) -> Dict[str, Any]:
223:     """Initialize PRP execution context.
224: 
225:     Creates .ce/active_prp_session file and initializes state tracking.
226: 
227:     Args:
228:         prp_id: PRP identifier (e.g., "PRP-003")
229:         prp_name: Optional PRP name for display
230: 
231:     Returns:
232:         {
233:             "success": True,
234:             "prp_id": "PRP-003",
235:             "started_at": "2025-10-12T14:30:00Z",
236:             "message": "PRP-003 context initialized"
237:         }
238: 
239:     Raises:
240:         RuntimeError: If another PRP is active (call cleanup first)
241:         ValueError: If prp_id format invalid
242:     """
243:     # Validate PRP ID format
244:     error = validate_prp_id_format(prp_id)
245:     if error:
246:         raise ValueError(
247:             f"{error}\n"
248:             f"ðŸ”§ Troubleshooting: Use format PRP-X or PRP-X.Y"
249:         )
250: 
251:     # Check if another PRP is active
252:     active = get_active_prp()
253:     if active:
254:         raise RuntimeError(
255:             f"Another PRP is active: {active['prp_id']}\n"
256:             f"ðŸ”§ Troubleshooting: Run 'ce prp cleanup {active['prp_id']}' or 'ce prp end {active['prp_id']}' first"
257:         )
258: 
259:     # Initialize state
260:     started_at = datetime.now(timezone.utc).isoformat()
261:     state = {
262:         "prp_id": prp_id,
263:         "prp_name": prp_name or prp_id,
264:         "started_at": started_at,
265:         "phase": "planning",
266:         "last_checkpoint": None,
267:         "checkpoint_count": 0,
268:         "validation_attempts": {
269:             "L1": 0,
270:             "L2": 0,
271:             "L3": 0,
272:             "L4": 0
273:         },
274:         "serena_memories": []
275:     }
276: 
277:     _write_state(state)
278:     logger.info(f"Started {prp_id} execution context")
279: 
280:     return {
281:         "success": True,
282:         "prp_id": prp_id,
283:         "started_at": started_at,
284:         "message": f"{prp_id} context initialized"
285:     }
286: 
287: 
288: def get_active_prp() -> Optional[Dict[str, Any]]:
289:     """Get current active PRP session.
290: 
291:     Returns:
292:         State dict if PRP active, None if no active session
293: 
294:     Example:
295:         >>> state = get_active_prp()
296:         >>> if state:
297:         ...     print(f"Active: {state['prp_id']}")
298:         ... else:
299:         ...     print("No active PRP")
300:     """
301:     if not STATE_FILE.exists():
302:         return None
303: 
304:     try:
305:         return json.loads(STATE_FILE.read_text())
306:     except (json.JSONDecodeError, OSError) as e:
307:         logger.warning(f"Failed to read state file: {e}")
308:         return None
309: 
310: 
311: def end_prp(prp_id: str) -> Dict[str, Any]:
312:     """End PRP execution context (without cleanup).
313: 
314:     Removes .ce/active_prp_session file. Use cleanup_prp() for full cleanup.
315: 
316:     Args:
317:         prp_id: PRP identifier to end
318: 
319:     Returns:
320:         {
321:             "success": True,
322:             "duration": "2h 15m",
323:             "checkpoints_created": 3
324:         }
325: 
326:     Raises:
327:         RuntimeError: If prp_id doesn't match active PRP
328:     """
329:     active = get_active_prp()
330:     if not active:
331:         raise RuntimeError(
332:             f"No active PRP session\n"
333:             f"ðŸ”§ Troubleshooting: Use 'ce prp status' to check current state"
334:         )
335: 
336:     if active["prp_id"] != prp_id:
337:         raise RuntimeError(
338:             f"PRP ID mismatch: active={active['prp_id']}, requested={prp_id}\n"
339:             f"ðŸ”§ Troubleshooting: End the active PRP first: 'ce prp end {active['prp_id']}'"
340:         )
341: 
342:     # Calculate duration
343:     started = datetime.fromisoformat(active["started_at"])
344:     ended = datetime.now(timezone.utc)
345:     duration_seconds = (ended - started).total_seconds()
346:     hours = int(duration_seconds // 3600)
347:     minutes = int((duration_seconds % 3600) // 60)
348:     duration = f"{hours}h {minutes}m" if hours > 0 else f"{minutes}m"
349: 
350:     # Remove state file
351:     STATE_FILE.unlink(missing_ok=True)
352:     logger.info(f"Ended {prp_id} execution context")
353: 
354:     return {
355:         "success": True,
356:         "duration": duration,
357:         "checkpoints_created": active["checkpoint_count"]
358:     }
359: 
360: 
361: def update_prp_phase(phase: str) -> Dict[str, Any]:
362:     """Update current PRP phase in state file.
363: 
364:     Args:
365:         phase: Phase name (e.g., "implementation", "testing", "validation")
366:                Valid phases: planning, implementation, testing, validation, complete
367: 
368:     Returns:
369:         Updated state dict
370: 
371:     Raises:
372:         RuntimeError: If no active PRP session
373:         ValueError: If phase not in valid phases list
374:     """
375:     if phase not in VALID_PHASES:
376:         raise ValueError(
377:             f"Invalid phase: '{phase}' (must be one of: {', '.join(VALID_PHASES)})\n"
378:             f"ðŸ”§ Troubleshooting: Use a valid phase name"
379:         )
380: 
381:     active = get_active_prp()
382:     if not active:
383:         raise RuntimeError(
384:             f"No active PRP session\n"
385:             f"ðŸ”§ Troubleshooting: Start a PRP first with 'ce prp start PRP-XXX'"
386:         )
387: 
388:     active["phase"] = phase
389:     _write_state(active)
390:     logger.info(f"Updated {active['prp_id']} phase to: {phase}")
391: 
392:     return active
393: 
394: 
395: # ============================================================================
396: # Checkpoint Management Functions
397: # ============================================================================
398: 
399: def create_checkpoint(phase: str, message: Optional[str] = None) -> Dict[str, Any]:
400:     """Create PRP-scoped git checkpoint.
401: 
402:     Args:
403:         phase: Phase identifier (e.g., "phase1", "phase2", "final")
404:         message: Optional checkpoint message (defaults to phase name)
405: 
406:     Returns:
407:         {
408:             "success": True,
409:             "tag_name": "checkpoint-PRP-003-phase1-20251012-143000",
410:             "commit_sha": "a1b2c3d",
411:             "message": "Phase 1 complete: Core logic implemented"
412:         }
413: 
414:     Raises:
415:         RuntimeError: If no active PRP or git operation fails
416:         RuntimeError: If working tree not clean (uncommitted changes)
417: 
418:     Side Effects:
419:         - Creates git annotated tag
420:         - Updates .ce/active_prp_session with last_checkpoint
421:         - Increments checkpoint_count
422:         - Serena memory handling:
423:           * If Serena available: writes checkpoint metadata to memory
424:           * If Serena unavailable: logs warning, continues successfully
425:           * Never fails on Serena unavailability
426:     """
427:     from .core import run_cmd
428: 
429:     # Verify active PRP
430:     active = get_active_prp()
431:     if not active:
432:         raise RuntimeError(
433:             f"No active PRP session\n"
434:             f"ðŸ”§ Troubleshooting: Start a PRP first with 'ce prp start PRP-XXX'"
435:         )
436: 
437:     # Check git working tree clean
438:     status_result = run_cmd("git status --porcelain")
439:     if not status_result["success"]:
440:         raise RuntimeError(
441:             f"Failed to check git status: {status_result['stderr']}\n"
442:             f"ðŸ”§ Troubleshooting: Ensure you're in a git repository"
443:         )
444: 
445:     if status_result["stdout"].strip():
446:         raise RuntimeError(
447:             f"Working tree has uncommitted changes\n"
448:             f"ðŸ”§ Troubleshooting: Commit or stash changes before creating checkpoint"
449:         )
450: 
451:     # Generate timestamp and tag name
452:     timestamp = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
453:     tag_name = f"checkpoint-{active['prp_id']}-{phase}-{timestamp}"
454: 
455:     # Get current commit SHA
456:     sha_result = run_cmd("git rev-parse HEAD")
457:     if not sha_result["success"]:
458:         raise RuntimeError(
459:             f"Failed to get commit SHA: {sha_result['stderr']}\n"
460:             f"ðŸ”§ Troubleshooting: Ensure you're in a git repository with commits"
461:         )
462:     commit_sha = sha_result["stdout"].strip()[:7]
463: 
464:     # Create annotated tag
465:     tag_message = message or f"{phase} checkpoint"
466:     tag_result = run_cmd(f'git tag -a "{tag_name}" -m "{tag_message}"')
467:     if not tag_result["success"]:
468:         raise RuntimeError(
469:             f"Failed to create checkpoint tag: {tag_result['stderr']}\n"
470:             f"ðŸ”§ Troubleshooting: Ensure git is configured correctly"
471:         )
472: 
473:     # Update state
474:     active["last_checkpoint"] = tag_name
475:     active["checkpoint_count"] += 1
476:     _write_state(active)
477: 
478:     logger.info(f"Created checkpoint: {tag_name}")
479: 
480:     return {
481:         "success": True,
482:         "tag_name": tag_name,
483:         "commit_sha": commit_sha,
484:         "message": tag_message
485:     }
486: 
487: 
488: def list_checkpoints(prp_id: Optional[str] = None) -> List[Dict[str, Any]]:
489:     """List all checkpoints for PRP(s).
490: 
491:     Args:
492:         prp_id: Optional PRP filter (None = all PRPs)
493: 
494:     Returns:
495:         List of checkpoint dicts:
496:         [
497:             {
498:                 "tag_name": "checkpoint-PRP-003-phase1-20251012-143000",
499:                 "prp_id": "PRP-003",
500:                 "phase": "phase1",
501:                 "timestamp": "2025-10-12T14:30:00Z",
502:                 "commit_sha": "a1b2c3d",
503:                 "message": "Phase 1 complete"
504:             },
505:             ...
506:         ]
507: 
508:     Example:
509:         >>> checkpoints = list_checkpoints("PRP-003")
510:         >>> for cp in checkpoints:
511:         ...     print(f"{cp['phase']}: {cp['message']}")
512:     """
513:     from .core import run_cmd
514: 
515:     # Get all tags
516:     tags_result = run_cmd("git tag -l 'checkpoint-*' --format='%(refname:short)|%(subject)|%(objectname:short)'")
517:     if not tags_result["success"]:
518:         logger.warning(f"Failed to list tags: {tags_result['stderr']}")
519:         return []
520: 
521:     if not tags_result["stdout"].strip():
522:         return []
523: 
524:     checkpoints = []
525:     for line in tags_result["stdout"].strip().split("\n"):
526:         parts = line.split("|")
527:         if len(parts) < 3:
528:             continue
529: 
530:         tag_name, tag_message, commit_sha = parts[0], parts[1], parts[2]
531: 
532:         # Parse tag name: checkpoint-{prp_id}-{phase}-{timestamp}
533:         if not tag_name.startswith("checkpoint-"):
534:             continue
535: 
536:         tag_parts = tag_name.split("-", 3)  # Split: ["checkpoint", "PRP", "X", "phase-YYYYMMDD-HHMMSS"]
537:         if len(tag_parts) < 4:
538:             continue
539: 
540:         checkpoint_prp_id = f"{tag_parts[1]}-{tag_parts[2]}"  # "PRP-X"
541: 
542:         # Filter by prp_id if provided
543:         if prp_id and checkpoint_prp_id != prp_id:
544:             continue
545: 
546:         # Extract phase and timestamp
547:         remaining = tag_parts[3]  # "phase1-20251012-143000"
548:         phase_timestamp = remaining.rsplit("-", 2)  # Split from right to preserve phase name
549:         if len(phase_timestamp) == 3:
550:             phase = phase_timestamp[0]
551:             timestamp_str = f"{phase_timestamp[1]}-{phase_timestamp[2]}"
552:             # Convert timestamp to ISO format
553:             try:
554:                 dt = datetime.strptime(timestamp_str, "%Y%m%d-%H%M%S")
555:                 timestamp_iso = dt.replace(tzinfo=timezone.utc).isoformat()
556:             except ValueError:
557:                 timestamp_iso = timestamp_str
558:         else:
559:             phase = remaining
560:             timestamp_iso = ""
561: 
562:         checkpoints.append({
563:             "tag_name": tag_name,
564:             "prp_id": checkpoint_prp_id,
565:             "phase": phase,
566:             "timestamp": timestamp_iso,
567:             "commit_sha": commit_sha,
568:             "message": tag_message
569:         })
570: 
571:     return checkpoints
572: 
573: 
574: def restore_checkpoint(prp_id: str, phase: Optional[str] = None) -> Dict[str, Any]:
575:     """Restore to PRP checkpoint.
576: 
577:     Args:
578:         prp_id: PRP identifier
579:         phase: Optional phase (defaults to latest checkpoint)
580: 
581:     Returns:
582:         {
583:             "success": True,
584:             "restored_to": "checkpoint-PRP-003-phase1-20251012-143000",
585:             "commit_sha": "a1b2c3d"
586:         }
587: 
588:     Raises:
589:         RuntimeError: If checkpoint not found or git operation fails
590:         RuntimeError: If working tree not clean (uncommitted changes)
591: 
592:     Warning:
593:         This is a destructive operation. Uncommitted changes will be lost.
594:     """
595:     from .core import run_cmd
596:     import sys
597: 
598:     # Check working tree clean
599:     status_result = run_cmd("git status --porcelain")
600:     if not status_result["success"]:
601:         raise RuntimeError(
602:             f"Failed to check git status: {status_result['stderr']}\n"
603:             f"ðŸ”§ Troubleshooting: Ensure you're in a git repository"
604:         )
605: 
606:     if status_result["stdout"].strip():
607:         raise RuntimeError(
608:             f"Working tree has uncommitted changes\n"
609:             f"ðŸ”§ Troubleshooting: Commit or stash changes before restoring checkpoint"
610:         )
611: 
612:     # Find checkpoint
613:     checkpoints = list_checkpoints(prp_id)
614:     if not checkpoints:
615:         raise RuntimeError(
616:             f"No checkpoints found for {prp_id}\n"
617:             f"ðŸ”§ Troubleshooting: Create a checkpoint first with 'ce prp checkpoint <phase>'"
618:         )
619: 
620:     # Select checkpoint
621:     if phase:
622:         checkpoint = next((cp for cp in checkpoints if cp["phase"] == phase), None)
623:         if not checkpoint:
624:             phases = [cp["phase"] for cp in checkpoints]
625:             raise RuntimeError(
626:                 f"No checkpoint found for phase '{phase}' in {prp_id}\n"
627:                 f"Available phases: {', '.join(phases)}\n"
628:                 f"ðŸ”§ Troubleshooting: Use 'ce prp list' to see available checkpoints"
629:             )
630:     else:
631:         # Use latest (by timestamp)
632:         checkpoints.sort(key=lambda x: x["timestamp"], reverse=True)
633:         checkpoint = checkpoints[0]
634: 
635:     # Confirmation if interactive
636:     if sys.stdout.isatty():
637:         response = input(f"Restore to {checkpoint['tag_name']}? This will discard uncommitted changes. [y/N] ")
638:         if response.lower() != "y":
639:             return {"success": False, "message": "Restore cancelled by user"}
640: 
641:     # Restore to checkpoint
642:     checkout_result = run_cmd(f"git checkout {checkpoint['tag_name']}")
643:     if not checkout_result["success"]:
644:         raise RuntimeError(
645:             f"Failed to restore checkpoint: {checkout_result['stderr']}\n"
646:             f"ðŸ”§ Troubleshooting: Ensure tag exists and git is configured correctly"
647:         )
648: 
649:     logger.info(f"Restored to checkpoint: {checkpoint['tag_name']}")
650: 
651:     return {
652:         "success": True,
653:         "restored_to": checkpoint["tag_name"],
654:         "commit_sha": checkpoint["commit_sha"]
655:     }
656: 
657: 
658: def delete_intermediate_checkpoints(prp_id: str, keep_final: bool = True) -> Dict[str, Any]:
659:     """Delete intermediate checkpoints (part of cleanup protocol).
660: 
661:     Args:
662:         prp_id: PRP identifier
663:         keep_final: Keep *-final checkpoint for rollback (default: True)
664: 
665:     Returns:
666:         {
667:             "success": True,
668:             "deleted_count": 2,
669:             "kept": ["checkpoint-PRP-003-final-20251012-160000"]
670:         }
671: 
672:     Process:
673:         1. List all checkpoints for prp_id
674:         2. Filter: keep *-final if keep_final=True
675:         3. Delete remaining tags: git tag -d {tag_name}
676:     """
677:     from .core import run_cmd
678: 
679:     checkpoints = list_checkpoints(prp_id)
680:     if not checkpoints:
681:         return {"success": True, "deleted_count": 0, "kept": []}
682: 
683:     to_delete = []
684:     kept = []
685: 
686:     for checkpoint in checkpoints:
687:         if keep_final and checkpoint["phase"] == "final":
688:             kept.append(checkpoint["tag_name"])
689:         else:
690:             to_delete.append(checkpoint["tag_name"])
691: 
692:     # Delete tags
693:     deleted_count = 0
694:     for tag_name in to_delete:
695:         result = run_cmd(f"git tag -d {tag_name}")
696:         if result["success"]:
697:             deleted_count += 1
698:             logger.info(f"Deleted checkpoint: {tag_name}")
699:         else:
700:             logger.warning(f"Failed to delete tag {tag_name}: {result['stderr']}")
701: 
702:     return {
703:         "success": True,
704:         "deleted_count": deleted_count,
705:         "kept": kept
706:     }
707: 
708: 
709: # ============================================================================
710: # Memory Isolation Functions
711: # ============================================================================
712: 
713: def write_prp_memory(category: str, name: str, content: str) -> Dict[str, Any]:
714:     """Write Serena memory with PRP namespace.
715: 
716:     Args:
717:         category: Memory category (checkpoint, learnings, temp)
718:         name: Memory identifier
719:         content: Memory content (markdown)
720: 
721:     Returns:
722:         {
723:             "success": True,
724:             "memory_name": "PRP-003-checkpoint-phase1",
725:             "serena_available": True
726:         }
727: 
728:     Raises:
729:         RuntimeError: If no active PRP
730:         Warning: If Serena MCP unavailable (logs warning, continues)
731: 
732:     Side Effects:
733:         - Calls serena.write_memory(f"{prp_id}-{category}-{name}", content)
734:         - Updates .ce/active_prp_session serena_memories list
735:     """
736:     active = get_active_prp()
737:     if not active:
738:         raise RuntimeError(
739:             f"No active PRP session\n"
740:             f"ðŸ”§ Troubleshooting: Start a PRP first with 'ce prp start PRP-XXX'"
741:         )
742: 
743:     memory_name = f"{active['prp_id']}-{category}-{name}"
744:     serena_available = False
745: 
746:     # Try to write to Serena (optional)
747:     try:
748:         # Check if mcp__serena__write_memory tool is available
749:         # For now, we'll just log that Serena is not available
750:         # In production, this would call the Serena MCP tool
751:         logger.warning(f"Serena MCP not available - skipping memory write for {memory_name}")
752:     except Exception as e:
753:         logger.warning(f"Failed to write Serena memory: {e}")
754: 
755:     # Update state file
756:     if memory_name not in active["serena_memories"]:
757:         active["serena_memories"].append(memory_name)
758:         _write_state(active)
759: 
760:     return {
761:         "success": True,
762:         "memory_name": memory_name,
763:         "serena_available": serena_available
764:     }
765: 
766: 
767: def read_prp_memory(category: str, name: str) -> Optional[str]:
768:     """Read Serena memory with PRP namespace.
769: 
770:     Args:
771:         category: Memory category
772:         name: Memory identifier
773: 
774:     Returns:
775:         Memory content if exists, None otherwise
776: 
777:     Raises:
778:         RuntimeError: If no active PRP
779:     """
780:     active = get_active_prp()
781:     if not active:
782:         raise RuntimeError(
783:             f"No active PRP session\n"
784:             f"ðŸ”§ Troubleshooting: Start a PRP first with 'ce prp start PRP-XXX'"
785:         )
786: 
787:     memory_name = f"{active['prp_id']}-{category}-{name}"
788: 
789:     # Try to read from Serena (optional)
790:     try:
791:         # In production, this would call the Serena MCP tool
792:         logger.warning(f"Serena MCP not available - cannot read memory {memory_name}")
793:         return None
794:     except Exception as e:
795:         logger.warning(f"Failed to read Serena memory: {e}")
796:         return None
797: 
798: 
799: def list_prp_memories(prp_id: Optional[str] = None) -> List[str]:
800:     """List all Serena memories for PRP(s).
801: 
802:     Args:
803:         prp_id: Optional PRP filter (None = current active PRP)
804: 
805:     Returns:
806:         List of memory names (e.g., ["PRP-003-checkpoint-phase1", ...])
807: 
808:     Process:
809:         1. Call serena.list_memories()
810:         2. Filter by prefix: {prp_id}-
811:         3. Return matching names
812:     """
813:     if prp_id is None:
814:         active = get_active_prp()
815:         if not active:
816:             return []
817:         prp_id = active["prp_id"]
818: 
819:     # Try to list from Serena (optional)
820:     try:
821:         # In production, this would call the Serena MCP tool
822:         logger.warning(f"Serena MCP not available - returning memories from state file")
823:         active = get_active_prp()
824:         if active and active["prp_id"] == prp_id:
825:             return active["serena_memories"]
826:         return []
827:     except Exception as e:
828:         logger.warning(f"Failed to list Serena memories: {e}")
829:         return []
830: 
831: 
832: # ============================================================================
833: # Cleanup Protocol Function
834: # ============================================================================
835: 
836: def cleanup_prp(prp_id: str) -> Dict[str, Any]:
837:     """Execute cleanup protocol for PRP (Model.md Section 5.6).
838: 
839:     Args:
840:         prp_id: PRP identifier to clean up
841: 
842:     Returns:
843:         {
844:             "success": True,
845:             "checkpoints_deleted": 2,
846:             "checkpoints_kept": ["checkpoint-PRP-003-final"],
847:             "memories_archived": ["PRP-003-learnings-auth-patterns"],
848:             "memories_deleted": ["PRP-003-checkpoint-*", "PRP-003-temp-*"],
849:             "context_health": {"drift_score": 5.2, "status": "healthy"}
850:         }
851: 
852:     Raises:
853:         RuntimeError: If cleanup operations fail
854: 
855:     Cleanup Protocol Steps:
856:         1. Delete intermediate git checkpoints (keep *-final)
857:         2. Archive learnings to project knowledge:
858:            - Read PRP-{id}-learnings-* memories
859:            - Merge into global "project-patterns" memory (append with timestamp + PRP-id prefix)
860:            - Delete PRP-{id}-learnings-* memories
861:         3. Delete ephemeral memories:
862:            - PRP-{id}-checkpoint-*
863:            - PRP-{id}-temp-*
864:         4. Reset validation state (if tracked)
865:         5. Run context health check:
866:            - ce context health
867:            - ce context prune
868:         6. Archive validation logs (if exist):
869:            - Move to PRPs/{prp_id}/validation-log.md
870:         7. Remove .ce/active_prp_session if prp_id matches active
871: 
872:     Side Effects:
873:         - Deletes git tags
874:         - Deletes/modifies Serena memories
875:         - Runs context health check
876:         - May remove active session file
877:     """
878:     from .core import run_cmd
879:     from .context import health as context_health
880: 
881:     result = {
882:         "success": True,
883:         "checkpoints_deleted": 0,
884:         "checkpoints_kept": [],
885:         "memories_archived": [],
886:         "memories_deleted": [],
887:         "context_health": {}
888:     }
889: 
890:     # Step 1: Delete intermediate checkpoints (keep *-final)
891:     checkpoint_result = delete_intermediate_checkpoints(prp_id, keep_final=True)
892:     result["checkpoints_deleted"] = checkpoint_result["deleted_count"]
893:     result["checkpoints_kept"] = checkpoint_result["kept"]
894: 
895:     # Step 2-3: Handle Serena memories (optional - skip if unavailable)
896:     memories = list_prp_memories(prp_id)
897: 
898:     # Archive learnings
899:     learnings = [m for m in memories if f"{prp_id}-learnings-" in m]
900:     if learnings:
901:         logger.info(f"Found {len(learnings)} learning memories to archive (Serena not implemented)")
902:         result["memories_archived"] = learnings
903: 
904:     # Delete ephemeral memories
905:     ephemeral = [m for m in memories if
906:                  f"{prp_id}-checkpoint-" in m or f"{prp_id}-temp-" in m]
907:     if ephemeral:
908:         logger.info(f"Found {len(ephemeral)} ephemeral memories to delete (Serena not implemented)")
909:         result["memories_deleted"] = ephemeral
910: 
911:     # Step 4: Reset validation state (already in state file)
912:     logger.info(f"Validation state reset for {prp_id}")
913: 
914:     # Step 5: Run context health check
915:     try:
916:         health = context_health()
917:         result["context_health"] = health
918:     except Exception as e:
919:         logger.warning(f"Context health check failed: {e}")
920: 
921:     # Step 6: Archive validation logs (if exist)
922:     # TODO: Implement when validation logging is added
923: 
924:     # Step 7: Remove active session if matches
925:     active = get_active_prp()
926:     if active and active["prp_id"] == prp_id:
927:         STATE_FILE.unlink(missing_ok=True)
928:         logger.info(f"Removed active session for {prp_id}")
929: 
930:     logger.info(f"Cleanup completed for {prp_id}")
931:     return result
</file>

<file path="tools/ce/resilience.py">
  1: """Resilience module - retry logic and circuit breaker for error recovery.
  2: 
  3: Provides decorators and utilities for handling transient failures and
  4: preventing cascading failures in production systems.
  5: """
  6: 
  7: import time
  8: import functools
  9: from typing import Callable, Any, Type, Tuple
 10: from datetime import datetime, timedelta
 11: 
 12: 
 13: def retry_with_backoff(
 14:     max_attempts: int = 3,
 15:     base_delay: float = 1.0,
 16:     max_delay: float = 60.0,
 17:     exponential_base: float = 2.0,
 18:     exceptions: Tuple[Type[Exception], ...] = (Exception,)
 19: ):
 20:     """Retry decorator with exponential backoff.
 21: 
 22:     Args:
 23:         max_attempts: Maximum retry attempts (default: 3)
 24:         base_delay: Initial delay in seconds (default: 1.0)
 25:         max_delay: Maximum delay in seconds (default: 60.0)
 26:         exponential_base: Backoff multiplier (default: 2.0)
 27:         exceptions: Tuple of exception types to retry (default: all)
 28: 
 29:     Returns:
 30:         Decorator function
 31: 
 32:     Example:
 33:         @retry_with_backoff(max_attempts=5, base_delay=2.0)
 34:         def fetch_data():
 35:             return api.get("/data")
 36: 
 37:     Note: Only retries on specified exceptions. Non-retryable errors propagate immediately.
 38:     """
 39:     def decorator(func: Callable) -> Callable:
 40:         @functools.wraps(func)
 41:         def wrapper(*args, **kwargs) -> Any:
 42:             for attempt in range(max_attempts):
 43:                 result = _try_call(func, args, kwargs, exceptions, attempt, max_attempts, base_delay, exponential_base, max_delay)
 44:                 if result is not None:
 45:                     return result
 46:             raise RuntimeError(
 47:                 "Retry logic error - should not reach here\n"
 48:                 "ðŸ”§ Troubleshooting: Check retry decorator logic - this indicates internal implementation bug"
 49:             )
 50: 
 51:         return wrapper
 52:     return decorator
 53: 
 54: 
 55: def _try_call(func: Callable, args: tuple, kwargs: dict, exceptions: Tuple,
 56:               attempt: int, max_attempts: int, base_delay: float,
 57:               exponential_base: float, max_delay: float) -> Any:
 58:     """Try calling function with retry logic.
 59: 
 60:     Returns function result on success, None on retryable failure.
 61:     Raises on final attempt failure.
 62:     """
 63:     try:
 64:         return func(*args, **kwargs)
 65:     except exceptions as e:
 66:         is_final_attempt = (attempt == max_attempts - 1)
 67:         if is_final_attempt:
 68:             _raise_retry_error(func, max_attempts, e)
 69: 
 70:         # Backoff and retry
 71:         delay = min(base_delay * (exponential_base ** attempt), max_delay)
 72:         time.sleep(delay)
 73:         return None
 74: 
 75: 
 76: def _raise_retry_error(func: Callable, max_attempts: int, last_error: Exception) -> None:
 77:     """Raise detailed retry error after exhausting attempts."""
 78:     func_name = getattr(func, '__name__', repr(func))
 79:     raise RuntimeError(
 80:         f"Failed after {max_attempts} attempts: {func_name}\n"
 81:         f"Last error: {str(last_error)}\n"
 82:         f"ðŸ”§ Troubleshooting: Check network connectivity, API rate limits"
 83:     ) from last_error
 84: 
 85: 
 86: class CircuitBreaker:
 87:     """Circuit breaker for preventing cascading failures.
 88: 
 89:     State machine: CLOSED â†’ OPEN â†’ HALF_OPEN â†’ CLOSED
 90:     - CLOSED: Normal operation, requests pass through
 91:     - OPEN: Failure threshold exceeded, requests fail fast
 92:     - HALF_OPEN: Testing recovery, limited requests pass through
 93: 
 94:     Example:
 95:         breaker = CircuitBreaker(name="serena-mcp", failure_threshold=5)
 96: 
 97:         @breaker.call
 98:         def call_serena():
 99:             return serena.read_file("test.py")
100: 
101:     Attributes:
102:         state: Current circuit state (closed/open/half_open)
103:         failure_count: Consecutive failure count
104:         last_failure_time: Timestamp of last failure
105:     """
106: 
107:     def __init__(
108:         self,
109:         name: str,
110:         failure_threshold: int = 5,
111:         recovery_timeout: int = 60,
112:         half_open_max_calls: int = 3
113:     ):
114:         """Initialize circuit breaker.
115: 
116:         Args:
117:             name: Circuit breaker identifier
118:             failure_threshold: Failures before opening circuit
119:             recovery_timeout: Seconds to wait before half-open attempt
120:             half_open_max_calls: Max calls in half-open state
121:         """
122:         self.name = name
123:         self.failure_threshold = failure_threshold
124:         self.recovery_timeout = recovery_timeout
125:         self.half_open_max_calls = half_open_max_calls
126: 
127:         # State
128:         self.state = "closed"  # closed, open, half_open
129:         self.failure_count = 0
130:         self.success_count = 0
131:         self.half_open_calls = 0
132:         self.last_failure_time = None
133: 
134:     def call(self, func: Callable) -> Callable:
135:         """Decorator to protect function with circuit breaker.
136: 
137:         Args:
138:             func: Function to protect
139: 
140:         Returns:
141:             Protected function
142: 
143:         Raises:
144:             CircuitBreakerOpenError: If circuit is open
145:         """
146:         @functools.wraps(func)
147:         def wrapper(*args, **kwargs) -> Any:
148:             # Check circuit state
149:             if self.state == "open":
150:                 if self._should_attempt_reset():
151:                     self._transition_to_half_open()
152:                 else:
153:                     raise CircuitBreakerOpenError(
154:                         f"Circuit breaker '{self.name}' is OPEN\n"
155:                         f"Failures: {self.failure_count}/{self.failure_threshold}\n"
156:                         f"ðŸ”§ Troubleshooting: Wait {self.recovery_timeout}s or check service health"
157:                     )
158: 
159:             # Execute function
160:             try:
161:                 result = func(*args, **kwargs)
162:                 self._on_success()
163:                 return result
164:             except Exception as e:
165:                 self._on_failure()
166:                 raise
167: 
168:         return wrapper
169: 
170:     def _should_attempt_reset(self) -> bool:
171:         """Check if enough time has passed to attempt reset."""
172:         if self.last_failure_time is None:
173:             return False
174: 
175:         elapsed = (datetime.now() - self.last_failure_time).total_seconds()
176:         return elapsed >= self.recovery_timeout
177: 
178:     def _transition_to_half_open(self):
179:         """Transition from open to half-open state."""
180:         self.state = "half_open"
181:         self.half_open_calls = 0
182: 
183:     def _on_success(self):
184:         """Handle successful call."""
185:         if self.state == "half_open":
186:             self.success_count += 1
187:             if self.success_count >= self.half_open_max_calls:
188:                 # Recovered - close circuit
189:                 self.state = "closed"
190:                 self.failure_count = 0
191:                 self.success_count = 0
192:         else:
193:             # Reset failure count on success in closed state
194:             self.failure_count = 0
195: 
196:     def _on_failure(self):
197:         """Handle failed call."""
198:         self.failure_count += 1
199:         self.last_failure_time = datetime.now()
200: 
201:         if self.state == "half_open":
202:             # Failed in half-open - reopen circuit
203:             self.state = "open"
204:             self.success_count = 0
205:         elif self.failure_count >= self.failure_threshold:
206:             # Threshold exceeded - open circuit
207:             self.state = "open"
208: 
209: 
210: class CircuitBreakerOpenError(Exception):
211:     """Raised when circuit breaker is open."""
212:     pass
</file>

<file path="tools/ce/shell_utils.py">
  1: """Python alternatives to bash utilities for efficiency.
  2: 
  3: This module provides pure Python implementations of common bash utilities,
  4: eliminating subprocess overhead and improving performance 10-50x.
  5: 
  6: Usage:
  7:     from ce.shell_utils import grep_text, count_lines, head, Pipeline
  8: 
  9: All functions use pure Python stdlib - no external dependencies required.
 10: """
 11: 
 12: import re
 13: from pathlib import Path
 14: from typing import List, Optional, Union
 15: 
 16: 
 17: def grep_text(pattern: str, text: str, context_lines: int = 0) -> List[str]:
 18:     """Search text with regex, optional context lines.
 19: 
 20:     Replaces: bash grep -C<n>
 21: 
 22:     Args:
 23:         pattern: Regex pattern to search for
 24:         text: Input text to search
 25:         context_lines: Number of lines before/after to include
 26: 
 27:     Returns:
 28:         List of matching lines (with context if specified)
 29: 
 30:     Example:
 31:         >>> text = "line1\\nerror here\\nline3"
 32:         >>> grep_text("error", text, context_lines=1)
 33:         ['line1', 'error here', 'line3']
 34: 
 35:     Performance: 10-50x faster than subprocess grep
 36:     """
 37:     lines = text.split('\n')
 38:     regex = re.compile(pattern)
 39:     matches = []
 40:     matched_indices = set()
 41: 
 42:     for i, line in enumerate(lines):
 43:         if regex.search(line):
 44:             start = max(0, i - context_lines)
 45:             end = min(len(lines), i + context_lines + 1)
 46:             matched_indices.update(range(start, end))
 47: 
 48:     return [lines[i] for i in sorted(matched_indices)]
 49: 
 50: 
 51: def count_lines(file_path: str) -> int:
 52:     """Count lines in file.
 53: 
 54:     Replaces: bash wc -l
 55: 
 56:     Args:
 57:         file_path: Path to file (absolute or relative)
 58: 
 59:     Returns:
 60:         Number of lines in file
 61: 
 62:     Raises:
 63:         FileNotFoundError: If file doesn't exist
 64: 
 65:     Example:
 66:         >>> count_lines("config.yml")
 67:         42
 68: 
 69:     Performance: Direct file read, no subprocess overhead
 70:     """
 71:     return len(Path(file_path).read_text().split('\n'))
 72: 
 73: 
 74: def head(file_path: str, n: int = 10) -> List[str]:
 75:     """Read first N lines from file.
 76: 
 77:     Replaces: bash head -n
 78: 
 79:     Args:
 80:         file_path: Path to file (absolute or relative)
 81:         n: Number of lines to read (default: 10)
 82: 
 83:     Returns:
 84:         First N lines as list
 85: 
 86:     Raises:
 87:         FileNotFoundError: If file doesn't exist
 88: 
 89:     Example:
 90:         >>> head("log.txt", n=5)
 91:         ['Line 1', 'Line 2', 'Line 3', 'Line 4', 'Line 5']
 92: 
 93:     Performance: Reads only beginning of file, efficient for large files
 94:     """
 95:     return Path(file_path).read_text().split('\n')[:n]
 96: 
 97: 
 98: def tail(file_path: str, n: int = 10) -> List[str]:
 99:     """Read last N lines from file.
100: 
101:     Replaces: bash tail -n
102: 
103:     Args:
104:         file_path: Path to file (absolute or relative)
105:         n: Number of lines to read (default: 10)
106: 
107:     Returns:
108:         Last N lines as list
109: 
110:     Raises:
111:         FileNotFoundError: If file doesn't exist
112: 
113:     Example:
114:         >>> tail("log.txt", n=5)
115:         ['Line 96', 'Line 97', 'Line 98', 'Line 99', 'Line 100']
116: 
117:     Performance: Efficient for large files, reads from end
118:     """
119:     return Path(file_path).read_text().split('\n')[-n:]
120: 
121: 
122: def find_files(
123:     root: str,
124:     pattern: str,
125:     exclude: Optional[List[str]] = None
126: ) -> List[str]:
127:     """Find files by glob pattern recursively.
128: 
129:     Replaces: bash find . -name "*.py"
130: 
131:     Args:
132:         root: Root directory to search from
133:         pattern: Glob pattern (e.g., "*.py", "**/*.md")
134:         exclude: Optional list of patterns to exclude
135: 
136:     Returns:
137:         List of matching file paths (sorted, relative to root)
138: 
139:     Example:
140:         >>> find_files("src", "*.py", exclude=["__pycache__"])
141:         ['src/main.py', 'src/utils.py']
142: 
143:     Performance: Uses pathlib.rglob(), faster than subprocess find
144:     """
145:     exclude = exclude or []
146:     results = []
147: 
148:     for path in Path(root).rglob(pattern):
149:         if not any(ex in str(path) for ex in exclude):
150:             results.append(str(path))
151: 
152:     return sorted(results)
153: 
154: 
155: def extract_fields(
156:     text: str,
157:     field_indices: List[int],
158:     delimiter: Optional[str] = None
159: ) -> List[List[str]]:
160:     """Extract specific fields from each line.
161: 
162:     Replaces: awk '{print $1, $3}'
163: 
164:     Args:
165:         text: Input text (multi-line string)
166:         field_indices: 1-based field indices (like awk $1, $2)
167:         delimiter: Field separator (None = whitespace)
168: 
169:     Returns:
170:         List of extracted field lists per line
171: 
172:     Example:
173:         >>> text = "user1 100 active\\nuser2 200 inactive"
174:         >>> extract_fields(text, field_indices=[1, 3])
175:         [['user1', 'active'], ['user2', 'inactive']]
176: 
177:     Performance: Pure Python string operations, 10-50x faster than awk subprocess
178:     """
179:     lines = text.strip().split('\n')
180:     results = []
181: 
182:     for line in lines:
183:         if not line.strip():
184:             continue
185:         fields = line.split(delimiter) if delimiter else line.split()
186:         extracted = []
187:         for i in field_indices:
188:             if i <= len(fields):
189:                 extracted.append(fields[i-1])
190:         if extracted:
191:             results.append(extracted)
192: 
193:     return results
194: 
195: 
196: def sum_column(
197:     text: str,
198:     column: int,
199:     delimiter: Optional[str] = None
200: ) -> float:
201:     """Sum numeric values in a column.
202: 
203:     Replaces: awk '{sum += $1} END {print sum}'
204: 
205:     Args:
206:         text: Input text (multi-line string)
207:         column: 1-based column index to sum
208:         delimiter: Field separator (None = whitespace)
209: 
210:     Returns:
211:         Sum of numeric values in column
212: 
213:     Example:
214:         >>> text = "item1 100\\nitem2 200\\nitem3 300"
215:         >>> sum_column(text, column=2)
216:         600.0
217: 
218:     Note: Non-numeric values are skipped (not treated as errors)
219: 
220:     Performance: Type-safe Python arithmetic, no subprocess overhead
221:     """
222:     lines = text.strip().split('\n')
223:     total = 0.0
224: 
225:     for line in lines:
226:         if not line.strip():
227:             continue
228:         fields = line.split(delimiter) if delimiter else line.split()
229:         if column <= len(fields):
230:             try:
231:                 total += float(fields[column-1])
232:             except ValueError:
233:                 continue
234: 
235:     return total
236: 
237: 
238: def filter_and_extract(
239:     text: str,
240:     pattern: str,
241:     field_index: int,
242:     delimiter: Optional[str] = None
243: ) -> List[str]:
244:     """Pattern match lines and extract specific field.
245: 
246:     Replaces: awk '/pattern/ {print $2}'
247: 
248:     Args:
249:         text: Input text (multi-line string)
250:         pattern: Regex pattern to match lines
251:         field_index: 1-based field to extract from matching lines
252:         delimiter: Field separator (None = whitespace)
253: 
254:     Returns:
255:         List of extracted fields from matching lines
256: 
257:     Example:
258:         >>> text = "ERROR user1\\nINFO user2\\nERROR user3"
259:         >>> filter_and_extract(text, "ERROR", field_index=2)
260:         ['user1', 'user3']
261: 
262:     Performance: Combines grep_text and extract_fields for efficiency
263:     """
264:     matching_lines = grep_text(pattern, text, context_lines=0)
265:     results = []
266: 
267:     for line in matching_lines:
268:         if not line.strip():
269:             continue
270:         fields = line.split(delimiter) if delimiter else line.split()
271:         if field_index <= len(fields):
272:             results.append(fields[field_index-1])
273: 
274:     return results
275: 
276: 
277: class Pipeline:
278:     """Composable pipeline for chaining shell_utils operations.
279: 
280:     Eliminates subprocess overhead by chaining Python operations.
281:     10-50x faster than equivalent bash pipes.
282: 
283:     Usage:
284:         # Create pipeline from file
285:         result = Pipeline.from_file("log.txt").grep("ERROR", context_lines=1).count()
286: 
287:         # Create pipeline from text
288:         text = "line1\\nerror\\nline3"
289:         lines = Pipeline.from_text(text).grep("error").lines()
290: 
291:     Performance: Chaining operations avoids intermediate string copies
292:     and subprocess forks. Typical 10-50x speedup vs bash equivalents.
293:     """
294: 
295:     def __init__(self, data: Union[str, List[str]]) -> None:
296:         """Initialize pipeline with data.
297: 
298:         Args:
299:             data: String content or list of lines
300:         """
301:         if isinstance(data, str):
302:             self._lines = data.split('\n')
303:             self._text = data
304:         else:
305:             self._lines = data
306:             self._text = '\n'.join(data)
307: 
308:     @classmethod
309:     def from_file(cls, file_path: str) -> "Pipeline":
310:         """Create pipeline from file contents.
311: 
312:         Args:
313:             file_path: Path to file (absolute or relative)
314: 
315:         Returns:
316:             Pipeline instance initialized with file contents
317: 
318:         Example:
319:             >>> result = Pipeline.from_file("log.txt").grep("ERROR").count()
320:         """
321:         text = Path(file_path).read_text()
322:         return cls(text)
323: 
324:     @classmethod
325:     def from_text(cls, text: str) -> "Pipeline":
326:         """Create pipeline from text string.
327: 
328:         Args:
329:             text: Multi-line text string
330: 
331:         Returns:
332:             Pipeline instance initialized with text
333: 
334:         Example:
335:             >>> result = Pipeline.from_text("a\\nb\\nc").head(2).text()
336:         """
337:         return cls(text)
338: 
339:     def grep(self, pattern: str, context_lines: int = 0) -> "Pipeline":
340:         """Filter lines matching regex pattern.
341: 
342:         Args:
343:             pattern: Regex pattern to match
344:             context_lines: Lines before/after to include
345: 
346:         Returns:
347:             New Pipeline with filtered lines
348: 
349:         Example:
350:             >>> Pipeline.from_text("a\\nerror\\nb").grep("error").text()
351:             'error'
352:         """
353:         # Use current lines, not reconstructed text
354:         regex = re.compile(pattern)
355:         matched_indices = set()
356:         
357:         for i, line in enumerate(self._lines):
358:             if regex.search(line):
359:                 start = max(0, i - context_lines)
360:                 end = min(len(self._lines), i + context_lines + 1)
361:                 matched_indices.update(range(start, end))
362:         
363:         filtered_lines = [self._lines[i] for i in sorted(matched_indices)]
364:         return Pipeline(filtered_lines)
365: 
366:     def head(self, n: int = 10) -> "Pipeline":
367:         """Keep first N lines.
368: 
369:         Args:
370:             n: Number of lines to keep
371: 
372:         Returns:
373:             New Pipeline with first N lines
374: 
375:         Example:
376:             >>> Pipeline.from_text("a\\nb\\nc").head(2).text()
377:             'a\\nb'
378:         """
379:         return Pipeline(self._lines[:n])
380: 
381:     def tail(self, n: int = 10) -> "Pipeline":
382:         """Keep last N lines.
383: 
384:         Args:
385:             n: Number of lines to keep
386: 
387:         Returns:
388:             New Pipeline with last N lines
389: 
390:         Example:
391:             >>> Pipeline.from_text("a\\nb\\nc").tail(2).text()
392:             'b\\nc'
393:         """
394:         return Pipeline(self._lines[-n:])
395: 
396:     def extract_fields(
397:         self,
398:         field_indices: List[int],
399:         delimiter: Optional[str] = None
400:     ) -> "Pipeline":
401:         """Extract specific fields from each line.
402: 
403:         Args:
404:             field_indices: 1-based field indices to extract
405:             delimiter: Field separator (None = whitespace)
406: 
407:         Returns:
408:             New Pipeline with extracted fields as lines
409: 
410:         Example:
411:             >>> Pipeline.from_text("a 1\\nb 2").extract_fields([1]).text()
412:             'a\\nb'
413:         """
414:         extracted = extract_fields(self._text, field_indices, delimiter)
415:         # Convert lists back to lines
416:         lines = [' '.join(row) for row in extracted]
417:         return Pipeline(lines)
418: 
419:     def count(self) -> int:
420:         """Count lines in pipeline.
421: 
422:         Returns:
423:             Number of lines
424: 
425:         Example:
426:             >>> Pipeline.from_text("a\\nb\\nc").count()
427:             3
428:         """
429:         return len([l for l in self._lines if l.strip()])
430: 
431:     def sum_column(
432:         self,
433:         column: int,
434:         delimiter: Optional[str] = None
435:     ) -> float:
436:         """Sum numeric values in a column.
437: 
438:         Args:
439:             column: 1-based column index
440:             delimiter: Field separator (None = whitespace)
441: 
442:         Returns:
443:             Sum of numeric values
444: 
445:         Example:
446:             >>> Pipeline.from_text("a 100\\nb 200").sum_column(2)
447:             300.0
448:         """
449:         return sum_column(self._text, column, delimiter)
450: 
451:     def text(self) -> str:
452:         """Get pipeline contents as text.
453: 
454:         Returns:
455:             Multi-line string
456: 
457:         Example:
458:             >>> Pipeline.from_text("a\\nb").head(1).text()
459:             'a'
460:         """
461:         return self._text
462: 
463:     def lines(self) -> List[str]:
464:         """Get pipeline contents as line list.
465: 
466:         Returns:
467:             List of lines
468: 
469:         Example:
470:             >>> Pipeline.from_text("a\\nb\\nc").grep("[ab]").lines()
471:             ['a', 'b']
472:         """
473:         return self._lines
474: 
475:     def first(self) -> Optional[str]:
476:         """Get first line.
477: 
478:         Returns:
479:             First non-empty line or None
480: 
481:         Example:
482:             >>> Pipeline.from_text("\\na\\nb").first()
483:             'a'
484:         """
485:         for line in self._lines:
486:             if line.strip():
487:                 return line
488:         return None
489: 
490:     def last(self) -> Optional[str]:
491:         """Get last line.
492: 
493:         Returns:
494:             Last non-empty line or None
495: 
496:         Example:
497:             >>> Pipeline.from_text("a\\nb\\n").last()
498:             'b'
499:         """
500:         for line in reversed(self._lines):
501:             if line.strip():
502:                 return line
503:         return None
</file>

<file path="tools/ce/update_context.py">
   1: """Context sync operations for maintaining CE/Serena alignment with codebase.
   2: 
   3: This module provides the /update-context command functionality for syncing
   4: knowledge systems with actual implementations.
   5: """
   6: 
   7: import ast
   8: import logging
   9: import re
  10: import sys
  11: import yaml
  12: from datetime import datetime, timezone
  13: from pathlib import Path
  14: from typing import Any, Dict, List, Optional, Tuple
  15: 
  16: import frontmatter
  17: 
  18: logger = logging.getLogger(__name__)
  19: 
  20: 
  21: def is_interactive() -> bool:
  22:     """Check if stdin is connected to a terminal (interactive mode)."""
  23:     return sys.stdin.isatty()
  24: 
  25: # Pattern detection rules from examples/
  26: PATTERN_FILES = {
  27:     "error_handling": "examples/patterns/error-handling.py",
  28:     "no_fishy_fallbacks": "examples/patterns/no-fishy-fallbacks.py",
  29:     "naming_conventions": "examples/patterns/naming.py"
  30: }
  31: 
  32: PATTERN_CHECKS = {
  33:     "error_handling": [
  34:         ("bare_except", r"except:\s*$", "Use specific exception types"),
  35:         ("missing_troubleshooting", r'raise \w+Error\([^ðŸ”§]+\)$', "Add ðŸ”§ Troubleshooting guidance")
  36:     ],
  37:     "naming_conventions": [
  38:         ("version_suffix", r"def \w+_v\d+", "Use descriptive names, not versions"),
  39:     ],
  40:     "kiss_violations": [
  41:         ("deep_nesting", r"^                    (if |for |while |try:|elif |with )", "Reduce nesting depth (max 4 levels)")
  42:     ]
  43: }
  44: 
  45: 
  46: def atomic_write(file_path: Path, content: str) -> None:
  47:     """Write file atomically using temp file + rename pattern.
  48: 
  49:     Args:
  50:         file_path: Target file path
  51:         content: Content to write
  52: 
  53:     Raises:
  54:         RuntimeError: If write operation fails
  55:             ðŸ”§ Troubleshooting: Check file permissions and disk space
  56: 
  57:     Note: Prevents file corruption by writing to temp file first,
  58:     then replacing original atomically. Based on pattern from prp.py:215-219.
  59:     """
  60:     try:
  61:         # Write to temp file
  62:         temp_file = file_path.with_suffix(file_path.suffix + ".tmp")
  63:         temp_file.write_text(content, encoding="utf-8")
  64: 
  65:         # Atomic replace
  66:         temp_file.replace(file_path)
  67:     except Exception as e:
  68:         raise RuntimeError(
  69:             f"Failed to write {file_path}: {e}\n"
  70:             f"ðŸ”§ Troubleshooting: Check file permissions and disk space"
  71:         ) from e
  72: 
  73: 
  74: def verify_function_exists_ast(function_name: str, search_dir: Path) -> bool:
  75:     """Verify function exists in codebase using AST parsing.
  76: 
  77:     Args:
  78:         function_name: Name of function to find (e.g., "sync_context")
  79:         search_dir: Directory to search (e.g., tools/ce/)
  80: 
  81:     Returns:
  82:         True if function found in any Python file, False otherwise
  83: 
  84:     Raises:
  85:         RuntimeError: If search directory doesn't exist
  86:             ðŸ”§ Troubleshooting: Verify search_dir path is correct
  87:     """
  88:     if not search_dir.exists():
  89:         raise RuntimeError(
  90:             f"Search directory not found: {search_dir}\n"
  91:             f"ðŸ”§ Troubleshooting: Verify search_dir path is correct"
  92:         )
  93: 
  94:     # Scan all Python files
  95:     for py_file in search_dir.glob("*.py"):
  96:         try:
  97:             content = py_file.read_text(encoding="utf-8")
  98:             tree = ast.parse(content, filename=str(py_file))
  99: 
 100:             # Walk AST looking for function definitions
 101:             for node in ast.walk(tree):
 102:                 if isinstance(node, ast.FunctionDef):
 103:                     if node.name == function_name:
 104:                         return True
 105:         except SyntaxError:
 106:             # Skip files with syntax errors
 107:             continue
 108:         except Exception as e:
 109:             logger.warning(f"Failed to parse {py_file}: {e}")
 110:             continue
 111: 
 112:     return False
 113: 
 114: 
 115: def read_prp_header(file_path: Path) -> Tuple[Dict[str, Any], str]:
 116:     """Read PRP YAML header using safe YAML loading.
 117: 
 118:     Args:
 119:         file_path: Path to PRP markdown file
 120: 
 121:     Returns:
 122:         Tuple of (metadata dict, content string)
 123: 
 124:     Raises:
 125:         FileNotFoundError: If file doesn't exist
 126:         ValueError: If YAML header is invalid
 127: 
 128:     Security Note:
 129:         Uses yaml.safe_load() to prevent code injection via !!python/object directives.
 130:         Only safe YAML constructs are parsed (no arbitrary Python code execution).
 131:     """
 132:     if not file_path.exists():
 133:         raise FileNotFoundError(
 134:             f"PRP file not found: {file_path}\n"
 135:             f"ðŸ”§ Troubleshooting:\n"
 136:             f"   - Verify file path is correct\n"
 137:             f"   - Check if file was moved or renamed\n"
 138:             f"   - Use: ls {file_path.parent} to list directory"
 139:         )
 140: 
 141:     try:
 142:         # Use yaml.safe_load() for security (prevents code injection)
 143:         content = file_path.read_text(encoding="utf-8")
 144:         # Extract YAML frontmatter manually for explicit safe loading
 145:         if content.startswith("---"):
 146:             # Find closing --- delimiter
 147:             end_marker = content.find("---", 3)
 148:             if end_marker != -1:
 149:                 yaml_content = content[3:end_marker].strip()
 150:                 markdown_content = content[end_marker + 3:].strip()
 151: 
 152:                 # Parse YAML safely
 153:                 metadata = yaml.safe_load(yaml_content) or {}
 154:                 return metadata, markdown_content
 155: 
 156:         # Fallback to frontmatter.load() with safe loader for backwards compatibility
 157:         post = frontmatter.load(file_path)
 158:         return post.metadata, post.content
 159:     except yaml.YAMLError as e:
 160:         raise ValueError(
 161:             f"Failed to parse YAML header in {file_path}: {e}\n"
 162:             f"ðŸ”§ Troubleshooting:\n"
 163:             f"   - Check YAML syntax with: head -n 20 {file_path}\n"
 164:             f"   - Ensure --- delimiters are present\n"
 165:             f"   - Validate YAML structure (no !!python/object directives)"
 166:         ) from e
 167:     except Exception as e:
 168:         raise ValueError(
 169:             f"Failed to read PRP header in {file_path}: {e}\n"
 170:             f"ðŸ”§ Troubleshooting:\n"
 171:             f"   - Check file permissions: ls -la {file_path}\n"
 172:             f"   - Ensure file is readable text"
 173:         ) from e
 174: 
 175: 
 176: def transform_drift_to_initial(
 177:     violations: List[str],
 178:     drift_score: float,
 179:     missing_examples: List[Dict[str, Any]]
 180: ) -> str:
 181:     """Transform drift report â†’ INITIAL.md blueprint format.
 182: 
 183:     Args:
 184:         violations: List of violation messages with format:
 185:                    "File {path} has {issue} (violates {pattern}): {fix}"
 186:         drift_score: Percentage score (0-100)
 187:         missing_examples: List of PRPs missing examples with metadata:
 188:                          [{"prp_id": "PRP-10", "feature_name": "...",
 189:                            "suggested_path": "...", "rationale": "..."}]
 190: 
 191:     Returns:
 192:         INITIAL.md formatted string with:
 193:         - Feature: Drift summary with breakdown
 194:         - Context: Root causes and impact
 195:         - Examples: Top 5 violations + up to 3 missing examples
 196:         - Acceptance Criteria: Standard remediation checklist
 197:         - Technical Notes: File count, effort estimate, complexity
 198: 
 199:     Raises:
 200:         ValueError: If violations empty and missing_examples empty
 201:                    If drift_score invalid (not 0-100)
 202: 
 203:     Edge Cases:
 204:         - Empty violations + empty missing: Raises ValueError
 205:         - drift_score outside 0-100: Raises ValueError
 206:         - More than 5 violations: Shows top 5 only
 207:         - More than 3 missing examples: Shows top 3 only
 208:         - No file paths extractable: files_affected = 0
 209: 
 210:     Example:
 211:         >>> violations = ["File tools/ce/foo.py has bare_except: Use specific"]
 212:         >>> missing = [{"prp_id": "PRP-10", "suggested_path": "ex.py",
 213:         ...            "feature_name": "Feature", "rationale": "Important"}]
 214:         >>> result = transform_drift_to_initial(violations, 12.5, missing)
 215:         >>> assert "# Drift Remediation" in result
 216:         >>> assert "12.5%" in result
 217:         >>> assert "PRP-10" in result
 218:     """
 219:     # Validation
 220:     if not violations and not missing_examples:
 221:         raise ValueError(
 222:             "Cannot generate INITIAL.md: no violations and no missing examples\n"
 223:             "ðŸ”§ Troubleshooting: Drift detection returned empty results"
 224:         )
 225: 
 226:     if not (0 <= drift_score <= 100):
 227:         raise ValueError(
 228:             f"Invalid drift_score: {drift_score} (must be 0-100)\n"
 229:             "ðŸ”§ Troubleshooting: Check drift calculation returns percentage"
 230:         )
 231: 
 232:     now = datetime.now(timezone.utc).strftime("%Y-%m-%d")
 233: 
 234:     # Count violations by category (extract pattern from violation string)
 235:     # Pattern format: "(violates examples/patterns/{category}.py)"
 236:     error_handling = len([v for v in violations if "error-handling.py" in v or "error_handling.py" in v])
 237:     naming = len([v for v in violations if "naming.py" in v])
 238:     kiss = len([v for v in violations if "kiss.py" in v or "nesting" in v.lower()])
 239: 
 240:     # Categorize drift level
 241:     if drift_score < 5:
 242:         drift_level = "âœ… OK"
 243:     elif drift_score < 15:
 244:         drift_level = "âš ï¸ WARNING"
 245:     else:
 246:         drift_level = "ðŸš¨ CRITICAL"
 247: 
 248:     # Calculate effort estimate (15 min per violation + 30 min per missing example)
 249:     effort_hours = (len(violations) * 0.25) + (len(missing_examples) * 0.5)
 250:     effort_hours = max(1, round(effort_hours))  # Minimum 1 hour
 251: 
 252:     # Calculate complexity
 253:     total_items = len(violations) + len(missing_examples)
 254:     if total_items < 5:
 255:         complexity = "LOW"
 256:     elif total_items < 15:
 257:         complexity = "MEDIUM"
 258:     else:
 259:         complexity = "HIGH"
 260: 
 261:     # Extract unique file paths for count
 262:     # Expected format: "File {path} has {issue} (violates {pattern}): {fix}"
 263:     files_affected = set()
 264:     for v in violations:
 265:         if "File " in v and " has " in v:
 266:             # Extract file path: "File tools/ce/foo.py has ..."
 267:             try:
 268:                 file_part = v.split(" has ")[0].replace("File ", "").strip()
 269:                 if file_part:  # Only add non-empty paths
 270:                     files_affected.add(file_part)
 271:             except (IndexError, AttributeError):
 272:                 # Malformed violation string, skip gracefully
 273:                 continue
 274: 
 275:     # Build INITIAL.md content
 276:     initial = f"""# Drift Remediation - {now}
 277: 
 278: ## Feature
 279: 
 280: Address {len(violations)} drift violations detected in codebase scan on {now}.
 281: 
 282: **Drift Score**: {drift_score:.1f}% ({drift_level})
 283: 
 284: **Violations Breakdown**:
 285: - Error Handling: {error_handling}
 286: - Naming Conventions: {naming}
 287: - KISS Violations: {kiss}
 288: - Missing Examples: {len(missing_examples)}
 289: 
 290: ## Context
 291: 
 292: Context Engineering drift detection found violations between documented patterns (CLAUDE.md, examples/) and actual implementation.
 293: 
 294: **Root Causes**:
 295: 1. New code written without pattern awareness
 296: 2. Missing examples for critical PRPs
 297: 3. Pattern evolution without documentation updates
 298: 
 299: **Impact**:
 300: - Code quality inconsistency
 301: - Reduced onboarding effectiveness
 302: - Pattern erosion over time
 303: 
 304: ## Examples
 305: 
 306: """
 307: 
 308:     # Add top 5 violations
 309:     for i, violation in enumerate(violations[:5], 1):
 310:         initial += f"### Violation {i}\n\n"
 311:         initial += f"{violation}\n\n"
 312: 
 313:     # Add missing examples (up to 3)
 314:     if missing_examples:
 315:         initial += "### Missing Examples\n\n"
 316:         for missing in missing_examples[:3]:
 317:             initial += f"**{missing['prp_id']}**: {missing['feature_name']}\n"
 318:             initial += f"- **Missing**: `{missing['suggested_path']}`\n"
 319:             initial += f"- **Rationale**: {missing['rationale']}\n\n"
 320: 
 321:     # Add Acceptance Criteria
 322:     initial += """## Acceptance Criteria
 323: 
 324: - [ ] All HIGH priority violations resolved
 325: - [ ] Missing examples created for critical PRPs
 326: - [ ] L4 validation passes (ce validate --level 4)
 327: - [ ] Drift score < 5% after remediation
 328: - [ ] Pattern documentation updated if intentional drift
 329: 
 330: """
 331: 
 332:     # Add Technical Notes with high-level summary
 333:     initial += f"""## Technical Notes
 334: 
 335: **Files Affected**: {len(files_affected)}
 336: **Estimated Effort**: {effort_hours}h based on violation count
 337: **Complexity**: {complexity}
 338: **Total Items**: {len(violations)} violations + {len(missing_examples)} missing examples
 339: 
 340: **Priority Focus**:
 341: - Address HIGH priority violations first
 342: - Create missing examples for critical PRPs
 343: - Run L4 validation after each fix
 344: """
 345: 
 346:     return initial
 347: 
 348: 
 349: def detect_drift_violations() -> Dict[str, Any]:
 350:     """Run drift detection and return structured results.
 351: 
 352:     Returns:
 353:         {
 354:             "drift_score": 12.5,
 355:             "violations": ["file.py:42 - Error", ...],
 356:             "missing_examples": [{"prp_id": "PRP-10", ...}],
 357:             "has_drift": True
 358:         }
 359: 
 360:     Raises:
 361:         RuntimeError: If detection fails with troubleshooting guidance
 362: 
 363:     Example:
 364:         >>> result = detect_drift_violations()
 365:         >>> assert "drift_score" in result
 366:         >>> assert isinstance(result["violations"], list)
 367:     """
 368:     logger.info("Running drift detection...")
 369:     try:
 370:         # Call existing detection functions
 371:         drift_result = verify_codebase_matches_examples()
 372:         missing_examples = detect_missing_examples_for_prps()
 373: 
 374:         drift_score = drift_result["drift_score"]
 375:         violations = drift_result["violations"]
 376:         has_drift = drift_score >= 5 or len(missing_examples) > 0
 377: 
 378:         return {
 379:             "drift_score": drift_score,
 380:             "violations": violations,
 381:             "missing_examples": missing_examples,
 382:             "has_drift": has_drift
 383:         }
 384:     except Exception as e:
 385:         raise RuntimeError(
 386:             f"Drift detection failed: {e}\n"
 387:             f"ðŸ”§ Troubleshooting:\n"
 388:             f"   - Ensure examples/ directory exists\n"
 389:             f"   - Check PRPs have valid YAML headers\n"
 390:             f"   - Verify tools/ce/ directory is accessible\n"
 391:             f"   - Run: cd tools && uv run ce validate --level 1"
 392:         ) from e
 393: 
 394: 
 395: def generate_drift_blueprint(drift_result: Dict, missing_examples: List) -> Path:
 396:     """Generate DEDRIFT-INITIAL.md blueprint in tmp/ce/.
 397: 
 398:     Args:
 399:         drift_result: Detection results from detect_drift_violations()
 400:         missing_examples: List of PRPs missing examples
 401: 
 402:     Returns:
 403:         Path to generated blueprint file
 404: 
 405:     Raises:
 406:         RuntimeError: If blueprint generation fails
 407: 
 408:     Example:
 409:         >>> drift = detect_drift_violations()
 410:         >>> missing = drift["missing_examples"]
 411:         >>> path = generate_drift_blueprint(drift, missing)
 412:         >>> assert path.exists()
 413:         >>> assert "DEDRIFT-INITIAL.md" in path.name
 414:     """
 415:     logger.info("Generating remediation blueprint...")
 416:     try:
 417:         # Use PRP-15.1 transform function
 418:         blueprint = transform_drift_to_initial(
 419:             drift_result["violations"],
 420:             drift_result["drift_score"],
 421:             missing_examples
 422:         )
 423: 
 424:         # Determine project root
 425:         current_dir = Path.cwd()
 426:         if current_dir.name == "tools":
 427:             project_root = current_dir.parent
 428:         else:
 429:             project_root = current_dir
 430: 
 431:         # Create tmp/ce/ directory
 432:         tmp_ce_dir = project_root / "tmp" / "ce"
 433:         tmp_ce_dir.mkdir(parents=True, exist_ok=True)
 434: 
 435:         # Write blueprint atomically
 436:         blueprint_path = tmp_ce_dir / "DEDRIFT-INITIAL.md"
 437:         atomic_write(blueprint_path, blueprint)
 438: 
 439:         logger.info(f"Blueprint generated: {blueprint_path}")
 440:         return blueprint_path
 441: 
 442:     except Exception as e:
 443:         raise RuntimeError(
 444:             f"Blueprint generation failed: {e}\n"
 445:             f"ðŸ”§ Troubleshooting:\n"
 446:             f"   - Check tmp/ce/ directory permissions\n"
 447:             f"   - Verify transform_drift_to_initial() is available (PRP-15.1)\n"
 448:             f"   - Check disk space: df -h\n"
 449:             f"   - Run: ls -la tmp/"
 450:         ) from e
 451: 
 452: 
 453: def display_drift_summary(drift_score: float, violations: List[str],
 454:                           missing_examples: List[Dict], blueprint_path: Path):
 455:     """Display drift summary with direct output (no box-drawing).
 456: 
 457:     Args:
 458:         drift_score: Percentage score (0-100)
 459:         violations: List of violation messages
 460:         missing_examples: List of PRPs missing examples
 461:         blueprint_path: Path to generated blueprint
 462: 
 463:     Example:
 464:         >>> display_drift_summary(12.5, violations, missing, path)
 465:         # Prints direct output with Unicode separators
 466:     """
 467:     print("\n" + "â”" * 60)
 468:     print("ðŸ“Š Drift Summary")
 469:     print("â”" * 60)
 470: 
 471:     # Drift level indicator
 472:     level = "âš ï¸ WARNING" if drift_score < 15 else "ðŸš¨ CRITICAL"
 473:     print(f"Drift Score: {drift_score:.1f}% ({level})")
 474:     print(f"Total Violations: {len(violations) + len(missing_examples)}")
 475:     print()
 476: 
 477:     # Breakdown by category
 478:     # Pattern format: "(violates examples/patterns/{category}.py)"
 479:     print("Breakdown:")
 480:     if violations:
 481:         # Categorize violations using pattern file detection (consistent with PRP-15.1)
 482:         error_count = len([v for v in violations if "error-handling.py" in v or "error_handling.py" in v])
 483:         naming_count = len([v for v in violations if "naming.py" in v])
 484:         kiss_count = len([v for v in violations if "kiss.py" in v or "nesting" in v.lower()])
 485: 
 486:         if error_count > 0:
 487:             print(f"  â€¢ Error Handling: {error_count} violation{'s' if error_count != 1 else ''}")
 488:         if naming_count > 0:
 489:             print(f"  â€¢ Naming Conventions: {naming_count} violation{'s' if naming_count != 1 else ''}")
 490:         if kiss_count > 0:
 491:             print(f"  â€¢ KISS Violations: {kiss_count} violation{'s' if kiss_count != 1 else ''}")
 492: 
 493:     if missing_examples:
 494:         print(f"  â€¢ Missing Examples: {len(missing_examples)} PRP{'s' if len(missing_examples) != 1 else ''}")
 495: 
 496:     print()
 497:     print(f"Blueprint: {blueprint_path}")
 498:     print("â”" * 60)
 499:     print()
 500: 
 501: 
 502: def generate_prp_yaml_header(violation_count: int, missing_count: int, timestamp: str) -> str:
 503:     """Generate YAML header for DEDRIFT maintenance PRP.
 504: 
 505:     Args:
 506:         violation_count: Number of code violations
 507:         missing_count: Number of missing examples
 508:         timestamp: Formatted timestamp for PRP ID (e.g., "20251015-120530")
 509: 
 510:     Returns:
 511:         YAML header string with metadata
 512: 
 513:     Example:
 514:         >>> header = generate_prp_yaml_header(5, 2, "20251015-120530")
 515:         >>> assert "prp_id:" in header
 516:         >>> assert "DEDRIFT-20251015-120530" in header
 517:         >>> assert "effort_hours:" in header
 518:     """
 519:     total_items = violation_count + missing_count
 520: 
 521:     # Effort estimation: 15 min per violation + 30 min per missing example
 522:     # NOTE: Same formula as PRP-15.1 transform function for consistency
 523:     effort_hours = (violation_count * 0.25) + (missing_count * 0.5)
 524:     effort_hours = max(1, round(effort_hours))  # Minimum 1 hour
 525: 
 526:     # Risk assessment based on item count
 527:     if total_items < 5:
 528:         risk = "LOW"
 529:     elif total_items < 10:
 530:         risk = "MEDIUM"
 531:     else:
 532:         risk = "HIGH"
 533: 
 534:     now = datetime.now().isoformat()
 535: 
 536:     return f"""---
 537: name: "Drift Remediation - {timestamp}"
 538: description: "Address drift violations detected in codebase scan"
 539: prp_id: "DEDRIFT-{timestamp}"
 540: status: "new"
 541: created_date: "{now}Z"
 542: last_updated: "{now}Z"
 543: updated_by: "drift-remediation-workflow"
 544: context_sync:
 545:   ce_updated: false
 546:   serena_updated: false
 547: version: 1
 548: priority: "MEDIUM"
 549: effort_hours: {effort_hours}
 550: risk: "{risk}"
 551: ---
 552: 
 553: """
 554: 
 555: 
 556: # ======================================================================
 557: # PRP-15.3: Drift Remediation Workflow Automation
 558: # ======================================================================
 559: 
 560: def generate_maintenance_prp(blueprint_path: Path) -> Path:
 561:     """Generate complete maintenance PRP file from blueprint.
 562: 
 563:     Args:
 564:         blueprint_path: Path to DEDRIFT-INITIAL.md blueprint
 565: 
 566:     Returns:
 567:         Path to generated PRP file in PRPs/system/
 568: 
 569:     Raises:
 570:         RuntimeError: If PRP generation fails
 571: 
 572:     Example:
 573:         >>> blueprint = Path("tmp/ce/DEDRIFT-INITIAL.md")
 574:         >>> prp = generate_maintenance_prp(blueprint)
 575:         >>> assert prp.exists()
 576:         >>> assert "DEDRIFT_PRP-" in prp.name
 577:     """
 578:     logger.info("Generating maintenance PRP file...")
 579:     try:
 580:         # Read blueprint content
 581:         blueprint_content = blueprint_path.read_text()
 582: 
 583:         # Extract metadata from blueprint for YAML header
 584:         # Count violations and missing examples from content
 585:         violation_count = blueprint_content.count("### Violation")
 586:         missing_count = blueprint_content.count("**Missing**:")
 587: 
 588:         # Generate timestamp for PRP ID
 589:         timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
 590: 
 591:         # Generate YAML header (PRP-15.2 function)
 592:         yaml_header = generate_prp_yaml_header(violation_count, missing_count, timestamp)
 593: 
 594:         # Combine YAML + blueprint content
 595:         prp_content = yaml_header + blueprint_content
 596: 
 597:         # Determine project root and create PRPs/system/ directory
 598:         current_dir = Path.cwd()
 599:         if current_dir.name == "tools":
 600:             project_root = current_dir.parent
 601:         else:
 602:             project_root = current_dir
 603: 
 604:         prp_system_dir = project_root / "PRPs" / "system"
 605:         prp_system_dir.mkdir(parents=True, exist_ok=True)
 606: 
 607:         # Write PRP file atomically
 608:         prp_path = prp_system_dir / f"DEDRIFT_PRP-{timestamp}.md"
 609:         atomic_write(prp_path, prp_content)
 610: 
 611:         logger.info(f"Maintenance PRP generated: {prp_path}")
 612:         return prp_path
 613: 
 614:     except Exception as e:
 615:         raise RuntimeError(
 616:             f"PRP generation failed: {e}\n"
 617:             f"ðŸ”§ Troubleshooting:\n"
 618:             f"   - Check PRPs/system/ directory permissions\n"
 619:             f"   - Verify blueprint file exists: {blueprint_path}\n"
 620:             f"   - Check disk space: df -h"
 621:         ) from e
 622: 
 623: 
 624: def remediate_drift_workflow(yolo_mode: bool = False, auto_execute: bool = False) -> Dict[str, Any]:
 625:     """Execute drift remediation workflow.
 626: 
 627:     Args:
 628:         yolo_mode: If True, skip approval gate (--remediate flag)
 629:         auto_execute: If True, automatically execute PRP without user approval
 630: 
 631:     Returns:
 632:         {
 633:             "success": bool,
 634:             "prp_path": Optional[Path],
 635:             "blueprint_path": Optional[Path],
 636:             "executed": bool,  # True if auto_execute=True and PRP was executed
 637:             "fixes": List[str],  # List of fixes applied (if executed=True)
 638:             "errors": List[str]
 639:         }
 640: 
 641:     Workflow:
 642:         1. Detect drift violations (PRP-15.2)
 643:         2. Transform to INITIAL.md format (PRP-15.1)
 644:         3. Generate blueprint file (PRP-15.2)
 645:         4. Display drift summary (PRP-15.2)
 646:         5. Ask approval (vanilla) OR skip approval (YOLO)
 647:         6. Generate maintenance PRP (PRP-15.3)
 648:         7. Display /execute-prp command for manual execution
 649: 
 650:     Raises:
 651:         None - all errors captured in errors list
 652: 
 653:     Example (Vanilla Mode):
 654:         >>> result = remediate_drift_workflow(yolo_mode=False)
 655:         # Prompts: "Proceed with remediation? (yes/no):"
 656:         # If yes: Generates PRP, displays command
 657:         # If no: Workflow stops, blueprint saved
 658: 
 659:     Example (YOLO Mode):
 660:         >>> result = remediate_drift_workflow(yolo_mode=True)
 661:         # Skips approval prompt
 662:         # Auto-generates PRP, displays command
 663:     """
 664:     mode_label = "YOLO mode (no approval)" if yolo_mode else "Interactive mode"
 665:     logger.info(f"Starting drift remediation workflow ({mode_label})...")
 666:     errors = []
 667: 
 668:     # Step 1: Detect drift (PRP-15.2 function)
 669:     try:
 670:         drift = detect_drift_violations()
 671:     except RuntimeError as e:
 672:         return {
 673:             "success": False,
 674:             "prp_path": None,
 675:             "blueprint_path": None,
 676:             "errors": [str(e)]
 677:         }
 678: 
 679:     # Early exit if no drift
 680:     if not drift["has_drift"]:
 681:         print(f"\nâœ… No drift detected (score: {drift['drift_score']:.1f}%)")
 682:         print("Context is healthy - no remediation needed.\n")
 683:         return {
 684:             "success": True,
 685:             "prp_path": None,
 686:             "blueprint_path": None,
 687:             "executed": False,
 688:             "fixes": [],
 689:             "errors": []
 690:         }
 691: 
 692:     # Step 2: Generate blueprint (PRP-15.2 function)
 693:     try:
 694:         blueprint_path = generate_drift_blueprint(drift, drift["missing_examples"])
 695:     except RuntimeError as e:
 696:         return {
 697:             "success": False,
 698:             "prp_path": None,
 699:             "blueprint_path": None,
 700:             "errors": [str(e)]
 701:         }
 702: 
 703:     # Step 3: Display summary (PRP-15.2 function)
 704:     display_drift_summary(
 705:         drift["drift_score"],
 706:         drift["violations"],
 707:         drift["missing_examples"],
 708:         blueprint_path
 709:     )
 710: 
 711:     # Step 4: Approval gate (vanilla only)
 712:     if not yolo_mode:
 713:         # Check if running in interactive mode
 714:         if not is_interactive():
 715:             # Non-interactive mode without --remediate: skip remediation gracefully
 716:             print(f"\nâ­ï¸ Non-interactive mode detected (no TTY)")
 717:             print(f"ðŸ“„ Blueprint saved: {blueprint_path}")
 718:             print(f"\nðŸ’¡ For automated remediation, use: ce update-context --remediate\n")
 719:             return {
 720:                 "success": True,
 721:                 "prp_path": None,
 722:                 "blueprint_path": blueprint_path,
 723:                 "errors": []
 724:             }
 725: 
 726:         # Interactive mode: ask for approval
 727:         print(f"\nReview INITIAL.md: {blueprint_path}")
 728:         approval = input("Proceed with remediation? (yes/no): ").strip().lower()
 729: 
 730:         if approval not in ["yes", "y"]:
 731:             print("âš ï¸ Remediation skipped by user")
 732:             print(f"Blueprint saved: {blueprint_path}\n")
 733:             return {
 734:                 "success": True,
 735:                 "prp_path": None,
 736:                 "blueprint_path": blueprint_path,
 737:                 "errors": []
 738:             }
 739: 
 740:         logger.info("User approved remediation - proceeding...")
 741: 
 742:     # Step 5: Generate maintenance PRP (PRP-15.3 function)
 743:     logger.info("Generating maintenance PRP...")
 744:     try:
 745:         prp_path = generate_maintenance_prp(blueprint_path)
 746:     except Exception as e:
 747:         errors.append(f"PRP generation failed: {e}")
 748:         return {
 749:             "success": False,
 750:             "prp_path": None,
 751:             "blueprint_path": blueprint_path,
 752:             "errors": errors
 753:         }
 754: 
 755:     # Step 6: Auto-execute if requested
 756:     if auto_execute:
 757:         logger.info(f"Auto-executing PRP: {prp_path}")
 758:         try:
 759:             # Import here to avoid circular imports
 760:             from .prp import execute_prp as execute_prp_impl
 761: 
 762:             exec_result = execute_prp_impl(str(prp_path))
 763: 
 764:             if not exec_result.get("success", False):
 765:                 raise RuntimeError(
 766:                     f"PRP execution failed: {exec_result.get('error', 'Unknown error')}\n"
 767:                     f"ðŸ”§ Troubleshooting:\n"
 768:                     f"   - Check PRP: {prp_path}\n"
 769:                     f"   - Review errors above\n"
 770:                     f"   - Try manual execution: /execute-prp {prp_path}"
 771:                 )
 772: 
 773:             fixes = exec_result.get("fixes", [])
 774:             print(f"\nâœ… Remediation executed: {len(fixes)} fixes applied")
 775:             logger.info(f"PRP executed successfully: {len(fixes)} fixes applied")
 776: 
 777:             return {
 778:                 "success": True,
 779:                 "prp_path": prp_path,
 780:                 "blueprint_path": blueprint_path,
 781:                 "executed": True,
 782:                 "fixes": fixes,
 783:                 "errors": []
 784:             }
 785:         except Exception as e:
 786:             error_msg = f"Auto-execution failed: {e}"
 787:             logger.error(error_msg)
 788:             errors.append(error_msg)
 789:             return {
 790:                 "success": False,
 791:                 "prp_path": prp_path,
 792:                 "blueprint_path": blueprint_path,
 793:                 "executed": False,
 794:                 "fixes": [],
 795:                 "errors": errors
 796:             }
 797: 
 798:     # Step 6: Display next step (manual execution)
 799:     logger.info("PRP ready for execution...")
 800: 
 801:     print("\n" + "â”" * 60)
 802:     print("ðŸ”§ Next Step: Execute PRP")
 803:     print("â”" * 60)
 804:     print(f"Run: /execute-prp {prp_path}")
 805:     print("â”" * 60)
 806:     print()
 807: 
 808:     # Workflow complete - PRP ready for manual execution
 809:     print(f"âœ… PRP Generated: {prp_path}")
 810:     print(f"ðŸ“„ Blueprint: {blueprint_path}\n")
 811: 
 812:     return {
 813:         "success": True,
 814:         "prp_path": prp_path,
 815:         "blueprint_path": blueprint_path,
 816:         "executed": False,
 817:         "fixes": [],
 818:         "errors": []
 819:     }
 820: 
 821: 
 822: def update_context_sync_flags(
 823:     file_path: Path,
 824:     ce_updated: bool,
 825:     serena_updated: bool
 826: ) -> None:
 827:     """Update context_sync flags in PRP YAML header.
 828: 
 829:     Args:
 830:         file_path: Path to PRP markdown file
 831:         ce_updated: Whether CE content was updated
 832:         serena_updated: Always False (Serena verification disabled due to MCP architecture)
 833: 
 834:     Raises:
 835:         ValueError: If YAML update fails
 836: 
 837:     Note:
 838:         - Serena verification removed (Python subprocess cannot access parent's stdio MCP)
 839:         - Only updates timestamps if flags actually changed (no false positives)
 840:     """
 841:     metadata, content = read_prp_header(file_path)
 842: 
 843:     # Initialize context_sync if missing
 844:     if "context_sync" not in metadata:
 845:         metadata["context_sync"] = {}
 846: 
 847:     # Track if anything actually changed
 848:     old_ce_updated = metadata["context_sync"].get("ce_updated", False)
 849:     old_serena_updated = metadata["context_sync"].get("serena_updated", False)
 850:     flags_changed = (old_ce_updated != ce_updated) or (old_serena_updated != serena_updated)
 851: 
 852:     # Only update if flags changed
 853:     if flags_changed:
 854:         metadata["context_sync"]["ce_updated"] = ce_updated
 855:         metadata["context_sync"]["serena_updated"] = serena_updated
 856:         metadata["context_sync"]["last_sync"] = datetime.now(timezone.utc).isoformat()
 857:         metadata["updated_by"] = "update-context-command"
 858:         metadata["updated"] = datetime.now(timezone.utc).isoformat()
 859: 
 860:         # Write back atomically
 861:         try:
 862:             post = frontmatter.Post(content, **metadata)
 863:             prp_content = frontmatter.dumps(post)
 864:             atomic_write(file_path, prp_content)
 865:             logger.info(f"Updated context_sync flags: {file_path}")
 866:         except Exception as e:
 867:             raise ValueError(
 868:                 f"Failed to write YAML header to {file_path}: {e}\n"
 869:                 f"ðŸ”§ Troubleshooting:\n"
 870:                 f"   - Check file permissions: ls -la {file_path}\n"
 871:                 f"   - Ensure disk space available: df -h\n"
 872:                 f"   - Verify file not locked by another process"
 873:             ) from e
 874:     else:
 875:         logger.debug(f"No flag changes detected for {file_path.name} - skipping update")
 876: 
 877: 
 878: def get_prp_status(file_path: Path) -> str:
 879:     """Extract status field from PRP YAML header.
 880: 
 881:     Args:
 882:         file_path: Path to PRP markdown file
 883: 
 884:     Returns:
 885:         Status string (e.g., 'new', 'executed', 'archived')
 886:     """
 887:     metadata, _ = read_prp_header(file_path)
 888:     return metadata.get("status", "unknown")
 889: 
 890: 
 891: def discover_prps(target_prp: Optional[str] = None) -> List[Path]:
 892:     """Scan PRPs/ directory recursively for markdown files.
 893: 
 894:     Args:
 895:         target_prp: Optional specific PRP file path for targeted sync
 896: 
 897:     Returns:
 898:         List of PRP file paths
 899: 
 900:     Raises:
 901:         FileNotFoundError: If target_prp specified but not found
 902:     """
 903:     # Determine project root - if we're in tools/, go up one level
 904:     current_dir = Path.cwd()
 905:     if current_dir.name == "tools":
 906:         project_root = current_dir.parent
 907:     else:
 908:         project_root = current_dir
 909: 
 910:     if target_prp:
 911:         # Targeted sync - single PRP
 912:         prp_path = project_root / target_prp
 913:         if not prp_path.exists():
 914:             raise FileNotFoundError(
 915:                 f"Target PRP not found: {target_prp}\n"
 916:                 f"ðŸ”§ Troubleshooting:\n"
 917:                 f"   - Check path is relative to project root\n"
 918:                 f"   - Use: ls PRPs/executed/ to list available PRPs\n"
 919:                 f"   - Verify file extension is .md"
 920:             )
 921:         return [prp_path]
 922: 
 923:     # Universal sync - all PRPs
 924:     prps_dir = project_root / "PRPs"
 925:     if not prps_dir.exists():
 926:         logger.warning(f"PRPs directory not found: {prps_dir}")
 927:         return []
 928: 
 929:     # Scan feature-requests and executed directories
 930:     prp_files = []
 931:     for subdir in ["feature-requests", "executed", "archived"]:
 932:         subdir_path = prps_dir / subdir
 933:         if subdir_path.exists():
 934:             prp_files.extend(subdir_path.glob("*.md"))
 935: 
 936:     logger.info(f"Discovered {len(prp_files)} PRP files")
 937:     return prp_files
 938: 
 939: 
 940: def extract_expected_functions(content: str) -> List[str]:
 941:     """Extract function/class names from PRP content using regex.
 942: 
 943:     Looks for:
 944:     - `function_name()` backtick references
 945:     - `class ClassName` backtick references
 946:     - def function_name() in code blocks
 947:     - class ClassName: in code blocks
 948: 
 949:     Args:
 950:         content: PRP markdown content
 951: 
 952:     Returns:
 953:         List of function/class names
 954:     """
 955:     functions = set()
 956: 
 957:     # Pattern 1: Backtick references `function_name()`
 958:     backtick_refs = re.findall(r'`(\w+)\(\)`', content)
 959:     functions.update(backtick_refs)
 960: 
 961:     # Pattern 2: Backtick class references `class ClassName`
 962:     class_refs = re.findall(r'`class (\w+)`', content)
 963:     functions.update(class_refs)
 964: 
 965:     # Pattern 3: Function definitions in code blocks
 966:     func_defs = re.findall(r'^\s*def (\w+)\(', content, re.MULTILINE)
 967:     functions.update(func_defs)
 968: 
 969:     # Pattern 4: Class definitions in code blocks
 970:     class_defs = re.findall(r'^\s*class (\w+)[\(:]', content, re.MULTILINE)
 971:     functions.update(class_defs)
 972: 
 973:     return sorted(list(functions))
 974: 
 975: 
 976: # Serena verification removed - Python subprocess cannot access parent's stdio MCP servers
 977: # MCP architecture limitation: stdio transport requires local subprocess spawn
 978: # Serena is internal to Claude Code session and not accessible from uv run subprocess
 979: 
 980: 
 981: def should_transition_to_executed(file_path: Path) -> bool:
 982:     """Check if PRP should transition from feature-requests to executed.
 983: 
 984:     Rules:
 985:     - Current status must be "new" or "in_progress"
 986:     - ce_updated must be True (implementation verified)
 987:     - File must be in feature-requests/ directory
 988: 
 989:     Args:
 990:         file_path: Path to PRP file
 991: 
 992:     Returns:
 993:         True if should transition to executed
 994:     """
 995:     metadata, _ = read_prp_header(file_path)
 996: 
 997:     # Check file location
 998:     if "feature-requests" not in str(file_path):
 999:         return False
1000: 
1001:     # Check status
1002:     status = metadata.get("status", "unknown")
1003:     if status not in ["new", "in_progress"]:
1004:         return False
1005: 
1006:     # Check ce_updated flag
1007:     context_sync = metadata.get("context_sync", {})
1008:     ce_updated = context_sync.get("ce_updated", False)
1009: 
1010:     return ce_updated
1011: 
1012: 
1013: def move_prp_to_executed(file_path: Path) -> Path:
1014:     """Move PRP from feature-requests/ to executed/.
1015: 
1016:     Uses pathlib rename for atomic operation.
1017: 
1018:     Args:
1019:         file_path: Current path to PRP file
1020: 
1021:     Returns:
1022:         New path in executed/ directory
1023: 
1024:     Raises:
1025:         RuntimeError: If move fails
1026:     """
1027:     # Calculate new path
1028:     current_dir = Path.cwd()
1029:     if current_dir.name == "tools":
1030:         project_root = current_dir.parent
1031:     else:
1032:         project_root = current_dir
1033:     executed_dir = project_root / "PRPs" / "executed"
1034: 
1035:     # Create executed directory if needed
1036:     executed_dir.mkdir(parents=True, exist_ok=True)
1037: 
1038:     new_path = executed_dir / file_path.name
1039: 
1040:     try:
1041:         # Atomic move
1042:         file_path.rename(new_path)
1043:         logger.info(f"Moved PRP: {file_path.name} â†’ executed/")
1044:         return new_path
1045:     except Exception as e:
1046:         raise RuntimeError(
1047:             f"Failed to move PRP to executed: {e}\n"
1048:             f"ðŸ”§ Troubleshooting:\n"
1049:             f"   - Check permissions: ls -la {file_path}\n"
1050:             f"   - Ensure target doesn't exist: ls {new_path}\n"
1051:             f"   - Verify disk space: df -h"
1052:         ) from e
1053: 
1054: 
1055: def move_prp_to_archived(file_path: Path) -> Path:
1056:     """Move PRP to archived/ directory.
1057: 
1058:     Args:
1059:         file_path: Current path to PRP file
1060: 
1061:     Returns:
1062:         New path in archived/ directory
1063: 
1064:     Raises:
1065:         RuntimeError: If move fails
1066:     """
1067:     current_dir = Path.cwd()
1068:     if current_dir.name == "tools":
1069:         project_root = current_dir.parent
1070:     else:
1071:         project_root = current_dir
1072:     archived_dir = project_root / "PRPs" / "archived"
1073: 
1074:     # Create archived directory if needed
1075:     archived_dir.mkdir(parents=True, exist_ok=True)
1076: 
1077:     new_path = archived_dir / file_path.name
1078: 
1079:     try:
1080:         file_path.rename(new_path)
1081:         logger.info(f"Archived PRP: {file_path.name} â†’ archived/")
1082:         return new_path
1083:     except Exception as e:
1084:         raise RuntimeError(
1085:             f"Failed to archive PRP: {e}\n"
1086:             f"ðŸ”§ Troubleshooting: Check permissions and disk space"
1087:         ) from e
1088: 
1089: 
1090: def detect_archived_prps() -> List[Path]:
1091:     """Identify superseded/deprecated PRPs for archival.
1092: 
1093:     Looks for:
1094:     - status == "archived" in YAML
1095:     - "superseded_by" field in metadata
1096: 
1097:     Returns:
1098:         List of PRP paths that should be archived
1099:     """
1100:     archived_candidates = []
1101:     all_prps = discover_prps()
1102: 
1103:     for prp_path in all_prps:
1104:         # Skip if already in archived/
1105:         if "archived" in str(prp_path):
1106:             continue
1107: 
1108:         try:
1109:             metadata, _ = read_prp_header(prp_path)
1110: 
1111:             # Check status
1112:             if metadata.get("status") == "archived":
1113:                 archived_candidates.append(prp_path)
1114:                 continue
1115: 
1116:             # Check superseded_by field
1117:             if "superseded_by" in metadata:
1118:                 archived_candidates.append(prp_path)
1119: 
1120:         except Exception as e:
1121:             logger.warning(f"Skipping {prp_path.name} - invalid YAML: {e}")
1122:             continue
1123: 
1124:     return archived_candidates
1125: 
1126: 
1127: def load_pattern_checks() -> Dict[str, List[Tuple[str, str, str]]]:
1128:     """Load pattern checks from PATTERN_CHECKS.
1129: 
1130:     Returns:
1131:         {
1132:             "error_handling": [
1133:                 ("bare_except", "regex", "fix description"),
1134:                 ...
1135:             ]
1136:         }
1137:     """
1138:     return PATTERN_CHECKS
1139: 
1140: 
1141: def verify_codebase_matches_examples() -> Dict[str, Any]:
1142:     """Check if codebase follows patterns documented in examples/.
1143: 
1144:     Returns:
1145:         {
1146:             "violations": [
1147:                 "File tools/ce/foo.py uses bare except (violates examples/patterns/error-handling.py)",
1148:                 ...
1149:             ],
1150:             "drift_score": 15.3  # Percentage of files violating patterns
1151:         }
1152: 
1153:     Refactored to reduce nesting depth from 5 to 4 levels.
1154:     """
1155:     from .pattern_detectors import check_file_for_violations
1156: 
1157:     current_dir = Path.cwd()
1158:     if current_dir.name == "tools":
1159:         project_root = current_dir.parent
1160:     else:
1161:         project_root = current_dir
1162:     examples_dir = project_root / "examples"
1163: 
1164:     # Skip if examples/ doesn't exist
1165:     if not examples_dir.exists():
1166:         logger.info("examples/ directory not found - skipping drift detection")
1167:         return {"violations": [], "drift_score": 0.0}
1168: 
1169:     violations = []
1170:     pattern_checks = load_pattern_checks()
1171: 
1172:     # Scan tools/ce/ for violations
1173:     tools_ce_dir = project_root / "tools" / "ce"
1174:     if not tools_ce_dir.exists():
1175:         return {"violations": [], "drift_score": 0.0}
1176: 
1177:     python_files = list(tools_ce_dir.glob("*.py"))
1178:     files_with_violations = set()
1179: 
1180:     # Process each file (delegated to reduce nesting)
1181:     for py_file in python_files:
1182:         file_violations, has_violations = check_file_for_violations(
1183:             py_file, pattern_checks, project_root
1184:         )
1185:         violations.extend(file_violations)
1186:         if has_violations:
1187:             files_with_violations.add(py_file)
1188: 
1189:     # Calculate drift score based on violation count, not file count
1190:     drift_score = 0.0
1191:     if python_files:
1192:         total_checks = len(python_files) * sum(len(checks) for checks in pattern_checks.values())
1193:         if total_checks > 0:
1194:             drift_score = (len(violations) / total_checks) * 100
1195: 
1196:     return {
1197:         "violations": violations,
1198:         "drift_score": drift_score
1199:     }
1200: 
1201: 
1202: def detect_missing_examples_for_prps() -> List[Dict[str, Any]]:
1203:     """Detect executed PRPs missing corresponding examples/ documentation.
1204: 
1205:     Returns:
1206:         [
1207:             {
1208:                 "prp_id": "PRP-13",
1209:                 "feature_name": "Production Hardening",
1210:                 "complexity": "high",
1211:                 "missing_example": "error_recovery",
1212:                 "suggested_path": "examples/patterns/error-recovery.py",
1213:                 "rationale": "Complex error recovery logic should be documented"
1214:             },
1215:             ...
1216:         ]
1217: 
1218:     Refactored to reduce nesting depth from 5 to 4 levels.
1219:     """
1220:     from .pattern_detectors import check_prp_for_missing_examples
1221: 
1222:     current_dir = Path.cwd()
1223:     if current_dir.name == "tools":
1224:         project_root = current_dir.parent
1225:     else:
1226:         project_root = current_dir
1227:     examples_dir = project_root / "examples"
1228:     missing_examples = []
1229: 
1230:     # Define keyword patterns
1231:     keywords_to_examples = {
1232:         "error recovery": ("error_recovery", "examples/patterns/error-recovery.py",
1233:                            "Complex error recovery logic should be documented"),
1234:         "strategy pattern": ("strategy_pattern_testing", "examples/patterns/strategy-testing.py",
1235:                              "Strategy pattern with mocks is reusable pattern"),
1236:         "pipeline": ("pipeline_testing", "examples/patterns/pipeline-testing.py",
1237:                      "Pipeline orchestration pattern should be documented")
1238:     }
1239: 
1240:     # Get all executed PRPs
1241:     executed_prps = (project_root / "PRPs" / "executed").glob("*.md")
1242: 
1243:     # Check each PRP (delegated to reduce nesting)
1244:     for prp_path in executed_prps:
1245:         prp_missing = check_prp_for_missing_examples(
1246:             prp_path, project_root, keywords_to_examples
1247:         )
1248:         missing_examples.extend(prp_missing)
1249: 
1250:     return missing_examples
1251: 
1252: 
1253: def generate_drift_report(violations: List[str], drift_score: float,
1254:                           missing_examples: List[Dict[str, Any]]) -> str:
1255:     """Generate formalized structured drift report with solution proposals.
1256: 
1257:     Args:
1258:         violations: List of violation messages
1259:         drift_score: Percentage of files violating patterns
1260:         missing_examples: List of PRPs missing examples
1261: 
1262:     Returns:
1263:         Markdown formatted drift report
1264:     """
1265:     now = datetime.now(timezone.utc).isoformat()
1266: 
1267:     # Classify drift score
1268:     drift_level = "âœ… OK" if drift_score < 5 else ("âš ï¸  WARNING" if drift_score < 15 else "ðŸš¨ CRITICAL")
1269: 
1270:     report = f"""## Context Drift Report - Examples/ Patterns
1271: 
1272: **Drift Score**: {drift_score:.1f}% ({drift_level})
1273: **Generated**: {now}
1274: **Violations Found**: {len(violations)}
1275: **Missing Examples**: {len(missing_examples)}
1276: 
1277: ### Part 1: Code Violating Documented Patterns
1278: 
1279: """
1280: 
1281:     if violations:
1282:         # Group violations by category
1283:         error_handling_violations = [v for v in violations if "error_handling" in v or "bare_except" in v]
1284:         naming_violations = [v for v in violations if "naming" in v or "version_suffix" in v]
1285:         kiss_violations = [v for v in violations if "kiss" in v or "nesting" in v]
1286: 
1287:         if error_handling_violations:
1288:             report += f"#### Error Handling ({len(error_handling_violations)} violations)\n\n"
1289:             for i, v in enumerate(error_handling_violations, 1):
1290:                 report += f"{i}. {v}\n"
1291:             report += "\n"
1292: 
1293:         if naming_violations:
1294:             report += f"#### Naming Conventions ({len(naming_violations)} violations)\n\n"
1295:             for i, v in enumerate(naming_violations, 1):
1296:                 report += f"{i}. {v}\n"
1297:             report += "\n"
1298: 
1299:         if kiss_violations:
1300:             report += f"#### KISS Violations ({len(kiss_violations)} violations)\n\n"
1301:             for i, v in enumerate(kiss_violations, 1):
1302:                 report += f"{i}. {v}\n"
1303:             report += "\n"
1304:     else:
1305:         report += "No violations detected - codebase follows documented patterns.\n\n"
1306: 
1307:     report += """### Part 2: Missing Pattern Documentation
1308: 
1309: **Critical PRPs Without Examples**:
1310: 
1311: """
1312: 
1313:     if missing_examples:
1314:         for i, missing in enumerate(missing_examples, 1):
1315:             report += f"""{i}. **{missing['prp_id']}**: {missing['feature_name']}
1316:    **Complexity**: {missing['complexity']}
1317:    **Missing Example**: {missing['missing_example']}
1318:    **Suggested Path**: {missing['suggested_path']}
1319:    **Rationale**: {missing['rationale']}
1320:    **Action**: Create example showing this pattern
1321: 
1322: """
1323:     else:
1324:         report += "All critical PRPs have corresponding examples/ documentation.\n\n"
1325: 
1326:     report += """### Proposed Solutions Summary
1327: 
1328: 1. **Code Violations** (manual review):
1329: """
1330:     if violations:
1331:         for v in violations[:3]:  # Show first 3
1332:             report += f"   - Review and fix: {v}\n"
1333:         if len(violations) > 3:
1334:             report += f"   - Review {len(violations) - 3} other files listed in Part 1\n"
1335:     else:
1336:         report += "   - No violations to fix\n"
1337: 
1338:     report += """
1339: 2. **Missing Examples** (documentation needed):
1340: """
1341:     if missing_examples:
1342:         for missing in missing_examples[:3]:  # Show first 3
1343:             report += f"   - Create {missing['suggested_path']} (from {missing['prp_id']})\n"
1344:         if len(missing_examples) > 3:
1345:             report += f"   - Create {len(missing_examples) - 3} other examples listed in Part 2\n"
1346:     else:
1347:         report += "   - No missing examples\n"
1348: 
1349:     report += """
1350: 3. **Prevention**:
1351:    - Add pre-commit hook: ce validate --level 4 (pattern conformance)
1352:    - Run /update-context weekly to detect drift early
1353:    - Update CLAUDE.md when new patterns emerge
1354: 
1355: ### Next Steps
1356: 1. Review violations in Part 1 and fix manually
1357: 2. Create missing examples from Part 2
1358: 3. **ðŸ”§ CRITICAL - Validate Each Fix**:
1359:    - After fixing each violation, run: ce update-context
1360:    - Verify violation removed from drift report
1361:    - If still present: Analyze why fix didn't work, try different approach
1362: 4. Validate: ce validate --level 4
1363: 5. Update patterns if codebase evolution is intentional
1364: 6. Re-run /update-context to verify drift resolved
1365: 
1366: **Anti-Pattern**: Batch-apply all fixes without validation (violations may persist)
1367: **Correct Pattern**: Fix â†’ Validate â†’ Next fix (iterative verification)
1368: """
1369: 
1370:     return report
1371: 
1372: 
1373: def get_cache_ttl(cli_ttl: Optional[int] = None) -> int:
1374:     """Get cache TTL from CLI arg, config, or default.
1375: 
1376:     Priority:
1377:         1. CLI flag (--cache-ttl)
1378:         2. .ce/config.yml value
1379:         3. Hardcoded default (5 minutes)
1380: 
1381:     Args:
1382:         cli_ttl: TTL from command-line --cache-ttl flag
1383: 
1384:     Returns:
1385:         TTL in minutes
1386: 
1387:     Example:
1388:         >>> ttl = get_cache_ttl()
1389:         >>> assert ttl >= 1
1390:         >>> ttl = get_cache_ttl(cli_ttl=10)
1391:         >>> assert ttl == 10
1392:     """
1393:     # Priority 1: CLI flag
1394:     if cli_ttl is not None:
1395:         return cli_ttl
1396: 
1397:     # Priority 2: Config file
1398:     current_dir = Path.cwd()
1399:     if current_dir.name == "tools":
1400:         project_root = current_dir.parent
1401:     else:
1402:         project_root = current_dir
1403: 
1404:     config_path = project_root / ".ce" / "config.yml"
1405:     if config_path.exists():
1406:         try:
1407:             import yaml
1408:             config = yaml.safe_load(config_path.read_text())
1409:             ttl = config.get("cache", {}).get("analysis_ttl_minutes")
1410:             if ttl is not None:
1411:                 return int(ttl)
1412:         except Exception:
1413:             pass  # Fall back to default
1414: 
1415:     # Priority 3: Default
1416:     return 5
1417: 
1418: 
1419: def get_cached_analysis() -> Optional[Dict[str, Any]]:
1420:     """Read cached drift analysis from report file.
1421: 
1422:     Parses .ce/drift-report.md to extract cached analysis results.
1423: 
1424:     Returns:
1425:         Cached analysis dict or None if not found
1426: 
1427:     Example:
1428:         >>> cached = get_cached_analysis()
1429:         >>> if cached:
1430:         ...     assert "drift_score" in cached
1431:         ...     assert "generated_at" in cached
1432:     """
1433:     current_dir = Path.cwd()
1434:     if current_dir.name == "tools":
1435:         project_root = current_dir.parent
1436:     else:
1437:         project_root = current_dir
1438: 
1439:     report_path = project_root / ".ce" / "drift-report.md"
1440:     if not report_path.exists():
1441:         return None
1442: 
1443:     try:
1444:         content = report_path.read_text()
1445: 
1446:         # Extract timestamp from report
1447:         # Format: **Generated**: 2025-10-16T20:03:32.185604+00:00
1448:         timestamp_match = re.search(
1449:             r'\*\*Generated\*\*: (.+?)$',
1450:             content,
1451:             re.MULTILINE
1452:         )
1453:         if not timestamp_match:
1454:             return None
1455: 
1456:         generated_at = timestamp_match.group(1).strip()
1457: 
1458:         # Extract drift score
1459:         score_match = re.search(r'\*\*Drift Score\*\*: ([\d.]+)%', content)
1460:         if not score_match:
1461:             return None
1462: 
1463:         drift_score = float(score_match.group(1))
1464: 
1465:         # Extract violation count
1466:         violations_match = re.search(r'\*\*Violations Found\*\*: (\d+)', content)
1467:         violation_count = int(violations_match.group(1)) if violations_match else 0
1468: 
1469:         # Classify drift level
1470:         if drift_score < 5:
1471:             drift_level = "ok"
1472:         elif drift_score < 15:
1473:             drift_level = "warning"
1474:         else:
1475:             drift_level = "critical"
1476: 
1477:         return {
1478:             "drift_score": drift_score,
1479:             "drift_level": drift_level,
1480:             "violation_count": violation_count,
1481:             "report_path": str(report_path),
1482:             "generated_at": generated_at,
1483:             "cached": True
1484:         }
1485: 
1486:     except Exception as e:
1487:         logger.debug(f"Failed to read cache: {e}")
1488:         return None
1489: 
1490: 
1491: def get_cache_ttl(override_ttl: int = None) -> int:
1492:     """Get cache TTL from config or environment, with validation.
1493: 
1494:     Args:
1495:         override_ttl: Optional override from CLI --cache-ttl flag
1496: 
1497:     Returns:
1498:         Cache TTL in minutes (minimum 1, default 5)
1499: 
1500:     Sources (in priority order):
1501:         1. override_ttl parameter (CLI --cache-ttl)
1502:         2. CONTEXT_CACHE_TTL environment variable
1503:         3. .ce/config.yml cache.analysis_ttl_minutes
1504:         4. Default: 5 minutes
1505: 
1506:     ðŸ”§ Troubleshooting:
1507:         - Set env: export CONTEXT_CACHE_TTL=10
1508:         - Or configure: echo "cache: {analysis_ttl_minutes: 10}" >> .ce/config.yml
1509:     """
1510:     import os
1511: 
1512:     # Check CLI override first
1513:     if override_ttl is not None:
1514:         try:
1515:             ttl = max(1, int(override_ttl))  # Minimum 1 minute
1516:             logger.debug(f"Cache TTL from CLI: {ttl} minutes")
1517:             return ttl
1518:         except (ValueError, TypeError):
1519:             logger.warning(f"Invalid cache TTL override: {override_ttl}, ignoring")
1520: 
1521:     # Check environment variable
1522:     env_ttl = os.getenv("CONTEXT_CACHE_TTL")
1523:     if env_ttl:
1524:         try:
1525:             ttl = max(1, int(env_ttl))  # Minimum 1 minute
1526:             logger.debug(f"Cache TTL from env: {ttl} minutes")
1527:             return ttl
1528:         except ValueError:
1529:             logger.warning(f"Invalid CONTEXT_CACHE_TTL: {env_ttl}, using default")
1530: 
1531:     # Check .ce/config.yml
1532:     try:
1533:         current_dir = Path.cwd()
1534:         if current_dir.name == "tools":
1535:             project_root = current_dir.parent
1536:         else:
1537:             project_root = current_dir
1538: 
1539:         config_path = project_root / ".ce" / "config.yml"
1540:         if config_path.exists():
1541:             config = yaml.safe_load(config_path.read_text()) or {}
1542:             cache_config = config.get("cache", {})
1543:             ttl = cache_config.get("analysis_ttl_minutes")
1544:             if ttl:
1545:                 ttl = max(1, int(ttl))  # Minimum 1 minute
1546:                 logger.debug(f"Cache TTL from config: {ttl} minutes")
1547:                 return ttl
1548:     except Exception as e:
1549:         logger.debug(f"Failed to read cache config: {e}")
1550: 
1551:     # Default
1552:     logger.debug("Using default cache TTL: 5 minutes")
1553:     return 5
1554: 
1555: 
1556: def is_cache_valid(cached: Dict[str, Any], ttl_minutes: int = 0) -> bool:
1557:     """Check if cached analysis is still valid.
1558: 
1559:     Args:
1560:         cached: Cached analysis dict with 'generated_at' field
1561:         ttl_minutes: Cache time-to-live in minutes. If 0, uses get_cache_ttl()
1562: 
1563:     Returns:
1564:         True if cache is fresh (< TTL), False otherwise
1565: 
1566:     Example:
1567:         >>> cached = {"generated_at": "2025-10-17T10:00:00+00:00"}
1568:         >>> is_valid = is_cache_valid(cached, ttl_minutes=5)
1569:         >>> assert isinstance(is_valid, bool)
1570:     """
1571:     # Use configured TTL if not specified
1572:     if ttl_minutes == 0:
1573:         ttl_minutes = get_cache_ttl()
1574: 
1575:     try:
1576:         # Parse timestamp (handle multiple formats)
1577:         generated_str = cached["generated_at"]
1578: 
1579:         # Replace timezone suffix for consistent parsing
1580:         generated_str = generated_str.replace("+00:00", "+00:00")
1581: 
1582:         generated_at = datetime.fromisoformat(generated_str)
1583: 
1584:         # Ensure timezone aware
1585:         if generated_at.tzinfo is None:
1586:             generated_at = generated_at.replace(tzinfo=timezone.utc)
1587: 
1588:         now = datetime.now(timezone.utc)
1589:         age_minutes = (now - generated_at).total_seconds() / 60
1590: 
1591:         is_valid = age_minutes < ttl_minutes
1592:         if not is_valid:
1593:             logger.debug(f"Cache expired: {age_minutes:.1f}m old, TTL: {ttl_minutes}m")
1594:         return is_valid
1595: 
1596:     except Exception as e:
1597:         logger.debug(f"Cache validation failed: {e}")
1598:         return False
1599: 
1600: 
1601: def analyze_context_drift() -> Dict[str, Any]:
1602:     """Run drift analysis and generate report.
1603: 
1604:     Fast drift detection without metadata updates - optimized for CI/CD.
1605: 
1606:     Returns:
1607:         {
1608:             "drift_score": 17.9,
1609:             "drift_level": "critical",  # ok, warning, critical
1610:             "violations": ["..."],
1611:             "violation_count": 5,
1612:             "missing_examples": [...],
1613:             "report_path": ".ce/drift-report.md",
1614:             "generated_at": "2025-10-16T20:15:00Z",
1615:             "duration_seconds": 2.3
1616:         }
1617: 
1618:     Raises:
1619:         RuntimeError: If analysis fails with troubleshooting guidance
1620: 
1621:     Example:
1622:         >>> result = analyze_context_drift()
1623:         >>> assert result["drift_level"] in ["ok", "warning", "critical"]
1624:         >>> assert 0 <= result["drift_score"] <= 100
1625:     """
1626:     import time
1627:     start_time = time.time()
1628: 
1629:     try:
1630:         # Run drift detection (existing functions)
1631:         drift_result = verify_codebase_matches_examples()
1632:         missing_examples = detect_missing_examples_for_prps()
1633: 
1634:         # Generate report
1635:         report = generate_drift_report(
1636:             drift_result["violations"],
1637:             drift_result["drift_score"],
1638:             missing_examples
1639:         )
1640: 
1641:         # Save report
1642:         current_dir = Path.cwd()
1643:         if current_dir.name == "tools":
1644:             project_root = current_dir.parent
1645:         else:
1646:             project_root = current_dir
1647: 
1648:         ce_dir = project_root / ".ce"
1649:         ce_dir.mkdir(exist_ok=True)
1650:         report_path = ce_dir / "drift-report.md"
1651:         atomic_write(report_path, report)
1652: 
1653:         # Calculate duration
1654:         duration = time.time() - start_time
1655: 
1656:         # Classify drift level
1657:         drift_score = drift_result["drift_score"]
1658:         if drift_score < 5:
1659:             drift_level = "ok"
1660:         elif drift_score < 15:
1661:             drift_level = "warning"
1662:         else:
1663:             drift_level = "critical"
1664: 
1665:         return {
1666:             "drift_score": drift_score,
1667:             "drift_level": drift_level,
1668:             "violations": drift_result["violations"],
1669:             "violation_count": len(drift_result["violations"]),
1670:             "missing_examples": missing_examples,
1671:             "report_path": str(report_path),
1672:             "generated_at": datetime.now(timezone.utc).isoformat(),
1673:             "duration_seconds": round(duration, 1)
1674:         }
1675: 
1676:     except Exception as e:
1677:         raise RuntimeError(
1678:             f"Drift analysis failed: {e}\n"
1679:             f"ðŸ”§ Troubleshooting:\n"
1680:             f"   - Ensure examples/ directory exists\n"
1681:             f"   - Check PRPs have valid YAML headers\n"
1682:             f"   - Verify tools/ce/ directory is accessible\n"
1683:             f"   - Run: cd tools && uv run ce validate --level 1"
1684:         ) from e
1685: 
1686: 
1687: def sync_context(target_prp: Optional[str] = None) -> Dict[str, Any]:
1688:     """Execute context sync workflow.
1689: 
1690:     Args:
1691:         target_prp: Optional PRP file path for targeted sync
1692: 
1693:     Returns:
1694:         {
1695:             "success": True,
1696:             "prps_scanned": 15,
1697:             "prps_updated": 8,
1698:             "prps_moved": 2,
1699:             "ce_updated_count": 8,
1700:             "serena_updated_count": 5,
1701:             "errors": []
1702:         }
1703:     """
1704:     logger.info("Starting context sync...")
1705: 
1706:     # Initialize counters
1707:     prps_scanned = 0
1708:     prps_updated = 0
1709:     prps_moved = 0
1710:     ce_updated_count = 0
1711:     serena_updated_count = 0
1712:     errors = []
1713: 
1714:     # Discover PRPs
1715:     try:
1716:         prp_files = discover_prps(target_prp)
1717:     except Exception as e:
1718:         logger.error(f"Failed to discover PRPs: {e}")
1719:         return {
1720:             "success": False,
1721:             "prps_scanned": 0,
1722:             "prps_updated": 0,
1723:             "prps_moved": 0,
1724:             "ce_updated_count": 0,
1725:             "serena_updated_count": 0,
1726:             "errors": [str(e)]
1727:         }
1728: 
1729:     # Process each PRP
1730:     for prp_path in prp_files:
1731:         prps_scanned += 1
1732: 
1733:         try:
1734:             # Read PRP
1735:             metadata, content = read_prp_header(prp_path)
1736: 
1737:             # Extract expected functions
1738:             expected_functions = extract_expected_functions(content)
1739: 
1740:             # Verify functions actually exist in codebase using AST
1741:             current_dir = Path.cwd()
1742:             if current_dir.name == "tools":
1743:                 project_root = current_dir.parent
1744:             else:
1745:                 project_root = current_dir
1746:             tools_ce_dir = project_root / "tools" / "ce"
1747: 
1748:             ce_verified = False
1749:             if expected_functions and tools_ce_dir.exists():
1750:                 # Check if ALL expected functions exist
1751:                 all_found = all(
1752:                     verify_function_exists_ast(func, tools_ce_dir)
1753:                     for func in expected_functions
1754:                 )
1755:                 ce_verified = all_found
1756: 
1757:             # Serena verification disabled (subprocess cannot access parent's stdio MCP)
1758:             serena_verified = False
1759: 
1760:             # Update context_sync flags
1761:             update_context_sync_flags(prp_path, ce_verified, serena_verified)
1762:             prps_updated += 1
1763: 
1764:             if ce_verified:
1765:                 ce_updated_count += 1
1766:             if serena_verified:
1767:                 serena_updated_count += 1
1768: 
1769:             # Check status transition
1770:             if should_transition_to_executed(prp_path):
1771:                 new_path = move_prp_to_executed(prp_path)
1772:                 prps_moved += 1
1773:                 prp_path = new_path  # Update path for drift detection
1774: 
1775:         except Exception as e:
1776:             error_msg = f"Error processing {prp_path.name}: {e}"
1777:             logger.error(error_msg)
1778:             errors.append(error_msg)
1779:             continue
1780: 
1781:     # Drift detection (universal sync only) with caching
1782:     if not target_prp:
1783:         logger.info("Running drift detection...")
1784: 
1785:         # Check cache with configured TTL (reads from env/config/default)
1786:         cached = get_cached_analysis()
1787:         if cached and is_cache_valid(cached):  # Uses get_cache_ttl() internally
1788:             logger.info(f"Using cached drift analysis ({cached['drift_score']:.1f}%)")
1789:             drift_score = cached["drift_score"]
1790:             report_path = Path(cached["report_path"])
1791:         else:
1792:             # Run fresh analysis
1793:             logger.info("Running fresh drift analysis (cache expired or not found)")
1794:             analysis_result = analyze_context_drift()
1795:             drift_score = analysis_result["drift_score"]
1796:             report_path = Path(analysis_result["report_path"])
1797: 
1798:         # Display warning if drift detected
1799:         if drift_score >= 5:
1800:             logger.warning(
1801:                 f"Examples drift detected: {drift_score:.1f}%\n"
1802:                 f"ðŸ“Š Report saved: {report_path}\n"
1803:                 f"ðŸ”§ Review and apply fixes: cat {report_path}"
1804:             )
1805: 
1806:     logger.info("Context sync completed")
1807: 
1808:     return {
1809:         "success": len(errors) == 0,
1810:         "prps_scanned": prps_scanned,
1811:         "prps_updated": prps_updated,
1812:         "prps_moved": prps_moved,
1813:         "ce_updated_count": ce_updated_count,
1814:         "serena_updated_count": serena_updated_count,
1815:         "errors": errors
1816:     }
</file>

<file path="tools/ce/validate_permissions.py">
  1: """Permission validation utility - replaces jq/grep for settings checks."""
  2: import json
  3: from pathlib import Path
  4: from typing import Dict, List
  5: 
  6: 
  7: def load_settings() -> Dict:
  8:     """Load .claude/settings.local.json
  9: 
 10:     Returns:
 11:         Dict with permissions configuration
 12: 
 13:     Raises:
 14:         FileNotFoundError: If settings file doesn't exist
 15:         json.JSONDecodeError: If settings file is malformed
 16:     """
 17:     settings_path = Path(__file__).parent.parent.parent / ".claude/settings.local.json"
 18: 
 19:     if not settings_path.exists():
 20:         raise FileNotFoundError(
 21:             f"Settings file not found: {settings_path}\n"
 22:             "ðŸ”§ Troubleshooting: Ensure .claude/settings.local.json exists"
 23:         )
 24: 
 25:     return json.loads(settings_path.read_text())
 26: 
 27: 
 28: def count_permissions() -> Dict[str, int]:
 29:     """Count allow/deny tools.
 30: 
 31:     Returns:
 32:         Dict with 'allow' and 'deny' counts
 33:     """
 34:     settings = load_settings()
 35:     return {
 36:         "allow": len(settings["permissions"]["allow"]),
 37:         "deny": len(settings["permissions"]["deny"])
 38:     }
 39: 
 40: 
 41: def search_tool(pattern: str, permission_type: str = "allow") -> List[str]:
 42:     """Search for tools matching pattern in allow/deny list.
 43: 
 44:     Args:
 45:         pattern: String pattern to search for (case-sensitive)
 46:         permission_type: Either "allow" or "deny"
 47: 
 48:     Returns:
 49:         List of matching tool names
 50:     """
 51:     settings = load_settings()
 52:     tools = settings["permissions"][permission_type]
 53:     return [t for t in tools if pattern in t]
 54: 
 55: 
 56: def verify_tool_exists(tool_name: str) -> Dict[str, bool]:
 57:     """Check if tool exists in allow or deny list.
 58: 
 59:     Args:
 60:         tool_name: Exact tool name to search for
 61: 
 62:     Returns:
 63:         Dict with 'in_allow' and 'in_deny' boolean flags
 64:     """
 65:     settings = load_settings()
 66:     return {
 67:         "in_allow": tool_name in settings["permissions"]["allow"],
 68:         "in_deny": tool_name in settings["permissions"]["deny"]
 69:     }
 70: 
 71: 
 72: def categorize_tools() -> Dict[str, List[str]]:
 73:     """Group allowed tools by category.
 74: 
 75:     Returns:
 76:         Dict mapping category names to lists of tool names
 77:     """
 78:     settings = load_settings()
 79:     allowed = settings["permissions"]["allow"]
 80: 
 81:     categories = {
 82:         "bash": [t for t in allowed if t.startswith("Bash(")],
 83:         "serena": [t for t in allowed if t.startswith("mcp__serena__")],
 84:         "filesystem": [t for t in allowed if t.startswith("mcp__filesystem__")],
 85:         "git": [t for t in allowed if t.startswith("mcp__git__")],
 86:         "context7": [t for t in allowed if t.startswith("mcp__context7__")],
 87:         "sequential": [t for t in allowed if t.startswith("mcp__sequential-thinking__")],
 88:         "linear": [t for t in allowed if t.startswith("mcp__linear-server__")],
 89:         "repomix": [t for t in allowed if t.startswith("mcp__repomix__")],
 90:         "special": [t for t in allowed if t.startswith(("Read(", "WebFetch(", "SlashCommand("))]
 91:     }
 92: 
 93:     return categories
 94: 
 95: 
 96: if __name__ == "__main__":
 97:     import sys
 98: 
 99:     if len(sys.argv) < 2:
100:         print("Usage: python validate_permissions.py [count|search|verify|categorize]")
101:         print("\nCommands:")
102:         print("  count                    - Show allow/deny counts")
103:         print("  search <pattern> [type]  - Search for pattern in allow/deny list")
104:         print("  verify <tool_name>       - Check if tool is in allow/deny")
105:         print("  categorize               - Show tools grouped by category")
106:         sys.exit(1)
107: 
108:     action = sys.argv[1]
109: 
110:     try:
111:         if action == "count":
112:             counts = count_permissions()
113:             print(f"Allow: {counts['allow']}")
114:             print(f"Deny: {counts['deny']}")
115: 
116:         elif action == "search" and len(sys.argv) >= 3:
117:             pattern = sys.argv[2]
118:             perm_type = sys.argv[3] if len(sys.argv) >= 4 else "allow"
119:             matches = search_tool(pattern, perm_type)
120:             if matches:
121:                 for match in matches:
122:                     print(match)
123:             else:
124:                 print(f"No matches found for pattern '{pattern}' in {perm_type} list")
125: 
126:         elif action == "verify" and len(sys.argv) >= 3:
127:             tool = sys.argv[2]
128:             result = verify_tool_exists(tool)
129:             print(f"In allow: {result['in_allow']}")
130:             print(f"In deny: {result['in_deny']}")
131: 
132:         elif action == "categorize":
133:             cats = categorize_tools()
134:             total = sum(len(tools) for tools in cats.values())
135:             print(f"Total allowed tools: {total}\n")
136:             for cat, tools in cats.items():
137:                 print(f"{cat.upper()} ({len(tools)}):")
138:                 for tool in sorted(tools):
139:                     print(f"  - {tool}")
140:                 print()
141: 
142:         else:
143:             print(f"Unknown action: {action}")
144:             print("Use: count, search, verify, or categorize")
145:             sys.exit(1)
146: 
147:     except Exception as e:
148:         print(f"âŒ Error: {e}")
149:         sys.exit(1)
</file>

<file path="tools/ce/validate.py">
  1: """Validation gates: 4-level validation system."""
  2: 
  3: import sys
  4: import re
  5: import time
  6: from typing import Dict, Any, List, Optional
  7: from pathlib import Path
  8: from datetime import datetime, timezone
  9: 
 10: from .core import run_cmd
 11: from .pattern_extractor import extract_patterns_from_prp
 12: from .drift_analyzer import analyze_implementation, calculate_drift_score, get_auto_fix_suggestions
 13: from .mermaid_validator import lint_all_markdown_mermaid
 14: 
 15: 
 16: def validate_level_1() -> Dict[str, Any]:
 17:     """Run Level 1 validation: Syntax & Style (lint + type-check + markdown-lint).
 18: 
 19:     Returns:
 20:         Dict with: success (bool), errors (List[str]), duration (float)
 21: 
 22:     Raises:
 23:         RuntimeError: If validation commands fail to execute
 24: 
 25:     Note: Real validation - no mocked results.
 26:     """
 27:     errors = []
 28:     total_duration = 0.0
 29: 
 30:     # Run lint (optional - skip if not configured)
 31:     lint_result = run_cmd("npm run lint", capture_output=True)
 32:     total_duration += lint_result["duration"]
 33: 
 34:     if not lint_result["success"] and "Missing script" not in lint_result["stderr"]:
 35:         errors.append(f"Lint failed:\n{lint_result['stderr']}")
 36: 
 37:     # Run type-check (optional - skip if not configured)
 38:     typecheck_result = run_cmd("npm run type-check", capture_output=True)
 39:     total_duration += typecheck_result["duration"]
 40: 
 41:     if not typecheck_result["success"] and "Missing script" not in typecheck_result["stderr"]:
 42:         errors.append(f"Type-check failed:\n{typecheck_result['stderr']}")
 43: 
 44:     # Run markdown-lint (accept minor errors in old research files)
 45:     markdownlint_result = run_cmd("npm run lint:md", capture_output=True)
 46:     total_duration += markdownlint_result["duration"]
 47: 
 48:     # Check if errors are only in old research files (acceptable)
 49:     if not markdownlint_result["success"]:
 50:         stderr = markdownlint_result["stderr"]
 51:         # Count errors and check if they're only in old research files
 52:         error_lines = [line for line in stderr.split("\n") if "99-context-mastery-exploration-original.md" in line or "MD046" in line]
 53:         critical_errors = [line for line in stderr.split("\n") if line.startswith("docs/") or line.startswith("PRPs/") or line.startswith("examples/")]
 54:         critical_errors = [e for e in critical_errors if "99-context-mastery-exploration-original.md" not in e]
 55: 
 56:         if critical_errors:
 57:             errors.append(f"Markdown lint failed:\n{markdownlint_result['stderr']}")
 58: 
 59:     # Run mermaid validation
 60:     mermaid_start = time.time()
 61:     mermaid_result = lint_all_markdown_mermaid(".", auto_fix=True)
 62:     mermaid_duration = time.time() - mermaid_start
 63:     total_duration += mermaid_duration
 64: 
 65:     if not mermaid_result["success"]:
 66:         errors.append(f"Mermaid validation failed: {len(mermaid_result['errors'])} issues")
 67:         for error in mermaid_result['errors'][:5]:  # Show first 5
 68:             errors.append(f"  - {error}")
 69:     elif mermaid_result["fixes_applied"]:
 70:         print(f"âœ… Mermaid auto-fixes applied: {len(mermaid_result['fixes_applied'])} fixes", file=sys.stderr)
 71: 
 72:     return {
 73:         "success": len(errors) == 0,
 74:         "errors": errors,
 75:         "duration": total_duration,
 76:         "level": 1
 77:     }
 78: 
 79: 
 80: def validate_level_2() -> Dict[str, Any]:
 81:     """Run Level 2 validation: Unit Tests.
 82: 
 83:     Returns:
 84:         Dict with: success (bool), errors (List[str]), duration (float)
 85: 
 86:     Raises:
 87:         RuntimeError: If test command fails to execute
 88: 
 89:     Note: Real test execution - no mocked test pass.
 90:     """
 91:     result = run_cmd("npm test", capture_output=True)
 92: 
 93:     errors = []
 94:     if not result["success"]:
 95:         errors.append(f"Unit tests failed:\n{result['stderr']}")
 96: 
 97:     return {
 98:         "success": result["success"],
 99:         "errors": errors,
100:         "duration": result["duration"],
101:         "level": 2
102:     }
103: 
104: 
105: def validate_level_3() -> Dict[str, Any]:
106:     """Run Level 3 validation: Integration Tests.
107: 
108:     Returns:
109:         Dict with: success (bool), errors (List[str]), duration (float)
110: 
111:     Raises:
112:         RuntimeError: If integration test command fails to execute
113: 
114:     Note: Real integration test execution.
115:     """
116:     result = run_cmd("npm run test:integration", capture_output=True)
117: 
118:     errors = []
119:     if not result["success"]:
120:         errors.append(f"Integration tests failed:\n{result['stderr']}")
121: 
122:     return {
123:         "success": result["success"],
124:         "errors": errors,
125:         "duration": result["duration"],
126:         "level": 3
127:     }
128: 
129: 
130: def validate_level_4(
131:     prp_path: str,
132:     implementation_paths: Optional[List[str]] = None
133: ) -> Dict[str, Any]:
134:     """Run Level 4 validation: Pattern Conformance.
135: 
136:     Args:
137:         prp_path: Path to PRP markdown file
138:         implementation_paths: Files to analyze; auto-detected if None via:
139:             1. Parse PRP IMPLEMENTATION BLUEPRINT for file references
140:                (searches for patterns: "Modify: path/file.py", "Create: path/file.py")
141:             2. Fallback: git diff --name-only main...HEAD
142:             3. Fallback: Interactive prompt for user to specify files
143: 
144:     Returns:
145:         {
146:             "success": bool,
147:             "drift_score": float,
148:             "threshold_action": str,  # auto_accept | auto_fix | escalate
149:             "decision": Optional[str],  # if escalated: accepted | rejected | examples_updated
150:             "justification": Optional[str],
151:             "duration": float,
152:             "level": 4
153:         }
154: 
155:     Raises:
156:         RuntimeError: If PRP parsing fails or Serena MCP unavailable
157: 
158:     Process:
159:         1. Extract patterns from PRP EXAMPLES
160:         2. Analyze implementation with Serena MCP
161:         3. Calculate drift score
162:         4. Apply threshold logic (auto-accept/fix/escalate)
163:         5. If escalated: prompt user, persist decision
164:         6. Return validation result
165:     """
166:     start_time = time.time()
167: 
168:     try:
169:         # Step 1: Auto-detect implementation paths if not provided
170:         if implementation_paths is None:
171:             implementation_paths = _auto_detect_implementation_paths(prp_path)
172: 
173:         # Step 2: Extract expected patterns from PRP
174:         expected_patterns = extract_patterns_from_prp(prp_path)
175: 
176:         # Step 3: Analyze implementation
177:         analysis_result = analyze_implementation(prp_path, implementation_paths)
178:         detected_patterns = analysis_result["detected_patterns"]
179: 
180:         # Step 4: Calculate drift score
181:         drift_result = calculate_drift_score(expected_patterns, detected_patterns)
182:         drift_score = drift_result["drift_score"]
183:         threshold_action = drift_result["threshold_action"]
184: 
185:         # Step 5: Handle based on threshold
186:         decision = None
187:         justification = None
188: 
189:         if threshold_action == "auto_accept":
190:             # Drift < 10%: Auto-accept
191:             success = True
192:         elif threshold_action == "auto_fix":
193:             # Drift 10-30%: Display suggestions (MVP: no auto-apply)
194:             success = True
195:             suggestions = get_auto_fix_suggestions(drift_result["mismatches"])
196:             print("\nâš ï¸  MODERATE DRIFT DETECTED - SUGGESTIONS:")
197:             for suggestion in suggestions:
198:                 print(f"   {suggestion}")
199:             print()
200:         else:
201:             # Drift >= 30%: Escalate to user
202:             success, decision, justification = _handle_user_escalation(
203:                 prp_path,
204:                 drift_result,
205:                 implementation_paths
206:             )
207: 
208:             # Persist decision to PRP if user decided
209:             if decision:
210:                 _persist_drift_decision(prp_path, drift_result, decision, justification)
211: 
212:         duration = time.time() - start_time
213: 
214:         return {
215:             "success": success,
216:             "drift_score": drift_score,
217:             "threshold_action": threshold_action,
218:             "decision": decision,
219:             "justification": justification,
220:             "duration": round(duration, 2),
221:             "level": 4,
222:             "files_analyzed": analysis_result["files_analyzed"],
223:             "category_scores": drift_result["category_scores"]
224:         }
225: 
226:     except Exception as e:
227:         duration = time.time() - start_time
228:         return {
229:             "success": False,
230:             "drift_score": 100.0,
231:             "threshold_action": "escalate",
232:             "decision": None,
233:             "justification": None,
234:             "duration": round(duration, 2),
235:             "level": 4,
236:             "error": str(e)
237:         }
238: 
239: 
240: def _auto_detect_implementation_paths(prp_path: str) -> List[str]:
241:     """Auto-detect implementation file paths from PRP or git."""
242:     # Strategy 1: Parse PRP IMPLEMENTATION BLUEPRINT
243:     paths = _parse_prp_blueprint_paths(prp_path)
244:     if paths:
245:         return paths
246: 
247:     # Strategy 2: Git diff (changed files)
248:     result = run_cmd("git diff --name-only main...HEAD", capture_output=True)
249:     if result["success"] and result["stdout"].strip():
250:         paths = [p.strip() for p in result["stdout"].strip().split("\n")]
251:         # Filter for code files only
252:         code_extensions = {".py", ".ts", ".tsx", ".js", ".jsx", ".go", ".rs", ".java"}
253:         paths = [p for p in paths if Path(p).suffix in code_extensions]
254:         if paths:
255:             return paths
256: 
257:     # Strategy 3: Interactive prompt
258:     print("\nðŸ” Unable to auto-detect implementation files.")
259:     print("Please specify file paths to analyze (comma-separated):")
260:     user_input = input("> ").strip()
261:     if user_input:
262:         return [p.strip() for p in user_input.split(",")]
263: 
264:     raise RuntimeError(
265:         "No implementation files specified\n"
266:         "ðŸ”§ Troubleshooting: Specify --files flag or add file references to PRP"
267:     )
268: 
269: 
270: def _parse_prp_blueprint_paths(prp_path: str) -> List[str]:
271:     """Parse implementation file paths from PRP IMPLEMENTATION BLUEPRINT section."""
272:     content = Path(prp_path).read_text()
273: 
274:     # Find IMPLEMENTATION BLUEPRINT section
275:     blueprint_match = re.search(
276:         r"##\s+.*?IMPLEMENTATION\s+BLUEPRINT.*?\n(.*?)(?=\n##|\Z)",
277:         content,
278:         re.DOTALL | re.IGNORECASE
279:     )
280: 
281:     if not blueprint_match:
282:         return []
283: 
284:     blueprint_text = blueprint_match.group(1)
285: 
286:     # Extract file paths from patterns like "Modify: path/file.py", "Create: path/file.py"
287:     file_patterns = re.findall(
288:         r"(?:Modify|Create|Update|Add):\s*([`]?)([a-zA-Z0-9_/\.\-]+\.(py|ts|tsx|js|jsx|go|rs|java))\1",
289:         blueprint_text,
290:         re.IGNORECASE
291:     )
292: 
293:     paths = [match[1] for match in file_patterns]
294:     return list(set(paths))  # Deduplicate
295: 
296: 
297: def _handle_user_escalation(
298:     prp_path: str,
299:     drift_result: Dict[str, Any],
300:     implementation_paths: List[str]
301: ) -> tuple[bool, Optional[str], Optional[str]]:
302:     """Interactive CLI for high-drift cases requiring human decision.
303: 
304:     Returns:
305:         (success, decision, justification) tuple
306:     """
307:     from .drift import get_drift_history, drift_summary
308:     import logging
309: 
310:     logger = logging.getLogger(__name__)
311:     drift_score = drift_result["drift_score"]
312:     category_scores = drift_result["category_scores"]
313:     mismatches = drift_result["mismatches"]
314: 
315:     print("\n" + "=" * 80)
316:     print(f"ðŸš¨ HIGH DRIFT DETECTED: {drift_score:.1f}%")
317:     print("=" * 80)
318:     print(f"\nPRP: {prp_path}")
319:     print(f"Implementation: {', '.join(implementation_paths)}")
320: 
321:     # NEW: Show drift history for context
322:     try:
323:         history = get_drift_history(last_n=5)
324:         if history:
325:             print("\nðŸ“Š RECENT DRIFT HISTORY (for context):\n")
326:             print(f"{'PRP':<12} {'Score':<8} {'Action':<18} {'Date':<12}")
327:             print("â”€" * 50)
328:             for h in history:
329:                 dd = h["drift_decision"]
330:                 prp_id = h["prp_id"]
331:                 score = dd["score"]
332:                 action = dd["action"]
333:                 timestamp = dd.get("timestamp", "N/A")[:10]
334:                 print(f"{prp_id:<12} {score:<8.2f} {action:<18} {timestamp:<12}")
335:             print()
336: 
337:             # Show summary stats
338:             summary = drift_summary()
339:             print(f"Historical Average: {summary['avg_drift_score']:.2f}%")
340:             print(f"Accepted: {summary['decisions'].get('accepted', 0)} | "
341:                   f"Rejected: {summary['decisions'].get('rejected', 0)}\n")
342:     except Exception as e:
343:         logger.warning(f"Could not load drift history: {e}")
344: 
345:     print("\nDRIFT BREAKDOWN:")
346:     print("â”" * 80)
347:     print(f"{'Category':<25} {'Expected':<20} {'Detected':<20} {'Drift':<10}")
348:     print("â”€" * 80)
349: 
350:     for category, score in category_scores.items():
351:         print(f"{category:<25} {'(see PRP)':<20} {'(varies)':<20} {score:.1f}%")
352: 
353:     print("â”" * 80)
354: 
355:     # Show affected patterns
356:     if mismatches:
357:         print("\nAFFECTED PATTERNS:")
358:         for mismatch in mismatches[:5]:  # Show first 5
359:             expected = mismatch["expected"]
360:             detected = mismatch.get("detected", "None")
361:             category = mismatch["category"]
362:             print(f"â€¢ {category}: Expected '{expected}', Detected {detected}")
363: 
364:     print("\nOPTIONS:")
365:     print("[A] Accept drift (add DRIFT_JUSTIFICATION to PRP)")
366:     print("[R] Reject and halt (requires manual refactoring)")
367:     print("[U] Update EXAMPLES in PRP (update specification)")
368:     print("[Q] Quit without saving")
369:     print()
370: 
371:     while True:
372:         choice = input("Your choice (A/R/U/Q): ").strip().upper()
373: 
374:         if choice == "A":
375:             justification = input("Justification for accepting drift: ").strip()
376:             if not justification:
377:                 print("âš ï¸  Justification required. Try again.")
378:                 continue
379:             return (True, "accepted", justification)
380: 
381:         elif choice == "R":
382:             print("\nâŒ L4 validation REJECTED - Manual refactoring required")
383:             return (False, "rejected", "User rejected high drift")
384: 
385:         elif choice == "U":
386:             print("\nâ„¹ï¸  Update EXAMPLES section in PRP manually, then re-run validation")
387:             return (False, "examples_updated", "User chose to update PRP EXAMPLES")
388: 
389:         elif choice == "Q":
390:             print("\nâŒ L4 validation aborted")
391:             return (False, None, None)
392: 
393:         else:
394:             print("âš ï¸  Invalid choice. Please enter A, R, U, or Q.")
395: 
396: 
397: def _persist_drift_decision(
398:     prp_path: str,
399:     drift_result: Dict[str, Any],
400:     decision: str,
401:     justification: Optional[str]
402: ):
403:     """Persist DRIFT_JUSTIFICATION to PRP YAML header."""
404:     content = Path(prp_path).read_text()
405: 
406:     # Build drift decision YAML
407:     drift_yaml = f"""drift_decision:
408:   score: {drift_result['drift_score']}
409:   action: "{decision}"
410:   justification: "{justification or 'N/A'}"
411:   timestamp: "{datetime.now(timezone.utc).isoformat()}"
412:   category_breakdown:
413: """
414: 
415:     for category, score in drift_result["category_scores"].items():
416:         drift_yaml += f"    {category}: {score}\n"
417: 
418:     drift_yaml += f'  reviewer: "human"\n'
419: 
420:     # Insert into YAML header (after last YAML field before ---)
421:     yaml_end_match = re.search(r"(---\s*\n)", content)
422:     if yaml_end_match:
423:         # Insert before closing ---
424:         insert_pos = yaml_end_match.start()
425:         new_content = content[:insert_pos] + drift_yaml + content[insert_pos:]
426:         Path(prp_path).write_text(new_content)
427:         print(f"\nâœ… Drift decision persisted to {prp_path}")
428:     else:
429:         print(f"\nâš ï¸  Warning: Could not find YAML header in {prp_path}")
430: 
431: 
432: def calculate_confidence(results: Dict[int, Dict[str, Any]]) -> int:
433:     """Calculate confidence score (1-10) based on validation results.
434: 
435:     Scoring breakdown:
436:     - Baseline: 6 (untested code)
437:     - Level 1 (Syntax & Style): +1
438:     - Level 2 (Unit Tests): +2 (with >80% coverage)
439:     - Level 3 (Integration): +1
440:     - Level 4 (Pattern Conformance): +1 (NEW)
441:     - Max: 10/10 (production-ready)
442: 
443:     Requirements for +1 from L4:
444:     - drift_score < 10% (auto-accept threshold)
445:     - OR drift_score < 30% AND decision = "accepted" with justification
446: 
447:     Args:
448:         results: Dict mapping level (1-4) to validation results
449: 
450:     Returns:
451:         Confidence score 1-10
452: 
453:     Examples:
454:         >>> results = {1: {"success": True}, 2: {"success": True, "coverage": 0.85}}
455:         >>> calculate_confidence(results)
456:         9  # Without L3, L4
457: 
458:         >>> results = {
459:         ...     1: {"success": True},
460:         ...     2: {"success": True, "coverage": 0.85},
461:         ...     3: {"success": True},
462:         ...     4: {"success": True, "drift_score": 8.5}
463:         ... }
464:         >>> calculate_confidence(results)
465:         10  # All gates pass
466:     """
467:     score = 6  # Baseline
468: 
469:     if results.get(1, {}).get("success"):
470:         score += 1
471: 
472:     if results.get(2, {}).get("success") and results.get(2, {}).get("coverage", 0) > 0.8:
473:         score += 2
474: 
475:     if results.get(3, {}).get("success"):
476:         score += 1
477: 
478:     # Level 4: Pattern conformance (NEW)
479:     l4_result = results.get(4, {})
480:     if l4_result.get("success"):
481:         drift_score = l4_result.get("drift_score", 100)
482:         decision = l4_result.get("decision")
483: 
484:         # Pass L4 if:
485:         # 1. drift < 10% (auto-accept)
486:         # 2. drift < 30% AND explicitly accepted with justification
487:         if drift_score < 10.0 or (drift_score < 30.0 and decision == "accepted"):
488:             score += 1
489: 
490:     return min(score, 10)
491: 
492: 
493: def validate_all() -> Dict[str, Any]:
494:     """Run all validation levels sequentially.
495: 
496:     Returns:
497:         Dict with: success (bool), results (Dict[int, Dict]),
498:                    total_duration (float), confidence_score (int)
499: 
500:     Note: Runs all levels even if early ones fail (for comprehensive report).
501:     """
502:     results = {}
503:     total_duration = 0.0
504: 
505:     # Level 1
506:     try:
507:         results[1] = validate_level_1()
508:         total_duration += results[1]["duration"]
509:     except Exception as e:
510:         results[1] = {
511:             "success": False,
512:             "errors": [f"Level 1 exception: {str(e)}"],
513:             "duration": 0.0,
514:             "level": 1
515:         }
516: 
517:     # Level 2
518:     try:
519:         results[2] = validate_level_2()
520:         total_duration += results[2]["duration"]
521:     except Exception as e:
522:         results[2] = {
523:             "success": False,
524:             "errors": [f"Level 2 exception: {str(e)}"],
525:             "duration": 0.0,
526:             "level": 2
527:         }
528: 
529:     # Level 3
530:     try:
531:         results[3] = validate_level_3()
532:         total_duration += results[3]["duration"]
533:     except Exception as e:
534:         results[3] = {
535:             "success": False,
536:             "errors": [f"Level 3 exception: {str(e)}"],
537:             "duration": 0.0,
538:             "level": 3
539:         }
540: 
541:     # Overall success: all levels must pass
542:     overall_success = all(r["success"] for r in results.values())
543: 
544:     # Calculate confidence score
545:     confidence_score = calculate_confidence(results)
546: 
547:     return {
548:         "success": overall_success,
549:         "results": results,
550:         "total_duration": total_duration,
551:         "confidence_score": confidence_score
552:     }
</file>

<file path="tools/ce/validation_loop.py">
  1: """Validation loop with self-healing capabilities.
  2: 
  3: Orchestrates L1-L4 validation levels with automatic error detection,
  4: parsing, and self-healing fixes. Includes escalation triggers for
  5: human intervention when automated fixes are insufficient.
  6: """
  7: 
  8: import re
  9: from typing import Dict, Any, List
 10: from pathlib import Path
 11: 
 12: from .exceptions import EscalationRequired
 13: 
 14: 
 15: def run_validation_loop(
 16:     phase: Dict[str, Any],
 17:     prp_path: str,
 18:     max_attempts: int = 3
 19: ) -> Dict[str, Any]:
 20:     """Run L1-L4 validation loop with self-healing.
 21: 
 22:     Args:
 23:         phase: Phase dict with validation_command
 24:         prp_path: Path to PRP file (for L4 validation)
 25:         max_attempts: Max self-healing attempts (default: 3)
 26: 
 27:     Returns:
 28:         {
 29:             "success": True,
 30:             "validation_levels": {
 31:                 "L1": {"passed": True, "attempts": 1, "errors": []},
 32:                 "L2": {"passed": True, "attempts": 2, "errors": ["..."]},
 33:                 "L3": {"passed": True, "attempts": 1, "errors": []},
 34:                 "L4": {"passed": True, "attempts": 1, "errors": []}
 35:             },
 36:             "self_healed": ["L2: Fixed import error"],
 37:             "escalated": [],
 38:             "attempts": 1
 39:         }
 40: 
 41:     Raises:
 42:         EscalationRequired: If validation fails after max_attempts or trigger hit
 43: 
 44:     Process:
 45:         1. Run L1 (Syntax): validate_level_1() with self-healing
 46:         2. Run L2 (Unit Tests): Custom validation from phase with self-healing
 47:         3. Run L3 (Integration): validate_level_3() with self-healing
 48:         4. Run L4 (Pattern Conformance): validate_level_4(prp_path)
 49: 
 50:         For each level:
 51:         - If pass: continue to next level
 52:         - If fail: enter self-healing loop (max 3 attempts)
 53:           1. Parse error
 54:           2. Check escalation triggers
 55:           3. Apply fix
 56:           4. Re-run validation
 57:         - If still failing after max_attempts: escalate to human
 58:     """
 59:     from .validate import validate_level_1, validate_level_3, validate_level_4
 60: 
 61:     print(f"  ðŸ§ª Running validation...")
 62: 
 63:     validation_levels = {}
 64:     self_healed = []
 65:     escalated = []
 66:     all_passed = True
 67: 
 68:     # L1: Syntax & Style (with self-healing)
 69:     print(f"    L1: Syntax & Style...")
 70:     l1_passed = False
 71:     l1_attempts = 0
 72:     l1_errors = []
 73:     error_history = []
 74: 
 75:     for attempt in range(1, max_attempts + 1):
 76:         l1_attempts = attempt
 77:         try:
 78:             l1_result = validate_level_1()
 79:             if not l1_result["success"]:
 80:                 # Validation failed - try self-healing
 81:                 l1_errors = l1_result.get("errors", [])
 82:                 print(f"    âŒ L1 failed (attempt {attempt}/{max_attempts}): {len(l1_errors)} errors")
 83:                 combined_error = "\n".join(l1_errors)
 84:                 _try_self_heal(combined_error, "L1", attempt, max_attempts, error_history)
 85:                 continue
 86: 
 87:             # Success path
 88:             l1_passed = True
 89:             print(f"    âœ… L1 passed ({l1_result['duration']:.2f}s)")
 90:             if attempt > 1:
 91:                 self_healed.append(f"L1: Fixed after {attempt} attempts")
 92:             break
 93: 
 94:         except EscalationRequired:
 95:             raise  # Propagate escalation
 96:         except Exception as e:
 97:             l1_errors = [str(e)]
 98:             print(f"    âŒ L1 exception (attempt {attempt}): {str(e)}")
 99:             if attempt == max_attempts:
100:                 break
101: 
102:     validation_levels["L1"] = {
103:         "passed": l1_passed,
104:         "attempts": l1_attempts,
105:         "errors": l1_errors
106:     }
107:     if not l1_passed:
108:         all_passed = False
109:         print(f"    âŒ L1 failed after {l1_attempts} attempts - escalating")
110:         error = parse_validation_error("\n".join(l1_errors), "L1")
111:         escalate_to_human(error, "persistent_error")
112: 
113:     # L2: Unit Tests (with self-healing)
114:     l2_passed = False
115:     l2_attempts = 0
116:     l2_errors = []
117:     error_history_l2 = []
118: 
119:     if phase.get("validation_command"):
120:         print(f"    L2: Running {phase['validation_command']}...")
121:         from .core import run_cmd
122: 
123:         for attempt in range(1, max_attempts + 1):
124:             l2_attempts = attempt
125:             try:
126:                 l2_result = run_cmd(phase["validation_command"])
127:                 if not l2_result["success"]:
128:                     # Validation failed - try self-healing
129:                     l2_errors = [l2_result.get("stderr", "Test failed")]
130:                     print(f"    âŒ L2 failed (attempt {attempt}/{max_attempts})")
131:                     print(f"       {l2_result.get('stderr', 'Unknown error')[:200]}")
132:                     error_output = l2_result.get("stderr", "")
133:                     _try_self_heal(error_output, "L2", attempt, max_attempts, error_history_l2)
134:                     continue
135: 
136:                 # Success path
137:                 l2_passed = True
138:                 print(f"    âœ… L2 passed ({l2_result['duration']:.2f}s)")
139:                 if attempt > 1:
140:                     self_healed.append(f"L2: Fixed after {attempt} attempts")
141:                 break
142: 
143:             except EscalationRequired:
144:                 raise
145:             except Exception as e:
146:                 l2_errors = [str(e)]
147:                 print(f"    âŒ L2 exception (attempt {attempt}): {str(e)}")
148:                 if attempt == max_attempts:
149:                     break
150: 
151:         validation_levels["L2"] = {
152:             "passed": l2_passed,
153:             "attempts": l2_attempts,
154:             "errors": l2_errors
155:         }
156:         if not l2_passed:
157:             all_passed = False
158:             print(f"    âŒ L2 failed after {l2_attempts} attempts - escalating")
159:             error = parse_validation_error("\n".join(l2_errors), "L2")
160:             escalate_to_human(error, "persistent_error")
161: 
162:     else:
163:         # No validation command - skip L2
164:         print(f"    âš ï¸  L2 skipped: No validation command specified")
165:         validation_levels["L2"] = {"passed": True, "attempts": 1, "errors": [], "skipped": True}
166: 
167:     # L3: Integration Tests (MVP: no self-healing for integration tests)
168:     try:
169:         print(f"    L3: Integration Tests...")
170:         l3_result = validate_level_3()
171:         validation_levels["L3"] = {
172:             "passed": l3_result["success"],
173:             "attempts": 1,
174:             "errors": l3_result.get("errors", [])
175:         }
176:         if l3_result["success"]:
177:             print(f"    âœ… L3 passed ({l3_result['duration']:.2f}s)")
178:         else:
179:             print(f"    âŒ L3 failed - integration tests require manual review")
180:             all_passed = False
181:             # Integration test failures typically require architectural changes
182:             error = parse_validation_error(str(l3_result.get("errors", [])), "L3")
183:             escalate_to_human(error, "architectural")
184:     except EscalationRequired:
185:         raise
186:     except Exception as e:
187:         print(f"    âš ï¸  L3 skipped: {str(e)}")
188:         validation_levels["L3"] = {"passed": True, "attempts": 1, "errors": [], "skipped": True}
189: 
190:     # L4: Pattern Conformance
191:     try:
192:         print(f"    L4: Pattern Conformance...")
193:         l4_result = validate_level_4(prp_path)
194:         validation_levels["L4"] = {
195:             "passed": l4_result["success"],
196:             "attempts": 1,
197:             "errors": [],
198:             "drift_score": l4_result.get("drift_score", 0)
199:         }
200:         if l4_result["success"]:
201:             print(f"    âœ… L4 passed (drift: {l4_result.get('drift_score', 0):.1f}%)")
202:         else:
203:             print(f"    âŒ L4 failed (drift: {l4_result.get('drift_score', 100):.1f}%)")
204:             all_passed = False
205:     except Exception as e:
206:         print(f"    âš ï¸  L4 skipped: {str(e)}")
207:         validation_levels["L4"] = {"passed": True, "attempts": 1, "errors": [], "skipped": True}
208: 
209:     print(f"  {'âœ…' if all_passed else 'âŒ'} Validation {'complete' if all_passed else 'failed'}")
210: 
211:     return {
212:         "success": all_passed,
213:         "validation_levels": validation_levels,
214:         "self_healed": self_healed,
215:         "escalated": escalated,
216:         "attempts": 1
217:     }
218: 
219: 
220: def _try_self_heal(
221:     error_output: str,
222:     level: str,
223:     attempt: int,
224:     max_attempts: int,
225:     error_history: List[str]
226: ) -> bool:
227:     """Try self-healing for validation error.
228: 
229:     Args:
230:         error_output: Raw error output to parse
231:         level: Validation level (L1, L2, etc.)
232:         attempt: Current attempt number
233:         max_attempts: Maximum attempts allowed
234:         error_history: List of previous error messages
235: 
236:     Returns:
237:         True if should continue trying, False if should stop
238: 
239:     Raises:
240:         EscalationRequired: If escalation triggered
241:     """
242:     if attempt >= max_attempts:
243:         return False
244: 
245:     error = parse_validation_error(error_output, level)
246:     error_history.append(error["message"])
247: 
248:     # Check escalation triggers
249:     if check_escalation_triggers(error, attempt, error_history):
250:         escalate_to_human(error, "persistent_error")
251: 
252:     # Apply self-healing
253:     print(f"      ðŸ”§ Attempting self-heal...")
254:     fix_result = apply_self_healing_fix(error, attempt)
255:     if fix_result["success"]:
256:         print(f"      âœ… Applied fix: {fix_result['description']}")
257:     else:
258:         print(f"      âš ï¸  Auto-fix failed: {fix_result['description']}")
259: 
260:     return True
261: 
262: 
263: def calculate_confidence_score(validation_results: Dict[str, Any]) -> str:
264:     """Calculate confidence score (1-10) based on validation results.
265: 
266:     Args:
267:         validation_results: Dict with L1-L4 results per phase
268: 
269:     Returns:
270:         "8/10" or "10/10"
271: 
272:     Scoring:
273:         - All L1-L4 passed on first attempt: 10/10
274:         - All passed, 1-2 self-heals: 9/10
275:         - All passed, 3+ self-heals: 8/10
276:         - L1-L3 passed, L4 skipped: 7/10
277:         - L1-L2 passed, L3-L4 skipped: 5/10
278:     """
279:     if not validation_results:
280:         return "6/10"  # No validation = baseline
281: 
282:     total_attempts = 0
283:     all_passed = True
284: 
285:     for _, phase_result in validation_results.items():
286:         if not phase_result.get("success"):
287:             all_passed = False
288: 
289:         # Count total attempts across all levels
290:         for _, level_result in phase_result.get("validation_levels", {}).items():
291:             total_attempts += level_result.get("attempts", 1) - 1  # -1 because first attempt doesn't count as retry
292: 
293:     if not all_passed:
294:         return "5/10"  # Validation failures
295: 
296:     # All passed - score by attempts
297:     if total_attempts == 0:
298:         return "10/10"  # Perfect
299:     elif total_attempts <= 2:
300:         return "9/10"  # Minor issues
301:     else:
302:         return "8/10"  # Multiple retries
303: 
304: 
305: def parse_validation_error(output: str, _level: str) -> Dict[str, Any]:
306:     """Parse validation error output into structured format.
307: 
308:     Args:
309:         output: Raw error output (stderr + stdout)
310:         _level: Validation level (L1, L2, L3, L4) - reserved for future use
311: 
312:     Returns:
313:         {
314:             "type": "assertion_error",  # assertion_error, import_error, syntax_error, etc.
315:             "file": "src/auth.py",
316:             "line": 42,
317:             "function": "authenticate",
318:             "message": "Expected User, got None",
319:             "traceback": "<full traceback>",
320:             "suggested_fix": "Check return value"
321:         }
322: 
323:     Process:
324:         1. Detect error type (assertion, import, syntax, type, etc.)
325:         2. Extract file:line location
326:         3. Extract function/class context
327:         4. Extract error message
328:         5. Generate suggested fix hint
329:     """
330:     error = {
331:         "type": "unknown_error",
332:         "file": "unknown",
333:         "line": 0,
334:         "function": None,
335:         "message": output[:200] if output else "Unknown error",
336:         "traceback": output,
337:         "suggested_fix": "Manual review required"
338:     }
339: 
340:     # Detect error type from output patterns
341:     if "ImportError" in output or "ModuleNotFoundError" in output or "cannot import" in output:
342:         error["type"] = "import_error"
343:         error["suggested_fix"] = "Add missing import statement"
344: 
345:         # Extract module name: "No module named 'jwt'" or "cannot import name 'User'"
346:         import_match = re.search(r"No module named '([^']+)'", output)
347:         if import_match:
348:             error["message"] = f"No module named '{import_match.group(1)}'"
349:             error["suggested_fix"] = f"Install or import {import_match.group(1)}"
350:         else:
351:             name_match = re.search(r"cannot import name '([^']+)'", output)
352:             if name_match:
353:                 error["message"] = f"cannot import name '{name_match.group(1)}'"
354:                 error["suggested_fix"] = f"Check import of {name_match.group(1)}"
355: 
356:     elif "AssertionError" in output or "assert" in output.lower():
357:         error["type"] = "assertion_error"
358:         error["suggested_fix"] = "Check assertion logic"
359: 
360:     elif "SyntaxError" in output:
361:         error["type"] = "syntax_error"
362:         error["suggested_fix"] = "Fix syntax error"
363: 
364:     elif "TypeError" in output:
365:         error["type"] = "type_error"
366:         error["suggested_fix"] = "Check type annotations and conversions"
367: 
368:     elif "NameError" in output or "is not defined" in output:
369:         error["type"] = "name_error"
370:         error["suggested_fix"] = "Define missing variable or import"
371: 
372:     elif "AttributeError" in output:
373:         error["type"] = "attribute_error"
374:         error["suggested_fix"] = "Check attribute exists on object"
375: 
376:     # Extract file:line location (common patterns)
377:     # Pattern 1: File "path/to/file.py", line 42
378:     file_match = re.search(r'File "([^"]+)", line (\d+)', output)
379:     if file_match:
380:         error["file"] = file_match.group(1)
381:         error["line"] = int(file_match.group(2))
382: 
383:     # Pattern 2: path/to/file.py:42:
384:     location_match = re.search(r'([^:\s]+\.py):(\d+):', output)
385:     if location_match:
386:         error["file"] = location_match.group(1)
387:         error["line"] = int(location_match.group(2))
388: 
389:     # Extract function/class context
390:     func_match = re.search(r'in (\w+)', output)
391:     if func_match:
392:         error["function"] = func_match.group(1)
393: 
394:     return error
395: 
396: 
397: def check_escalation_triggers(
398:     error: Dict[str, Any],
399:     attempt: int,
400:     error_history: List[str]
401: ) -> bool:
402:     """Check if error triggers human escalation.
403: 
404:     Args:
405:         error: Parsed error dict
406:         attempt: Current attempt number
407:         error_history: List of previous error messages for this validation
408: 
409:     Returns:
410:         True if escalation required, False to continue self-healing
411: 
412:     Escalation Triggers:
413:         1. Same error after 3 attempts (error message unchanged)
414:         2. Ambiguous error messages (generic "something went wrong")
415:         3. Architectural changes required (detected by keywords: "refactor", "redesign")
416:         4. External dependency issues (network errors, API failures, missing packages)
417:         5. Security concerns (vulnerability, secret exposure, permission escalation)
418:     """
419:     # Trigger 1: Same error after 3 attempts
420:     if attempt >= 3 and len(error_history) >= 3:
421:         # Check if all 3 error messages are identical
422:         if len(set(error_history[-3:])) == 1:
423:             return True
424: 
425:     # Trigger 2: Ambiguous error messages
426:     ambiguous_patterns = [
427:         "something went wrong",
428:         "unexpected error",
429:         "failed",
430:         "error occurred",
431:         "unknown error"
432:     ]
433:     error_msg = error.get("message", "").lower()
434:     if any(pattern in error_msg for pattern in ambiguous_patterns):
435:         # Only escalate if also no file/line info
436:         if error.get("file") == "unknown" and error.get("line") == 0:
437:             return True
438: 
439:     # Trigger 3: Architectural changes required
440:     architecture_keywords = [
441:         "refactor",
442:         "redesign",
443:         "architecture",
444:         "restructure",
445:         "circular import",
446:         "coupling"
447:     ]
448:     full_error = error.get("traceback", "") + error.get("message", "")
449:     if any(keyword in full_error.lower() for keyword in architecture_keywords):
450:         return True
451: 
452:     # Trigger 4: External dependency issues
453:     dependency_keywords = [
454:         "connection refused",
455:         "network error",
456:         "timeout",
457:         "api error",
458:         "http error",
459:         "could not resolve host",
460:         "package not found",
461:         "pypi",
462:         "npm error"
463:     ]
464:     if any(keyword in full_error.lower() for keyword in dependency_keywords):
465:         return True
466: 
467:     # Trigger 5: Security concerns
468:     security_keywords = [
469:         "cve-",
470:         "vulnerability",
471:         "secret",
472:         "password",
473:         "api key",
474:         "token",
475:         "credential",
476:         "permission denied",
477:         "access denied",
478:         "unauthorized",
479:         "security"
480:     ]
481:     if any(keyword in full_error.lower() for keyword in security_keywords):
482:         return True
483: 
484:     return False
485: 
486: 
487: def apply_self_healing_fix(error: Dict[str, Any], _attempt: int) -> Dict[str, Any]:
488:     """Apply self-healing fix based on error type.
489: 
490:     Args:
491:         error: Parsed error dict from parse_validation_error()
492:         _attempt: Current attempt number (1-3) - reserved for future use
493: 
494:     Returns:
495:         {
496:             "success": True,
497:             "fix_type": "import_added",
498:             "location": "src/auth.py:3",
499:             "description": "Added missing import: from models import User"
500:         }
501: 
502:     Process:
503:         1. Check escalation triggers first (done in run_validation_loop)
504:         2. Match error type to fix strategy:
505:            - import_error â†’ add_missing_import()
506:            - assertion_error â†’ Manual review (escalate)
507:            - type_error â†’ Manual review (escalate)
508:            - syntax_error â†’ Manual review (escalate)
509:            - name_error â†’ Manual review (escalate)
510:         3. Apply fix using file operations
511:         4. Log fix for debugging
512:     """
513:     error_type = error.get("type", "unknown_error")
514: 
515:     # Import errors - can auto-fix by adding import statement
516:     if error_type == "import_error":
517:         try:
518:             filepath = error.get("file", "unknown")
519:             message = error.get("message", "")
520: 
521:             # Extract module/class name
522:             if "No module named" in message:
523:                 match = re.search(r"No module named '([^']+)'", message)
524:                 if match:
525:                     module = match.group(1)
526:                     return _add_import_statement(filepath, f"import {module}")
527:             elif "cannot import name" in message:
528:                 match = re.search(r"cannot import name '([^']+)'", message)
529:                 if match:
530:                     name = match.group(1)
531:                     # Try common import patterns
532:                     return _add_import_statement(filepath, f"from . import {name}")
533: 
534:         except Exception as e:
535:             return {
536:                 "success": False,
537:                 "fix_type": "import_error_failed",
538:                 "description": f"Failed to fix import: {str(e)}"
539:             }
540: 
541:     # Other error types - require manual intervention or more complex logic
542:     # These will be handled by escalation triggers
543:     return {
544:         "success": False,
545:         "fix_type": f"{error_type}_not_implemented",
546:         "description": f"Auto-fix not implemented for {error_type} - escalate to human"
547:     }
548: 
549: 
550: def _add_import_statement(filepath: str, import_stmt: str) -> Dict[str, Any]:
551:     """Add import statement to file.
552: 
553:     Args:
554:         filepath: Path to Python file
555:         import_stmt: Import statement to add (e.g., "import jwt" or "from models import User")
556: 
557:     Returns:
558:         Fix result dict
559:     """
560:     try:
561:         file_path = Path(filepath)
562:         if not file_path.exists():
563:             return {
564:                 "success": False,
565:                 "fix_type": "import_add_failed",
566:                 "description": f"File not found: {filepath}"
567:             }
568: 
569:         # Read current content
570:         content = file_path.read_text()
571:         lines = content.split("\n")
572: 
573:         # Find position to insert import (after existing imports or at top)
574:         insert_pos = 0
575:         for i, line in enumerate(lines):
576:             if line.startswith("import ") or line.startswith("from "):
577:                 insert_pos = i + 1
578:             elif line.strip() and not line.startswith("#") and not line.startswith('"""'):
579:                 # Found first non-import, non-comment line
580:                 break
581: 
582:         # Insert import statement
583:         lines.insert(insert_pos, import_stmt)
584: 
585:         # Write back
586:         file_path.write_text("\n".join(lines))
587: 
588:         return {
589:             "success": True,
590:             "fix_type": "import_added",
591:             "location": f"{filepath}:{insert_pos + 1}",
592:             "description": f"Added import: {import_stmt}"
593:         }
594: 
595:     except Exception as e:
596:         return {
597:             "success": False,
598:             "fix_type": "import_add_failed",
599:             "description": f"Error adding import: {str(e)}"
600:         }
601: 
602: 
603: def escalate_to_human(error: Dict[str, Any], reason: str) -> None:
604:     """Escalate to human with detailed error report.
605: 
606:     Args:
607:         error: Parsed error dict
608:         reason: Escalation trigger reason
609: 
610:     Raises:
611:         EscalationRequired: Always (signals need for human intervention)
612: 
613:     Process:
614:         1. Format error report with type and location
615:         2. Include full error message and traceback
616:         3. Provide escalation reason
617:         4. Generate troubleshooting guidance based on error type
618:     """
619:     # Build context-specific troubleshooting guidance
620:     troubleshooting_lines = ["Steps to resolve:"]
621: 
622:     if reason == "persistent_error":
623:         troubleshooting_lines.extend([
624:             "1. Review error details - same error occurred 3 times",
625:             "2. Check if fix logic matches error type",
626:             "3. Consider if architectural change needed",
627:             "4. Review validation command output manually"
628:         ])
629: 
630:     elif reason == "ambiguous_error":
631:         troubleshooting_lines.extend([
632:             "1. Run validation command manually for full context",
633:             "2. Check logs for additional error details",
634:             "3. Add debug print statements if needed",
635:             "4. Review recent code changes"
636:         ])
637: 
638:     elif reason == "architectural":
639:         troubleshooting_lines.extend([
640:             "1. Review error for architectural keywords (refactor, redesign, circular)",
641:             "2. Consider if code structure needs reorganization",
642:             "3. Check for circular dependencies",
643:             "4. May require human design decision"
644:         ])
645: 
646:     elif reason == "dependencies":
647:         troubleshooting_lines.extend([
648:             "1. Check network connectivity",
649:             "2. Verify package repository access (PyPI, npm, etc.)",
650:             "3. Review dependency versions in requirements",
651:             "4. Check for transitive dependency conflicts"
652:         ])
653: 
654:     elif reason == "security":
655:         troubleshooting_lines.extend([
656:             "1. DO NOT auto-fix security-related errors",
657:             "2. Review error for exposed secrets/credentials",
658:             "3. Check for permission/access issues",
659:             "4. Consult security documentation if CVE mentioned"
660:         ])
661: 
662:     else:
663:         troubleshooting_lines.extend([
664:             "1. Review error details above",
665:             "2. Check file and line number for context",
666:             "3. Run validation command manually",
667:             "4. Consult documentation for error type"
668:         ])
669: 
670:     # Add error-type-specific guidance
671:     error_type = error.get("type", "unknown")
672:     if error_type == "import_error":
673:         troubleshooting_lines.append("5. Check if module is installed: pip list | grep <module>")
674:     elif error_type == "assertion_error":
675:         troubleshooting_lines.append("5. Review test logic and expected vs actual values")
676:     elif error_type == "type_error":
677:         troubleshooting_lines.append("5. Check type annotations and ensure type compatibility")
678: 
679:     troubleshooting = "\n".join(troubleshooting_lines)
680: 
681:     raise EscalationRequired(
682:         reason=reason,
683:         error=error,
684:         troubleshooting=troubleshooting
685:     )
</file>

<file path="tools/bootstrap.sh">
 1: #!/bin/bash
 2: # Bootstrap script for Context Engineering CLI Tools
 3: # One-command setup
 4: 
 5: set -euo pipefail
 6: 
 7: echo "=== Context Engineering CLI Tools Bootstrap ==="
 8: echo ""
 9: 
10: # Check prerequisites
11: echo "ðŸ“‹ Checking prerequisites..."
12: 
13: if ! command -v uv &> /dev/null; then
14:     echo "âŒ UV package manager not found"
15:     echo "ðŸ”§ Install with: curl -LsSf https://astral.sh/uv/install.sh | sh"
16:     exit 1
17: fi
18: 
19: if ! command -v git &> /dev/null; then
20:     echo "âŒ Git not found"
21:     echo "ðŸ”§ Install git first"
22:     exit 1
23: fi
24: 
25: echo "âœ… Prerequisites OK"
26: echo ""
27: 
28: # Create virtual environment
29: echo "ðŸ”§ Creating virtual environment..."
30: if [ -d ".venv" ]; then
31:     echo "   .venv already exists, skipping"
32: else
33:     uv venv
34:     echo "âœ… Virtual environment created"
35: fi
36: echo ""
37: 
38: # Install package
39: echo "ðŸ“¦ Installing ce-tools..."
40: uv pip install -e .
41: echo "âœ… ce-tools installed"
42: echo ""
43: 
44: # Run tests to verify installation
45: echo "ðŸ§ª Running tests to verify installation..."
46: if uv run pytest tests/ -v --tb=short; then
47:     echo "âœ… All tests passed"
48: else
49:     echo "âš ï¸  Some tests failed (this is OK if npm commands not available)"
50: fi
51: echo ""
52: 
53: # Final instructions
54: echo "=== Bootstrap Complete ==="
55: echo ""
56: echo "CLI is ready to use:"
57: echo "  ce --help"
58: echo "  ce validate --level all"
59: echo "  ce git status"
60: echo "  ce context health"
61: echo ""
62: echo "To activate virtual environment manually:"
63: echo "  source .venv/bin/activate"
64: echo ""
65: echo "Or use directly with uv run:"
66: echo "  uv run ce --help"
67: echo ""
</file>

<file path=".ce/PRPs/executed/system/PRP-0-CONTEXT-ENGINEERING.md">
  1: ---
  2: prp_id: PRP-0
  3: title: Context Engineering Framework Installation
  4: status: executed
  5: created: {TIMESTAMP}
  6: executed: {TIMESTAMP}
  7: batch: 0
  8: phase: 0
  9: order: 0
 10: estimated_hours: {INSTALL_TIME}
 11: complexity: low
 12: risk_level: low
 13: ce_version: {CE_VERSION}
 14: installation_method: {METHOD}
 15: ---
 16: 
 17: # PRP-0: Context Engineering Framework Installation
 18: 
 19: ## ðŸ“‹ TL;DR
 20: 
 21: **What**: Installed Context Engineering (CE) framework version {CE_VERSION} into {PROJECT_NAME}
 22: 
 23: **When**: {TIMESTAMP}
 24: 
 25: **How**: {INSTALLATION_METHOD}
 26: 
 27: **Result**: CE 1.1 framework fully installed with /system/ organization
 28: 
 29: ---
 30: 
 31: ## Installation Details
 32: 
 33: ### CE Version
 34: 
 35: **Version**: {CE_VERSION}
 36: **Release Date**: {CE_RELEASE_DATE}
 37: **Distribution Package**: `ce-infrastructure.xml` ({SIZE}KB)
 38: 
 39: ### Installation Method
 40: 
 41: **Method**: {METHOD}
 42: 
 43: **Options**:
 44: - `greenfield` - New project, no existing CE
 45: - `mature-project` - Existing code, added CE
 46: - `existing-ce` - Upgraded CE 1.0 â†’ CE 1.1
 47: - `partial-ce` - Completed incomplete installation
 48: 
 49: ### Installation Date
 50: 
 51: **Started**: {START_TIMESTAMP}
 52: **Completed**: {END_TIMESTAMP}
 53: **Duration**: {DURATION} minutes
 54: 
 55: ---
 56: 
 57: ## Components Installed
 58: 
 59: ### Framework Memories (23 files)
 60: 
 61: **Location**: `.serena/memories/system/`
 62: 
 63: **Critical Memories (6)**:
 64: - code-style-conventions.md
 65: - suggested-commands.md
 66: - task-completion-checklist.md
 67: - testing-standards.md
 68: - tool-usage-syntropy.md
 69: - use-syntropy-tools-not-bash.md
 70: 
 71: **Regular Memories (17)**:
 72: - batch-generation-patterns.md
 73: - ce-tool-patterns.md
 74: - complexity-estimation.md
 75: - context-drift-monitoring.md
 76: - file-structure-best-practices.md
 77: - git-worktree-patterns.md
 78: - prp-execution-checklist.md
 79: - prp-sizing-thresholds.md
 80: - sequential-thinking-usage.md
 81: - syntropy-mcp-architecture.md
 82: - syntropy-tool-management.md
 83: - testing-strategy-pattern.md
 84: - token-optimization-strategies.md
 85: - tool-selection-decision-tree.md
 86: - validation-levels.md
 87: - worktree-conflict-resolution.md
 88: - xml-package-generation.md
 89: 
 90: ### Framework Examples (21 files)
 91: 
 92: **Location**: `.ce/examples/system/`
 93: 
 94: **Files Installed**:
 95: {LIST_OF_EXAMPLE_FILES}
 96: 
 97: ### Framework Commands (11 files)
 98: 
 99: **Location**: `.claude/commands/`
100: 
101: **Commands Installed**:
102: - batch-exe-prp.md
103: - batch-gen-prp.md
104: - denoise.md
105: - execute-prp.md
106: - generate-prp.md
107: - peer-review.md
108: - sync-with-syntropy.md
109: - syntropy-health.md
110: - tools-misuse-scan.md
111: - update-context.md
112: - vacuum.md
113: 
114: ### Tool Source Code (33 files)
115: 
116: **Location**: `tools/`
117: 
118: **Modules Installed**:
119: {LIST_OF_TOOL_MODULES}
120: 
121: ### CLAUDE.md Sections
122: 
123: **Framework Sections Added**:
124: - Communication
125: - Core Principles
126: - UV Package Management
127: - Ad-Hoc Code Policy
128: - Quick Commands
129: - Tool Naming Convention
130: - Allowed Tools Summary
131: - Command Permissions
132: - Quick Tool Selection
133: - Project Structure
134: - Testing Standards
135: - Code Quality
136: 
137: ---
138: 
139: ## Validation Results
140: 
141: ### Component Verification
142: 
143: ```bash
144: # Run validation checklist
145: {VALIDATION_COMMAND_OUTPUT}
146: ```
147: 
148: **Results**:
149: - âœ… 23 system memories installed
150: - âœ… 21 system examples installed
151: - âœ… 11 framework commands installed
152: - âœ… 33 tool files installed
153: - âœ… CLAUDE.md framework sections added
154: - âœ… CE 1.1 /system/ organization verified
155: 
156: ### Token Counts
157: 
158: **Framework Documentation**:
159: - System memories: ~{MEMORY_TOKENS}k tokens
160: - System examples: ~{EXAMPLE_TOKENS}k tokens
161: - Commands: ~{COMMAND_TOKENS}k tokens
162: - **Total**: ~{TOTAL_TOKENS}k tokens
163: 
164: ---
165: 
166: ## Project Integration
167: 
168: ### Existing Project Context
169: 
170: **Project Name**: {PROJECT_NAME}
171: **Project Type**: {PROJECT_TYPE}
172: **Primary Language**: {PRIMARY_LANGUAGE}
173: **Repository**: {REPO_URL}
174: 
175: **Pre-Installation State**:
176: {PRE_INSTALL_STATE}
177: 
178: ### Post-Installation Configuration
179: 
180: **Syntropy MCP**:
181: - Status: {MCP_STATUS}
182: - Servers: {MCP_SERVERS}
183: 
184: **Linear Integration**:
185: - Status: {LINEAR_STATUS}
186: - Project: {LINEAR_PROJECT}
187: - Team: {LINEAR_TEAM}
188: 
189: **UV Tools**:
190: - Status: {UV_STATUS}
191: - Python Version: {PYTHON_VERSION}
192: 
193: ---
194: 
195: ## Next Steps
196: 
197: ### Immediate Actions
198: 
199: 1. **Test framework commands**:
200:    ```bash
201:    /generate-prp
202:    /vacuum --dry-run
203:    /syntropy-health
204:    ```
205: 
206: 2. **Configure project settings**:
207:    - Update `.claude/settings.local.json` with project-specific permissions
208:    - Update `CLAUDE.md` with user sections
209: 
210: 3. **Create first user PRP**:
211:    - Document initial feature or bug fix
212:    - Practice PRP workflow
213: 
214: ### Recommended Reading
215: 
216: **Essential Documentation**:
217: - `.ce/examples/system/TOOL-USAGE-GUIDE.md` - Tool selection reference
218: - `.ce/examples/system/prp-decomposition-patterns.md` - PRP sizing
219: - `.serena/memories/system/task-completion-checklist.md` - Workflow checklist
220: 
221: **Migration Guide**:
222: - `examples/INITIALIZATION.md` - Complete CE 1.1 initialization guide (5-phase workflow, scenario-aware)
223: 
224: ---
225: 
226: ## Troubleshooting
227: 
228: **Issue**: Commands not found
229: 
230: **Solution**:
231: ```bash
232: # Verify commands directory
233: ls .claude/commands/
234: 
235: # Check Claude Code settings
236: cat .claude/settings.local.json | grep commands
237: ```
238: 
239: **Issue**: MCP servers not connected
240: 
241: **Solution**:
242: ```bash
243: # Remove auth cache
244: rm -rf ~/.mcp-auth
245: 
246: # Restart Claude Code
247: /mcp
248: ```
249: 
250: ---
251: 
252: ## Changelog
253: 
254: ### {TIMESTAMP} - Initial Installation
255: 
256: - Installed CE {CE_VERSION} via {INSTALLATION_METHOD}
257: - Added 23 framework memories to `.serena/memories/system/`
258: - Added 21 framework examples to `.ce/examples/system/`
259: - Added 11 framework commands to `.claude/commands/`
260: - Configured CLAUDE.md with framework sections
261: 
262: ---
263: 
264: ## References
265: 
266: - **CE Framework Documentation**: `.ce/examples/system/`
267: - **Migration Guides**: `examples/workflows/migration-*.md`
268: - **Master Initialization Guide**: `examples/INITIALIZATION.md`
269: - **Repomix Packages**: `.ce/ce-infrastructure.xml`, `.ce/ce-workflow-docs.xml`
270: 
271: ---
272: 
273: **Installation Completed**: {TIMESTAMP}
274: **Installed By**: {INSTALLER_NAME}
275: **CE Version**: {CE_VERSION}
</file>

<file path=".claude/commands/denoise.md">
  1: # /denoise - Boil Out Document Noise
  2: 
  3: Boil out noise from documentsâ€”remove verbosity while strictly guaranteeing complete retention of all essential information.
  4: 
  5: ## Usage
  6: 
  7: ```bash
  8: # Denoise single document
  9: /denoise path/to/document.md
 10: 
 11: # Denoise with custom compression target (default: 60-75% reduction)
 12: /denoise path/to/document.md --target 70
 13: 
 14: # Preview changes without writing (dry-run)
 15: /denoise path/to/document.md --dry-run
 16: 
 17: # Denoise multiple documents
 18: /denoise docs/*.md
 19: 
 20: # Denoise and show statistics
 21: /denoise path/to/document.md --verbose
 22: ```
 23: 
 24: ## What It Does
 25: 
 26: **Removes**:
 27: - Verbose explanations (keeps essential meaning)
 28: - Redundant examples (keeps one representative)
 29: - Long code blocks (keeps signature + key logic)
 30: - Repetitive sections
 31: - Wordy transitions
 32: - Multiple ways of saying same thing
 33: 
 34: **Preserves**:
 35: - All essential information
 36: - Technical accuracy
 37: - Commands and references
 38: - Critical warnings and errors
 39: - Quick reference data
 40: - Structure and organization
 41: 
 42: **Guaranteed**: Zero information loss, only noise removal
 43: 
 44: ## Algorithm
 45: 
 46: ### Phase 1: Structural Analysis
 47: - Parse markdown sections
 48: - Identify redundant patterns
 49: - Detect verbose vs concise sections
 50: - Map cross-references
 51: 
 52: ### Phase 2: Content Classification
 53: - **Keep**: Commands, references, warnings, key facts
 54: - **Compress**: Long explanations â†’ bullet points
 55: - **Deduplicate**: Multiple examples â†’ single best
 56: - **Condense**: Multi-paragraph â†’ 2-3 lines
 57: 
 58: ### Phase 3: Optimization
 59: - Convert verbose text to scannable format
 60: - Preserve all unique information
 61: - Maintain readability
 62: - Keep structure intact
 63: 
 64: ### Phase 4: Validation
 65: - Verify no information loss
 66: - Check all references intact
 67: - Ensure commands preserved
 68: - Validate structure
 69: 
 70: ## Output
 71: 
 72: ```
 73: ðŸ“„ Denoising: CLAUDE.md
 74: 
 75: Before: 1040 lines
 76: After:  259 lines
 77: Reduction: 75% (781 lines)
 78: 
 79: âœ… Preserved:
 80:   - All 5 core principles
 81:   - 48 allowed tools summary
 82:   - 15 quick commands
 83:   - 12 troubleshooting entries
 84:   - All resource paths
 85: 
 86: âŒ Removed:
 87:   - 6 verbose sections (150+ lines)
 88:   - 12 redundant code examples
 89:   - 8 long explanations
 90:   - 4 duplicate references
 91: 
 92: Validation: PASS âœ…
 93: All essential information preserved.
 94: 
 95: Written to: CLAUDE.md
 96: ```
 97: 
 98: ## Examples
 99: 
100: ### Before (Verbose)
101: ```markdown
102: ## Working Directory
103: 
104: **Default Context:** Project root (`/Users/bprzybysz/nc-src/ctx-eng-plus`)
105: 
106: **For tools/ commands:** Always prefix with `cd tools &&` or use full paths from root.
107: 
108: **Note:** Claude Code doesn't have a persistent working directory setting per project.
109: Always specify context explicitly:
110: 
111: ```bash
112: # Correct patterns
113: cd tools && uv run ce --help
114: cd tools && uv run pytest tests/ -v
115: uv run -C tools ce validate --level all  # Using uv -C flag
116: 
117: # Avoid (relative paths from wrong location)
118: uv run ce --help  # Will fail if not in tools/
119: ```
120: ```
121: 
122: ### After (Denoised)
123: ```markdown
124: ## Working Directory
125: 
126: **Default**: `/Users/bprzybysz/nc-src/ctx-eng-plus`
127: 
128: **For tools/ commands**: Use `cd tools &&` or `uv run -C tools`
129: ```
130: 
131: ## Configuration
132: 
133: **Location**: `.ce/denoise-config.yml` (optional)
134: 
135: ```yaml
136: denoise:
137:   # Target compression ratio (default: 60-75%)
138:   target_reduction: 70
139: 
140:   # Preserve these sections verbatim
141:   preserve_sections:
142:     - "Quick Commands"
143:     - "Troubleshooting"
144: 
145:   # Maximum examples to keep per section
146:   max_examples: 1
147: 
148:   # Condense code blocks longer than N lines
149:   condense_code_threshold: 20
150: ```
151: 
152: ## Rules
153: 
154: ### MUST Preserve
155: - Commands and CLI references
156: - Configuration examples
157: - Error messages and troubleshooting
158: - Technical specifications
159: - Quick reference tables
160: - Resource paths
161: 
162: ### MAY Compress
163: - Long explanations (â†’ bullet points)
164: - Multiple examples (â†’ best one)
165: - Verbose prose (â†’ concise)
166: - Repetitive sections (â†’ single instance)
167: 
168: ### MUST NOT Change
169: - Technical accuracy
170: - Command syntax
171: - File paths
172: - URLs and references
173: - Code logic (only formatting/length)
174: 
175: ## When to Use
176: 
177: **Good for**:
178: - Documentation that grew organically
179: - Multiple authors with varying styles
180: - Copy-paste accumulation
181: - Tutorial-style docs needing quick reference format
182: - Token-heavy files
183: 
184: **Not for**:
185: - Tutorial content (intentional verbosity)
186: - Step-by-step guides with explanations
187: - API documentation with full examples
188: - Already concise documents
189: 
190: ## Implementation
191: 
192: This command uses:
193: 1. **LLM-based analysis** (via Syntropy thinking tool)
194:    - Semantic understanding of content
195:    - Intelligent redundancy detection
196:    - Context-aware compression
197: 
198: 2. **Rule-based optimization**
199:    - Markdown structure preservation
200:    - Code block condensing
201:    - Bullet point conversion
202: 
203: 3. **Validation layer**
204:    - Information extraction (before/after)
205:    - Completeness check
206:    - Cross-reference validation
207: 
208: ## Integration
209: 
210: **Pre-commit hook** (optional):
211: ```yaml
212: # .claude/settings.local.json
213: {
214:   "hooks": {
215:     "preCommit": {
216:       "command": "/denoise docs/*.md --dry-run --verbose",
217:       "description": "Check for document bloat before commit"
218:     }
219:   }
220: }
221: ```
222: 
223: **Weekly maintenance**:
224: ```bash
225: # Add to crontab or CI/CD
226: /denoise CLAUDE.md syntropy-mcp/CLAUDE.md --verbose
227: ```
228: 
229: ## Related
230: 
231: - `/peer-review` - Review document quality
232: - `/update-context` - Sync context with codebase
233: - `ce validate --level 1` - Markdown linting
</file>

<file path=".claude/commands/sync-with-syntropy.md">
 1: # /sync-with-syntropy - Sync Settings with Syntropy MCP Tool State
 2: 
 3: Updates `.claude/settings.local.json` to match Syntropy MCP's current tool enable/disable state.
 4: 
 5: ## Workflow
 6: 
 7: 1. **Call Syntropy MCP**:
 8:    ```
 9:    mcp__syntropy__list_all_tools()
10:    ```
11: 
12:    Expected response:
13:    ```json
14:    {
15:      "total_tools": 87,
16:      "enabled_tools": 45,
17:      "disabled_tools": 42,
18:      "tools": [
19:        {"name": "mcp__syntropy__serena_find_symbol", "description": "...", "status": "enabled"},
20:        {"name": "mcp__syntropy__filesystem_read_file", "description": "...", "status": "disabled"}
21:      ]
22:    }
23:    ```
24: 
25: 2. **Load Settings**:
26:    - Read `.claude/settings.local.json`
27:    - If missing, create with structure: `{"allow": [], "deny": [], "ask": []}`
28: 
29: 3. **Process Disabled Tools**:
30:    For each tool with `status: "disabled"`:
31:    - Remove from `allow` list if present
32:    - Remove from `deny` list if present
33:    - Remove from `ask` list if present
34: 
35: 4. **Process Enabled Tools**:
36:    For each tool with `status: "enabled"`:
37:    - If NOT in `allow`, `deny`, OR `ask` lists â†’ Add to `allow` list
38:    - If already in any list â†’ No change
39: 
40: 5. **Backup and Validate**:
41:    - Backup: Copy settings to `.claude/settings.local.json.backup`
42:    - Validate JSON syntax: Parse and re-stringify to ensure validity
43:    - If validation fails â†’ Abort, restore backup, show error
44: 
45: 6. **Write Settings**:
46:    - Write updated settings with `indent=2`
47:    - Preserve all other settings (hooks, etc.)
48: 
49: 7. **Output Summary**:
50:    ```
51:    âœ“ Synced settings with Syntropy MCP tool state
52: 
53:    Removed 2 disabled tools:
54:      - mcp__syntropy__filesystem_read_file
55:      - mcp__syntropy__git_git_status
56: 
57:    Added 1 enabled tool to allow list:
58:      - mcp__syntropy__serena_find_symbol
59: 
60:    No changes: 45 tools already correct
61:    ```
62: 
63: ## Error Handling
64: 
65: **Syntropy MCP Not Connected**:
66: ```
67: âŒ Error: Syntropy MCP not connected
68: 
69:    Please ensure Syntropy MCP server is running:
70:    - Check MCP status with /mcp
71:    - Restart MCP if needed
72: ```
73: 
74: **Invalid JSON**:
75: ```
76: âŒ Error: Settings file invalid after update
77: 
78:    Validation error: {error details}
79: 
80:    Settings restored from backup.
81:    ðŸ”§ Check .claude/settings.local.json.backup for last good state
82: ```
83: 
84: **Empty Tool List**:
85: ```
86: âš  No tools returned from Syntropy MCP
87: 
88:   Skipping settings update (nothing to sync)
89: ```
</file>

<file path=".claude/commands/syntropy-health.md">
 1: # /syntropy-health - Syntropy MCP Health Check
 2: 
 3: Check health status of Syntropy MCP server and all underlying servers.
 4: 
 5: ## Usage
 6: 
 7: ```bash
 8: # Quick check (default - fast, essential servers only)
 9: /syntropy-health
10: 
11: # Full diagnostic check (all servers, detailed info)
12: /syntropy-health full
13: ```
14: 
15: ## What It Checks
16: 
17: **Quick Mode** (default):
18: - Essential servers: Serena, Filesystem, Git, Linear, Thinking
19: - Response times for each server
20: - Connection status
21: - Tool call counts (if available)
22: 
23: **Full Mode**:
24: - All 9 MCP servers (including lazy-loaded: Context7, Repomix, GitHub, Perplexity)
25: - Detailed diagnostics
26: - Last errors (if any)
27: - Call statistics
28: 
29: ## Output Format
30: 
31: ```
32: ðŸ¥ Syntropy MCP Health Check
33: 
34: âœ… serena: 124ms (15 calls)
35: âœ… filesystem: 45ms (8 calls)
36: âœ… git: 67ms (3 calls)
37: âš ï¸  linear: 1205ms (2 calls) - SLOW
38: âŒ github: Not connected
39: 
40: Overall: 4/5 servers healthy
41: ```
42: 
43: ## Troubleshooting
44: 
45: ### "Not connected" errors
46: 
47: Clear MCP auth cache and restart:
48: ```bash
49: rm -rf ~/.mcp-auth
50: ```
51: 
52: ### Slow response times (>1s)
53: 
54: Server may be starting up or overloaded. Wait 30s and retry.
55: 
56: ### All servers failing
57: 
58: Check if MCP servers are running:
59: ```bash
60: ps aux | grep mcp
61: ```
62: 
63: ## When to Use
64: 
65: **Recommended on session start** to verify MCP infrastructure:
66: - After Claude Code restart
67: - When seeing unexpected tool failures
68: - Before starting major work (PRPs, refactoring)
69: - After clearing auth cache
70: 
71: ## Implementation
72: 
73: This command calls the `healthcheck` tool with appropriate parameters:
74: - Default: `{"detailed": false, "timeout_ms": 2000}`
75: - Full: `{"detailed": true, "timeout_ms": 5000}`
76: 
77: ## Related
78: 
79: - `/update-context` - Updates project context (requires healthy Serena)
80: - `/generate-prp` - Requires healthy Serena + Filesystem
81: - `/execute-prp` - Requires multiple healthy servers
</file>

<file path=".claude/commands/tools-misuse-scan.md">
  1: # Tool Misuse Scan Command
  2: 
  3: **Purpose**: Detect and categorize tool misuse patterns in Claude Code sessions
  4: 
  5: **Target**: AI agents working with Context Engineering codebase
  6: 
  7: **Last Updated**: 2025-10-17
  8: 
  9: ---
 10: 
 11: ## Command Usage
 12: 
 13: ```bash
 14: /tools-misuse-scan
 15: ```
 16: 
 17: **What it does**:
 18: - Scans conversation history for denied tool errors
 19: - Categorizes into: (1) Bash anti-patterns, (2) Denied tools without substitutes
 20: - Provides remediation suggestions with proper tool alternatives
 21: - Generates structured report for debugging and improvement
 22: 
 23: ---
 24: 
 25: ## Detection Patterns
 26: 
 27: ### Category 1: Bash Anti-patterns
 28: 
 29: **Pattern**: Bash text processing operations that should use designated tools
 30: 
 31: **Detection Rules**:
 32: ```regex
 33: # Bash head/tail with file piping
 34: Bash\(.*\|\s*head\s+-\d+
 35: Bash\(.*\|\s*tail\s+-\d+
 36: 
 37: # Bash grep with file piping
 38: Bash\(.*\|\s*grep\s+
 39: 
 40: # Direct head/tail commands
 41: Bash\(head\s+-\d+
 42: Bash\(tail\s+-\d+
 43: 
 44: # Python subprocess without uv
 45: Bash\(python3?\s+
 46: Bash\(python3?\s+-m
 47: ```
 48: 
 49: **Remediation Mappings**:
 50: | Anti-pattern | Correct Tool | Example |
 51: |-------------|--------------|---------|
 52: | `Bash("cat file \| head -50")` | `Read(file, limit=50)` | Read first 50 lines |
 53: | `Bash("cat file \| tail -100")` | `Read(file, offset=-100)` | Read last 100 lines |
 54: | `Bash("grep pattern file")` | `shell_utils.grep_text()` | Search with context |
 55: | `Bash("python script.py")` | `uv run python script.py` | Proper env management |
 56: 
 57: ### Category 2: Denied Tools
 58: 
 59: **Pattern**: MCP tools explicitly denied with no direct substitute
 60: 
 61: **Detection Rules**:
 62: ```regex
 63: # Tool denied error messages
 64: has been denied
 65: permission denied.*mcp__
 66: Tool.*not available
 67: ```
 68: 
 69: **Known Denied Tools**:
 70: | Tool | Reason | Alternative |
 71: |------|--------|-------------|
 72: | `mcp__serena__replace_symbol_body` (old) | Legacy tool (pre-Syntropy aggregator) | `mcp__syntropy__serena_replace_symbol_body` (new, allowed) or `Edit` (native) |
 73: 
 74: ---
 75: 
 76: ## Analysis Workflow
 77: 
 78: ### Step 1: Error Collection
 79: 
 80: Scan conversation for:
 81: - "has been denied" messages
 82: - "permission denied" errors
 83: - Failed tool invocations with error responses
 84: 
 85: ### Step 2: Categorization
 86: 
 87: **Group 1: Bash Misuse**
 88: - Extract Bash command from error context
 89: - Match against anti-pattern regex
 90: - Identify correct tool replacement
 91: - Generate remediation suggestion
 92: 
 93: **Group 2: Denied Tools**
 94: - Extract tool name from error message
 95: - Check known alternatives table
 96: - Provide workaround guidance
 97: 
 98: ### Step 3: Report Generation
 99: 
100: **Output Format**:
101: ```markdown
102: ## Tool Misuse Analysis Report
103: 
104: **Session**: {timestamp}
105: **Total Errors Found**: {count}
106: 
107: ### Category 1: Bash Anti-patterns ({count})
108: 
109: 1. **Error**: Bash("cat file | head -50")
110:    - **Issue**: Text processing with piping
111:    - **Remedy**: Use `Read(file, limit=50)`
112:    - **Location**: Message #{n}
113:    - **Performance Impact**: 10-50x slower (subprocess overhead)
114: 
115: ### Category 2: Denied Tools ({count})
116: 
117: 1. **Error**: mcp__serena__replace_symbol_body (old naming)
118:    - **Issue**: Legacy tool (pre-Syntropy aggregator)
119:    - **Remedy**: Use `mcp__syntropy__serena_replace_symbol_body` (new, allowed) or `Edit` (native)
120:    - **Location**: Message #{n}
121:    - **Documentation**: See examples/TOOL-USAGE-GUIDE.md
122: ```
123: 
124: ---
125: 
126: ## Implementation Instructions
127: 
128: When running this command:
129: 
130: 1. **Scan Phase** (2 passes for thoroughness):
131:    - Pass 1: Search for "denied" and "permission" keywords
132:    - Pass 2: Validate each error against detection patterns
133: 
134: 2. **Categorization Phase**:
135:    - Apply regex patterns to extract tool names and commands
136:    - Match against known anti-patterns and denied tools
137:    - Generate remediation suggestions
138: 
139: 3. **Validation Phase**:
140:    - Cross-reference with `examples/tool-usage-patterns.md`
141:    - Verify remediation suggestions are accurate
142:    - Check that alternatives exist and are allowed
143: 
144: 4. **Report Phase**:
145:    - Generate structured markdown report
146:    - Include location references (message numbers)
147:    - Add performance impact notes where applicable
148:    - Link to relevant documentation
149: 
150: ---
151: 
152: ## Quality Checks
153: 
154: **Before finalizing report**:
155: 
156: - âœ… All errors categorized correctly
157: - âœ… Remediation suggestions are actionable
158: - âœ… Alternatives verified in tool-usage-patterns.md
159: - âœ… Location references are accurate
160: - âœ… No false positives (legitimate tool uses)
161: - âœ… Performance impact documented (bash subprocess overhead)
162: 
163: ---
164: 
165: ## Expected Output Example
166: 
167: ```markdown
168: ## Tool Misuse Analysis Report
169: 
170: **Session**: 2025-10-17
171: **Total Errors Found**: 7
172: 
173: ### Category 1: Bash Anti-patterns (6 errors)
174: 
175: 1. **Error**: Bash("head -50 file")
176:    - **Issue**: Direct head command without tool
177:    - **Remedy**: Use `Read(file, limit=50)` or `shell_utils.head(file, 50)`
178:    - **Performance**: 10-50x faster with Python utilities
179: 
180: 2. **Error**: Bash("tail -100 file")
181:    - **Issue**: Direct tail command without tool
182:    - **Remedy**: Use `Read(file, offset=-100)` or `shell_utils.tail(file, 100)`
183:    - **Performance**: 10-50x faster with Python utilities
184: 
185: 3. **Error**: Bash("grep pattern file")
186:    - **Issue**: Text search with subprocess
187:    - **Remedy**: Use `shell_utils.grep_text(pattern, file_content)`
188:    - **Performance**: No subprocess fork overhead
189: 
190: 4-6. [Similar patterns...]
191: 
192: ### Category 2: Denied Tools (1 error)
193: 
194: 1. **Error**: mcp__serena__replace_symbol_body (old naming)
195:    - **Issue**: Legacy tool (pre-Syntropy aggregator)
196:    - **Remedy**: Use `mcp__syntropy__serena_replace_symbol_body` (new, allowed)
197:    - **Alternative**: Use `Edit` (native) for direct file modifications
198:    - **Documentation**: examples/TOOL-USAGE-GUIDE.md
199: 
200: ---
201: 
202: ## Recommendations
203: 
204: 1. **Update Documentation**: Ensure examples/tool-usage-patterns.md covers all anti-patterns
205: 2. **Add Pre-commit Validation**: Consider tool usage linting in CI/CD
206: 3. **Agent Training**: Review remediation patterns with agents
207: 4. **Performance Monitoring**: Track subprocess overhead reduction after fixes
208: ```
209: 
210: ---
211: 
212: ## References
213: 
214: - **Tool Usage Patterns**: `examples/tool-usage-patterns.md`
215: - **Serena MCP Restrictions**: `.serena/memories/serena-mcp-tool-restrictions.md`
216: - **CLAUDE.md**: Text processing anti-patterns section
217: - **shell_utils Module**: `tools/ce/shell_utils.py`
218: 
219: ---
220: 
221: ## Notes
222: 
223: - This command is retrospective analysis, not real-time enforcement
224: - Helps identify patterns for documentation updates
225: - Useful for agent training and tool usage improvement
226: - Run periodically after major development sessions
</file>

<file path="tools/ce/blend.py">
  1: """Blending operations module."""
  2: 
  3: import sys
  4: import logging
  5: from pathlib import Path
  6: from typing import Dict, Any
  7: 
  8: from .blending.core import BlendingOrchestrator
  9: 
 10: logger = logging.getLogger(__name__)
 11: 
 12: 
 13: def setup_logging(verbose: bool = False) -> None:
 14:     """Configure logging."""
 15:     level = logging.DEBUG if verbose else logging.INFO
 16:     logging.basicConfig(
 17:         level=level,
 18:         format='%(message)s'
 19:     )
 20: 
 21: 
 22: def load_config(config_path: Path) -> Dict[str, Any]:
 23:     """
 24:     Load blend configuration from YAML.
 25: 
 26:     Args:
 27:         config_path: Path to blend-config.yml
 28: 
 29:     Returns:
 30:         Configuration dict
 31: 
 32:     Raises:
 33:         FileNotFoundError: If config file not found
 34:         ValueError: If config invalid
 35:     """
 36:     if not config_path.exists():
 37:         raise FileNotFoundError(
 38:             f"Config file not found: {config_path}\n"
 39:             f"ðŸ”§ Create .ce/blend-config.yml (see PRP-34.1.1)"
 40:         )
 41: 
 42:     import yaml
 43: 
 44:     try:
 45:         with open(config_path) as f:
 46:             config = yaml.safe_load(f)
 47: 
 48:         if not config or 'domains' not in config:
 49:             raise ValueError("Config missing 'domains' section")
 50: 
 51:         return config
 52: 
 53:     except yaml.YAMLError as e:
 54:         raise ValueError(
 55:             f"Invalid YAML config: {e}\n"
 56:             f"ðŸ”§ Check syntax: {config_path}"
 57:         ) from e
 58: 
 59: 
 60: def run_blend(args) -> int:
 61:     """
 62:     Execute blending operation.
 63: 
 64:     Args:
 65:         args: Parsed CLI arguments (from argparse.Namespace)
 66: 
 67:     Returns:
 68:         Exit code (0 = success, 1 = failure)
 69:     """
 70:     # Setup logging first
 71:     setup_logging(getattr(args, 'verbose', False))
 72: 
 73:     try:
 74:         # Load configuration
 75:         config_path = Path(args.config)
 76:         config = load_config(config_path)
 77: 
 78:         # Initialize orchestrator
 79:         orchestrator = BlendingOrchestrator(
 80:             config=config,
 81:             dry_run=args.dry_run
 82:         )
 83: 
 84:         # Determine target directory
 85:         target_dir = Path(args.target_dir).resolve()
 86:         if not target_dir.exists():
 87:             raise ValueError(
 88:                 f"Target directory not found: {target_dir}\n"
 89:                 f"ðŸ”§ Provide valid project directory"
 90:             )
 91: 
 92:         # Run phases
 93:         if args.all:
 94:             # Run all 4 phases
 95:             phases = ['detect', 'classify', 'blend']
 96:             if not args.skip_cleanup:
 97:                 phases.append('cleanup')
 98: 
 99:             for phase in phases:
100:                 result = orchestrator.run_phase(phase, target_dir)
101:                 logger.info(f"âœ“ Phase {phase} complete")
102: 
103:                 # Interactive mode - ask before next phase
104:                 if args.interactive and phase != phases[-1]:
105:                     response = input(f"Continue to {phases[phases.index(phase) + 1]}? [Y/n] ")
106:                     if response.lower() == 'n':
107:                         logger.info("Stopped by user")
108:                         return 0
109: 
110:         elif args.phase:
111:             # Run specific phase
112:             result = orchestrator.run_phase(args.phase, target_dir)
113:             logger.info(f"âœ“ Phase {args.phase} complete")
114: 
115:         elif args.cleanup_only:
116:             # Run cleanup only (requires prior blend)
117:             result = orchestrator.run_phase('cleanup', target_dir)
118:             logger.info("âœ“ Cleanup complete")
119: 
120:         elif args.rollback:
121:             # Restore backups
122:             logger.info("ðŸ”„ Rolling back blending operations...")
123:             # Stub - implement in validation.py (PRP-34.1.1)
124:             logger.warning("Rollback not yet fully implemented")
125:             return 1
126: 
127:         else:
128:             logger.error("No operation specified (use --all, --phase, --cleanup-only, or --rollback)")
129:             return 1
130: 
131:         logger.info("âœ… Blending complete!")
132:         return 0
133: 
134:     except Exception as e:
135:         logger.error(f"âŒ Blending failed: {e}")
136:         if getattr(args, 'verbose', False):
137:             logger.exception("Full traceback:")
138:         return 1
</file>

<file path="tools/ce/init_project.py">
  1: #!/usr/bin/env python3
  2: """
  3: CE Framework Project Initializer - Core Module
  4: 
  5: Implements the 4-phase pipeline for installing CE framework on target projects:
  6: 1. Extract: Unpack ce-infrastructure.xml to target project
  7: 2. Blend: Merge framework + user files (CLAUDE.md, settings, commands)
  8: 3. Initialize: Run uv sync to install dependencies
  9: 4. Verify: Validate installation and report status
 10: """
 11: 
 12: import json
 13: import shutil
 14: import subprocess
 15: import sys
 16: from pathlib import Path
 17: from typing import Dict, List, Optional
 18: 
 19: 
 20: class ProjectInitializer:
 21:     """
 22:     Core initializer for CE Framework installation on target projects.
 23: 
 24:     Handles 4-phase pipeline:
 25:     - extract: Unpack repomix package to .ce/
 26:     - blend: Merge framework + user files
 27:     - initialize: Install Python dependencies
 28:     - verify: Validate installation
 29:     """
 30: 
 31:     def __init__(self, target_project: Path, dry_run: bool = False):
 32:         """
 33:         Initialize ProjectInitializer.
 34: 
 35:         Args:
 36:             target_project: Path to target project root
 37:             dry_run: If True, show actions without executing
 38:         """
 39:         self.target_project = Path(target_project).resolve()
 40:         self.dry_run = dry_run
 41:         self.ce_dir = self.target_project / ".ce"
 42:         self.tools_dir = self.ce_dir / "tools"
 43: 
 44:         # Paths to framework packages (in ctx-eng-plus repo)
 45:         self.ctx_eng_root = Path(__file__).parent.parent.parent.resolve()
 46:         self.infrastructure_xml = self.ctx_eng_root / ".ce" / "ce-infrastructure.xml"
 47:         self.workflow_xml = self.ctx_eng_root / ".ce" / "ce-workflow-docs.xml"
 48: 
 49:     def run(self, phase: str = "all") -> Dict:
 50:         """
 51:         Run initialization pipeline.
 52: 
 53:         Args:
 54:             phase: Which phase(s) to run - "all", "extract", "blend", "initialize", "verify"
 55: 
 56:         Returns:
 57:             Dict with status info for each phase executed
 58: 
 59:         Raises:
 60:             ValueError: If invalid phase specified
 61:         """
 62:         valid_phases = ["all", "extract", "blend", "initialize", "verify"]
 63:         if phase not in valid_phases:
 64:             raise ValueError(f"Invalid phase '{phase}'. Must be one of: {valid_phases}")
 65: 
 66:         results = {}
 67: 
 68:         if phase == "all":
 69:             results["extract"] = self.extract()
 70:             results["blend"] = self.blend()
 71:             results["initialize"] = self.initialize()
 72:             results["verify"] = self.verify()
 73:         else:
 74:             # Run single phase
 75:             method = getattr(self, phase)
 76:             results[phase] = method()
 77: 
 78:         return results
 79: 
 80:     def extract(self) -> Dict:
 81:         """
 82:         Extract ce-infrastructure.xml to target project.
 83: 
 84:         Steps:
 85:         1. Check if ce-infrastructure.xml exists
 86:         2. Extract to .ce/ directory
 87:         3. Reorganize tools/ to .ce/tools/
 88:         4. Copy ce-workflow-docs.xml to .ce/
 89: 
 90:         Returns:
 91:             Dict with extraction status and file counts
 92:         """
 93:         status = {"success": False, "files_extracted": 0, "message": ""}
 94: 
 95:         # Check for infrastructure package
 96:         if not self.infrastructure_xml.exists():
 97:             status["message"] = (
 98:                 f"âŒ ce-infrastructure.xml not found at {self.infrastructure_xml}\n"
 99:                 f"ðŸ”§ Ensure you're running from ctx-eng-plus repo root"
100:             )
101:             return status
102: 
103:         if self.dry_run:
104:             status["success"] = True
105:             status["message"] = f"[DRY-RUN] Would extract to {self.ce_dir}"
106:             return status
107: 
108:         # Check for existing .ce/ directory
109:         if self.ce_dir.exists():
110:             status["message"] = (
111:                 f"âš ï¸  Warning: .ce/ directory already exists at {self.ce_dir}\n"
112:                 f"ðŸ”§ Backup and remove existing directory, or use --force flag"
113:             )
114:             return status
115: 
116:         try:
117:             # Import repomix_unpack module
118:             from ce.repomix_unpack import extract_files
119: 
120:             # Extract to temporary location first
121:             temp_extract = self.target_project / "tmp" / "ce-extraction"
122:             temp_extract.mkdir(parents=True, exist_ok=True)
123: 
124:             # Extract files
125:             files_extracted = extract_files(
126:                 xml_path=self.infrastructure_xml,
127:                 target_dir=temp_extract,
128:                 verbose=False
129:             )
130: 
131:             if files_extracted == 0:
132:                 status["message"] = "âŒ No files extracted from package"
133:                 return status
134: 
135:             # Reorganize: Move system/ to .ce/, tools/ to .ce/tools/
136:             self.ce_dir.mkdir(parents=True, exist_ok=True)
137: 
138:             # Move system/ contents to .ce/
139:             system_dir = temp_extract / "system"
140:             if system_dir.exists():
141:                 for item in system_dir.iterdir():
142:                     shutil.move(str(item), str(self.ce_dir / item.name))
143: 
144:             # Move tools/ to .ce/tools/
145:             tools_source = temp_extract / "tools"
146:             if tools_source.exists():
147:                 shutil.move(str(tools_source), str(self.tools_dir))
148: 
149:             # Copy ce-workflow-docs.xml (reference package)
150:             if self.workflow_xml.exists():
151:                 shutil.copy2(self.workflow_xml, self.ce_dir / "ce-workflow-docs.xml")
152: 
153:             # Cleanup temp directory
154:             shutil.rmtree(temp_extract.parent)
155: 
156:             status["success"] = True
157:             status["files_extracted"] = files_extracted
158:             status["message"] = f"âœ… Extracted {files_extracted} files to {self.ce_dir}"
159: 
160:         except Exception as e:
161:             status["message"] = f"âŒ Extraction failed: {str(e)}\nðŸ”§ Check error details above"
162: 
163:         return status
164: 
165:     def blend(self) -> Dict:
166:         """
167:         Blend framework + user files.
168: 
169:         Delegates to: uv run ce blend --all --target-dir <target>
170: 
171:         Returns:
172:             Dict with blend status and stdout/stderr
173:         """
174:         status = {"success": False, "stdout": "", "stderr": ""}
175: 
176:         if self.dry_run:
177:             status["success"] = True
178:             status["stdout"] = f"[DRY-RUN] Would run: uv run ce blend --all --target-dir {self.target_project}"
179:             return status
180: 
181:         try:
182:             # Run blend command
183:             result = subprocess.run(
184:                 ["uv", "run", "ce", "blend", "--all", "--target-dir", str(self.target_project)],
185:                 cwd=self.ctx_eng_root / "tools",
186:                 capture_output=True,
187:                 text=True,
188:                 timeout=120
189:             )
190: 
191:             status["stdout"] = result.stdout
192:             status["stderr"] = result.stderr
193:             status["success"] = result.returncode == 0
194: 
195:             if not status["success"]:
196:                 status["message"] = (
197:                     f"âŒ Blend phase failed (exit code {result.returncode})\n"
198:                     f"ðŸ”§ Check blend tool output:\n{result.stderr}"
199:                 )
200:             else:
201:                 status["message"] = "âœ… Blend phase completed"
202: 
203:         except subprocess.TimeoutExpired:
204:             status["message"] = "âŒ Blend phase timed out (120s limit)\nðŸ”§ Check for hanging processes"
205:         except FileNotFoundError:
206:             status["message"] = (
207:                 "âŒ uv not found in PATH\n"
208:                 "ðŸ”§ Install UV: curl -LsSf https://astral.sh/uv/install.sh | sh"
209:             )
210:         except Exception as e:
211:             status["message"] = f"âŒ Blend phase failed: {str(e)}"
212: 
213:         return status
214: 
215:     def initialize(self) -> Dict:
216:         """
217:         Initialize Python environment.
218: 
219:         Runs: uv sync in .ce/tools/ directory
220: 
221:         Returns:
222:             Dict with initialization status and command output
223:         """
224:         status = {"success": False, "stdout": "", "stderr": ""}
225: 
226:         if not self.tools_dir.exists():
227:             status["message"] = (
228:                 f"âŒ Tools directory not found: {self.tools_dir}\n"
229:                 f"ðŸ”§ Run extract phase first"
230:             )
231:             return status
232: 
233:         if self.dry_run:
234:             status["success"] = True
235:             status["stdout"] = f"[DRY-RUN] Would run: uv sync in {self.tools_dir}"
236:             return status
237: 
238:         try:
239:             # Run uv sync
240:             result = subprocess.run(
241:                 ["uv", "sync"],
242:                 cwd=self.tools_dir,
243:                 capture_output=True,
244:                 text=True,
245:                 timeout=300  # 5 minutes for dependency installation
246:             )
247: 
248:             status["stdout"] = result.stdout
249:             status["stderr"] = result.stderr
250:             status["success"] = result.returncode == 0
251: 
252:             if not status["success"]:
253:                 status["message"] = (
254:                     f"âŒ UV sync failed (exit code {result.returncode})\n"
255:                     f"ðŸ”§ Check pyproject.toml and dependency versions:\n{result.stderr}"
256:                 )
257:             else:
258:                 status["message"] = "âœ… Python environment initialized"
259: 
260:         except subprocess.TimeoutExpired:
261:             status["message"] = "âŒ UV sync timed out (300s limit)\nðŸ”§ Check network connection or package mirrors"
262:         except FileNotFoundError:
263:             status["message"] = (
264:                 "âŒ uv not found in PATH\n"
265:                 "ðŸ”§ Install UV: curl -LsSf https://astral.sh/uv/install.sh | sh"
266:             )
267:         except Exception as e:
268:             status["message"] = f"âŒ Initialize phase failed: {str(e)}"
269: 
270:         return status
271: 
272:     def verify(self) -> Dict:
273:         """
274:         Verify installation.
275: 
276:         Checks:
277:         1. Critical files exist (.ce/tools/, .claude/, .serena/)
278:         2. settings.local.json is valid JSON
279:         3. pyproject.toml exists
280:         4. Reports summary
281: 
282:         Returns:
283:             Dict with verification results and warnings
284:         """
285:         status = {"success": True, "warnings": [], "checks": []}
286: 
287:         # Critical files to check
288:         critical_files = [
289:             self.ce_dir / "tools" / "pyproject.toml",
290:             self.target_project / ".claude" / "settings.local.json",
291:             self.target_project / ".serena" / "memories",
292:             self.ce_dir / "RULES.md"
293:         ]
294: 
295:         for file_path in critical_files:
296:             if file_path.exists():
297:                 status["checks"].append(f"âœ… {file_path.relative_to(self.target_project)}")
298:             else:
299:                 status["warnings"].append(f"âš ï¸  Missing: {file_path.relative_to(self.target_project)}")
300:                 status["success"] = False
301: 
302:         # Validate settings.local.json
303:         settings_file = self.target_project / ".claude" / "settings.local.json"
304:         if settings_file.exists():
305:             try:
306:                 with open(settings_file) as f:
307:                     json.load(f)
308:                 status["checks"].append("âœ… settings.local.json is valid JSON")
309:             except json.JSONDecodeError as e:
310:                 status["warnings"].append(f"âš ï¸  Invalid JSON in settings.local.json: {str(e)}")
311:                 status["success"] = False
312:         else:
313:             status["warnings"].append("âš ï¸  settings.local.json not found")
314: 
315:         # Check Python installation
316:         venv_dir = self.tools_dir / ".venv"
317:         if venv_dir.exists():
318:             status["checks"].append("âœ… Python virtual environment created")
319:         else:
320:             status["warnings"].append("âš ï¸  Virtual environment not found (run initialize phase)")
321: 
322:         # Summary message
323:         if status["success"]:
324:             status["message"] = f"âœ… Installation verified ({len(status['checks'])} checks passed)"
325:         else:
326:             status["message"] = (
327:                 f"âš ï¸  Installation incomplete ({len(status['warnings'])} warnings)\n"
328:                 f"ðŸ”§ Review warnings above and re-run failed phases"
329:             )
330: 
331:         return status
332: 
333: 
334: def main():
335:     """CLI entry point for testing."""
336:     if len(sys.argv) < 2:
337:         print("Usage: python init_project.py <target-project-path> [--dry-run]")
338:         sys.exit(1)
339: 
340:     target = Path(sys.argv[1])
341:     dry_run = "--dry-run" in sys.argv
342: 
343:     initializer = ProjectInitializer(target, dry_run=dry_run)
344:     results = initializer.run(phase="all")
345: 
346:     # Print results
347:     for phase, result in results.items():
348:         print(f"\n=== Phase: {phase} ===")
349:         print(result.get("message", "No message"))
350:         if not result.get("success", True):
351:             sys.exit(1)
352: 
353:     print("\nâœ… Initialization complete!")
354: 
355: 
356: if __name__ == "__main__":
357:     main()
</file>

<file path="tools/ce/repomix_unpack.py">
  1: #!/usr/bin/env python3
  2: """
  3: Repomix XML unpacker - extracts files from repomix-generated XML packages.
  4: 
  5: Since repomix CLI doesn't yet support --unpack, this utility extracts files
  6: from XML packages generated by repomix.
  7: """
  8: 
  9: import argparse
 10: import re
 11: import sys
 12: from pathlib import Path
 13: from typing import List, Tuple
 14: 
 15: 
 16: def parse_repomix_xml(xml_content: str) -> List[Tuple[str, str]]:
 17:     """
 18:     Parse repomix XML and extract (file_path, content) tuples.
 19: 
 20:     Args:
 21:         xml_content: Raw XML content from repomix package
 22: 
 23:     Returns:
 24:         List of (file_path, file_content) tuples
 25:     """
 26:     files = []
 27: 
 28:     # Pattern to match <file path="...">content</file>
 29:     # Using non-greedy match and DOTALL to handle multiline content
 30:     pattern = r'<file path="([^"]+)">\n(.*?)\n</file>'
 31: 
 32:     matches = re.finditer(pattern, xml_content, re.DOTALL)
 33: 
 34:     for match in matches:
 35:         file_path = match.group(1)
 36:         content = match.group(2)
 37: 
 38:         # Remove line number prefix (format: "   123â†’")
 39:         # Each line in repomix XML has line numbers
 40:         lines = content.split('\n')
 41:         cleaned_lines = []
 42:         for line in lines:
 43:             # Match line number format: spaces + number + â†’
 44:             cleaned = re.sub(r'^\s*\d+â†’', '', line)
 45:             cleaned_lines.append(cleaned)
 46: 
 47:         cleaned_content = '\n'.join(cleaned_lines)
 48:         files.append((file_path, cleaned_content))
 49: 
 50:     return files
 51: 
 52: 
 53: def extract_files(xml_path: Path, target_dir: Path, verbose: bool = False) -> int:
 54:     """
 55:     Extract all files from repomix XML to target directory.
 56: 
 57:     Args:
 58:         xml_path: Path to repomix XML file
 59:         target_dir: Directory to extract files to
 60:         verbose: Print extraction progress
 61: 
 62:     Returns:
 63:         Number of files extracted
 64:     """
 65:     if not xml_path.exists():
 66:         print(f"âŒ Error: XML file not found: {xml_path}", file=sys.stderr)
 67:         return 0
 68: 
 69:     # Read XML content
 70:     if verbose:
 71:         print(f"ðŸ“¦ Reading package: {xml_path}")
 72: 
 73:     xml_content = xml_path.read_text()
 74: 
 75:     # Parse files
 76:     files = parse_repomix_xml(xml_content)
 77: 
 78:     if not files:
 79:         print(f"âš ï¸  Warning: No files found in package", file=sys.stderr)
 80:         return 0
 81: 
 82:     if verbose:
 83:         print(f"ðŸ“‹ Found {len(files)} files to extract")
 84: 
 85:     # Extract each file
 86:     extracted = 0
 87:     for file_path, content in files:
 88:         # Construct full path
 89:         full_path = target_dir / file_path
 90: 
 91:         # Create parent directories
 92:         full_path.parent.mkdir(parents=True, exist_ok=True)
 93: 
 94:         # Write file
 95:         full_path.write_text(content)
 96: 
 97:         if verbose:
 98:             print(f"   âœ“ {file_path}")
 99: 
100:         extracted += 1
101: 
102:     return extracted
103: 
104: 
105: def main():
106:     """Main entry point for CLI."""
107:     parser = argparse.ArgumentParser(
108:         description="Extract files from repomix XML package",
109:         formatter_class=argparse.RawDescriptionHelpFormatter,
110:         epilog="""
111: Examples:
112:   # Extract to current directory
113:   python repomix_unpack.py ce-infrastructure.xml
114: 
115:   # Extract to specific directory
116:   python repomix_unpack.py ce-infrastructure.xml --target /path/to/project
117: 
118:   # Verbose output
119:   python repomix_unpack.py ce-infrastructure.xml -v
120:         """
121:     )
122: 
123:     parser.add_argument(
124:         "xml_file",
125:         type=Path,
126:         help="Path to repomix XML file"
127:     )
128: 
129:     parser.add_argument(
130:         "--target",
131:         type=Path,
132:         default=Path.cwd(),
133:         help="Target directory for extraction (default: current directory)"
134:     )
135: 
136:     parser.add_argument(
137:         "-v", "--verbose",
138:         action="store_true",
139:         help="Verbose output"
140:     )
141: 
142:     args = parser.parse_args()
143: 
144:     # Extract files
145:     count = extract_files(args.xml_file, args.target, args.verbose)
146: 
147:     if count > 0:
148:         print(f"\nâœ… Extracted {count} files to {args.target}")
149:         return 0
150:     else:
151:         print(f"\nâŒ Extraction failed", file=sys.stderr)
152:         return 1
153: 
154: 
155: if __name__ == "__main__":
156:     sys.exit(main())
</file>

<file path="tools/pyproject.toml">
 1: [project]
 2: name = "ce-tools"
 3: version = "0.1.0"
 4: description = "Context Engineering CLI Tools"
 5: readme = "README.md"
 6: requires-python = ">=3.10"
 7: dependencies = [
 8:     "anthropic>=0.40.0",
 9:     "deepdiff>=6.0",
10:     "diagrams>=0.24.4",
11:     "jsonschema>=4.25.1",
12:     "python-frontmatter>=1.1.0",
13:     "pyyaml>=6.0",
14: ]
15: 
16: [project.scripts]
17: ce = "ce.__main__:main"
18: 
19: [build-system]
20: requires = ["hatchling"]
21: build-backend = "hatchling.build"
22: 
23: [tool.pytest.ini_options]
24: testpaths = ["tests"]
25: python_files = "test_*.py"
26: python_functions = "test_*"
27: addopts = "-v --tb=short"
28: 
29: [tool.hatch.build.targets.wheel]
30: packages = ["ce"]
31: 
32: [dependency-groups]
33: dev = [
34:     "pytest>=8.4.2",
35:     "pytest-cov>=7.0.0",
36: ]
</file>

<file path=".claude/commands/batch-gen-prp.md">
  1: # /batch-gen-prp - Batch PRP Generation with Parallel Subagents
  2: 
  3: Decomposes large plan documents into staged, parallelizable PRPs with automatic dependency analysis and concurrent generation using subagents.
  4: 
  5: **Architecture**: Coordinator spawns parallel subagents running `/generate-prp` in batch mode
  6: 
  7: ## Usage
  8: 
  9: ```bash
 10: /batch-gen-prp <plan-file-path>
 11: 
 12: # Examples:
 13: /batch-gen-prp TOOL-PERMISSION-LOCKDOWN-PLAN.md
 14: /batch-gen-prp feature-requests/AUTH-SYSTEM-PLAN.md
 15: /batch-gen-prp PRPs/plans/BIG-FEATURE-PLAN.md
 16: ```
 17: 
 18: ## What It Does
 19: 
 20: 1. **Parses plan document** â†’ Extracts phases with metadata
 21: 2. **Builds dependency graph** â†’ Analyzes explicit dependencies + file conflicts
 22: 3. **Assigns stages** â†’ Groups independent PRPs for parallel execution
 23: 4. **Shows plan** â†’ User confirms generation strategy
 24: 5. **Spawns subagents** â†’ Parallel generation per stage (Sonnet model)
 25: 6. **Monitors progress** â†’ Health checks via file timestamp polling (30s intervals)
 26: 7. **Aggregates results** â†’ Collects generated PRPs + Linear issues
 27: 8. **Outputs summary** â†’ Shows all generated PRPs grouped by stage
 28: 
 29: **Time Savings**: 8 PRPs sequential (30 min) â†’ parallel (10-12 min) = **60% faster**
 30: 
 31: ---
 32: 
 33: ## Plan Document Format
 34: 
 35: ### Structure
 36: 
 37: ```markdown
 38: # [Plan Title]
 39: 
 40: ## Overview
 41: [High-level description of what this plan achieves]
 42: 
 43: ## Success Criteria
 44: - [ ] Criterion 1
 45: - [ ] Criterion 2
 46: 
 47: ## Phases
 48: 
 49: ### Phase 1: [Name]
 50: 
 51: **Goal**: [One-sentence objective]
 52: 
 53: **Estimated Hours**: [X.X]
 54: 
 55: **Complexity**: [low|medium|high]
 56: 
 57: **Files Modified**:
 58: - path/to/file1.ext
 59: - path/to/file2.ext
 60: 
 61: **Dependencies**: [None | Phase 1, Phase 2]
 62: 
 63: **Implementation Steps**:
 64: 1. Step 1
 65: 2. Step 2
 66: 
 67: **Validation Gates**:
 68: - [ ] Validation 1
 69: - [ ] Validation 2
 70: 
 71: **Conflict Notes**: [Optional - explicit conflict warnings]
 72: 
 73: ---
 74: 
 75: ### Phase 2: [Name]
 76: [Same structure repeated]
 77: ```
 78: 
 79: ### Example: TOOL-PERMISSION-LOCKDOWN-PLAN.md
 80: 
 81: ```markdown
 82: # Tool & Permission Lockdown
 83: 
 84: ## Overview
 85: Implement comprehensive tool deny list and command permission system to reduce token usage and improve security.
 86: 
 87: ## Success Criteria
 88: - [ ] 55 MCP tools denied
 89: - [ ] Command permissions documented
 90: - [ ] Token usage reduced by 44k (96%)
 91: 
 92: ## Phases
 93: 
 94: ### Phase 1: Tool Deny List
 95: 
 96: **Goal**: Add 55 denied tools to settings.local.json
 97: 
 98: **Estimated Hours**: 0.42
 99: 
100: **Complexity**: low
101: 
102: **Files Modified**:
103: - .claude/settings.local.json
104: 
105: **Dependencies**: None
106: 
107: **Implementation Steps**:
108: 1. Read existing settings.local.json
109: 2. Add 55 tools to deny array (Filesystem: 8, Git: 5, GitHub: 26, Repomix: 4, Playwright: 6, Perplexity: 1, Syntropy: 5)
110: 3. Validate JSON syntax
111: 
112: **Validation Gates**:
113: - [ ] JSON validates
114: - [ ] 55 tools in deny list
115: - [ ] No duplicates
116: 
117: ---
118: 
119: ### Phase 2: Usage Guide
120: 
121: **Goal**: Create comprehensive tool usage guide
122: 
123: **Estimated Hours**: 0.50
124: 
125: **Complexity**: low
126: 
127: **Files Modified**:
128: - TOOL-USAGE-GUIDE.md
129: 
130: **Dependencies**: Phase 1
131: 
132: **Implementation Steps**:
133: 1. Create TOOL-USAGE-GUIDE.md
134: 2. Add decision tree flowchart
135: 3. Document all 55 denied tools with native alternatives
136: 4. Add common tasks with examples
137: 
138: **Validation Gates**:
139: - [ ] All 55 denied tools documented
140: - [ ] Decision tree included
141: - [ ] 9 common task examples
142: ```
143: 
144: ---
145: 
146: ## Workflow
147: 
148: ### Step 1: Parse Plan Document
149: 
150: ```python
151: # Read plan file
152: plan_content = Read(file_path=plan_path)
153: 
154: # Extract phases
155: phases = []
156: for section in parse_markdown(plan_content):
157:     if section.heading.startswith("### Phase"):
158:         phase = {
159:             "name": extract_after_colon(section.heading),
160:             "goal": extract_field(section, "Goal"),
161:             "estimated_hours": float(extract_field(section, "Estimated Hours")),
162:             "complexity": extract_field(section, "Complexity"),
163:             "files_modified": extract_list(section, "Files Modified"),
164:             "dependencies": extract_field(section, "Dependencies"),
165:             "implementation_steps": extract_list(section, "Implementation Steps"),
166:             "validation_gates": extract_list(section, "Validation Gates"),
167:             "conflict_notes": extract_field(section, "Conflict Notes", optional=True)
168:         }
169:         phases.append(phase)
170: ```
171: 
172: **Output**:
173: ```
174: Found 5 phases:
175:   Phase 1: Tool Deny List (0.42h, low)
176:   Phase 2: Usage Guide (0.50h, low)
177:   Phase 3: Worktree Docs (0.58h, low)
178:   Phase 4: Command Permissions (0.42h, low)
179:   Phase 5: Doc Updates (0.50h, low)
180: ```
181: 
182: ### Step 2: Build Dependency Graph
183: 
184: **Explicit Dependencies**:
185: ```python
186: dep_graph = {}
187: for i, phase in enumerate(phases):
188:     phase_num = i + 1
189:     phase_letter = chr(64 + phase_num)  # A, B, C, ...
190: 
191:     dep_graph[phase_num] = {
192:         "name": phase.name,
193:         "letter": phase_letter,
194:         "phase": phase,
195:         "dependencies": parse_dependency_string(phase.dependencies),
196:         "files": phase.files_modified
197:     }
198: ```
199: 
200: **Implicit Dependencies (File Conflicts)**:
201: ```python
202: file_map = defaultdict(list)
203: for phase_num, data in dep_graph.items():
204:     for file in data.files:
205:         file_map[file].append(phase_num)
206: 
207: # Detect conflicts
208: conflicts = {}
209: for file, phases in file_map.items():
210:     if len(phases) > 1:
211:         conflicts[file] = phases
212: ```
213: 
214: **Output**:
215: ```
216: Dependency graph:
217:   Phase 1 (A): Tool Deny List
218:     Dependencies: None
219:     Files: .claude/settings.local.json
220: 
221:   Phase 2 (B): Usage Guide
222:     Dependencies: Phase 1
223:     Files: TOOL-USAGE-GUIDE.md
224: 
225:   Phase 3 (C): Worktree Docs
226:     Dependencies: Phase 1
227:     Files: CLAUDE.md
228: 
229:   Phase 4 (D): Command Permissions
230:     Dependencies: Phase 2, Phase 3
231:     Files: .claude/settings.local.json
232: 
233:   Phase 5 (E): Doc Updates
234:     Dependencies: Phase 1
235:     Files: CLAUDE.md
236: 
237: âš  File conflicts detected:
238:   - .claude/settings.local.json: Phase 1, Phase 4
239:     â†’ Phase 4 will merge after Phase 1 (MEDIUM conflict)
240: 
241:   - CLAUDE.md: Phase 3, Phase 5
242:     â†’ Phase 5 will merge after Phase 3 (LOW conflict - different sections)
243: ```
244: 
245: ### Step 3: Assign Stages
246: 
247: **Algorithm**: Topological sort + conflict resolution
248: 
249: ```python
250: def detect_circular_dependency(dep_graph, assigned):
251:     """Detect circular dependency and return cycle path
252: 
253:     H3: Show actual cycle path in error message
254: 
255:     Returns: list of phase numbers forming cycle, or None if no cycle
256:     """
257:     def dfs(node, visited, stack):
258:         visited.add(node)
259:         stack.append(node)
260: 
261:         for dep in dep_graph[node].dependencies:
262:             if dep not in assigned:  # Only check unassigned deps
263:                 if dep not in visited:
264:                     cycle = dfs(dep, visited, stack)
265:                     if cycle:
266:                         return cycle
267:                 elif dep in stack:
268:                     # Found cycle
269:                     cycle_start = stack.index(dep)
270:                     return stack[cycle_start:] + [dep]
271: 
272:         stack.pop()
273:         return None
274: 
275:     # Check all unassigned nodes
276:     for node in dep_graph:
277:         if node not in assigned:
278:             cycle = dfs(node, set(), [])
279:             if cycle:
280:                 return cycle
281: 
282:     return None
283: 
284: def assign_stages(dep_graph, file_map):
285:     """Group phases into stages maximizing parallelism"""
286:     stages = []
287:     assigned = set()
288: 
289:     while len(assigned) < len(dep_graph):
290:         # Find phases with all dependencies satisfied
291:         ready = [
292:             phase_num for phase_num in dep_graph
293:             if phase_num not in assigned
294:             and all(dep in assigned for dep in dep_graph[phase_num].dependencies)
295:         ]
296: 
297:         if not ready:
298:             # H3: Detect and show circular dependency path
299:             cycle_path = detect_circular_dependency(dep_graph, assigned)
300:             if cycle_path:
301:                 cycle_str = " â†’ ".join([f"Phase {p}" for p in cycle_path])
302:                 raise CircularDependencyError(f"Circular dependency detected: {cycle_str}")
303:             else:
304:                 raise CircularDependencyError("Circular dependency detected (unable to determine path)")
305: 
306:         # Group by file conflicts
307:         parallel_groups = []
308:         for phase_num in ready:
309:             # Check if phase conflicts with any group
310:             placed = False
311:             for group in parallel_groups:
312:                 if not has_file_conflict(phase_num, group, file_map):
313:                     group.append(phase_num)
314:                     placed = True
315:                     break
316: 
317:             if not placed:
318:                 parallel_groups.append([phase_num])
319: 
320:         # Create stage
321:         stage_type = "parallel" if all(len(g) == 1 for g in parallel_groups) or len(ready) > 1 else "sequential"
322:         stages.append({
323:             "stage_num": len(stages) + 1,
324:             "type": stage_type,
325:             "phases": ready
326:         })
327: 
328:         assigned.update(ready)
329: 
330:     return stages
331: ```
332: 
333: **Output**:
334: ```
335: Stage assignment:
336:   Stage 1 (parallel): Phase 1 (A)
337:   Stage 2 (parallel): Phase 2 (B), Phase 3 (C), Phase 5 (E)
338:   Stage 3 (sequential): Phase 4 (D)
339: 
340: Estimated execution time:
341:   Sequential: 2.42 hours
342:   Parallel: 1.42 hours
343:   Savings: 41% (1.0 hours)
344: ```
345: 
346: ### Step 4: Calculate PRP IDs
347: 
348: **Format**: `PRP-X.Y.Z`
349: - X = Batch ID (next free PRP number)
350: - Y = Stage number
351: - Z = Order within stage
352: 
353: ```python
354: import re
355: from glob import glob
356: 
357: def extract_prp_number(filename):
358:     """Extract root batch number from PRP filename
359: 
360:     C2: Handles both sequential and batch PRP formats
361: 
362:     Examples:
363:         PRP-42-feature.md â†’ 42
364:         PRP-43.2.1-feature.md â†’ 43 (ignore stage/order)
365:         PRP-100.5.3-complex.md â†’ 100
366: 
367:     Returns: int (batch number) or 0 if no match
368:     """
369:     match = re.search(r'PRP-(\d+)', filename)
370:     if match:
371:         return int(match.group(1))
372:     return 0
373: 
374: # Find next batch ID
375: existing_prps = glob("PRPs/feature-requests/PRP-*.md")
376: if existing_prps:
377:     max_id = max([extract_prp_number(p) for p in existing_prps])
378:     batch_id = max_id + 1
379: else:
380:     batch_id = 1  # First batch
381: 
382: # Assign PRP IDs
383: prp_ids = {}
384: execution_order = 1
385: for stage in stages:
386:     for i, phase_num in enumerate(stage.phases):
387:         prp_id = f"{batch_id}.{stage.stage_num}.{i+1}"
388:         prp_ids[phase_num] = {
389:             "prp_id": prp_id,
390:             "stage": stage.stage_num,
391:             "execution_order": execution_order,
392:             "merge_order": execution_order
393:         }
394:         execution_order += 1
395: ```
396: 
397: **Output**:
398: ```
399: PRP ID Assignment (Batch 43):
400:   PRP-43.1.1: Phase 1 - Tool Deny List
401:   PRP-43.2.1: Phase 2 - Usage Guide
402:   PRP-43.2.2: Phase 3 - Worktree Docs
403:   PRP-43.2.3: Phase 5 - Doc Updates
404:   PRP-43.3.1: Phase 4 - Command Permissions
405: ```
406: 
407: ### Step 5: Show Plan to User
408: 
409: ```
410: ðŸ“‹ Batch PRP Generation Plan
411: ============================================================
412: 
413: Input: TOOL-PERMISSION-LOCKDOWN-PLAN.md
414: Phases detected: 5
415: 
416: Dependency graph:
417:   Phase 1 (A): Tool Deny List (no deps)
418:   Phase 2 (B): Usage Guide (depends: Phase 1)
419:   Phase 3 (C): Worktree Docs (depends: Phase 1)
420:   Phase 4 (D): Command Permissions (depends: Phase 2, Phase 3)
421:   Phase 5 (E): Doc Updates (depends: Phase 1)
422: 
423: Stage assignment:
424:   Stage 1 (parallel): PRP-43.1.1
425:   Stage 2 (parallel): PRP-43.2.1, PRP-43.2.2, PRP-43.2.3
426:   Stage 3 (sequential): PRP-43.3.1
427: 
428: âš  File conflicts detected:
429:   - .claude/settings.local.json: PRP-43.1.1, PRP-43.3.1 (MEDIUM)
430:     â†’ PRP-43.3.1 will merge after PRP-43.1.1
431: 
432:   - CLAUDE.md: PRP-43.2.2, PRP-43.2.3 (LOW)
433:     â†’ Different sections, conflicts unlikely
434: 
435: Estimated execution time:
436:   Sequential: 2.42h
437:   Parallel: 1.42h
438:   Savings: 41% (1.0h)
439: 
440: Generated PRPs will be created at:
441:   PRPs/feature-requests/PRP-43.1.1-tool-deny-list.md
442:   PRPs/feature-requests/PRP-43.2.1-usage-guide.md
443:   PRPs/feature-requests/PRP-43.2.2-worktree-docs.md
444:   PRPs/feature-requests/PRP-43.2.3-doc-updates.md
445:   PRPs/feature-requests/PRP-43.3.1-command-permissions.md
446: 
447: Proceed with generation? [y/N]:
448: ```
449: 
450: ### Step 6: Spawn Subagents (Stage by Stage)
451: 
452: **Create monitoring directory**:
453: ```bash
454: mkdir -p .tmp/batch-gen
455: ```
456: 
457: **For each stage**:
458: ```python
459: for stage in stages:
460:     print(f"\nðŸ”§ Stage {stage.stage_num} ({stage.type})")
461:     print("=" * 60)
462: 
463:     if stage.type == "parallel" and len(stage.phases) > 1:
464:         # Spawn parallel subagents
465:         agents = []
466:         for phase_num in stage.phases:
467:             prp_info = prp_ids[phase_num]
468:             phase_data = dep_graph[phase_num]
469: 
470:             # Build JSON input for subagent
471:             batch_input = {
472:                 "batch_mode": True,
473:                 "prp_id": prp_info.prp_id,
474:                 "feature_name": phase_data.name,
475:                 "goal": phase_data.phase.goal,
476:                 "estimated_hours": phase_data.phase.estimated_hours,
477:                 "complexity": phase_data.phase.complexity,
478:                 "files_modified": phase_data.phase.files_modified,
479:                 "dependencies": [prp_ids[dep].prp_id for dep in phase_data.dependencies],
480:                 "implementation_steps": phase_data.phase.implementation_steps,
481:                 "validation_gates": phase_data.phase.validation_gates,
482:                 "stage": f"stage-{stage.stage_num}-parallel",
483:                 "execution_order": prp_info.execution_order,
484:                 "merge_order": prp_info.merge_order,
485:                 "conflict_potential": calculate_conflict(phase_num, file_map),
486:                 "conflict_notes": phase_data.phase.conflict_notes or "",
487:                 "worktree_path": f"../ctx-eng-plus-prp-{prp_info.prp_id.replace('.', '-')}",
488:                 "branch_name": f"prp-{prp_info.prp_id.replace('.', '-')}-{slugify(phase_data.name)}",
489:                 "create_linear_issue": True,
490:                 "plan_context": f"Part of {plan_title} initiative"
491:             }
492: 
493:             # Spawn subagent
494:             agent = Task(
495:                 description=f"Generate PRP-{prp_info.prp_id}",
496:                 prompt=f"""
497: You are generating a PRP in batch mode for the /batch-gen-prp coordinator.
498: 
499: Use the /generate-prp command with this structured JSON input:
500: 
501: ```json
502: {json.dumps(batch_input, indent=2)}
503: ```
504: 
505: Follow the "Batch Mode Workflow" section of /generate-prp:
506: 1. Parse JSON input
507: 2. Write heartbeat to .tmp/batch-gen/PRP-{prp_info.prp_id}.status
508: 3. Generate PRP file with all metadata
509: 4. Create Linear issue
510: 5. Return JSON report
511: 
512: **IMPORTANT**:
513: - Write heartbeat every 10-15 seconds
514: - Return JSON report at end (coordinator needs it)
515: - On error, still return JSON with status: FAILED
516: """,
517:                 subagent_type="general-purpose",
518:                 model="sonnet"  # User specified: use Sonnet, not haiku
519:             )
520:             agents.append((phase_num, agent))
521: 
522:             print(f"  Spawned agent for PRP-{prp_info.prp_id}: {phase_data.name}")
523: 
524:         # Monitor agents
525:         results = monitor_parallel_agents(agents, prp_ids, timeout=300)  # 5 min timeout
526:     else:
527:         # Sequential execution
528:         results = []
529:         for phase_num in stage.phases:
530:             result = generate_prp_sequential(phase_num, dep_graph, prp_ids)
531:             results.append(result)
532: 
533:     # Show stage results
534:     show_stage_results(stage, results)
535: ```
536: 
537: ### Step 7: Monitor Agents (Health Check Protocol)
538: 
539: **Monitoring function** (runs every 30 seconds):
540: ```python
541: def monitor_parallel_agents(agents, prp_ids, timeout=300):
542:     """Monitor subagents via file timestamp polling"""
543:     start_time = time.now()
544:     results = {}
545:     agent_status = {}
546:     failed_polls = defaultdict(int)  # Track consecutive failures
547: 
548:     # Initialize monitoring
549:     for phase_num, agent in agents:
550:         prp_id = prp_ids[phase_num].prp_id
551:         agent_status[prp_id] = {
552:             "agent": agent,
553:             "phase_num": phase_num,
554:             "status": "STARTING",
555:             "progress": 0,
556:             "last_heartbeat": start_time,
557:             "stalled_warnings": 0
558:         }
559: 
560:     # Poll every 30 seconds
561:     while len(results) < len(agents):
562:         time.sleep(30)
563: 
564:         # Clear screen and show header
565:         print("\n" + "=" * 60)
566:         print(f"ðŸ“Š Monitoring {len(agents)} Agents")
567:         print("=" * 60 + "\n")
568: 
569:         for prp_id, status_data in agent_status.items():
570:             if prp_id in results:
571:                 continue  # Already completed
572: 
573:             # Check heartbeat file
574:             heartbeat_file = f".tmp/batch-gen/PRP-{prp_id}.status"
575:             prp_file_pattern = f"PRPs/feature-requests/PRP-{prp_id}-*.md"
576: 
577:             # Check if PRP file exists (completion signal)
578:             prp_files = glob(prp_file_pattern)
579:             if prp_files:
580:                 # Agent completed successfully
581:                 results[prp_id] = {
582:                     "status": "SUCCESS",
583:                     "file_path": prp_files[0]
584:                 }
585:                 print(f"PRP-{prp_id}: {dep_graph[status_data.phase_num].name}")
586:                 print(f"  Status: [COMPLETED] âœ“ DONE")
587:                 continue
588: 
589:             # Check heartbeat file
590:             if os.path.exists(heartbeat_file):
591:                 # H5: Handle corrupted heartbeat files
592:                 try:
593:                     with open(heartbeat_file, 'r') as f:
594:                         heartbeat = json.load(f)
595:                     age = time.now() - heartbeat.timestamp
596:                 except (json.JSONDecodeError, IOError, KeyError) as e:
597:                     # Treat corrupted heartbeat as missing
598:                     failed_polls[prp_id] += 1
599:                     age = time.now() - start_time
600:                     print(f"PRP-{prp_id}: {dep_graph[status_data.phase_num].name}")
601:                     print(f"  Status: [CORRUPTED_HEARTBEAT] âš  WARNING (poll {failed_polls[prp_id]}/2)")
602:                     print(f"  Error: {str(e)}")
603:                     print()
604:                     continue
605: 
606:                 age = time.now() - heartbeat.timestamp
607: 
608:                 status_data.status = heartbeat.status
609:                 status_data.progress = heartbeat.progress
610:                 status_data.last_heartbeat = heartbeat.timestamp
611:                 failed_polls[prp_id] = 0  # Reset failure counter
612: 
613:                 # Determine health
614:                 if heartbeat.status == "FAILED":
615:                     results[prp_id] = {
616:                         "status": "FAILED",
617:                         "error": heartbeat.get("error", "Unknown error")
618:                     }
619:                     health = "âŒ FAILED"
620:                 elif age < 120:  # 2 minutes
621:                     health = "âœ“ HEALTHY"
622:                 elif age < 300:  # 5 minutes
623:                     health = "âš  WARNING"
624:                     status_data.stalled_warnings += 1
625:                 else:
626:                     health = "âŒ STALLED"
627: 
628:                 # Display
629:                 print(f"PRP-{prp_id}: {dep_graph[status_data.phase_num].name}")
630:                 print(f"  Status: [{heartbeat.status}{'.' * (15 - len(heartbeat.status))}] {health} ({int(age)}s ago)")
631:                 if "current_step" in heartbeat:
632:                     print(f"  Step: {heartbeat.current_step}")
633:                 print(f"  Progress: {heartbeat.progress}%")
634:             else:
635:                 # No heartbeat file yet
636:                 failed_polls[prp_id] += 1
637:                 age = time.now() - start_time
638: 
639:                 if failed_polls[prp_id] >= 2:
640:                     # Kill agent after 2 consecutive failed polls (user requirement)
641:                     print(f"PRP-{prp_id}: {dep_graph[status_data.phase_num].name}")
642:                     print(f"  Status: [NO_HEARTBEAT] âŒ KILLED (2 failed polls)")
643: 
644:                     # Kill agent
645:                     # Note: Task API doesn't support kill, so mark as failed
646:                     results[prp_id] = {
647:                         "status": "FAILED",
648:                         "error": "Agent killed: No heartbeat after 2 polls (60s)"
649:                     }
650:                 elif age < 60:  # 1 minute grace period
651:                     print(f"PRP-{prp_id}: {dep_graph[status_data.phase_num].name}")
652:                     print(f"  Status: [STARTING....] â³ INITIALIZING ({int(age)}s)")
653:                 else:
654:                     print(f"PRP-{prp_id}: {dep_graph[status_data.phase_num].name}")
655:                     print(f"  Status: [NO_HEARTBEAT] âš  WARNING ({int(age)}s, poll {failed_polls[prp_id]}/2)")
656: 
657:             print()  # Blank line between PRPs
658: 
659:         # Check timeout
660:         if time.now() - start_time > timeout:
661:             print("\nâš  Timeout reached. Killing stalled agents...")
662:             for prp_id, status_data in agent_status.items():
663:                 if prp_id not in results:
664:                     results[prp_id] = {
665:                         "status": "FAILED",
666:                         "error": f"Timeout after {timeout}s"
667:                     }
668:             break
669: 
670:     # M3: Cleanup heartbeat files
671:     for prp_id in agent_status.keys():
672:         heartbeat_file = f".tmp/batch-gen/PRP-{prp_id}.status"
673:         if os.path.exists(heartbeat_file):
674:             try:
675:                 os.remove(heartbeat_file)
676:             except OSError:
677:                 pass  # Ignore cleanup errors
678: 
679:     return results
680: ```
681: 
682: **Display Example** (during monitoring):
683: ```
684: ============================================================
685: ðŸ“Š Monitoring 3 Agents
686: ============================================================
687: 
688: PRP-43.2.1: Usage Guide
689:   Status: [WRITING........] âœ“ HEALTHY (15s ago)
690:   Step: Generating Implementation Steps section
691:   Progress: 65%
692: 
693: PRP-43.2.2: Worktree Docs
694:   Status: [COMPLETED] âœ“ DONE
695: 
696: PRP-43.2.3: Doc Updates
697:   Status: [RESEARCHING....] âš  WARNING (3m 20s ago)
698:   Progress: 30%
699: 
700: Health Summary: 1 HEALTHY, 1 WARNING, 0 STALLED
701: Next poll in 30s...
702: ```
703: 
704: ### Step 8: Aggregate Results
705: 
706: ```python
707: # Collect all results across stages
708: all_results = {
709:     "batch_id": batch_id,
710:     "plan_file": plan_path,
711:     "total_prps": len(phases),
712:     "successful": 0,
713:     "failed": 0,
714:     "stages": []
715: }
716: 
717: for stage in stages:
718:     stage_results = {
719:         "stage_num": stage.stage_num,
720:         "type": stage.type,
721:         "prps": []
722:     }
723: 
724:     for phase_num in stage.phases:
725:         prp_id = prp_ids[phase_num].prp_id
726:         result = results.get(prp_id, {"status": "UNKNOWN"})
727: 
728:         stage_results.prps.append({
729:             "prp_id": prp_id,
730:             "phase_name": dep_graph[phase_num].name,
731:             "status": result.status,
732:             "file_path": result.get("file_path"),
733:             "linear_issue": result.get("linear_issue"),
734:             "linear_url": result.get("linear_url"),
735:             "error": result.get("error")
736:         })
737: 
738:         if result.status == "SUCCESS":
739:             all_results.successful += 1
740:         else:
741:             all_results.failed += 1
742: 
743:     all_results.stages.append(stage_results)
744: ```
745: 
746: ### Step 9: Output Summary
747: 
748: ```
749: âœ… Batch PRP Generation Complete
750: ============================================================
751: 
752: Batch ID: 43
753: Plan: TOOL-PERMISSION-LOCKDOWN-PLAN.md
754: Generated: 5/5 PRPs (100% success rate)
755: 
756: Stage 1 (parallel):
757:   âœ“ PRP-43.1.1: Tool Deny List
758:     â†’ PRPs/feature-requests/PRP-43.1.1-tool-deny-list.md
759:     â†’ Linear: CTX-45 (https://linear.app/...)
760: 
761: Stage 2 (parallel - 3 agents):
762:   âœ“ PRP-43.2.1: Usage Guide
763:     â†’ PRPs/feature-requests/PRP-43.2.1-usage-guide.md
764:     â†’ Linear: CTX-46 (https://linear.app/...)
765: 
766:   âœ“ PRP-43.2.2: Worktree Docs
767:     â†’ PRPs/feature-requests/PRP-43.2.2-worktree-docs.md
768:     â†’ Linear: CTX-47 (https://linear.app/...)
769: 
770:   âœ“ PRP-43.2.3: Doc Updates
771:     â†’ PRPs/feature-requests/PRP-43.2.3-doc-updates.md
772:     â†’ Linear: CTX-48 (https://linear.app/...)
773: 
774: Stage 3 (sequential):
775:   âœ“ PRP-43.3.1: Command Permissions
776:     â†’ PRPs/feature-requests/PRP-43.3.1-command-permissions.md
777:     â†’ Linear: CTX-49 (https://linear.app/...)
778: 
779: Execution time: 12m 34s
780: Time saved: 41% vs sequential (17m 45s)
781: 
782: Next steps:
783:   1. Review generated PRPs in PRPs/feature-requests/
784:   2. Execute with:
785:      /batch-exe-prp --batch 43
786:      or stage-by-stage:
787:      /batch-exe-prp --batch 43 --stage 1
788:      /batch-exe-prp --batch 43 --stage 2
789:      /batch-exe-prp --batch 43 --stage 3
790: ```
791: 
792: **If failures occurred**:
793: ```
794: âš  Partial Success: 4/5 PRPs generated
795: ============================================================
796: 
797: [... successful PRPs listed ...]
798: 
799: Failed:
800:   âŒ PRP-43.2.3: Doc Updates
801:     Error: Agent killed: No heartbeat after 2 polls (60s)
802: 
803:     Retry with:
804:     /generate-prp --prp-id 43.2.3 --retry
805: 
806: Or regenerate entire stage:
807:     /batch-gen-prp TOOL-PERMISSION-LOCKDOWN-PLAN.md --stage 2 --retry-failed
808: ```
809: 
810: ---
811: 
812: ## Integration with `/batch-exe-prp`
813: 
814: **Full workflow**:
815: ```bash
816: # Step 1: Plan decomposition + generation
817: /batch-gen-prp BIG-FEATURE-PLAN.md
818: # Output: 8 PRPs in PRPs/feature-requests/PRP-43.*.md
819: 
820: # Step 2: Execute by batch (all stages)
821: /batch-exe-prp --batch 43
822: # Executes Stage 1 â†’ Stage 2 (parallel) â†’ Stage 3
823: 
824: # OR: Execute stage-by-stage
825: /batch-exe-prp --batch 43 --stage 1
826: /batch-exe-prp --batch 43 --stage 2
827: /batch-exe-prp --batch 43 --stage 3
828: ```
829: 
830: **H6: Batch Filtering Integration**
831: 
832: âš ï¸ **NOTE**: The `--batch` flag syntax shown above assumes `/batch-exe-prp` supports batch ID filtering. This feature needs verification:
833: 
834: 1. **Check if `/batch-exe-prp` supports `--batch` flag**
835: 2. **If not**: Update `/batch-exe-prp` to add batch filtering by parsing PRP-X.Y.Z format
836: 3. **Or**: Update this doc to show manual PRP selection:
837:    ```bash
838:    # Manual approach (if --batch not supported)
839:    /execute-prp PRPs/feature-requests/PRP-43.1.1-*.md
840:    /execute-prp PRPs/feature-requests/PRP-43.2.1-*.md
841:    /execute-prp PRPs/feature-requests/PRP-43.2.2-*.md
842:    # ... etc
843:    ```
844: 
845: **Recommended**: Add `--batch` support to `/batch-exe-prp` for seamless workflow
846: 
847: **Batch metadata**: PRPs contain all necessary metadata for execution
848: - `stage`: Which stage the PRP belongs to
849: - `execution_order`: Order within batch
850: - `merge_order`: Global merge sequence
851: - `dependencies`: Other PRPs that must complete first
852: - `conflict_potential`: Merge conflict warning
853: 
854: ---
855: 
856: ## Error Handling
857: 
858: ### 1. Circular Dependencies
859: 
860: **Detection**: Topological sort fails
861: **Error**:
862: ```
863: âŒ Circular dependency detected:
864:   Phase 2 â†’ Phase 3 â†’ Phase 4 â†’ Phase 2
865: 
866: Please revise plan to break the cycle.
867: ```
868: 
869: ### 2. Agent Failures
870: 
871: **Behavior**: Continue with other agents (user requirement: "Continue with other 2? Yes")
872: 
873: **Example**:
874: ```
875: Stage 2 (parallel): 3 agents
876:   âœ“ PRP-43.2.1: SUCCESS
877:   âŒ PRP-43.2.2: FAILED (timeout)
878:   âœ“ PRP-43.2.3: SUCCESS
879: 
880: Result: 2/3 PRPs generated, proceed to Stage 3
881: Failed PRPs can be retried later
882: ```
883: 
884: ### 3. No Heartbeat (2 Failed Polls)
885: 
886: **User requirement**: "second polling status failed = kill"
887: 
888: **Behavior**:
889: - Poll 1 (30s): No heartbeat â†’ âš  WARNING
890: - Poll 2 (60s): Still no heartbeat â†’ âŒ KILL
891: - Mark as FAILED, continue with other agents
892: 
893: **H4: 60-Second Kill Timeout**
894: 
895: âš ï¸ **WARNING**: Agents are killed if no heartbeat for 60 seconds (2 polls Ã— 30s).
896: 
897: This is **intentionally aggressive** for PRP generation (typical time: 30-90s). Reasons:
898: - Prevents hung agents from blocking stage completion
899: - Faster feedback for failures
900: - PRP generation should be quick (< 60s for most)
901: 
902: **If your PRPs require >60s**:
903: - Research-heavy PRPs may timeout if Serena queries are slow
904: - Complex PRPs with deep codebase analysis may need more time
905: 
906: **Solutions**:
907: 1. **Adjust phase granularity**: Break large phases into smaller ones
908: 2. **Skip research**: Set research to optional in batch input
909: 3. **Increase timeout**: Modify monitoring code (not recommended)
910: 
911: **Comparison with /batch-exe-prp**: Execution timeout is 10 minutes (much longer because code execution takes longer than generation)
912: 
913: ### 4. Invalid Plan Format
914: 
915: **Validation errors**:
916: ```
917: âŒ Plan validation failed:
918:   - Phase 3: Missing "Estimated Hours" field
919:   - Phase 5: "Dependencies" field references non-existent "Phase 9"
920: 
921: Please fix plan format and retry.
922: ```
923: 
924: ---
925: 
926: ## Symmetry with `/batch-exe-prp`
927: 
928: ### Shared Patterns
929: 
930: | Aspect | `/batch-gen-prp` | `/batch-exe-prp` |
931: |--------|------------------|------------------|
932: | **Coordinator Role** | Parse plan â†’ spawn agents | Parse PRPs â†’ spawn agents |
933: | **Subagent Type** | `general-purpose` | `general-purpose` |
934: | **Model** | Sonnet (generation) | Auto (execution) |
935: | **Parallelism** | Stage-based | Stage-based |
936: | **Polling Interval** | 30 seconds | 30 seconds |
937: | **Health Signals** | File timestamps | Git commit timestamps |
938: | **Status Levels** | HEALTHY/WARNING/STALLED | HEALTHY/WARNING/STALLED |
939: | **Kill Policy** | 2 failed polls | 10 minutes no commits |
940: | **Error Handling** | Continue on partial failure | Continue on partial failure |
941: | **Output Format** | JSON aggregation | JSON aggregation |
942: 
943: ### Key Differences
944: 
945: | Aspect | `/batch-gen-prp` | `/batch-exe-prp` |
946: |--------|------------------|------------------|
947: | **Input** | Plan markdown file | Generated PRP files |
948: | **Output** | PRP files + Linear issues | Code changes in worktrees |
949: | **Work Location** | Single directory | Git worktrees |
950: | **Health Signal** | `.tmp/batch-gen/*.status` | Git log timestamps |
951: | **Cleanup** | Remove .tmp/ files | Remove worktrees |
952: 
953: ---
954: 
955: ## Example Plan Document
956: 
957: See `examples/TOOL-PERMISSION-LOCKDOWN-PLAN.md` for complete example.
958: 
959: ---
960: 
961: ## Tips
962: 
963: 1. **Clear goals**: Each phase should have one specific, measurable objective
964: 2. **Explicit dependencies**: Always declare "Dependencies: Phase N" if needed
965: 3. **File accuracy**: List all files that will be modified (used for conflict detection)
966: 4. **Validation gates**: Make them copy-pasteable bash commands
967: 5. **Reasonable hours**: 0.25-2h per phase (larger tasks = multiple phases)
968: 6. **Review before execution**: Generated PRPs may need minor adjustments
969: 
970: ---
971: 
972: ## Next Steps
973: 
974: After batch generation:
975: 1. Review all generated PRPs in `PRPs/feature-requests/PRP-{batch-id}.*`
976: 2. Adjust any PRP details if needed
977: 3. Execute entire batch: `/batch-exe-prp --batch {batch-id}`
978: 4. Or execute stage-by-stage for more control
</file>

<file path=".claude/commands/execute-prp.md">
  1: # /execute-prp - Execute PRP with Self-Healing
  2: 
  3: Automates PRP execution with phase-by-phase implementation, L1-L4 validation loops, self-healing error recovery, and automatic escalation triggers.
  4: 
  5: **Note**: This command defines the PRP execution protocol. `/batch-exe-prp` launches parallel agents that follow this protocol independently in separate git worktrees.
  6: 
  7: ## Usage
  8: 
  9: ```
 10: /execute-prp <prp-file-or-id>
 11: ```
 12: 
 13: **Examples:**
 14: ```
 15: /execute-prp PRP-5
 16: /execute-prp PRPs/feature-requests/PRP-5-context-sync-integration.md
 17: ```
 18: 
 19: ## What It Does
 20: 
 21: 1. **Parses PRP Blueprint**:
 22:    - Extracts phases from `## ðŸ”§ Implementation Blueprint`
 23:    - Validates phase structure (number, name, hours, goal, approach)
 24:    - Identifies files to create/modify and functions to implement
 25:    - Extracts validation commands and checkpoint instructions
 26: 
 27: 2. **Creates Git Branch** (in worktree if specified):
 28:    - If executing in git worktree: Uses existing branch from worktree setup
 29:    - If executing in main repo: Creates branch `prp-{prp_id}-{sanitized-name}`
 30:    - Format example: `prp-6-user-authentication`
 31:    - Enables parallel PRP execution (see CLAUDE.md "Git Worktree" section)
 32:    - **Pattern**: Branch created/verified BEFORE any file modifications
 33: 
 34: 3. **Initializes PRP Context**:
 35:    - Creates active PRP session in `.ce/active_prp_session`
 36:    - Initializes state tracking for phases
 37:    - Sets up checkpoint namespace: `checkpoint-{prp_id}-{phase}`
 38:    - Stores branch name and worktree path if applicable
 39: 
 40: 4. **Executes Each Phase**:
 41:    - Creates/modifies files using Serena MCP
 42:    - Implements functions from blueprint
 43:    - Logs progress to console
 44:    - Updates phase state
 45: 
 46: 4. **Runs Validation Loop** (L1-L4 with self-healing):
 47:    - **L1 (Syntax & Style)**: `validate_level_1()` - Linting, formatting, type checks
 48:    - **L2 (Unit Tests)**: Runs phase validation command (e.g., `pytest tests/test_auth.py`)
 49:    - **L3 (Integration)**: `validate_level_3()` - Integration tests
 50:    - **L4 (Pattern Conformance)**: `validate_level_4(prp_path)` - Drift detection (<30%)
 51: 
 52:    **Self-Healing** (L1-L2 only, max 3 attempts):
 53:    - Parses error output (type, file, line, message)
 54:    - Checks escalation triggers
 55:    - Applies automatic fixes (e.g., add missing imports)
 56:    - Re-runs validation
 57:    - Escalates to human if persistent/ambiguous
 58: 
 59:    **Error Escalation Triggers**:
 60:    - **Persistent**: Same error after 3 attempts
 61:    - **Ambiguous**: Generic error with no file/line info
 62:    - **Architectural**: Keywords: refactor, redesign, circular import
 63:    - **External**: Network errors, package issues
 64:    - **Security**: CVE, credentials, permissions
 65: 
 66: 5. **Creates Checkpoints**:
 67:    - Git commit after each phase: `git commit -m "Phase N: {goal}"`
 68:    - Git tag after validation gate: `checkpoint-{prp_id}-phase{N}-{timestamp}`
 69:    - Preserves rollback points for easy recovery
 70: 
 71: 6. **Post-Execution Sync** (if auto-sync enabled via PRP-5):
 72:    - Runs cleanup protocol (archives memories, deletes ephemeral state)
 73:    - Syncs context (reindexes new code)
 74:    - Creates final checkpoint
 75:    - Verifies drift < 10%
 76: 
 77: 7. **Calculates Confidence Score**:
 78:    - **10/10**: All L1-L4 passed on first attempt
 79:    - **9/10**: All passed, 1-2 self-heals
 80:    - **8/10**: All passed, 3+ self-heals
 81:    - **7/10**: L1-L3 passed, L4 skipped
 82:    - **5/10**: Validation failures
 83: 
 84: 8. **Ends PRP Context**:
 85:    - Removes active session
 86:    - Returns execution summary
 87: 
 88: ## Execution Protocol Specification
 89: 
 90: **This section defines the PRP execution protocol that `/batch-exe-prp` agents follow independently.**
 91: 
 92: ### Protocol Steps (Sequential per Phase)
 93: 
 94: ```
 95: For each phase in PRP blueprint:
 96:   1. Parse phase metadata (number, name, goal, hours)
 97:   2. Execute implementation steps:
 98:      - Read files (if modifying existing)
 99:      - Apply changes (Edit/Write tools)
100:      - Create new files if needed
101:   3. Run validation loop:
102:      a. L1 (Syntax & Style):
103:         - Run linting (ruff/pylint/eslint)
104:         - Run formatting (black/prettier)
105:         - Run type checking (mypy/tsc)
106:         - Self-heal if failed (max 3 attempts)
107:      b. L2 (Unit Tests):
108:         - Run phase validation command
109:         - Parse test failures
110:         - Self-heal if failed (max 3 attempts)
111:      c. L3 (Integration Tests):
112:         - Run integration test suite
113:         - NO self-healing (escalate on failure)
114:      d. L4 (Pattern Conformance):
115:         - Calculate drift score
116:         - ABORT if drift >30%
117:         - NO self-healing (escalate on failure)
118:   4. Create checkpoint:
119:      - Git commit: "Phase {N}: {goal}"
120:      - Git tag: "checkpoint-{prp_id}-phase{N}-{timestamp}"
121:   5. Output progress signal:
122:      - "STATUS:PHASE_COMPLETE:{N}/{total}"
123: 
124: Return JSON report:
125: {
126:   "prp_id": "PRP-X",
127:   "status": "SUCCESS|FAILED|PARTIAL",
128:   "phases_completed": N,
129:   "phases_total": M,
130:   "confidence_score": 1-10,
131:   "validation_results": {...},
132:   "self_heals": N,
133:   "commit_hash": "abc123",
134:   "execution_time": "Xm Ys",
135:   "files_modified": ["file1", "file2"],
136:   "errors": [...]
137: }
138: ```
139: 
140: ### Self-Healing Rules
141: 
142: **L1-L2 ONLY** (max 3 attempts per error):
143: 
144: **Auto-fixable**:
145: - Import errors â†’ Add missing import
146: - Formatting errors â†’ Apply formatter
147: - Simple type errors â†’ Add type hints
148: 
149: **DO NOT auto-fix**:
150: - Persistent errors (same error 3x) â†’ Escalate
151: - Ambiguous errors (no file/line) â†’ Escalate
152: - Architectural errors (circular import, refactor needed) â†’ Escalate
153: - External errors (network, package not found) â†’ Escalate
154: - Security errors (CVE, credentials) â†’ Escalate immediately
155: 
156: ### Health Check Protocol (for batch agents)
157: 
158: **Output every 5 minutes**:
159: ```
160: HEALTH:OK                          # All systems normal
161: HEALTH:ERROR:timeout               # Validation timeout
162: HEALTH:ERROR:import_error:file.py  # Specific error detected
163: ```
164: 
165: **Completion signals**:
166: ```
167: STATUS:COMPLETE:10/10              # Success (confidence score)
168: STATUS:FAILED:L3_timeout           # Failure reason
169: STATUS:PARTIAL:2/3                 # Partial (N of M phases done)
170: ```
171: 
172: ## CLI Command
173: 
174: ```bash
175: # Basic usage
176: cd tools
177: uv run ce execute <prp-id-or-path>
178: 
179: # Execute specific phase range
180: uv run ce execute PRP-5 --start-phase 2 --end-phase 3
181: 
182: # Dry run (parse blueprint without execution)
183: uv run ce execute PRP-5 --dry-run
184: 
185: # Skip validation (dangerous - for debugging only)
186: uv run ce execute PRP-5 --skip-validation
187: 
188: # JSON output (for scripting)
189: uv run ce execute PRP-5 --json
190: ```
191: 
192: ## Example Workflow
193: 
194: ```bash
195: # 1. Generate PRP from INITIAL.md
196: /generate-prp feature-requests/auth/INITIAL.md
197: # Output: PRPs/feature-requests/PRP-6-user-authentication-system.md
198: 
199: # 2. Review generated PRP
200: # - Check implementation blueprint phases
201: # - Verify validation commands
202: # - Adjust if needed
203: 
204: # 3. Execute PRP with auto-sync
205: cd tools
206: uv run ce context auto-sync --enable  # Enable auto-sync (PRP-5)
207: /execute-prp PRP-6
208: 
209: # Expected output:
210: # ================================================================================
211: # Phase 1: Core Logic Implementation
212: # Goal: Implement main authentication flow
213: # ================================================================================
214: #
215: #   ðŸ“ Create: src/auth.py - Authentication logic
216: #   ðŸ”§ Implement: authenticate_user
217: #   ðŸ”§ Implement: validate_token
218: #
219: #   ðŸ§ª Running validation...
220: #     L1: Syntax & Style...
221: #     âœ… L1 passed (0.45s)
222: #     L2: Running pytest tests/test_auth.py -v...
223: #     âœ… L2 passed (1.23s)
224: #     L3: Integration Tests...
225: #     âœ… L3 passed (2.15s)
226: #     L4: Pattern Conformance...
227: #     âœ… L4 passed (drift: 5.2%)
228: #   âœ… Validation complete
229: #
230: # âœ… Phase 1 complete
231: #
232: # ... (phases 2-3)
233: #
234: # ================================================================================
235: # Running post-execution sync...
236: # ================================================================================
237: #
238: # âœ… Post-sync complete: drift=3.1%
239: #    Cleanup: True
240: #    Memories archived: 2
241: #    Final checkpoint: checkpoint-PRP-6-final-20251013-120000
242: #
243: # âœ… Execution complete: 10/10 confidence (45m 23s)
244: ```
245: 
246: ## Self-Healing Capabilities
247: 
248: ### Auto-Fixable Errors (L1-L2)
249: 
250: **Import Errors**:
251: ```python
252: # Error: ImportError: No module named 'jwt'
253: # Fix: Adds "import jwt" to file at appropriate location
254: ```
255: 
256: ### Escalation Triggers (Human Intervention Required)
257: 
258: The system escalates to human when:
259: 
260: 1. **Persistent Error** (same error after 3 attempts):
261:    ```
262:    âŒ L2 failed after 3 attempts - escalating
263:    ðŸ”§ Troubleshooting:
264:       1. Review error details - same error occurred 3 times
265:       2. Check if fix logic matches error type
266:       3. Consider if architectural change needed
267:    ```
268: 
269: 2. **Ambiguous Error** (generic error with no file/line info):
270:    ```
271:    Error: "something went wrong"
272:    ðŸ”§ Troubleshooting:
273:       1. Run validation command manually for full context
274:       2. Check logs for additional error details
275:    ```
276: 
277: 3. **Architectural Changes** (keywords: refactor, redesign, circular import):
278:    ```
279:    Error: "circular import detected between auth.py and models.py"
280:    ðŸ”§ Troubleshooting:
281:       1. Review error for architectural keywords
282:       2. Consider if code structure needs reorganization
283:    ```
284: 
285: 4. **External Dependencies** (network errors, package issues):
286:    ```
287:    Error: "connection refused" or "package not found"
288:    ðŸ”§ Troubleshooting:
289:       1. Check network connectivity
290:       2. Verify package repository access (PyPI, npm)
291:    ```
292: 
293: 5. **Security Concerns** (CVE, credentials, permissions):
294:    ```
295:    Error: "vulnerability detected" or "secret exposed"
296:    ðŸ”§ Troubleshooting:
297:       1. DO NOT auto-fix security-related errors
298:       2. Review error for exposed secrets/credentials
299:    ```
300: 
301: ## Options
302: 
303: | Flag | Description | Example |
304: |------|-------------|---------|
305: | `--start-phase N` | Start execution from phase N | `--start-phase 2` |
306: | `--end-phase N` | Stop execution at phase N | `--end-phase 3` |
307: | `--skip-validation` | Skip validation loops (debugging only) | `--skip-validation` |
308: | `--dry-run` | Parse blueprint without execution | `--dry-run` |
309: | `--json` | Output results as JSON | `--json` |
310: 
311: ## Validation Gates
312: 
313: Each phase runs through 4 validation levels:
314: 
315: ### L1: Syntax & Style (Auto-healing: Yes)
316: - Linting (ruff, pylint, eslint)
317: - Formatting (black, prettier)
318: - Type checking (mypy, tsc)
319: - **Max 3 self-healing attempts**
320: 
321: ### L2: Unit Tests (Auto-healing: Yes)
322: - Runs phase validation command
323: - Parses test failures
324: - Attempts automatic fixes (import errors, simple logic)
325: - **Max 3 self-healing attempts**
326: 
327: ### L3: Integration Tests (Auto-healing: No)
328: - End-to-end functionality tests
329: - Manual intervention on failure
330: - Escalates architectural issues
331: 
332: ### L4: Pattern Conformance (Auto-healing: No)
333: - Compares implementation to INITIAL.md EXAMPLES
334: - Calculates drift score (0-100%)
335: - **Aborts if drift > 30%** (requires explicit user acceptance)
336: - User decision: accept/reject/update EXAMPLES
337: 
338: ## PRP State Isolation (via PRP-2)
339: 
340: Each execution maintains isolated state:
341: 
342: - **Checkpoints**: `checkpoint-{prp_id}-phase{N}-{timestamp}`
343: - **Memories**: `{prp_id}-checkpoint-*`, `{prp_id}-learnings-*`
344: - **Active Session**: `.ce/active_prp_session` (JSON with prp_id, phase, validation_attempts)
345: - **Cleanup**: Automatic ephemeral state removal after execution (if auto-sync enabled)
346: 
347: ## Context Sync Integration (via PRP-5)
348: 
349: With auto-sync enabled (`ce context auto-sync --enable`):
350: 
351: **Before execution**: N/A (pre-sync happens in generation phase)
352: 
353: **After execution** (Step 6.5):
354: 1. Runs cleanup protocol (PRP-2)
355: 2. Syncs context (reindexes new code)
356: 3. Verifies drift < 10%
357: 4. Creates final checkpoint
358: 5. Removes active PRP session
359: 
360: ## Claude Code Hooks
361: 
362: **Integrated Context Monitoring** (configured in `.claude/settings.local.json`):
363: 
364: **Active Hooks**:
365: 
366: - **SessionStart**: Context health check - Warns about drift on session start (>10%)
367: 
368: **Additional hooks available** (add to settings.local.json as needed):
369: 
370: - **UserPromptSubmit**: Auto-sync reminder (checks if auto-sync disabled)
371: - **PostToolUse**: Drift spike detector (alerts after Edit/Write if drift >40%)
372: 
373: **Current hook configuration**:
374: 
375: ```json
376: {
377:   "hooks": {
378:     "SessionStart": [
379:       {
380:         "matcher": "*",
381:         "hooks": [
382:           {
383:             "type": "command",
384:             "command": "cd tools && uv run ce context health --json | jq -r 'if .drift_score > 30 then \"âš ï¸ HIGH DRIFT: \" + (.drift_score | tostring) + \"% - Run: ce context sync\" elif .drift_score > 10 then \"âš ï¸ Moderate drift: \" + (.drift_score | tostring) + \"%\" else \"âœ… Context healthy: \" + (.drift_score | tostring) + \"%\" end'",
385:             "timeout": 5
386:           }
387:         ]
388:       }
389:     ]
390:   }
391: }
392: ```
393: 
394: **Use cases**:
395: 
396: - Daily development: SessionStart health check (drift warning)
397: - Long sessions: PostToolUse drift spike detector (alerts >40% drift)
398: - Exploration: Complements auto-sync for non-PRP work
399: 
400: **Note**: Hooks are optional. Auto-sync mode handles PRP workflow automatically.
401: 
402: **More info**: See official docs at <https://docs.claude.com/en/docs/claude-code/hooks>
403: 
404: ## Common Issues
405: 
406: ### Issue: "PRP file not found: PRP-5"
407: 
408: ```bash
409: # Solution: Use full path or ensure PRP is in PRPs/feature-requests/
410: /execute-prp PRPs/feature-requests/PRP-5-context-sync-integration.md
411: ```
412: 
413: ### Issue: Validation fails with "command not found"
414: 
415: ```bash
416: # Solution: Ensure validation command is correct in PRP blueprint
417: # Example: Use "uv run pytest tests/" not just "pytest tests/"
418: ```
419: 
420: ### Issue: Self-healing stuck in loop
421: 
422: ```bash
423: # Solution: Same error 3 times triggers escalation
424: # Review escalation message for troubleshooting guidance
425: ```
426: 
427: ### Issue: "Auto-sync failed"
428: 
429: ```bash
430: # Solution: Post-sync is non-blocking, execution still completes
431: # Run manual sync: cd tools && uv run ce context post-sync PRP-5
432: ```
433: 
434: ## Output Structure
435: 
436: ```json
437: {
438:   "success": true,
439:   "prp_id": "PRP-6",
440:   "phases_completed": 3,
441:   "validation_results": {
442:     "Phase1": {
443:       "success": true,
444:       "validation_levels": {
445:         "L1": {"passed": true, "attempts": 1, "errors": []},
446:         "L2": {"passed": true, "attempts": 2, "errors": ["import error"]},
447:         "L3": {"passed": true, "attempts": 1, "errors": []},
448:         "L4": {"passed": true, "attempts": 1, "drift_score": 5.2}
449:       },
450:       "self_healed": ["L2: Fixed after 2 attempts"]
451:     }
452:   },
453:   "checkpoints_created": [
454:     "checkpoint-PRP-6-phase1-20251013-100000",
455:     "checkpoint-PRP-6-phase2-20251013-110000",
456:     "checkpoint-PRP-6-phase3-20251013-120000"
457:   ],
458:   "confidence_score": "10/10",
459:   "execution_time": "45m 23s"
460: }
461: ```
462: 
463: ## Next Steps After Execution
464: 
465: 1. **Review Execution Summary**:
466:    - Check confidence score (target: 10/10)
467:    - Review self-healing actions taken
468:    - Verify all validation gates passed
469: 
470: 2. **Test Manually** (if confidence < 10/10):
471:    - Run validation commands manually
472:    - Review self-healing fixes
473:    - Address any escalated errors
474: 
475: 3. **Rollback if Needed** (via PRP-2):
476: 
477:    ```bash
478:    cd tools
479:    uv run ce prp restore PRP-6 phase2
480:    ```
481: 
482: 4. **Context Sync** (if auto-sync disabled):
483: 
484:    ```bash
485:    cd tools
486:    uv run ce context post-sync PRP-6
487:    ```
488: 
489: 5. **Peer Review**:
490: 
491:    ```bash
492:    /peer-review exe PRPs/feature-requests/PRP-6-user-authentication-system.md
493:    ```
494: 
495: ## Tips
496: 
497: 1. **Enable auto-sync** before execution: `ce context auto-sync --enable`
498: 2. **Use dry-run** to preview phases: `ce execute PRP-6 --dry-run`
499: 3. **Test incrementally**: Use `--start-phase` and `--end-phase` for partial execution
500: 4. **Trust self-healing**: Let L1-L2 auto-fixes run before manual intervention
501: 5. **Review escalations**: Escalation messages include specific troubleshooting guidance
502: 6. **Check confidence score**: 10/10 means production-ready, <8/10 needs review
503: 
504: ## Implementation Details
505: 
506: - **Module**: `tools/ce/execute.py`
507: - **Tests**: `tools/tests/test_execute.py` (20+ tests)
508: - **PRP Reference**: `PRPs/feature-requests/PRP-4-execute-prp-orchestration.md`
509: - **Self-Healing**: 90%+ success rate on L1-L2 errors
510: - **Speed Improvement**: 10-24x faster than manual implementation
511: 
512: ## Related Commands
513: 
514: - `/generate-prp` - Generate PRP from INITIAL.md
515: - `/peer-review exe <prp-file>` - Review execution quality
516: - `ce prp restore <prp-id> [phase]` - Rollback to checkpoint (PRP-2)
517: - `ce context post-sync <prp-id>` - Manual post-execution sync (PRP-5)
518: - `ce validate --level 4 <prp-path>` - Run L4 pattern conformance (PRP-1)
</file>

<file path="tools/ce/vacuum.py">
  1: """Vacuum command for project cleanup."""
  2: 
  3: import argparse
  4: import sys
  5: from datetime import datetime
  6: from pathlib import Path
  7: from typing import List
  8: 
  9: from .vacuum_strategies import (
 10:     BackupFileStrategy,
 11:     CleanupCandidate,
 12:     CommentedCodeStrategy,
 13:     ObsoleteDocStrategy,
 14:     OrphanTestStrategy,
 15:     TempFileStrategy,
 16:     UnreferencedCodeStrategy,
 17: )
 18: 
 19: 
 20: class VacuumCommand:
 21:     """Main vacuum command for project cleanup."""
 22: 
 23:     def __init__(self, project_root: Path):
 24:         """Initialize vacuum command.
 25: 
 26:         Args:
 27:             project_root: Path to project root directory
 28:         """
 29:         self.project_root = project_root
 30:         self.strategies = {
 31:             "temp-files": TempFileStrategy,
 32:             "backup-files": BackupFileStrategy,
 33:             "obsolete-docs": ObsoleteDocStrategy,
 34:             "unreferenced-code": UnreferencedCodeStrategy,
 35:             "orphan-tests": OrphanTestStrategy,
 36:             "commented-code": CommentedCodeStrategy,
 37:         }
 38: 
 39:     def run(
 40:         self,
 41:         dry_run: bool = True,
 42:         min_confidence: int = 0,
 43:         exclude_strategies: List[str] = None,
 44:         execute: bool = False,
 45:         force: bool = False,
 46:         auto: bool = False,
 47:         nuclear: bool = False,
 48:     ) -> int:
 49:         """Run vacuum command.
 50: 
 51:         Args:
 52:             dry_run: If True, only generate report without deleting
 53:             min_confidence: Minimum confidence threshold (0-100)
 54:             exclude_strategies: List of strategy names to skip
 55:             execute: Delete HIGH confidence items
 56:             force: Delete HIGH + MEDIUM confidence items
 57:             auto: Alias for force (delete HIGH + MEDIUM automatically)
 58:             nuclear: Delete ALL items (requires confirmation)
 59: 
 60:         Returns:
 61:             Exit code: 0 = clean, 1 = candidates found, 2 = error
 62:         """
 63:         exclude_strategies = exclude_strategies or []
 64: 
 65:         # Determine deletion threshold
 66:         if nuclear:
 67:             delete_threshold = 0  # Delete everything
 68:             if not self._confirm_nuclear():
 69:                 print("âŒ Nuclear mode cancelled by user")
 70:                 return 2
 71:         elif force or auto:
 72:             delete_threshold = 60  # Delete MEDIUM + HIGH
 73:         elif execute:
 74:             delete_threshold = 100  # Delete HIGH only
 75:         else:
 76:             delete_threshold = 101  # Dry-run: delete nothing
 77: 
 78:         # Run all strategies
 79:         all_candidates = []
 80:         for strategy_name, strategy_class in self.strategies.items():
 81:             if strategy_name in exclude_strategies:
 82:                 print(f"â­ï¸  Skipping {strategy_name}")
 83:                 continue
 84: 
 85:             print(f"ðŸ” Running {strategy_name}...")
 86:             strategy = strategy_class(self.project_root)
 87:             candidates = strategy.find_candidates()
 88: 
 89:             # Filter by minimum confidence
 90:             candidates = [c for c in candidates if c.confidence >= min_confidence]
 91: 
 92:             all_candidates.extend(candidates)
 93:             print(f"   Found {len(candidates)} candidates")
 94: 
 95:         # Generate report
 96:         report_path = self.project_root / ".ce" / "vacuum-report.md"
 97:         self._generate_report(all_candidates, report_path)
 98:         print(f"\nðŸ“„ Report generated: {report_path}")
 99: 
100:         # Delete files if not dry-run
101:         if delete_threshold <= 100:
102:             deleted_count = self._delete_candidates(all_candidates, delete_threshold)
103:             print(f"\nðŸ—‘ï¸  Deleted {deleted_count} items")
104: 
105:         # Return exit code
106:         if not all_candidates:
107:             print("\nâœ… No cleanup candidates found - project is clean!")
108:             return 0
109:         else:
110:             print(f"\nâš ï¸  Found {len(all_candidates)} cleanup candidates")
111:             return 1
112: 
113:     def _generate_report(self, candidates: List[CleanupCandidate], output_path: Path):
114:         """Generate vacuum report.
115: 
116:         Args:
117:             candidates: List of cleanup candidates
118:             output_path: Path to output report file
119:         """
120:         # Ensure output directory exists
121:         output_path.parent.mkdir(parents=True, exist_ok=True)
122: 
123:         # Group by confidence
124:         high = [c for c in candidates if c.confidence >= 100]
125:         medium = [c for c in candidates if 60 <= c.confidence < 100]
126:         low = [c for c in candidates if c.confidence < 60]
127: 
128:         # Calculate total size
129:         total_size = sum(c.size_bytes for c in candidates)
130: 
131:         # Generate report
132:         report = [
133:             f"# Vacuum Report - {datetime.now().isoformat()}",
134:             "",
135:             "## Summary",
136:             f"- Candidates found: {len(candidates)}",
137:             f"- Bytes reclaimable: {self._format_size(total_size)}",
138:             f"- HIGH confidence: {len(high)} items (safe to delete)",
139:             f"- MEDIUM confidence: {len(medium)} items (review recommended)",
140:             f"- LOW confidence: {len(low)} items (manual verification required)",
141:             "",
142:         ]
143: 
144:         # HIGH confidence section
145:         if high:
146:             report.extend([
147:                 "## HIGH Confidence (Safe to Delete)",
148:                 "",
149:                 "| Path | Reason | Size | Last Modified |",
150:                 "|------|--------|------|---------------|",
151:             ])
152:             for c in high:
153:                 rel_path = c.path.relative_to(self.project_root)
154:                 report.append(
155:                     f"| {rel_path} | {c.reason} | {self._format_size(c.size_bytes)} | {c.last_modified[:10]} |"
156:                 )
157:             report.append("")
158: 
159:         # MEDIUM confidence section
160:         if medium:
161:             report.extend([
162:                 "## MEDIUM Confidence (Review Needed)",
163:                 "",
164:                 "| Path | Reason | Confidence | Git History |",
165:                 "|------|--------|------------|-------------|",
166:             ])
167:             for c in medium:
168:                 rel_path = c.path.relative_to(self.project_root)
169:                 report.append(
170:                     f"| {rel_path} | {c.reason} | {c.confidence}% | {c.git_history} |"
171:                 )
172:             report.append("")
173: 
174:         # LOW confidence section
175:         if low:
176:             report.extend([
177:                 "## LOW Confidence (Manual Verification Required)",
178:                 "",
179:                 "| Path | Reason | Confidence | References |",
180:                 "|------|--------|------------|------------|",
181:             ])
182:             for c in low:
183:                 rel_path = c.path.relative_to(self.project_root)
184:                 refs = ", ".join(c.references[:3]) if c.references else "None"
185:                 report.append(f"| {rel_path} | {c.reason} | {c.confidence}% | {refs} |")
186:             report.append("")
187: 
188:         # Write report
189:         output_path.write_text("\n".join(report), encoding="utf-8")
190: 
191:     def _delete_candidates(self, candidates: List[CleanupCandidate], threshold: int) -> int:
192:         """Delete candidates meeting confidence threshold.
193: 
194:         Args:
195:             candidates: List of cleanup candidates
196:             threshold: Minimum confidence to delete
197: 
198:         Returns:
199:             Number of items deleted
200:         """
201:         import shutil
202: 
203:         deleted_count = 0
204: 
205:         for candidate in candidates:
206:             # Skip report-only candidates (never delete these)
207:             if candidate.report_only:
208:                 continue
209: 
210:             if candidate.confidence < threshold:
211:                 continue
212: 
213:             try:
214:                 if candidate.path.is_file():
215:                     candidate.path.unlink()
216:                     deleted_count += 1
217:                 elif candidate.path.is_dir():
218:                     shutil.rmtree(candidate.path)
219:                     deleted_count += 1
220:             except Exception as e:
221:                 print(f"âŒ Failed to delete {candidate.path}: {e}")
222: 
223:         return deleted_count
224: 
225:     def _confirm_nuclear(self) -> bool:
226:         """Ask user to confirm nuclear mode.
227: 
228:         Returns:
229:             True if user confirms, False otherwise
230:         """
231:         response = input("âš ï¸  NUCLEAR MODE: Delete ALL candidates including LOW confidence? (yes/no): ")
232:         return response.lower() == "yes"
233: 
234:     @staticmethod
235:     def _format_size(size_bytes: int) -> str:
236:         """Format file size in human-readable format.
237: 
238:         Args:
239:             size_bytes: Size in bytes
240: 
241:         Returns:
242:             Formatted size string
243:         """
244:         for unit in ["B", "KB", "MB", "GB"]:
245:             if size_bytes < 1024.0:
246:                 return f"{size_bytes:.1f} {unit}"
247:             size_bytes /= 1024.0
248:         return f"{size_bytes:.1f} TB"
249: 
250: 
251: def main():
252:     """CLI entry point for vacuum command."""
253:     parser = argparse.ArgumentParser(description="Clean up project noise")
254:     parser.add_argument(
255:         "--dry-run",
256:         action="store_true",
257:         default=True,
258:         help="Generate report only (default)",
259:     )
260:     parser.add_argument(
261:         "--execute",
262:         action="store_true",
263:         help="Delete HIGH confidence items",
264:     )
265:     parser.add_argument(
266:         "--force",
267:         action="store_true",
268:         help="Delete HIGH + MEDIUM confidence items",
269:     )
270:     parser.add_argument(
271:         "--nuclear",
272:         action="store_true",
273:         help="Delete ALL items (requires confirmation)",
274:     )
275:     parser.add_argument(
276:         "--min-confidence",
277:         type=int,
278:         default=0,
279:         help="Minimum confidence threshold (0-100)",
280:     )
281:     parser.add_argument(
282:         "--exclude-strategy",
283:         action="append",
284:         dest="exclude_strategies",
285:         help="Skip specific strategy",
286:     )
287: 
288:     args = parser.parse_args()
289: 
290:     # Find project root (where .ce/ directory exists)
291:     current = Path.cwd()
292:     project_root = None
293: 
294:     for parent in [current] + list(current.parents):
295:         if (parent / ".ce").exists():
296:             project_root = parent
297:             break
298: 
299:     if not project_root:
300:         print("âŒ Error: Not in a Context Engineering project (.ce/ not found)")
301:         return 2
302: 
303:     # Run vacuum command
304:     vacuum = VacuumCommand(project_root)
305:     return vacuum.run(
306:         dry_run=not (args.execute or args.force or args.nuclear),
307:         min_confidence=args.min_confidence,
308:         exclude_strategies=args.exclude_strategies or [],
309:         execute=args.execute,
310:         force=args.force,
311:         nuclear=args.nuclear,
312:     )
313: 
314: 
315: if __name__ == "__main__":
316:     sys.exit(main())
</file>

<file path=".claude/commands/batch-exe-prp.md">
   1: # /batch-exe-prp - Parallel PRP Execution with Monitoring
   2: 
   3: Execute multiple PRPs in parallel using subagents with health checks, real-time monitoring, and coordinated error handling.
   4: 
   5: ## Usage
   6: 
   7: ```
   8: /batch-exe-prp [options] <prp-1> <prp-2> ... <prp-n>
   9: /batch-exe-prp --batch <batch-id> [options]
  10: ```
  11: 
  12: **Examples:**
  13: ```bash
  14: # Execute 3 PRPs in parallel (auto-selects model per PRP)
  15: /batch-exe-prp PRP-A PRP-B PRP-C
  16: 
  17: # Execute entire batch by ID (auto-review enabled by default)
  18: /batch-exe-prp --batch 34
  19: 
  20: # Execute batch without automatic execution review
  21: /batch-exe-prp --batch 34 --no-auto-review
  22: 
  23: # Execute specific stage only (auto-review enabled by default)
  24: /batch-exe-prp --batch 34 --stage 2
  25: 
  26: # Resume interrupted batch execution
  27: /batch-exe-prp --batch 34 --resume
  28: 
  29: # Force all PRPs to use Haiku (override auto-selection)
  30: /batch-exe-prp --model haiku PRP-A PRP-B PRP-C
  31: 
  32: # Limit parallelism (useful for resource constraints)
  33: /batch-exe-prp --max-parallel 2 PRP-A PRP-B PRP-C PRP-D
  34: 
  35: # Dry run (parse all PRPs, show model assignments)
  36: /batch-exe-prp --dry-run PRP-A PRP-B PRP-C
  37: 
  38: # Continue on failure (don't abort if one PRP fails)
  39: /batch-exe-prp --continue-on-error PRP-A PRP-B PRP-C
  40: ```
  41: 
  42: ## What It Does
  43: 
  44: **Core Design**: Cohesive architecture with `/execute-prp`
  45: 
  46: - **Batch-exe-prp** (this command): Coordinator - analyzes PRPs, assigns models, launches parallel agents, monitors health, aggregates results
  47: - **Execute-prp** (single PRP engine): Execution logic - phases, validation gates, self-healing, checkpoints (reused by batch agents)
  48: - **No duplication**: Batch delegates to execute-prp via Task agents, doesn't reimplement PRP logic
  49: 
  50: ## Execution Modes
  51: 
  52: ### Mode 1: Individual PRPs (Original Behavior)
  53: 
  54: Execute specific PRPs in parallel, all at once:
  55: 
  56: ```bash
  57: /batch-exe-prp PRP-A PRP-B PRP-C
  58: ```
  59: 
  60: **Workflow**: Steps 1-8 below (analyze â†’ validate â†’ execute all â†’ merge â†’ cleanup)
  61: 
  62: ### Mode 2: Batch-Aware Execution (NEW)
  63: 
  64: Execute all PRPs in a batch, with stage-by-stage execution and automatic peer review:
  65: 
  66: ```bash
  67: /batch-exe-prp --batch 34
  68: ```
  69: 
  70: **Key Features**:
  71: 1. **Stage-aware**: Discovers all PRPs in batch, groups by stage, executes stages sequentially
  72: 2. **Auto-review**: Runs `/batch-peer-review --exe` after each stage completes (default, use `--no-auto-review` to disable)
  73: 3. **Quality gates**: Only proceeds to next stage if execution review passes
  74: 4. **Checkpointing**: Saves state after each stage for resume capability
  75: 5. **Resume support**: Continue from last checkpoint if interrupted
  76: 
  77: **Batch Execution Workflow**:
  78: 
  79: ```
  80: Stage 1: Execute PRPs
  81:    â†“
  82: Stage 1: Auto-review execution (/batch-peer-review --batch 34 --exe --stage 1)
  83:    â†“
  84:    â”œâ”€ Issues found (minor) â†’ Apply fixes automatically â†’ Continue
  85:    â”œâ”€ Issues found (critical) â†’ Pause, report, wait for user approval
  86:    â””â”€ No issues â†’ Continue
  87:    â†“
  88: Stage 1: Merge worktrees
  89:    â†“
  90: Checkpoint saved (.ce/tmp/batch-execution-34.json)
  91:    â†“
  92: Stage 2: Execute PRPs
  93:    â†“
  94: [Repeat review/merge/checkpoint cycle]
  95:    â†“
  96: ... Stage 3, 4, etc.
  97:    â†“
  98: Final cleanup
  99: ```
 100: 
 101: **Stage Discovery** (automatic):
 102: 1. Read master plan: `PRPs/feature-requests/PRP-34-INITIAL.md`
 103: 2. Scan for all batch PRPs: `PRPs/feature-requests/PRP-34.*.*.md`
 104: 3. Parse YAML headers, extract `stage` field
 105: 4. Group by stage: `{1: [PRP-34.1.1], 2: [PRP-34.2.1, PRP-34.2.2, ...], ...}`
 106: 5. Execute stages in order (1 â†’ 2 â†’ 3 â†’ 4)
 107: 
 108: **Execution Review** (automatic after each stage):
 109: - Calls `/batch-peer-review --batch 34 --exe --stage N` internally
 110: - Reviews all PRPs in stage N that just executed
 111: - Checks: Implementation matches specs, code quality, no guideline violations, etc. (9 checks)
 112: - Minor issues (typos, style): Auto-fix, continue
 113: - Critical issues (logic errors, security): Pause, escalate to user
 114: 
 115: **Pause/Resume**:
 116: ```bash
 117: # If batch execution paused (review failed, conflict, error)
 118: # Resume from last checkpoint
 119: /batch-exe-prp --batch 34 --resume
 120: 
 121: # Reads: .ce/tmp/batch-execution-34.json
 122: # Determines: Last completed stage = 2
 123: # Resumes: Execute Stage 3 (skips Stages 1-2)
 124: ```
 125: 
 126: **Benefits**:
 127: - ðŸ›¡ï¸ Quality gate between stages (catches errors early)
 128: - ðŸ”„ Resume from interruptions (don't lose progress)
 129: - ðŸ“Š Stage-level validation (validate integration within stage)
 130: - ðŸš€ Parallel within stages (max speed for independent PRPs)
 131: - ðŸŽ¯ Sequential across stages (respects dependencies)
 132: 
 133: ### 1. Analyze PRP Complexity & Auto-Assign Models (Sequential)
 134: **Time**: ~5-10 seconds per PRP
 135: 
 136: For each PRP, analyzes complexity and assigns optimal model (unless `--model` overrides):
 137: 
 138: **Complexity Analysis**:
 139: ```python
 140: def complexity_weight(complexity):
 141:     """Convert complexity label to numeric weight"""
 142:     weights = {"low": 0.5, "medium": 1.0, "high": 1.5}
 143:     return weights.get(complexity, 1.0)  # Default to medium
 144: 
 145: def analyze_prp_complexity(prp_path):
 146:     # Read PRP file
 147:     prp = read_prp(prp_path)
 148: 
 149:     # Extract metadata
 150:     complexity = prp.yaml_header.get('complexity', 'medium')  # low/medium/high
 151:     estimated_hours = prp.yaml_header.get('estimated_hours', 1.0)
 152:     files_modified = len(prp.yaml_header.get('files_modified', []))
 153:     phases = count_phases(prp.implementation_blueprint)
 154: 
 155:     # Calculate complexity score (0-100)
 156:     # Score breakdown:
 157:     # - Complexity weight: 0-60 points (low=20, medium=40, high=60)
 158:     # - Hours: 0-30 points (capped at 3 hours)
 159:     # - Files: 0-20 points (capped at 4 files)
 160:     # - Phases: 0-10 points (capped at 3+ phases)
 161:     score = (
 162:         complexity_weight(complexity) * 40 +  # 20/40/60 points
 163:         min(estimated_hours * 10, 30) +       # 30 points max
 164:         min(files_modified * 5, 20) +         # 20 points max
 165:         min(phases * 3, 10)                   # 10 points max
 166:     )
 167: 
 168:     return score
 169: 
 170: def assign_model(score):
 171:     if score < 40:
 172:         return "haiku"    # Simple: single-file edits, <0.5h, low complexity
 173:     elif score < 70:
 174:         return "sonnet"   # Medium: multi-file, 0.5-2h, some judgment
 175:     else:
 176:         return "opus"     # High: architectural, >2h, critical decisions
 177: ```
 178: 
 179: **Model Assignment Report**:
 180: ```
 181: ðŸ§  Model Assignment (Auto-Selected)
 182: ============================================================
 183: PRP-A: Tool Deny List Implementation
 184:   Complexity: low | Hours: 0.25-0.33 | Files: 1 | Phases: 3
 185:   Score: 37/100 â†’ Model: haiku âœ“
 186:   Rationale: Simple JSON edit, single file, no architectural decisions
 187:   Calculation: (0.5*40) + (0.29*10) + (1*5) + (3*3) = 20+2.9+5+9 = 37
 188: 
 189: PRP-B: Tool Usage Guide Creation
 190:   Complexity: low | Hours: 0.33-0.42 | Files: 1 | Phases: 3
 191:   Score: 38/100 â†’ Model: haiku âœ“
 192:   Rationale: Straightforward doc creation, clear structure
 193:   Calculation: (0.5*40) + (0.38*10) + (1*5) + (3*3) = 20+3.8+5+9 = 38
 194: 
 195: PRP-C: Worktree Migration
 196:   Complexity: medium | Hours: 0.42-0.50 | Files: 3 | Phases: 3
 197:   Score: 69/100 â†’ Model: sonnet âœ“
 198:   Rationale: Multi-file, doc structuring requires judgment
 199:   Calculation: (1.0*40) + (0.46*10) + (3*5) + (3*3) = 40+4.6+15+9 = 69
 200: 
 201: ============================================================
 202: Thresholds: Haiku <40, Sonnet 40-69, Opus â‰¥70
 203: Cost estimate: $0.05 (vs $0.25 all-sonnet = 80% savings)
 204: ```
 205: 
 206: ### 2. Pre-Flight Validation (Sequential)
 207: **Time**: ~10-30 seconds depending on PRP count
 208: 
 209: For each PRP:
 210: - âœ“ Validate PRP file exists and is readable
 211: - âœ“ Parse YAML headers (extract stage, worktree_path, conflict_potential)
 212: - âœ“ Check git worktree availability (if worktree_path specified)
 213: - âœ“ Verify no conflicting worktrees already active
 214: - âœ“ Run health check: `mcp__syntropy__healthcheck(detailed=True)`
 215: - âœ“ Estimate total execution time from PRP metadata
 216: 
 217: **Validation Report**:
 218: ```
 219: ðŸ“‹ Pre-Flight Validation
 220: ============================================================
 221: PRPs to execute: 3
 222: Parallelism: 3 (max)
 223: Model assignment: auto (haiku: 2, sonnet: 1)
 224: Total estimated time: 20-25 minutes (45m sequential, 55% savings)
 225: 
 226: PRP-A: Tool Deny List Implementation [HAIKU]
 227:   âœ“ File exists: PRPs/feature-requests/PRP-A-tool-deny-list.md
 228:   âœ“ Stage: stage-1-parallel
 229:   âœ“ Worktree: ../ctx-eng-plus-prp-a (available)
 230:   âœ“ Conflict potential: MEDIUM
 231:   â± Estimated: 15-20 minutes
 232: 
 233: PRP-B: Tool Usage Guide Creation [HAIKU]
 234:   âœ“ File exists: PRPs/feature-requests/PRP-B-tool-usage-guide.md
 235:   âœ“ Stage: stage-1-parallel
 236:   âœ“ Worktree: ../ctx-eng-plus-prp-b (available)
 237:   âœ“ Conflict potential: NONE
 238:   â± Estimated: 20-25 minutes
 239: 
 240: PRP-C: Worktree Migration [SONNET]
 241:   âœ“ File exists: PRPs/feature-requests/PRP-C-gitbutler-worktree-migration.md
 242:   âœ“ Stage: stage-1-parallel
 243:   âœ“ Worktree: ../ctx-eng-plus-prp-c (available)
 244:   âœ“ Conflict potential: LOW
 245:   â± Estimated: 25-30 minutes
 246: 
 247: âœ… All validations passed
 248: ðŸš€ Ready to execute
 249: ============================================================
 250: ```
 251: 
 252: **Abort conditions**:
 253: - âŒ Any PRP file not found
 254: - âŒ Conflicting stages (e.g., stage-1 + stage-2 PRPs)
 255: - âŒ Worktree path conflicts
 256: - âŒ MCP server health check failed
 257: - âŒ Git repo not clean (uncommitted changes)
 258: 
 259: ### 3. Create Git Worktrees (Sequential)
 260: **Time**: ~5-10 seconds
 261: 
 262: If PRPs specify `worktree_path` in YAML headers:
 263: ```bash
 264: git worktree add ../ctx-eng-plus-prp-a -b prp-a-tool-deny
 265: git worktree add ../ctx-eng-plus-prp-b -b prp-b-usage-guide
 266: git worktree add ../ctx-eng-plus-prp-c -b prp-c-worktree-migration
 267: ```
 268: 
 269: **Output**:
 270: ```
 271: ðŸŒ³ Creating worktrees...
 272:   âœ“ Created: ../ctx-eng-plus-prp-a (branch: prp-a-tool-deny)
 273:   âœ“ Created: ../ctx-eng-plus-prp-b (branch: prp-b-usage-guide)
 274:   âœ“ Created: ../ctx-eng-plus-prp-c (branch: prp-c-worktree-migration)
 275: ```
 276: 
 277: ### 4. Launch Parallel Execution (Parallel)
 278: **Time**: Variable (depends on PRP complexity)
 279: 
 280: **Delegates to `/execute-prp` via Task agents** (in single message with multiple Task calls):
 281: 
 282: ```python
 283: # Parallel agent launch (sent in single message)
 284: # Agent A: PRP-A (Haiku)
 285: Task(
 286:   subagent_type="general-purpose",
 287:   model="haiku",  # Auto-assigned based on complexity score
 288:   description="Execute PRP-A",
 289:   prompt=f"""
 290: You are a PRP execution agent. Execute the following PRP using /execute-prp logic.
 291: 
 292: **PRP**: PRPs/feature-requests/PRP-A-tool-deny-list.md
 293: **Worktree**: /Users/bprzybysz/nc-src/ctx-eng-plus-prp-a
 294: **Branch**: prp-a-tool-deny
 295: **Model**: haiku (auto-assigned: complexity score 25/100)
 296: 
 297: **Execution Protocol**:
 298: 1. Change working directory to worktree: `cd /Users/bprzybysz/nc-src/ctx-eng-plus-prp-a`
 299: 2. Read PRP file completely to understand all implementation steps
 300: 3. Execute /execute-prp logic (phases, validation gates, self-healing, checkpoints):
 301:    - Parse implementation blueprint (extract phases)
 302:    - For each phase:
 303:      a. Execute implementation steps
 304:      b. Run L1 validation (syntax & style)
 305:      c. Run L2 validation (unit tests)
 306:      d. Run L3 validation (integration tests)
 307:      e. Run L4 validation (pattern conformance, drift <30%)
 308:      f. Create checkpoint: git commit
 309:    - Self-heal L1-L2 errors (max 3 attempts)
 310:    - Escalate if persistent/architectural/security errors
 311: 4. Commit all changes to branch with message format: "PRP-A: <summary>"
 312: 5. Return execution report (see format below)
 313: 
 314: **Health Check Protocol** (output every 5 minutes):
 315: ```
 316: HEALTH:OK
 317: HEALTH:ERROR:<reason>
 318: ```
 319: 
 320: **Progress Updates** (output on phase completion):
 321: ```
 322: STATUS:PHASE_COMPLETE:1/3
 323: STATUS:VALIDATION:L2
 324: STATUS:SELF_HEAL:attempt_2
 325: ```
 326: 
 327: **Completion Signal**:
 328: ```
 329: STATUS:COMPLETE:10/10        # Success (confidence score)
 330: STATUS:FAILED:L3_timeout     # Failure reason
 331: STATUS:PARTIAL:2/3           # Partial completion
 332: ```
 333: 
 334: **Return Format** (final report):
 335: {{
 336:   "prp_id": "PRP-A",
 337:   "status": "SUCCESS|FAILED|PARTIAL",
 338:   "phases_completed": 3,
 339:   "phases_total": 3,
 340:   "confidence_score": 10,
 341:   "validation_results": {{
 342:     "L1": {{"passed": true, "attempts": 1}},
 343:     "L2": {{"passed": true, "attempts": 1}},
 344:     "L3": {{"passed": true, "attempts": 1}},
 345:     "L4": {{"passed": true, "drift_score": 4.2}}
 346:   }},
 347:   "self_heals": 0,
 348:   "commit_hash": "ab3118f",
 349:   "execution_time": "6m 12s",
 350:   "files_modified": [".claude/settings.local.json"],
 351:   "errors": []
 352: }}
 353: 
 354: **Error Handling**:
 355: - L1-L2: Attempt self-healing (max 3 attempts per error)
 356: - L3-L4: Escalate, no auto-healing
 357: - Persistent errors (same error 3x): Escalate
 358: - Architectural errors: Escalate immediately
 359: - Security errors: Escalate immediately, DO NOT auto-fix
 360: 
 361: **Constraints**:
 362: - Work only in worktree, never touch main repo
 363: - All validation gates must pass (L1-L4)
 364: - Drift score must be <30%
 365: - Create checkpoint (git commit) after each phase
 366: """
 367: )
 368: 
 369: # Agent B: PRP-B (Haiku)
 370: Task(
 371:   subagent_type="general-purpose",
 372:   model="haiku",
 373:   description="Execute PRP-B",
 374:   prompt="<same structure as PRP-A, with PRP-B details>"
 375: )
 376: 
 377: # Agent C: PRP-C (Sonnet - higher complexity)
 378: Task(
 379:   subagent_type="general-purpose",
 380:   model="sonnet",
 381:   description="Execute PRP-C",
 382:   prompt="<same structure as PRP-A, with PRP-C details>"
 383: )
 384: ```
 385: 
 386: **Key Points**:
 387: - **No logic duplication**: Agents follow /execute-prp protocol (phases, validation, self-healing)
 388: - **Model assignment**: Auto-selected based on complexity analysis (Step 1)
 389: - **Health monitoring**: Agents output health signals every 5 minutes
 390: - **Parallel launch**: All Task calls in single message for true parallelism
 391: 
 392: ### 5. Monitor Execution (Git Log Polling)
 393: **Time**: Continuous during execution
 394: 
 395: **Monitoring Mechanism**: Poll git logs every 60 seconds for checkpoint commits
 396: 
 397: **Polling Logic**:
 398: ```bash
 399: # For each worktree
 400: for worktree in worktrees:
 401:     latest_commit=$(git -C $worktree log -1 --oneline)
 402:     commit_time=$(git -C $worktree log -1 --format=%ct)
 403:     current_time=$(date +%s)
 404:     age=$((current_time - commit_time))
 405: 
 406:     if [ $age -lt 300 ]; then
 407:         echo "HEALTHY: Latest commit ${age}s ago"
 408:     elif [ $age -lt 600 ]; then
 409:         echo "WARNING: Last commit ${age}s ago (may be stalled)"
 410:     else
 411:         echo "STALLED: No commits for ${age}s (likely hung)"
 412:     fi
 413: done
 414: ```
 415: 
 416: **Monitoring Dashboard** (updates every 60 seconds):
 417: ```
 418: ðŸ“Š Batch Execution Status (Updated: 10:45:23)
 419: ============================================================
 420: Elapsed: 12m 34s / Estimated: 45m (28% complete)
 421: 
 422: PRP-A: Tool Deny List Implementation [HEALTHY]
 423:   Last commit: 2m ago "Phase 3: Validation complete"
 424:   Branch: prp-a-tool-deny-list
 425:   Status: Likely completing final phase
 426: 
 427: PRP-B: Tool Usage Guide Creation [HEALTHY]
 428:   Last commit: 1m ago "Phase 2: Implementation complete"
 429:   Branch: prp-b-tool-usage-guide
 430:   Status: Moving to Phase 3
 431: 
 432: PRP-C: Worktree Migration [WARNING]
 433:   Last commit: 8m ago "Phase 1: Preparation complete"
 434:   Branch: prp-c-worktree-migration
 435:   Status: May be stalled on Phase 2 (long-running step)
 436: 
 437: ============================================================
 438: Active: 3 | HEALTHY: 2 | WARNING: 1 | STALLED: 0
 439: Timeout: 60m per PRP (fallback if no commits for >10m)
 440: ```
 441: 
 442: **Health Status Criteria**:
 443: - **HEALTHY**: Last commit <5m ago
 444: - **WARNING**: Last commit 5-10m ago (may be long-running phase)
 445: - **STALLED**: Last commit >10m ago (likely hung, check agent timeout)
 446: - **FAILED**: Agent returned error or timeout exceeded
 447: 
 448: **Stall Handling**:
 449: ```
 450: âš ï¸ PRP-B stalled (no commits for 12m)
 451: Actions:
 452:   1. Check Task agent timeout (60m default)
 453:   2. Agent still running â†’ wait for timeout
 454:   3. Agent timed out â†’ mark FAILED, preserve worktree
 455:   4. Review partial work: cd ../ctx-eng-plus-prp-b && git log
 456: 
 457: Note: Cannot forcibly abort Task agents mid-execution
 458: Rely on Task timeout mechanism for automatic abort
 459: ```
 460: 
 461: ### 6. Aggregate Results (Sequential)
 462: **Time**: ~10-30 seconds
 463: 
 464: After all agents complete (or timeout):
 465: 
 466: ```python
 467: # Collect results from all agents
 468: results = {
 469:   "PRP-A": agent_a.result,
 470:   "PRP-B": agent_b.result,
 471:   "PRP-C": agent_c.result
 472: }
 473: 
 474: # Calculate aggregate metrics
 475: total_phases = sum(r.phases_completed for r in results.values())
 476: total_errors = sum(len(r.errors) for r in results.values())
 477: avg_confidence = mean(r.confidence_score for r in results.values())
 478: ```
 479: 
 480: **Aggregate Report**:
 481: ```
 482: ðŸ“Š Batch Execution Complete
 483: ============================================================
 484: Total Time: 18m 42s (estimated: 45m, 58% faster)
 485: PRPs Executed: 3 | Succeeded: 3 | Failed: 0
 486: 
 487: PRP-A: Tool Deny List Implementation [âœ… SUCCESS]
 488:   Phases: 3/3 completed
 489:   Validation: L1-L4 passed
 490:   Confidence: 10/10
 491:   Commit: ab3118f "Add 55 MCP tools to deny list, clean duplicates"
 492:   Execution time: 6m 12s
 493: 
 494: PRP-B: Tool Usage Guide Creation [âœ… SUCCESS]
 495:   Phases: 3/3 completed
 496:   Validation: L1-L4 passed
 497:   Confidence: 10/10
 498:   Commit: 43051cb "Create comprehensive tool usage guide"
 499:   Execution time: 8m 35s
 500: 
 501: PRP-C: Worktree Migration [âœ… SUCCESS]
 502:   Phases: 3/3 completed
 503:   Validation: L1-L4 passed
 504:   Confidence: 9/10 (1 self-heal: remove duplicate hooks)
 505:   Commit: 388508b "Migrate from GitButler to git worktree documentation"
 506:   Execution time: 10m 18s
 507: 
 508: ============================================================
 509: Aggregate Metrics:
 510:   Total phases: 9/9
 511:   Avg confidence: 9.7/10
 512:   Total errors: 0
 513:   Self-heals: 1
 514:   Time savings: 26m 18s (58%)
 515: 
 516: âœ… All PRPs executed successfully
 517: ```
 518: 
 519: ### 7. Merge Worktrees (Sequential)
 520: **Time**: ~30-60 seconds
 521: 
 522: If PRPs executed in worktrees, merge in `merge_order` from YAML headers:
 523: 
 524: ```bash
 525: # Switch to main branch
 526: git checkout main
 527: 
 528: # Merge in order (A â†’ B â†’ C)
 529: git merge prp-a-tool-deny --no-ff -m "Merge PRP-A: Tool Deny List Implementation"
 530: git merge prp-b-usage-guide --no-ff -m "Merge PRP-B: Tool Usage Guide Creation"
 531: git merge prp-c-worktree-migration --no-ff -m "Merge PRP-C: GitButler to Worktree Migration"
 532: ```
 533: 
 534: **Conflict Detection**:
 535: ```
 536: âš ï¸ Merge conflict detected: PRP-D
 537: File: .claude/settings.local.json
 538: Conflict markers found at lines 145-152
 539: 
 540: Conflict Resolution Required:
 541:   1. Read file to view conflict markers (<<<<<<< HEAD)
 542:   2. Decide on resolution strategy (keep both, prefer incoming, prefer current)
 543:   3. Use Edit tool to remove markers and merge changes
 544:   4. Stage resolved file: git add .claude/settings.local.json
 545:   5. Complete merge: git commit -m "Merge PRP-D: Resolve settings conflict"
 546: 
 547: Pausing batch execution until conflict resolved.
 548: ```
 549: 
 550: **If `--continue-on-error` flag set**:
 551: - Skip failed PRP merge
 552: - Log conflict details
 553: - Continue with remaining PRPs
 554: 
 555: ### 8. Cleanup (Sequential)
 556: **Time**: ~5-10 seconds
 557: 
 558: ```bash
 559: # Remove worktrees
 560: git worktree remove ../ctx-eng-plus-prp-a
 561: git worktree remove ../ctx-eng-plus-prp-b
 562: git worktree remove ../ctx-eng-plus-prp-c
 563: 
 564: # Prune stale references
 565: git worktree prune
 566: 
 567: # Update context (sync PRPs with codebase)
 568: cd tools && uv run ce update-context
 569: ```
 570: 
 571: **Output**:
 572: ```
 573: ðŸ§¹ Cleanup complete
 574:   âœ“ Removed 3 worktrees
 575:   âœ“ Pruned stale references
 576:   âœ“ Updated context (drift: 3.2%)
 577: ```
 578: 
 579: ## Options
 580: 
 581: | Flag | Description | Default |
 582: |------|-------------|---------|
 583: | `--batch <N>` | Execute all PRPs in batch N (stages sequentially) | `none` |
 584: | `--stage <N>` | Execute specific stage only (requires --batch) | `all` |
 585: | `--resume` | Resume from last checkpoint (requires --batch) | `false` |
 586: | `--no-auto-review` | Disable automatic execution review after each stage | `false` (review enabled by default for --batch) |
 587: | `--model <sonnet\|haiku\|opus>` | Model for subagents (overrides auto-selection) | `auto` |
 588: | `--max-parallel <N>` | Max concurrent PRPs within stage | `unlimited` |
 589: | `--dry-run` | Parse PRPs without execution | `false` |
 590: | `--continue-on-error` | Don't abort if one PRP fails | `false` |
 591: | `--no-merge` | Skip worktree merge step | `false` |
 592: | `--no-cleanup` | Keep worktrees after execution | `false` |
 593: | `--timeout <minutes>` | Max execution time per PRP | `60` |
 594: | `--health-check-interval <seconds>` | Health check frequency | `30` |
 595: | `--json` | Output results as JSON | `false` |
 596: 
 597: ## Model Selection Guidelines
 598: 
 599: **Default Behavior**: Automatic model assignment based on complexity analysis (see Step 1)
 600: 
 601: The batch executor analyzes each PRP and assigns the optimal model:
 602: - **Haiku**: Score <40 (simple, single-file, <0.5h)
 603: - **Sonnet**: Score 40-69 (medium, multi-file, 0.5-2h)
 604: - **Opus**: Score â‰¥70 (complex, architectural, >2h)
 605: 
 606: ### When to Override with `--model` Flag
 607: 
 608: **Scenario 1: Force Haiku (maximize cost savings)**
 609: ```bash
 610: /batch-exe-prp --model haiku PRP-A PRP-B PRP-C
 611: ```
 612: 
 613: **Use when**:
 614: - All PRPs are simple (you've verified manually)
 615: - Cost is priority over execution quality
 616: - You're willing to manually fix if Haiku makes mistakes
 617: 
 618: **Risk**: Haiku may struggle with:
 619: - Ambiguous requirements (needs clarification)
 620: - Multi-file coordination (may miss dependencies)
 621: - Complex judgment calls (doc structure, error handling)
 622: 
 623: **Example**: 3 simple PRPs (JSON edits, straightforward docs)
 624: - Auto (Haiku: 2, Sonnet: 1): ~$0.05
 625: - Forced Haiku: ~$0.03 (40% cheaper, but higher risk)
 626: 
 627: **Scenario 2: Force Sonnet (reliability priority)**
 628: ```bash
 629: /batch-exe-prp --model sonnet PRP-A PRP-B PRP-C
 630: ```
 631: 
 632: **Use when**:
 633: - Some PRPs auto-assigned Haiku, but you want consistency
 634: - You don't trust Haiku with your codebase
 635: - Budget allows, prefer reliability
 636: 
 637: **Benefit**: Higher success rate, fewer escalations, better judgment
 638: 
 639: **Example**: 3 mixed PRPs (auto: Haiku: 2, Sonnet: 1)
 640: - Auto: ~$0.05 (optimal)
 641: - Forced Sonnet: ~$0.25 (5x cost, minimal quality gain for simple PRPs)
 642: 
 643: **Scenario 3: Force Opus (critical changes)**
 644: ```bash
 645: /batch-exe-prp --model opus PRP-D PRP-E
 646: ```
 647: 
 648: **Use when**:
 649: - Database migrations, security patches, infrastructure changes
 650: - Confidence score must be 10/10
 651: - Rollback would be costly/dangerous
 652: 
 653: **Cost**: ~$1.50-3.00 per PRP (10-15x more than Sonnet)
 654: 
 655: ### Recommendation
 656: 
 657: **Trust auto-selection** unless you have specific reasons:
 658: - âœ… Optimizes cost/quality trade-off per PRP
 659: - âœ… Analyzes complexity objectively (not based on gut feeling)
 660: - âœ… Saves 50-80% vs all-Sonnet (typical batch)
 661: 
 662: **Override only when**:
 663: - ðŸ”§ You've manually reviewed all PRPs and disagree with assignments
 664: - ðŸ”§ You have budget constraints (force Haiku)
 665: - ðŸ”§ You have reliability requirements (force Sonnet/Opus)
 666: 
 667: ## Monitoring Protocol
 668: 
 669: ### Real-Time Status Updates
 670: 
 671: **Agent Health Signals** (every 30 seconds):
 672: ```
 673: HEALTH:OK                           # Agent running normally
 674: HEALTH:ERROR:timeout                # Validation timeout
 675: HEALTH:ERROR:import_error:foo.py    # Import error detected
 676: ```
 677: 
 678: **Progress Updates** (every phase completion):
 679: ```
 680: STATUS:PHASE_COMPLETE:1/3           # Phase 1 of 3 done
 681: STATUS:VALIDATION:L2                # Running L2 validation
 682: STATUS:SELF_HEAL:attempt_2          # Self-healing (attempt 2/3)
 683: ```
 684: 
 685: **Completion Signals**:
 686: ```
 687: STATUS:COMPLETE:10/10               # Success (confidence: 10/10)
 688: STATUS:FAILED:L3_timeout            # Failed at L3 validation
 689: STATUS:PARTIAL:2/3                  # Partial (2 of 3 phases done)
 690: ```
 691: 
 692: ### Error Aggregation
 693: 
 694: All errors logged to `.ce/batch-execution-{timestamp}.log`:
 695: 
 696: ```
 697: [2025-10-29 10:45:23] PRP-A | ERROR | L2 | ImportError: No module named 'foo'
 698: [2025-10-29 10:45:25] PRP-A | HEAL  | L2 | Added import foo at line 12
 699: [2025-10-29 10:45:30] PRP-A | OK    | L2 | Passed after 1 self-heal
 700: [2025-10-29 10:52:18] PRP-B | ERROR | L4 | Drift score 35% exceeds threshold
 701: [2025-10-29 10:52:20] PRP-B | ABORT | L4 | User acceptance required
 702: ```
 703: 
 704: ### Escalation Triggers
 705: 
 706: **Immediate Escalation** (pause batch execution):
 707: 1. **Security Error**: CVE detected, credentials exposed
 708: 2. **Critical Failure**: Agent crashed, worktree corrupted
 709: 3. **Merge Conflict**: Automatic merge failed (see Conflict Resolution)
 710: 4. **Drift Spike**: L4 drift >30% (user acceptance required)
 711: 5. **Resource Exhaustion**: Disk full, memory exhausted
 712: 
 713: **Deferred Escalation** (complete batch, report at end):
 714: 1. **Low Confidence**: Confidence score <7/10
 715: 2. **Self-Heal Excessive**: >5 self-healing attempts
 716: 3. **Partial Success**: Some phases completed, others skipped
 717: 
 718: ## Conflict Resolution
 719: 
 720: When merge conflicts occur:
 721: 
 722: **Step 1: Detect Conflict**
 723: ```bash
 724: git merge prp-d-command-perms --no-ff
 725: # Auto-merging .claude/settings.local.json
 726: # CONFLICT (content): Merge conflict in .claude/settings.local.json
 727: # Automatic merge failed; fix conflicts and then commit the result.
 728: ```
 729: 
 730: **Step 2: Analyze Conflict**
 731: ```python
 732: # Read conflicted file
 733: Read(file_path=".claude/settings.local.json")
 734: # Look for conflict markers: <<<<<<< HEAD, =======, >>>>>>>
 735: ```
 736: 
 737: **Step 3: Resolution Strategy**
 738: 
 739: **Option A: Keep Both Changes** (most common)
 740: ```python
 741: Edit(
 742:   file_path=".claude/settings.local.json",
 743:   old_string="""<<<<<<< HEAD
 744:   "deny": ["tool-a", "tool-b"]
 745: =======
 746:   "deny": ["tool-c", "tool-d"]
 747: >>>>>>> prp-d-command-perms""",
 748:   new_string="""  "deny": ["tool-a", "tool-b", "tool-c", "tool-d"]"""
 749: )
 750: ```
 751: 
 752: **Option B: Prefer Incoming** (last-merged wins)
 753: ```python
 754: Edit(
 755:   file_path=".claude/settings.local.json",
 756:   old_string="""<<<<<<< HEAD
 757:   "allow": ["Bash(git:*)"]
 758: =======
 759:   "allow": ["Bash(gh:*)"]
 760: >>>>>>> prp-d-command-perms""",
 761:   new_string="""  "allow": ["Bash(gh:*)"]"""  # Prefer incoming
 762: )
 763: ```
 764: 
 765: **Option C: Manual Decision** (conflicting logic)
 766: - User chooses which change to keep
 767: - Update PRP priority/dependency order
 768: - Re-run batch with adjusted order
 769: 
 770: **Step 4: Complete Merge**
 771: ```bash
 772: git add .claude/settings.local.json
 773: git commit -m "Merge PRP-D: Resolve settings conflict (kept both)"
 774: ```
 775: 
 776: ## Error Handling
 777: 
 778: ### Agent Failures
 779: 
 780: **Scenario 1: Agent Timeout**
 781: ```
 782: âŒ PRP-B agent timeout (60 minutes exceeded)
 783: Last status: L3 integration tests (running for 45m)
 784: 
 785: Actions taken:
 786:   1. Marked PRP-B as FAILED
 787:   2. Preserved worktree: ../ctx-eng-plus-prp-b
 788:   3. Logged partial results to .ce/batch-execution.log
 789: 
 790: Manual intervention required:
 791:   cd ../ctx-eng-plus-prp-b
 792:   # Review partial work, continue manually if needed
 793: ```
 794: 
 795: **Resolution**:
 796: - Review agent log for bottleneck
 797: - Increase timeout: `--timeout 90`
 798: - Split PRP into smaller phases
 799: 
 800: **Scenario 2: Agent Crash**
 801: ```
 802: âŒ PRP-C agent crashed (unexpected error)
 803: Stack trace: [shows error details]
 804: 
 805: Actions taken:
 806:   1. Marked PRP-C as FAILED
 807:   2. Worktree preserved for debugging
 808:   3. Other agents continue execution
 809: 
 810: Manual intervention:
 811:   Review stack trace for root cause
 812:   Check if bug in /execute-prp logic
 813:   Re-run individually: /execute-prp PRP-C
 814: ```
 815: 
 816: ### Validation Failures
 817: 
 818: **Scenario 3: L4 Drift Threshold Exceeded**
 819: ```
 820: âš ï¸ PRP-B validation failed: L4 drift 35% (threshold: 30%)
 821: 
 822: User decision required:
 823:   1. [A]ccept drift (update threshold in PRP)
 824:   2. [R]eject and rollback (restore to checkpoint)
 825:   3. [M]anually fix and re-validate
 826: 
 827: Pausing batch execution for user input...
 828: ```
 829: 
 830: **With `--continue-on-error`**:
 831: - Mark PRP-B as PARTIAL
 832: - Skip to next PRP (C)
 833: - Report failure in aggregate results
 834: 
 835: ### Checkpoint & Resume
 836: 
 837: **Purpose**: Save execution state after each stage for recovery from interruptions, failures, or manual pauses.
 838: 
 839: **Checkpoint Format**
 840: 
 841: **Location**: `.ce/tmp/batch-execution-{batch_id}.json`
 842: 
 843: **Example**: `.ce/tmp/batch-execution-34.json`
 844: 
 845: ```json
 846: {
 847:   "batch_id": 34,
 848:   "started": "2025-11-05T10:00:00Z",
 849:   "updated": "2025-11-05T11:30:00Z",
 850:   "master_plan": "PRPs/feature-requests/PRP-34-INITIAL.md",
 851:   "total_stages": 4,
 852:   "last_stage_completed": 2,
 853:   "current_stage": 3,
 854:   "status": "IN_PROGRESS",
 855:   "stages": {
 856:     "1": {
 857:       "status": "COMPLETED",
 858:       "started": "2025-11-05T10:00:00Z",
 859:       "completed": "2025-11-05T10:25:00Z",
 860:       "prps": ["PRP-34.1.1"],
 861:       "results": {
 862:         "PRP-34.1.1": {
 863:           "status": "SUCCESS",
 864:           "phases_completed": 4,
 865:           "confidence_score": 10,
 866:           "commit_hash": "ab3118f",
 867:           "execution_time": "18m 32s"
 868:         }
 869:       },
 870:       "review_status": "PASSED",
 871:       "review_findings": [],
 872:       "merged": true,
 873:       "merge_commits": ["920edd4"]
 874:     },
 875:     "2": {
 876:       "status": "COMPLETED",
 877:       "started": "2025-11-05T10:26:00Z",
 878:       "completed": "2025-11-05T11:12:00Z",
 879:       "prps": ["PRP-34.2.1", "PRP-34.2.2", "PRP-34.2.3", "PRP-34.2.4", "PRP-34.2.5", "PRP-34.2.6"],
 880:       "results": { /* 6 PRP results */ },
 881:       "review_status": "PASSED_WITH_FIXES",
 882:       "review_findings": [
 883:         "Minor: Fixed typo in classification.py line 45",
 884:         "Minor: Added missing type hint in blending.py line 102"
 885:       ],
 886:       "merged": true,
 887:       "merge_commits": ["9af5bcc", "b53e184", "c64f295", "d75e3a6", "e86f4b7", "f97g5c8"]
 888:     },
 889:     "3": {
 890:       "status": "IN_PROGRESS",
 891:       "started": "2025-11-05T11:15:00Z",
 892:       "completed": null,
 893:       "prps": ["PRP-34.3.1", "PRP-34.3.2", "PRP-34.3.3"],
 894:       "results": null,
 895:       "review_status": "PENDING",
 896:       "review_findings": [],
 897:       "merged": false,
 898:       "merge_commits": []
 899:     },
 900:     "4": {
 901:       "status": "PENDING",
 902:       "started": null,
 903:       "completed": null,
 904:       "prps": ["PRP-34.4.1"],
 905:       "results": null,
 906:       "review_status": "PENDING",
 907:       "review_findings": [],
 908:       "merged": false,
 909:       "merge_commits": []
 910:     }
 911:   },
 912:   "aggregate_metrics": {
 913:     "prps_completed": 7,
 914:     "prps_total": 11,
 915:     "stages_completed": 2,
 916:     "total_execution_time": "72m 18s",
 917:     "total_self_heals": 3,
 918:     "review_fixes_applied": 2
 919:   }
 920: }
 921: ```
 922: 
 923: ### When Checkpoints Are Created
 924: 
 925: **Checkpoint triggers** (automatic):
 926: 1. **After stage execution completes** (before review)
 927: 2. **After execution review completes** (with review results)
 928: 3. **After stage merge completes** (with merge commits)
 929: 4. **On manual interruption** (Ctrl+C, user abort)
 930: 5. **On error** (agent failure, validation failure, conflict)
 931: 
 932: **Checkpoint updates** (incremental):
 933: - Only changed fields updated
 934: - Preserves history of completed stages
 935: - Timestamp updated on every write
 936: 
 937: ### Resume Behavior
 938: 
 939: **Automatic detection**:
 940: ```bash
 941: # Resume from last checkpoint
 942: /batch-exe-prp --batch 34 --resume
 943: ```
 944: 
 945: **Resume workflow**:
 946: 1. **Read checkpoint**: `.ce/tmp/batch-execution-34.json`
 947: 2. **Validate checkpoint**:
 948:    - Check batch ID matches
 949:    - Verify master plan exists
 950:    - Confirm PRPs still exist
 951: 3. **Determine resume point**:
 952:    - `current_stage`: Stage that was executing when interrupted
 953:    - `last_stage_completed`: Last fully completed stage
 954:    - Resume from: `last_stage_completed + 1` OR `current_stage` (if partially done)
 955: 4. **Skip completed stages**:
 956:    - Stages 1-2: COMPLETED â†’ Skip
 957:    - Stage 3: IN_PROGRESS â†’ Check if PRPs executed, review pending
 958:    - Stage 4: PENDING â†’ Execute normally
 959: 5. **Resume execution**:
 960:    - If stage 3 PRPs all executed â†’ Run review only
 961:    - If stage 3 PRPs partially executed â†’ Resume execution, then review
 962:    - If stage 3 not started â†’ Execute from beginning
 963: 
 964: **Resume scenarios**:
 965: 
 966: **Scenario 1: Interrupted during execution**
 967: ```
 968: Status: Stage 2 executing, interrupted by Ctrl+C
 969: Checkpoint: stage 2 IN_PROGRESS, last_stage_completed = 1
 970: 
 971: Resume behavior:
 972:   1. Skip Stage 1 (already merged)
 973:   2. Check Stage 2 worktrees for partial progress
 974:   3. If PRPs executed: Run review, merge
 975:   4. If PRPs not executed: Re-execute Stage 2 from start
 976:   5. Continue to Stage 3
 977: ```
 978: 
 979: **Scenario 2: Paused during review (critical issue found)**
 980: ```
 981: Status: Stage 2 execution complete, review found critical issue
 982: Checkpoint: stage 2 COMPLETED (execution), review_status = FAILED
 983: 
 984: Resume behavior:
 985:   1. Skip Stage 1 (already merged)
 986:   2. Show Stage 2 review findings
 987:   3. Ask user: [F]ix manually and continue, [R]e-review, [S]kip stage
 988:   4. If fixed: Re-run review on Stage 2
 989:   5. If passed: Merge Stage 2, continue to Stage 3
 990: ```
 991: 
 992: **Scenario 3: Merge conflict**
 993: ```
 994: Status: Stage 3 execution complete, review passed, merge conflict on PRP-34.3.2
 995: Checkpoint: stage 3 COMPLETED (execution + review), merged = false (conflict)
 996: 
 997: Resume behavior:
 998:   1. Skip Stages 1-2 (already merged)
 999:   2. Detect unresolved merge conflict in Stage 3
1000:   3. Show conflict files, prompt user to resolve
1001:   4. After resolution: Complete Stage 3 merge
1002:   5. Continue to Stage 4
1003: ```
1004: 
1005: **Resume output**:
1006: ```
1007: ðŸ“‚ Resuming batch 34 from checkpoint
1008: ============================================================
1009: Checkpoint: .ce/tmp/batch-execution-34.json
1010: Last updated: 2025-11-05T11:30:00Z (15 minutes ago)
1011: 
1012: Progress:
1013:   âœ… Stage 1: Completed (1 PRP, merged)
1014:   âœ… Stage 2: Completed (6 PRPs, merged)
1015:   â¸ï¸  Stage 3: Execution completed, review pending
1016:   â³ Stage 4: Not started
1017: 
1018: Resuming from: Stage 3 execution review
1019: ============================================================
1020: 
1021: Running execution review for Stage 3...
1022: /batch-peer-review --batch 34 --exe --stage 3
1023: ```
1024: 
1025: **Checkpoint Cleanup**
1026: 
1027: **Automatic cleanup** (on successful batch completion):
1028: - Checkpoint kept for 7 days
1029: - Moved to: `.ce/tmp/batch-execution-archive/batch-34-{timestamp}.json`
1030: 
1031: **Manual cleanup**:
1032: ```bash
1033: # Remove checkpoint (abort batch, can't resume)
1034: rm .ce/tmp/batch-execution-34.json
1035: 
1036: # Archive checkpoint (preserve history)
1037: mkdir -p .ce/tmp/batch-execution-archive
1038: mv .ce/tmp/batch-execution-34.json .ce/tmp/batch-execution-archive/
1039: ```
1040: 
1041: ## Performance Metrics
1042: 
1043: ### Sequential vs Parallel
1044: 
1045: **3 PRPs (15m each)**:
1046: - Sequential: 45 minutes
1047: - Parallel (3 agents): ~18 minutes (60% faster)
1048: - Parallel (Haiku): ~15 minutes (67% faster, 90% cheaper)
1049: 
1050: **6 PRPs (15m each)**:
1051: - Sequential: 90 minutes
1052: - Parallel (unlimited): ~20 minutes (78% faster)
1053: - Parallel (--max-parallel 3): ~35 minutes (61% faster)
1054: 
1055: ### Resource Usage
1056: 
1057: **Per Agent** (Sonnet):
1058: - CPU: ~30-50% (1 core)
1059: - Memory: ~500MB-1GB
1060: - Tokens: ~10k-30k per PRP
1061: 
1062: **Total** (3 agents):
1063: - CPU: ~100-150% (1.5 cores)
1064: - Memory: ~1.5-3GB
1065: - Tokens: ~30k-90k total
1066: 
1067: **Recommendation**: Limit to 3-4 parallel agents on typical laptop (4-core, 16GB RAM)
1068: 
1069: ## Output Formats
1070: 
1071: ### Standard Output (default)
1072: 
1073: Human-readable dashboard with progress bars, health status, and aggregate report (shown in sections 4-5 above).
1074: 
1075: ### JSON Output (`--json`)
1076: 
1077: ```json
1078: {
1079:   "success": true,
1080:   "prps_total": 3,
1081:   "prps_succeeded": 3,
1082:   "prps_failed": 0,
1083:   "prps_partial": 0,
1084:   "execution_time": "18m 42s",
1085:   "estimated_time": "45m",
1086:   "time_savings": "26m 18s (58%)",
1087:   "model": "sonnet",
1088:   "max_parallel": 3,
1089:   "results": {
1090:     "PRP-A": {
1091:       "prp_id": "PRP-A",
1092:       "status": "SUCCESS",
1093:       "phases_completed": 3,
1094:       "phases_total": 3,
1095:       "confidence_score": 10,
1096:       "validation_results": {
1097:         "L1": {"passed": true, "attempts": 1},
1098:         "L2": {"passed": true, "attempts": 1},
1099:         "L3": {"passed": true, "attempts": 1},
1100:         "L4": {"passed": true, "drift_score": 4.2}
1101:       },
1102:       "self_heals": 0,
1103:       "commit_hash": "ab3118f",
1104:       "execution_time": "6m 12s",
1105:       "files_modified": [".claude/settings.local.json"],
1106:       "errors": []
1107:     },
1108:     "PRP-B": { /* similar structure */ },
1109:     "PRP-C": { /* similar structure */ }
1110:   },
1111:   "aggregate_metrics": {
1112:     "total_phases": 9,
1113:     "total_phases_completed": 9,
1114:     "avg_confidence_score": 9.7,
1115:     "total_self_heals": 1,
1116:     "total_errors": 0
1117:   },
1118:   "merge_status": {
1119:     "success": true,
1120:     "conflicts": [],
1121:     "commits": [
1122:       "920edd4 Merge PRP-A: Tool Deny List Implementation",
1123:       "9af5bcc Merge PRP-B: Tool Usage Guide Creation",
1124:       "b53e184 Merge PRP-C: GitButler to Worktree Migration"
1125:     ]
1126:   },
1127:   "cleanup_status": {
1128:     "worktrees_removed": 3,
1129:     "context_drift": 3.2
1130:   }
1131: }
1132: ```
1133: 
1134: ## Common Workflows
1135: 
1136: ### Workflow 1: Full Batch Execution (Recommended)
1137: 
1138: Execute entire batch with automatic quality gates:
1139: 
1140: ```bash
1141: # Execute all stages sequentially with auto-review (default)
1142: /batch-exe-prp --batch 34
1143: 
1144: # What happens:
1145: # 1. Discovers all PRP-34.*.* files
1146: # 2. Groups by stage (1, 2, 3, 4)
1147: # 3. For each stage:
1148: #    a. Execute PRPs in parallel
1149: #    b. Run execution review (auto-fix minor issues)
1150: #    c. Merge worktrees
1151: #    d. Save checkpoint
1152: # 4. If interrupted: resume with --resume flag
1153: 
1154: # Time: ~60-90 minutes (for 11-PRP batch)
1155: # Quality: High (peer review after each stage)
1156: ```
1157: 
1158: ### Workflow 2: Stage-by-Stage Manual Execution
1159: 
1160: Execute one stage at a time with manual validation:
1161: 
1162: ```bash
1163: # Stage 1: Foundation
1164: /batch-exe-prp --batch 34 --stage 1
1165: # Manual test, validate output
1166: 
1167: # Stage 2: Core modules (6 PRPs in parallel)
1168: /batch-exe-prp --batch 34 --stage 2
1169: # Manual test, validate integration
1170: 
1171: # Stage 3: Domain strategies
1172: /batch-exe-prp --batch 34 --stage 3
1173: 
1174: # Stage 4: Integration
1175: /batch-exe-prp --batch 34 --stage 4
1176: ```
1177: 
1178: ### Workflow 3: Resume from Interruption
1179: 
1180: Continue batch execution after pause/error:
1181: 
1182: ```bash
1183: # Scenario: Stage 2 execution review found critical issue
1184: # You fixed it manually, now resume
1185: 
1186: # Resume from checkpoint (skips completed stages)
1187: /batch-exe-prp --batch 34 --resume
1188: 
1189: # Reads checkpoint: .ce/tmp/batch-execution-34.json
1190: # Determines: Stage 1 completed, Stage 2 needs re-review
1191: # Runs: Stage 2 execution review â†’ merge â†’ Stage 3 â†’ Stage 4
1192: ```
1193: 
1194: ### Workflow 4: Individual PRPs (Legacy Mode)
1195: 
1196: Execute specific PRPs without batch-awareness:
1197: 
1198: ```bash
1199: # Extract stage-1 PRPs manually
1200: # Execute in parallel
1201: /batch-exe-prp PRP-34.1.1
1202: 
1203: # After completion, proceed to Stage 2
1204: /batch-exe-prp PRP-34.2.1 PRP-34.2.2 PRP-34.2.3 PRP-34.2.4 PRP-34.2.5 PRP-34.2.6
1205: 
1206: # Note: No auto-review, no checkpointing, manual merge management
1207: ```
1208: 
1209: ### Workflow 5: Cost-Optimized Execution
1210: 
1211: Use Haiku for simple PRPs, Sonnet for complex:
1212: 
1213: ```bash
1214: # Force Haiku for simple batch (if auto-selection assigns Sonnet)
1215: /batch-exe-prp --batch 34 --model haiku
1216: 
1217: # Cost savings: ~70% vs all-Sonnet
1218: # Trade-off: Lower quality, may need manual fixes
1219: ```
1220: 
1221: ### Workflow 6: Batch without Auto-Review
1222: 
1223: Execute batch quickly without peer review (risky):
1224: 
1225: ```bash
1226: # Disable auto-review (not recommended)
1227: /batch-exe-prp --batch 34 --no-auto-review
1228: 
1229: # Use when:
1230: # - Already peer-reviewed PRPs manually
1231: # - Low-risk changes (docs, comments)
1232: # - Time-critical deployment
1233: 
1234: # Risk: May merge code with quality issues
1235: ```
1236: 
1237: ### Workflow 7: Dry Run + Review
1238: 
1239: Preview batch execution plan before committing:
1240: 
1241: ```bash
1242: # Dry run: Show stages, PRPs, model assignments
1243: /batch-exe-prp --batch 34 --dry-run
1244: 
1245: # Output:
1246: # - Stage breakdown (which PRPs in each stage)
1247: # - Model assignments (Haiku vs Sonnet per PRP)
1248: # - Estimated time and cost
1249: # - Dependency graph
1250: 
1251: # Review output, adjust PRP complexities if needed
1252: 
1253: # Execute for real
1254: /batch-exe-prp --batch 34
1255: ```
1256: 
1257: ## Troubleshooting
1258: 
1259: ### Issue: "Worktree path conflicts"
1260: 
1261: **Symptom**: Pre-flight validation fails with "worktree path already exists"
1262: 
1263: **Cause**: Previous worktree not cleaned up
1264: 
1265: **Solution**:
1266: ```bash
1267: git worktree list  # Check existing worktrees
1268: git worktree remove ../ctx-eng-plus-prp-a
1269: git worktree prune
1270: ```
1271: 
1272: ### Issue: "Agent stalled (no health signal)"
1273: 
1274: **Symptom**: Agent shows "STALLED" after 2+ minutes
1275: 
1276: **Cause**: Long-running validation (e.g., integration tests)
1277: 
1278: **Solution**: Increase health check interval
1279: ```bash
1280: /batch-exe-prp --health-check-interval 60 PRP-A PRP-B
1281: ```
1282: 
1283: ### Issue: "All agents failed immediately"
1284: 
1285: **Symptom**: All agents marked FAILED within seconds
1286: 
1287: **Cause**: MCP server disconnected or permission denied
1288: 
1289: **Solution**:
1290: ```bash
1291: # Check MCP servers
1292: mcp__syntropy__healthcheck(detailed=True)
1293: 
1294: # Reconnect if needed
1295: /mcp
1296: 
1297: # Verify permissions in .claude/settings.local.json
1298: ```
1299: 
1300: ### Issue: "Merge conflicts on every PRP"
1301: 
1302: **Symptom**: Every merge attempt results in conflicts
1303: 
1304: **Cause**: PRPs modifying same file sections, wrong merge order
1305: 
1306: **Solution**:
1307: 1. Review PRP YAML headers: check `files_modified` and `conflict_potential`
1308: 2. Adjust `merge_order` to sequence conflicting PRPs
1309: 3. Re-run batch with adjusted order
1310: 4. Consider splitting conflicting PRPs into separate batches
1311: 
1312: ### Issue: "Batch not found" or "No PRPs discovered"
1313: 
1314: **Symptom**: `/batch-exe-prp --batch 34` reports "No PRPs found for batch 34"
1315: 
1316: **Cause**: PRPs not in expected location or naming format
1317: 
1318: **Solution**:
1319: ```bash
1320: # Check if master plan exists
1321: ls PRPs/feature-requests/PRP-34-INITIAL.md
1322: 
1323: # Check if batch PRPs exist
1324: ls PRPs/feature-requests/PRP-34.*.md
1325: 
1326: # Verify naming format: PRP-34.1.1.md, PRP-34.2.1.md, etc.
1327: # NOT: PRP-34-1-1.md or PRP34.1.1.md
1328: ```
1329: 
1330: ### Issue: "Checkpoint corrupted" or "Invalid checkpoint format"
1331: 
1332: **Symptom**: `/batch-exe-prp --batch 34 --resume` fails to read checkpoint
1333: 
1334: **Cause**: Checkpoint JSON malformed or manually edited
1335: 
1336: **Solution**:
1337: ```bash
1338: # Validate checkpoint JSON
1339: cat .ce/tmp/batch-execution-34.json | jq .
1340: 
1341: # If invalid: Start fresh (lose resume capability)
1342: rm .ce/tmp/batch-execution-34.json
1343: /batch-exe-prp --batch 34
1344: 
1345: # If valid but wrong state: Manually fix checkpoint
1346: # Example: Set last_stage_completed to correct value
1347: ```
1348: 
1349: ### Issue: "Execution review paused" with no clear reason
1350: 
1351: **Symptom**: Batch pauses after stage execution, says "review found critical issues" but doesn't show what
1352: 
1353: **Cause**: Review findings not displayed, need to check checkpoint
1354: 
1355: **Solution**:
1356: ```bash
1357: # Read checkpoint to see review findings
1358: cat .ce/tmp/batch-execution-34.json | jq '.stages["2"].review_findings'
1359: 
1360: # Output: ["Critical: Logic error in classification.py line 45"]
1361: 
1362: # Fix issue manually, then resume
1363: vim tools/ce/blending/classification.py
1364: # Fix line 45
1365: 
1366: # Resume (will re-run review)
1367: /batch-exe-prp --batch 34 --resume
1368: ```
1369: 
1370: ### Issue: "Stage already completed but PRPs not merged"
1371: 
1372: **Symptom**: Stage marked COMPLETED in checkpoint but changes not in main branch
1373: 
1374: **Cause**: Merge step failed or was skipped, checkpoint not updated
1375: 
1376: **Solution**:
1377: ```bash
1378: # Check if worktrees still exist
1379: git worktree list
1380: 
1381: # If worktrees exist: Manually merge
1382: git checkout main
1383: git merge prp-34-2-1 --no-ff
1384: git merge prp-34-2-2 --no-ff
1385: # ... (merge all stage PRPs)
1386: 
1387: # Update checkpoint manually
1388: # Set stages.2.merged = true
1389: 
1390: # Resume to next stage
1391: /batch-exe-prp --batch 34 --resume
1392: ```
1393: 
1394: ## CLI Command
1395: 
1396: ```bash
1397: # From project root
1398: cd tools
1399: 
1400: # Individual PRPs
1401: uv run ce batch-exe [options] <prp-1> <prp-2> ...
1402: 
1403: # Batch-aware execution
1404: uv run ce batch-exe --batch <N> [options]
1405: 
1406: # Options (same as slash command)
1407: --batch <N>              # Execute all PRPs in batch N
1408: --stage <N>              # Execute specific stage only
1409: --resume                 # Resume from checkpoint
1410: --no-auto-review         # Disable auto-review (not recommended)
1411: --model <sonnet|haiku|opus>  # Override auto-selection
1412: --max-parallel <N>       # Limit parallelism within stage
1413: --dry-run                # Preview execution plan
1414: --continue-on-error      # Don't abort on failure
1415: --no-merge               # Skip merge step
1416: --no-cleanup             # Keep worktrees
1417: --timeout <minutes>      # Max execution time per PRP
1418: --json                   # JSON output
1419: ```
1420: 
1421: ## Implementation Details
1422: 
1423: - **Module**: `tools/ce/batch_execute.py` (to be implemented)
1424: - **Tests**: `tools/tests/test_batch_execute.py` (integration tests)
1425: - **Dependencies**: `execute.py` (PRP execution logic), `core.py` (git operations)
1426: - **Agent Launch**: Uses Task tool with `subagent_type="general-purpose"`
1427: - **Monitoring**: Polling agent output every 30 seconds for health signals
1428: - **Error Recovery**: Preserves worktrees on failure for debugging
1429: 
1430: ### Checkpoint Implementation Requirements
1431: 
1432: **Critical**: When implementing `--batch` mode, checkpoint handling is MANDATORY:
1433: 
1434: 1. **Checkpoint Creation** (after each stage):
1435:    ```python
1436:    import json
1437:    from pathlib import Path
1438: 
1439:    checkpoint_path = Path(".ce/tmp/batch-execution-{batch_id}.json")
1440:    checkpoint_path.parent.mkdir(parents=True, exist_ok=True)
1441: 
1442:    checkpoint = {
1443:        "batch_id": batch_id,
1444:        "started": start_time.isoformat(),
1445:        "updated": datetime.now(timezone.utc).isoformat(),
1446:        "master_plan": master_plan_path,
1447:        "total_stages": len(stages),
1448:        "last_stage_completed": completed_stage_num,
1449:        "current_stage": current_stage_num,
1450:        "status": "IN_PROGRESS",  # or "COMPLETED", "FAILED", "PAUSED"
1451:        "stages": { ... }
1452:    }
1453: 
1454:    with open(checkpoint_path, 'w') as f:
1455:        json.dump(checkpoint, f, indent=2)
1456:    ```
1457: 
1458: 2. **Checkpoint Resume** (on `--resume` flag):
1459:    ```python
1460:    checkpoint_path = Path(f".ce/tmp/batch-execution-{batch_id}.json")
1461: 
1462:    if not checkpoint_path.exists():
1463:        print(f"âŒ No checkpoint found: {checkpoint_path}")
1464:        print(f"Start fresh: /batch-exe-prp --batch {batch_id}")
1465:        return
1466: 
1467:    with open(checkpoint_path) as f:
1468:        checkpoint = json.load(f)
1469: 
1470:    # Validate checkpoint
1471:    if checkpoint["batch_id"] != batch_id:
1472:        raise ValueError(f"Checkpoint batch ID mismatch")
1473: 
1474:    # Determine resume point
1475:    last_completed = checkpoint["last_stage_completed"]
1476:    resume_from_stage = last_completed + 1
1477: 
1478:    # Skip completed stages, resume from next
1479:    for stage_num in range(resume_from_stage, checkpoint["total_stages"] + 1):
1480:        execute_stage(stage_num)
1481:    ```
1482: 
1483: 3. **Auto-Review Integration** (after stage execution):
1484:    ```python
1485:    # After stage N execution completes
1486:    if not args.no_auto_review:
1487:        print(f"\nðŸ” Running execution review for Stage {stage_num}...")
1488: 
1489:        # Call /batch-peer-review internally via SlashCommand tool
1490:        review_result = SlashCommand(
1491:            command=f"/batch-peer-review --batch {batch_id} --exe --stage {stage_num}"
1492:        )
1493: 
1494:        # Update checkpoint with review results
1495:        checkpoint["stages"][str(stage_num)]["review_status"] = review_result.status
1496:        checkpoint["stages"][str(stage_num)]["review_findings"] = review_result.findings
1497: 
1498:        # Handle review failures
1499:        if review_result.status == "FAILED":
1500:            print(f"âŒ Stage {stage_num} execution review FAILED")
1501:            print(f"Findings: {review_result.findings}")
1502:            print(f"\nPausing batch execution.")
1503:            print(f"Fix issues and resume: /batch-exe-prp --batch {batch_id} --resume")
1504: 
1505:            checkpoint["status"] = "PAUSED"
1506:            save_checkpoint(checkpoint)
1507:            return  # Exit, don't proceed to next stage
1508: 
1509:        # Minor issues auto-fixed, continue
1510:        if review_result.fixes_applied:
1511:            print(f"âœ… Auto-fixed {len(review_result.fixes_applied)} minor issues")
1512:    ```
1513: 
1514: 4. **Stage Discovery** (for `--batch` mode):
1515:    ```python
1516:    def discover_batch_prps(batch_id):
1517:        """Find all PRPs in batch, group by stage"""
1518: 
1519:        # Find master plan
1520:        master_plan = Path(f"PRPs/feature-requests/PRP-{batch_id}-INITIAL.md")
1521:        if not master_plan.exists():
1522:            raise FileNotFoundError(f"Master plan not found: {master_plan}")
1523: 
1524:        # Find all batch PRPs: PRP-34.1.1.md, PRP-34.2.1.md, etc.
1525:        prp_files = list(Path("PRPs/feature-requests").glob(f"PRP-{batch_id}.*.*.md"))
1526: 
1527:        # Parse YAML headers, extract stage field
1528:        stages = {}
1529:        for prp_file in prp_files:
1530:            yaml_header = parse_yaml_header(prp_file)
1531:            stage = yaml_header.get("stage", 1)  # Default to stage 1
1532: 
1533:            if stage not in stages:
1534:                stages[stage] = []
1535:            stages[stage].append(prp_file)
1536: 
1537:        # Sort stages
1538:        return dict(sorted(stages.items()))
1539:    ```
1540: 
1541: 5. **Checkpoint Cleanup** (on success):
1542:    ```python
1543:    # After all stages complete successfully
1544:    checkpoint["status"] = "COMPLETED"
1545:    save_checkpoint(checkpoint)
1546: 
1547:    # Archive checkpoint
1548:    archive_dir = Path(".ce/tmp/batch-execution-archive")
1549:    archive_dir.mkdir(parents=True, exist_ok=True)
1550: 
1551:    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
1552:    archive_path = archive_dir / f"batch-{batch_id}-{timestamp}.json"
1553: 
1554:    checkpoint_path.rename(archive_path)
1555:    print(f"âœ… Checkpoint archived: {archive_path}")
1556:    ```
1557: 
1558: ## Related Commands
1559: 
1560: - `/execute-prp <prp>` - Execute single PRP
1561: - `/generate-prp <initial>` - Generate PRP with parallel metadata
1562: - `/peer-review exe <prp>` - Review executed PRP quality
1563: - `ce prp restore <prp-id> [phase]` - Rollback PRP to checkpoint
1564: 
1565: ## Security Notes
1566: 
1567: 1. **Isolation**: Each agent works in isolated worktree, cannot affect main repo
1568: 2. **Permissions**: Agents inherit same permissions as main session
1569: 3. **Secrets**: Never log credentials or API keys in batch execution logs
1570: 4. **Rollback**: All changes committed to branches, easy rollback via `git reset`
1571: 
1572: ## Future Enhancements
1573: 
1574: - **Auto-retry**: Retry failed PRPs with adjusted parameters
1575: - **Smart scheduling**: Prioritize high-conflict PRPs first
1576: - **Resource management**: Auto-throttle based on CPU/memory usage
1577: - **Dependency resolution**: Parse PRP dependencies, auto-sequence
1578: - **Web UI**: Real-time dashboard with progress graphs
</file>

<file path=".claude/commands/vacuum.md">
  1: # Vacuum
  2: 
  3: Clean up project noise: temp files, obsolete docs, unreferenced code, orphaned tests, dead links, commented code blocks.
  4: 
  5: ## Usage
  6: ```bash
  7: /vacuum [--execute|--force|--auto|--nuclear] [--exclude-strategy STRATEGY] [--min-confidence N]
  8: ```
  9: 
 10: ## Modes
 11: 
 12: **Default (dry-run)**: Generate report only, no deletions
 13: ```bash
 14: /vacuum
 15: ```
 16: 
 17: **Execute mode**: Delete HIGH confidence items only (100% safe: temp files, backups)
 18: ```bash
 19: /vacuum --execute
 20: ```
 21: 
 22: **Force/Auto mode**: Delete HIGH + MEDIUM confidence items (includes obsolete docs, orphan tests, dead links)
 23: ```bash
 24: /vacuum --force
 25: # or
 26: /vacuum --auto
 27: ```
 28: 
 29: **Nuclear mode**: Delete ALL items including LOW confidence (unreferenced code, commented blocks) - requires confirmation
 30: ```bash
 31: /vacuum --nuclear
 32: ```
 33: 
 34: ## Parameters
 35: 
 36: - `--execute`: Delete HIGH confidence items (â‰¥100%)
 37: - `--force`: Delete MEDIUM + HIGH confidence items (â‰¥60%)
 38: - `--auto`: Automatically delete MEDIUM + HIGH confidence items (same as --force)
 39: - `--nuclear`: Delete ALL items including LOW confidence (<60%) - requires "yes" confirmation
 40: - `--min-confidence N`: Set custom confidence threshold (0-100)
 41: - `--exclude-strategy STRATEGY`: Skip specific strategy (use multiple times for multiple strategies)
 42: 
 43: ## Strategies
 44: 
 45: ### 1. temp-files (HIGH: 100%)
 46: - `*.pyc`, `__pycache__/`, `.DS_Store`, `*.swp`, `.pytest_cache/`, `*.log`, `*.tmp`
 47: - **Auto-delete with --execute**
 48: 
 49: ### 2. backup-files (HIGH: 100%)
 50: - `*.bak`, `*~`, `*.orig`, `*.rej` (git merge artifacts)
 51: - **Auto-delete with --execute**
 52: 
 53: ### 3. obsolete-docs (MEDIUM: 70%)
 54: **Filename Patterns**:
 55: - Versioned docs: `*-v1.md`, `*-old.md`, `*-deprecated.md`
 56: - Temporary analysis docs: `ANALYSIS-*`, `CHANGELIST-*`, `REPORT-*`, `IMPLEMENTATION-*`, `DEPLOYMENT*`
 57: - Planning docs: `*-PLAN.md`, `*-REPLAN.md`, `*-SUMMARY-*.md`, `*_SOLUTION.md`
 58: - Root-level all-caps files (e.g., `VERSION` - with exceptions for LICENSE, MAKEFILE)
 59: 
 60: **Content Analysis** (reads first 20 lines):
 61: - Status markers: "**Status**: PENDING", "**Status**: READY FOR EXECUTION"
 62: - Execution tracking: "**Completed**:", "**Remaining Work**:"
 63: - Planning markers: "## ðŸ“‹ EXECUTIVE SUMMARY", "## ðŸŽ¯ EXECUTION SUMMARY"
 64: - Solution patterns: "**Problem**:", "**Solution**:", "## Solution Options"
 65: - Date indicators: "**Date**: 2025-10-*" (time-bound docs)
 66: - Workflow markers: "**Workflow:**", "Source Plan:", "This plan addresses"
 67: 
 68: **Protected Patterns**:
 69: - `.ce/**`, `.claude/**`, `.serena/**` (framework/config dirs)
 70: - `syntropy-mcp/**` (MCP server)
 71: - `tmp/finalizing/**` (work-in-progress)
 72: - `PRPs/**/*.md` with YAML headers (real PRPs)
 73: 
 74: - **Delete with --force/--auto**
 75: 
 76: ### 4. orphan-tests (MEDIUM: 60%)
 77: - `test_foo.py` where `foo.py` doesn't exist
 78: - **Delete with --force/--auto**
 79: 
 80: ### 5. unreferenced-code (LOW: 40%)
 81: - Python files where:
 82:   - ALL definitions (functions/classes) are unreferenced elsewhere
 83:   - AND the file itself is not imported by any other module
 84: - Uses Serena MCP for symbol analysis (~15s)
 85: - **Manual review only** (use --nuclear with caution)
 86: - Common false positives: CLI entry points, scripts meant to be run directly
 87: 
 88: ### 6. commented-code (LOW: 30%)
 89: - Commented code blocks â‰¥20 lines
 90: - Excludes docstrings, license headers, teaching examples
 91: - **Manual review only**
 92: 
 93: ## Safety Mechanisms
 94: 
 95: **NEVER_DELETE Paths**:
 96: - `.ce/**` - Framework boilerplate
 97: - `.claude/**` - Claude Code configuration (all commands, settings)
 98: - `.serena/**` - Serena memories and configuration
 99: - `syntropy-mcp/**` - Syntropy MCP server directory
100: - `tmp/**` files < 2 days old - Recent work-in-progress (older files deleted)
101: - `PRPs/**/*.md` - All PRP files with YAML headers (managed by update-context)
102: - `pyproject.toml`, `README.md`, `CLAUDE.md`, `WARP.md`, `VERSION`
103: - `examples/**` - Pattern documentation
104: - `**/__init__.py`, `**/cli.py`, `**/__main__.py` - Entry points
105: 
106: **Note**: Analysis files (CHANGELIST-*, ANALYSIS-*, REPORT-*, IMPLEMENTATION-*) are **NOT** protected and can be cleaned up if obsolete, as they're temporary artifacts of PRP execution.
107: 
108: **Protection Rules**:
109: 1. Files modified in last 30 days get lower confidence scores
110: 2. Files referenced in markdown docs are flagged
111: 3. Files with git activity are protected
112: 4. Dry-run by default (explicit --execute required)
113: 
114: ## Output
115: 
116: **Report**: `.ce/vacuum-report.md`
117: 
118: Sections:
119: 1. **Summary**: Candidate count, bytes reclaimable, confidence breakdown
120: 2. **HIGH Confidence**: Safe to delete (path, reason, size, last modified)
121: 3. **MEDIUM Confidence**: Review recommended (path, reason, confidence, git history)
122: 4. **LOW Confidence**: Manual verification required (path, reason, confidence, references)
123: 
124: ## Examples
125: 
126: **Basic cleanup** (temp files + backups):
127: ```bash
128: /vacuum --execute
129: ```
130: 
131: **Deep cleanup** (temp files + backups + obsolete docs + orphan tests):
132: ```bash
133: /vacuum --force
134: ```
135: 
136: **Skip slow strategy**:
137: ```bash
138: /vacuum --exclude-strategy unreferenced-code
139: ```
140: 
141: **Custom threshold** (only delete 80%+ confidence):
142: ```bash
143: /vacuum --execute --min-confidence 80
144: ```
145: 
146: **Review only** (generate report without deleting):
147: ```bash
148: /vacuum
149: # Or explicitly:
150: cd tools && uv run ce vacuum --dry-run
151: ```
152: 
153: ## Exit Codes
154: 
155: - `0`: No candidates found (clean)
156: - `1`: Candidates found (check report)
157: - `2`: Error occurred
158: 
159: ## Performance
160: 
161: **Parallel execution** for fast strategies (temp-files, backup-files, obsolete-docs, orphan-tests, commented-code):
162: - ~5 seconds total
163: 
164: **Sequential execution** for slow strategy (unreferenced-code via Serena):
165: - ~15 seconds
166: 
167: **Total time**: ~20 seconds for all strategies
168: 
169: **Skip slow strategy** for quick cleanup:
170: ```bash
171: /vacuum --execute --exclude-strategy unreferenced-code
172: # ~5 seconds
173: ```
174: 
175: ## Workflow Integration
176: 
177: **After /update-context**:
178: ```bash
179: # 1. Sync context
180: /update-context
181: 
182: # 2. Clean up project
183: /vacuum --execute
184: 
185: # 3. Review report
186: cat .ce/vacuum-report.md
187: ```
188: 
189: **Before commits**:
190: ```bash
191: # Clean temp files before committing
192: /vacuum --execute
193: git add .
194: git commit -m "Implement feature X"
195: ```
196: 
197: ## Troubleshooting
198: 
199: **"Strategy not found"**: Check spelling, available strategies:
200: - `temp-files`, `backup-files`, `obsolete-docs`, `unreferenced-code`, `orphan-tests`, `commented-code`
201: 
202: **"Serena unavailable"**: unreferenced-code strategy will skip gracefully, other strategies run normally
203: 
204: **"Permission denied"**: File in use or protected by OS - check .ce/vacuum-report.md for details
205: 
206: **False positive**: File flagged incorrectly? Add to PROTECTED_PATTERNS in `tools/ce/vacuum_strategies/base.py` or use lower confidence threshold
207: 
208: ## Advanced Usage
209: 
210: **Review LOW confidence candidates** before nuclear mode:
211: ```bash
212: # 1. Generate report
213: /vacuum
214: 
215: # 2. Review .ce/vacuum-report.md LOW confidence section
216: 
217: # 3. If satisfied, run nuclear mode
218: /vacuum --nuclear
219: # (type "yes" to confirm)
220: ```
221: 
222: **Chain with git commit**:
223: ```bash
224: /vacuum --execute && git add . && git commit -m "Clean up project noise"
225: ```
226: 
227: ## Goal
228: 
229: Maintain clean project state by identifying and safely removing noise while preserving all vital content through confidence-based safety mechanisms.
</file>

<file path="tools/ce/__main__.py">
  1: """Context Engineering CLI - Main entry point.
  2: 
  3: This module provides the CLI interface with argparse configuration.
  4: All command handlers are delegated to cli_handlers module for better organization.
  5: """
  6: 
  7: import argparse
  8: import sys
  9: 
 10: from . import __version__
 11: from .cli_handlers import (
 12:     cmd_validate,
 13:     cmd_git,
 14:     cmd_context,
 15:     cmd_drift,
 16:     cmd_run_py,
 17:     cmd_prp_validate,
 18:     cmd_prp_generate,
 19:     cmd_prp_execute,
 20:     cmd_prp_analyze,
 21:     cmd_pipeline_validate,
 22:     cmd_pipeline_render,
 23:     cmd_metrics,
 24:     cmd_analyze_context,
 25:     cmd_update_context,
 26:     cmd_vacuum,
 27:     cmd_blend,
 28:     cmd_cleanup,
 29:     cmd_init_project,
 30: )
 31: 
 32: 
 33: def main():
 34:     """Main CLI entry point."""
 35:     parser = argparse.ArgumentParser(
 36:         description="Context Engineering CLI Tools",
 37:         formatter_class=argparse.RawDescriptionHelpFormatter,
 38:         epilog="""
 39: Examples:
 40:   ce validate --level all
 41:   ce git status
 42:   ce git checkpoint "Phase 1 complete"
 43:   ce context sync
 44:   ce context health --json
 45:   ce run_py "print('hello')"
 46:   ce run_py "x = [1,2,3]; print(sum(x))"
 47:   ce run_py tmp/script.py
 48:   ce run_py --code "import sys; print(sys.version)"
 49:   ce run_py --file tmp/script.py --args "--input data.csv"
 50:         """
 51:     )
 52: 
 53:     parser.add_argument("--version", action="version", version=f"ce {__version__}")
 54: 
 55:     subparsers = parser.add_subparsers(dest="command", help="Command to execute")
 56: 
 57:     # === VALIDATE COMMAND ===
 58:     validate_parser = subparsers.add_parser(
 59:         "validate",
 60:         help="Run validation gates"
 61:     )
 62:     validate_parser.add_argument(
 63:         "--level",
 64:         choices=["1", "2", "3", "4", "all"],
 65:         default="all",
 66:         help="Validation level (1=lint/type, 2=unit tests, 3=integration, 4=pattern conformance, all=all levels)"
 67:     )
 68:     validate_parser.add_argument(
 69:         "--prp",
 70:         help="Path to PRP file (required for level 4)"
 71:     )
 72:     validate_parser.add_argument(
 73:         "--files",
 74:         help="Comma-separated list of implementation files (for level 4, optional - auto-detected if not provided)"
 75:     )
 76:     validate_parser.add_argument(
 77:         "--json",
 78:         action="store_true",
 79:         help="Output as JSON"
 80:     )
 81: 
 82:     # === GIT COMMAND ===
 83:     git_parser = subparsers.add_parser(
 84:         "git",
 85:         help="Git operations"
 86:     )
 87:     git_parser.add_argument(
 88:         "action",
 89:         choices=["status", "checkpoint", "diff"],
 90:         help="Git action to perform"
 91:     )
 92:     git_parser.add_argument(
 93:         "--message",
 94:         help="Checkpoint message (for checkpoint action)"
 95:     )
 96:     git_parser.add_argument(
 97:         "--since",
 98:         help="Git ref for diff (default: HEAD~5)"
 99:     )
100:     git_parser.add_argument(
101:         "--json",
102:         action="store_true",
103:         help="Output as JSON"
104:     )
105: 
106:     # === CONTEXT COMMAND ===
107:     context_parser = subparsers.add_parser(
108:         "context",
109:         help="Context management"
110:     )
111:     context_parser.add_argument(
112:         "action",
113:         choices=["sync", "health", "prune", "pre-sync", "post-sync", "auto-sync"],
114:         help="Context action to perform"
115:     )
116:     # Common flags
117:     context_parser.add_argument(
118:         "--json",
119:         action="store_true",
120:         help="Output as JSON"
121:     )
122:     # For health action
123:     context_parser.add_argument(
124:         "--verbose",
125:         action="store_true",
126:         help="Verbose health report with component breakdown (for health)"
127:     )
128:     # For prune action
129:     context_parser.add_argument(
130:         "--age",
131:         type=int,
132:         help="Age in days for pruning (default: 7, for prune)"
133:     )
134:     context_parser.add_argument(
135:         "--dry-run",
136:         action="store_true",
137:         help="Dry run mode (for prune)"
138:     )
139:     # For pre-sync action
140:     context_parser.add_argument(
141:         "--force",
142:         action="store_true",
143:         help="Skip drift abort check (for pre-sync, dangerous)"
144:     )
145:     # For post-sync action
146:     context_parser.add_argument(
147:         "--prp-id",
148:         help="PRP identifier (for post-sync)"
149:     )
150:     context_parser.add_argument(
151:         "--skip-cleanup",
152:         action="store_true",
153:         help="Skip cleanup protocol (for post-sync)"
154:     )
155:     # For auto-sync action
156:     auto_sync_group = context_parser.add_mutually_exclusive_group()
157:     auto_sync_group.add_argument(
158:         "--enable",
159:         action="store_true",
160:         help="Enable auto-sync mode (for auto-sync)"
161:     )
162:     auto_sync_group.add_argument(
163:         "--disable",
164:         action="store_true",
165:         help="Disable auto-sync mode (for auto-sync)"
166:     )
167:     auto_sync_group.add_argument(
168:         "--status",
169:         action="store_true",
170:         help="Check auto-sync status (for auto-sync)"
171:     )
172: 
173:     # === DRIFT COMMAND ===
174:     drift_parser = subparsers.add_parser(
175:         "drift",
176:         help="Drift history tracking and analysis"
177:     )
178:     drift_parser.add_argument(
179:         "action",
180:         choices=["history", "show", "summary", "compare"],
181:         help="Drift action to perform"
182:     )
183:     drift_parser.add_argument(
184:         "--last",
185:         type=int,
186:         help="Show last N decisions (for history)"
187:     )
188:     drift_parser.add_argument(
189:         "--prp-id",
190:         help="Filter by PRP ID (for history/show)"
191:     )
192:     drift_parser.add_argument(
193:         "--prp-id2",
194:         help="Second PRP ID (for compare)"
195:     )
196:     drift_parser.add_argument(
197:         "--action-filter",
198:         choices=["accepted", "rejected", "examples_updated"],
199:         help="Filter by action type (for history)"
200:     )
201:     drift_parser.add_argument(
202:         "--json",
203:         action="store_true",
204:         help="Output as JSON"
205:     )
206: 
207:     # === RUN_PY COMMAND ===
208:     runpy_parser = subparsers.add_parser(
209:         "run_py",
210:         help="Execute Python code (auto-detect or explicit mode)"
211:     )
212:     runpy_group = runpy_parser.add_mutually_exclusive_group(required=False)
213:     runpy_group.add_argument(
214:         "input",
215:         nargs="?",
216:         help="Auto-detect: code (â‰¤3 LOC) or file path (tmp/*.py)"
217:     )
218:     runpy_parser.add_argument(
219:         "--code",
220:         help="Explicit: Ad-hoc Python code (max 3 LOC)"
221:     )
222:     runpy_parser.add_argument(
223:         "--file",
224:         help="Explicit: Path to Python file in tmp/ folder"
225:     )
226:     runpy_parser.add_argument(
227:         "--args",
228:         dest="script_args",
229:         help="Arguments to pass to Python script"
230:     )
231:     runpy_parser.add_argument(
232:         "--json",
233:         action="store_true",
234:         help="Output execution summary as JSON"
235:     )
236: 
237:     # === PRP COMMAND ===
238:     prp_parser = subparsers.add_parser(
239:         "prp", help="PRP management commands"
240:     )
241:     prp_subparsers = prp_parser.add_subparsers(dest="prp_command", required=True)
242: 
243:     # prp validate subcommand
244:     prp_validate_parser = prp_subparsers.add_parser(
245:         "validate", help="Validate PRP YAML header"
246:     )
247:     prp_validate_parser.add_argument(
248:         "file", help="Path to PRP markdown file"
249:     )
250:     prp_validate_parser.add_argument(
251:         "--json", action="store_true", help="Output as JSON"
252:     )
253: 
254:     # prp generate subcommand
255:     prp_generate_parser = prp_subparsers.add_parser(
256:         "generate", help="Generate PRP from INITIAL.md"
257:     )
258:     prp_generate_parser.add_argument(
259:         "initial_md", help="Path to INITIAL.md file"
260:     )
261:     prp_generate_parser.add_argument(
262:         "-o", "--output",
263:         help="Output directory for PRP (default: PRPs/feature-requests)"
264:     )
265:     prp_generate_parser.add_argument(
266:         "--json", action="store_true", help="Output as JSON"
267:     )
268:     prp_generate_parser.add_argument(
269:         "--join-prp",
270:         help="Update existing PRP's Linear issue (PRP number, ID like 'PRP-12', or file path)"
271:     )
272:     prp_generate_parser.add_argument(
273:         "--use-thinking",
274:         action="store_true",
275:         default=True,
276:         help="Use sequential thinking for analysis (default: True)"
277:     )
278:     prp_generate_parser.add_argument(
279:         "--no-thinking",
280:         dest="use_thinking",
281:         action="store_false",
282:         help="Disable sequential thinking (use heuristics)"
283:     )
284: 
285:     # prp execute subcommand
286:     prp_execute_parser = prp_subparsers.add_parser(
287:         "execute", help="Execute PRP implementation"
288:     )
289:     prp_execute_parser.add_argument(
290:         "prp_id", help="PRP identifier (e.g., PRP-4)"
291:     )
292:     prp_execute_parser.add_argument(
293:         "--start-phase", type=int, help="Start from specific phase"
294:     )
295:     prp_execute_parser.add_argument(
296:         "--end-phase", type=int, help="End at specific phase"
297:     )
298:     prp_execute_parser.add_argument(
299:         "--skip-validation", action="store_true", help="Skip validation loops"
300:     )
301:     prp_execute_parser.add_argument(
302:         "--dry-run", action="store_true", help="Parse blueprint only, don't execute"
303:     )
304:     prp_execute_parser.add_argument(
305:         "--json", action="store_true", help="Output as JSON"
306:     )
307: 
308:     # prp analyze subcommand
309:     prp_analyze_parser = prp_subparsers.add_parser(
310:         "analyze", help="Analyze PRP size and complexity"
311:     )
312:     prp_analyze_parser.add_argument(
313:         "file", help="Path to PRP markdown file"
314:     )
315:     prp_analyze_parser.add_argument(
316:         "--json", action="store_true", help="Output as JSON"
317:     )
318: 
319:     # === PIPELINE COMMAND ===
320:     pipeline_parser = subparsers.add_parser(
321:         "pipeline", help="CI/CD pipeline management commands"
322:     )
323:     pipeline_subparsers = pipeline_parser.add_subparsers(dest="pipeline_command", required=True)
324: 
325:     # pipeline validate subcommand
326:     pipeline_validate_parser = pipeline_subparsers.add_parser(
327:         "validate", help="Validate abstract pipeline definition"
328:     )
329:     pipeline_validate_parser.add_argument(
330:         "pipeline_file", help="Path to abstract pipeline YAML file"
331:     )
332: 
333:     # pipeline render subcommand
334:     pipeline_render_parser = pipeline_subparsers.add_parser(
335:         "render", help="Render abstract pipeline to platform-specific format"
336:     )
337:     pipeline_render_parser.add_argument(
338:         "pipeline_file", help="Path to abstract pipeline YAML file"
339:     )
340:     pipeline_render_parser.add_argument(
341:         "--executor", type=str, choices=["github-actions", "mock"],
342:         default="github-actions", help="Platform executor to use"
343:     )
344:     pipeline_render_parser.add_argument(
345:         "-o", "--output", help="Output file path"
346:     )
347: 
348:     # === METRICS COMMAND ===
349:     metrics_parser = subparsers.add_parser(
350:         "metrics",
351:         help="Display system metrics and success rates"
352:     )
353:     metrics_parser.add_argument(
354:         "--format",
355:         choices=["text", "json"],
356:         default="text",
357:         help="Output format (default: text)"
358:     )
359:     metrics_parser.add_argument(
360:         "--file",
361:         default="metrics.json",
362:         help="Path to metrics file (default: metrics.json)"
363:     )
364: 
365:     # === ANALYZE-CONTEXT COMMAND ===
366:     analyze_context_parser = subparsers.add_parser(
367:         "analyze-context",
368:         aliases=["analyse-context"],
369:         help="Analyze context drift without updating metadata (fast check for CI/CD)"
370:     )
371:     analyze_context_parser.add_argument(
372:         "--json",
373:         action="store_true",
374:         help="Output JSON for scripting"
375:     )
376:     analyze_context_parser.add_argument(
377:         "--force",
378:         action="store_true",
379:         help="Force re-analysis, bypass cache"
380:     )
381:     analyze_context_parser.add_argument(
382:         "--cache-ttl",
383:         type=int,
384:         help="Cache TTL in minutes (default: from config or 5)"
385:     )
386: 
387:     # === UPDATE-CONTEXT COMMAND ===
388:     update_context_parser = subparsers.add_parser(
389:         "update-context",
390:         help="Sync CE/Serena with codebase changes"
391:     )
392:     update_context_parser.add_argument(
393:         "--prp",
394:         help="Target specific PRP file (path relative to project root)"
395:     )
396:     update_context_parser.add_argument(
397:         "--remediate",
398:         action="store_true",
399:         help="Auto-remediate drift violations (YOLO mode - skips approval)"
400:     )
401:     update_context_parser.add_argument(
402:         "--json",
403:         action="store_true",
404:         help="Output as JSON"
405:     )
406: 
407:     # === VACUUM COMMAND ===
408:     vacuum_parser = subparsers.add_parser(
409:         "vacuum",
410:         help="Clean up project noise (temp files, obsolete docs, unreferenced code)"
411:     )
412:     vacuum_parser.add_argument(
413:         "--dry-run",
414:         action="store_true",
415:         default=True,
416:         help="Generate report only (default)"
417:     )
418:     vacuum_parser.add_argument(
419:         "--execute",
420:         action="store_true",
421:         help="Delete HIGH confidence items (temp files, backups)"
422:     )
423:     vacuum_parser.add_argument(
424:         "--force",
425:         action="store_true",
426:         help="Delete HIGH + MEDIUM confidence items"
427:     )
428:     vacuum_parser.add_argument(
429:         "--auto",
430:         action="store_true",
431:         help="Automatically delete HIGH + MEDIUM confidence items (same as --force)"
432:     )
433:     vacuum_parser.add_argument(
434:         "--nuclear",
435:         action="store_true",
436:         help="Delete ALL items including LOW confidence (requires confirmation)"
437:     )
438:     vacuum_parser.add_argument(
439:         "--min-confidence",
440:         type=int,
441:         default=0,
442:         help="Minimum confidence threshold 0-100 (default: 0)"
443:     )
444:     vacuum_parser.add_argument(
445:         "--exclude-strategy",
446:         action="append",
447:         dest="exclude_strategies",
448:         help="Skip specific strategy (can be used multiple times)"
449:     )
450: 
451:     # === CLEANUP COMMAND ===
452:     cleanup_parser = subparsers.add_parser(
453:         "cleanup",
454:         help="Remove legacy directories after CE 1.1 migration"
455:     )
456:     cleanup_parser.add_argument(
457:         "--dry-run",
458:         action="store_true",
459:         default=True,
460:         help="Show what would be removed without deleting (default: True)"
461:     )
462:     cleanup_parser.add_argument(
463:         "--execute",
464:         action="store_true",
465:         default=False,
466:         help="Execute cleanup (remove legacy directories)"
467:     )
468: 
469:     # === BLEND COMMAND ===
470:     blend_parser = subparsers.add_parser(
471:         "blend",
472:         help="CE Framework Blending Tool - Migrate and blend framework files"
473:     )
474:     # Operation modes
475:     mode_group = blend_parser.add_mutually_exclusive_group(required=True)
476:     mode_group.add_argument('--all', action='store_true', help='Run all 4 phases')
477:     mode_group.add_argument('--phase', choices=['detect', 'classify', 'blend', 'cleanup'], help='Run specific phase')
478:     mode_group.add_argument('--cleanup-only', action='store_true', help='Run cleanup only')
479:     mode_group.add_argument('--rollback', action='store_true', help='Restore backups')
480:     # Domain selection (optional)
481:     blend_parser.add_argument('--domain', help='Blend specific domain only (settings, claude_md, memories, examples, prps, commands)')
482:     # Behavior flags
483:     blend_parser.add_argument('--dry-run', action='store_true', help='Show what would be done without executing')
484:     blend_parser.add_argument('--interactive', action='store_true', help='Ask before each phase')
485:     blend_parser.add_argument('--skip-cleanup', action='store_true', help='Skip Phase D (keep legacy dirs)')
486:     blend_parser.add_argument('--fast', action='store_true', help='Fast mode (Haiku only, skip expensive ops)')
487:     blend_parser.add_argument('--quality', action='store_true', help='Quality mode (Sonnet for all LLM calls)')
488:     blend_parser.add_argument('--scan', action='store_true', help='Scan mode (detect + classify only, no blending)')
489:     # Configuration
490:     blend_parser.add_argument('--config', default='.ce/blend-config.yml', help='Path to blend config (default: .ce/blend-config.yml)')
491:     blend_parser.add_argument('--target-dir', default='.', help='Target project directory (default: current)')
492:     # Debugging
493:     blend_parser.add_argument('-v', '--verbose', action='store_true', help='Verbose output')
494: 
495:     # === INIT-PROJECT COMMAND ===
496:     init_project_parser = subparsers.add_parser(
497:         "init-project",
498:         help="Initialize CE Framework in a target project"
499:     )
500:     init_project_parser.add_argument(
501:         "target_dir",
502:         help="Target directory for CE Framework initialization"
503:     )
504:     init_project_parser.add_argument(
505:         "--dry-run",
506:         action="store_true",
507:         help="Show what would be done without executing"
508:     )
509:     init_project_parser.add_argument(
510:         "--phase",
511:         choices=["extract", "blend", "initialize", "verify", "all"],
512:         default="all",
513:         help="Run specific initialization phase (default: all)"
514:     )
515:     init_project_parser.add_argument(
516:         "--blend-only",
517:         action="store_true",
518:         help="Skip extraction phase (for re-initialization)"
519:     )
520: 
521:     # Parse arguments
522:     args = parser.parse_args()
523: 
524:     if not args.command:
525:         parser.print_help()
526:         return 0
527: 
528:     # Execute command
529:     if args.command == "validate":
530:         return cmd_validate(args)
531:     elif args.command == "git":
532:         return cmd_git(args)
533:     elif args.command == "context":
534:         return cmd_context(args)
535:     elif args.command == "drift":
536:         return cmd_drift(args)
537:     elif args.command == "run_py":
538:         return cmd_run_py(args)
539:     elif args.command == "prp":
540:         if args.prp_command == "validate":
541:             return cmd_prp_validate(args)
542:         elif args.prp_command == "generate":
543:             return cmd_prp_generate(args)
544:         elif args.prp_command == "execute":
545:             return cmd_prp_execute(args)
546:         elif args.prp_command == "analyze":
547:             return cmd_prp_analyze(args)
548:     elif args.command == "pipeline":
549:         if args.pipeline_command == "validate":
550:             return cmd_pipeline_validate(args)
551:         elif args.pipeline_command == "render":
552:             return cmd_pipeline_render(args)
553:     elif args.command == "metrics":
554:         return cmd_metrics(args)
555:     elif args.command in ["analyze-context", "analyse-context"]:
556:         return cmd_analyze_context(args)
557:     elif args.command == "update-context":
558:         return cmd_update_context(args)
559:     elif args.command == "vacuum":
560:         return cmd_vacuum(args)
561:     elif args.command == "cleanup":
562:         return cmd_cleanup(args)
563:     elif args.command == "blend":
564:         return cmd_blend(args)
565:     elif args.command == "init-project":
566:         return cmd_init_project(args)
567:     else:
568:         print(f"Unknown command: {args.command}", file=sys.stderr)
569:         return 1
570: 
571: 
572: if __name__ == "__main__":
573:     sys.exit(main())
</file>

<file path="tools/ce/cli_handlers.py">
   1: """CLI command handlers with delegation pattern.
   2: 
   3: Extracted from __main__.py to reduce nesting depth and improve maintainability.
   4: Each handler follows KISS principle with max 4 nesting levels.
   5: """
   6: 
   7: import sys
   8: import json
   9: from typing import Any, Dict
  10: 
  11: from .core import git_status, git_checkpoint, git_diff, run_py
  12: from .validate import validate_level_1, validate_level_2, validate_level_3, validate_level_4, validate_all
  13: from .context import (
  14:     sync, health, prune,
  15:     pre_generation_sync, post_execution_sync,
  16:     context_health_verbose, drift_report_markdown,
  17:     enable_auto_sync, disable_auto_sync, get_auto_sync_status
  18: )
  19: from .generate import generate_prp
  20: from .drift import (
  21:     get_drift_history,
  22:     drift_summary,
  23:     show_drift_decision,
  24:     compare_drift_decisions
  25: )
  26: from .pipeline import load_abstract_pipeline, validate_pipeline
  27: from .executors.github_actions import GitHubActionsExecutor
  28: from .executors.mock import MockExecutor
  29: from .metrics import MetricsCollector
  30: from .update_context import sync_context
  31: from .blend import run_blend as blend_run_blend
  32: from .blending.cleanup import cleanup_legacy_dirs
  33: 
  34: # Conditional import for init_project (implemented in PRP-36.2.2)
  35: try:
  36:     from .init_project import ProjectInitializer
  37:     _HAS_INIT_PROJECT = True
  38: except ImportError:
  39:     _HAS_INIT_PROJECT = False
  40: 
  41: 
  42: def format_output(data: Dict[str, Any], as_json: bool = False) -> str:
  43:     """Format output for display.
  44: 
  45:     Args:
  46:         data: Data to format
  47:         as_json: If True, return JSON string
  48: 
  49:     Returns:
  50:         Formatted string
  51:     """
  52:     if as_json:
  53:         return json.dumps(data, indent=2)
  54: 
  55:     # Human-readable format
  56:     lines = []
  57:     for key, value in data.items():
  58:         if isinstance(value, list):
  59:             lines.append(f"{key}:")
  60:             for item in value:
  61:                 lines.append(f"  - {item}")
  62:         elif isinstance(value, dict):
  63:             lines.append(f"{key}:")
  64:             for k, v in value.items():
  65:                 lines.append(f"  {k}: {v}")
  66:         else:
  67:             lines.append(f"{key}: {value}")
  68: 
  69:     return "\n".join(lines)
  70: 
  71: 
  72: # === VALIDATE COMMAND ===
  73: 
  74: def cmd_validate(args) -> int:
  75:     """Execute validate command."""
  76:     try:
  77:         if args.level == "1":
  78:             result = validate_level_1()
  79:         elif args.level == "2":
  80:             result = validate_level_2()
  81:         elif args.level == "3":
  82:             result = validate_level_3()
  83:         elif args.level == "4":
  84:             if not args.prp:
  85:                 print("âŒ Level 4 validation requires --prp argument", file=sys.stderr)
  86:                 return 1
  87: 
  88:             files = None
  89:             if args.files:
  90:                 files = [f.strip() for f in args.files.split(",")]
  91: 
  92:             result = validate_level_4(prp_path=args.prp, implementation_paths=files)
  93:         else:  # "all"
  94:             result = validate_all()
  95: 
  96:         print(format_output(result, args.json))
  97:         return 0 if result["success"] else 1
  98: 
  99:     except Exception as e:
 100:         print(f"âŒ Validation failed: {str(e)}", file=sys.stderr)
 101:         return 1
 102: 
 103: 
 104: # === GIT COMMAND ===
 105: 
 106: def cmd_git(args) -> int:
 107:     """Execute git command."""
 108:     try:
 109:         if args.action == "status":
 110:             result = git_status()
 111:             print(format_output(result, args.json))
 112:             return 0
 113: 
 114:         if args.action == "checkpoint":
 115:             message = args.message or "Context Engineering checkpoint"
 116:             checkpoint_id = git_checkpoint(message)
 117:             result = {"checkpoint_id": checkpoint_id, "message": message}
 118:             print(format_output(result, args.json))
 119:             return 0
 120: 
 121:         if args.action == "diff":
 122:             since = args.since or "HEAD~5"
 123:             files = git_diff(since=since, name_only=True)
 124:             result = {"changed_files": files, "count": len(files), "since": since}
 125:             print(format_output(result, args.json))
 126:             return 0
 127: 
 128:         print(f"Unknown git action: {args.action}", file=sys.stderr)
 129:         return 1
 130: 
 131:     except Exception as e:
 132:         print(f"âŒ Git operation failed: {str(e)}", file=sys.stderr)
 133:         return 1
 134: 
 135: 
 136: # === CONTEXT COMMAND (delegated) ===
 137: 
 138: def _handle_context_sync(args) -> int:
 139:     """Handle context sync action."""
 140:     result = sync()
 141:     print(format_output(result, args.json))
 142:     return 0
 143: 
 144: 
 145: def _handle_context_health(args) -> int:
 146:     """Handle context health action."""
 147:     verbose = getattr(args, 'verbose', False)
 148: 
 149:     if verbose:
 150:         result = context_health_verbose()
 151:         if args.json:
 152:             print(format_output(result, True))
 153:         else:
 154:             print(drift_report_markdown())
 155:         return 0 if result["threshold"] != "critical" else 1
 156: 
 157:     result = health()
 158:     print(format_output(result, args.json))
 159: 
 160:     if not args.json:
 161:         print()
 162:         if result["healthy"]:
 163:             print("âœ… Context is healthy")
 164:         else:
 165:             print("âš ï¸  Context needs attention:")
 166:             for rec in result["recommendations"]:
 167:                 print(f"  â€¢ {rec}")
 168: 
 169:     return 0 if result["healthy"] else 1
 170: 
 171: 
 172: def _handle_context_prune(args) -> int:
 173:     """Handle context prune action."""
 174:     age = args.age or 7
 175:     dry_run = args.dry_run or False
 176:     result = prune(age_days=age, dry_run=dry_run)
 177:     print(format_output(result, args.json))
 178:     return 0
 179: 
 180: 
 181: def _handle_context_pre_sync(args) -> int:
 182:     """Handle context pre-sync action."""
 183:     force = getattr(args, 'force', False)
 184:     result = pre_generation_sync(force=force)
 185:     if args.json:
 186:         print(format_output(result, True))
 187:     else:
 188:         print(f"âœ… Pre-generation sync complete")
 189:         print(f"   Drift score: {result['drift_score']:.1f}%")
 190:         print(f"   Git clean: {result['git_clean']}")
 191:     return 0
 192: 
 193: 
 194: def _handle_context_post_sync(args) -> int:
 195:     """Handle context post-sync action."""
 196:     prp_id = getattr(args, 'prp_id', None)
 197:     if not prp_id:
 198:         print("âŒ post-sync requires --prp-id argument", file=sys.stderr)
 199:         return 1
 200: 
 201:     skip_cleanup = getattr(args, 'skip_cleanup', False)
 202:     result = post_execution_sync(prp_id, skip_cleanup=skip_cleanup)
 203:     if args.json:
 204:         print(format_output(result, True))
 205:     else:
 206:         print(f"âœ… Post-execution sync complete (PRP-{prp_id})")
 207:         print(f"   Cleanup: {result['cleanup_completed']}")
 208:         print(f"   Drift score: {result['drift_score']:.1f}%")
 209:         if result['final_checkpoint']:
 210:             print(f"   Checkpoint: {result['final_checkpoint']}")
 211:     return 0
 212: 
 213: 
 214: def _handle_context_auto_sync(args) -> int:
 215:     """Handle context auto-sync action."""
 216:     subaction = getattr(args, 'subaction', None)
 217: 
 218:     if subaction == "enable" or getattr(args, 'enable', False):
 219:         result = enable_auto_sync()
 220:         if args.json:
 221:             print(format_output(result, True))
 222:         else:
 223:             print(f"âœ… {result['mode'].title()}: Auto-sync enabled")
 224:             print(f"   Steps 2.5 and 6.5 will run automatically")
 225:         return 0
 226: 
 227:     if subaction == "disable" or getattr(args, 'disable', False):
 228:         result = disable_auto_sync()
 229:         if args.json:
 230:             print(format_output(result, True))
 231:         else:
 232:             print(f"âœ… {result['mode'].title()}: Auto-sync disabled")
 233:             print(f"   Manual sync required")
 234:         return 0
 235: 
 236:     if subaction == "status" or getattr(args, 'status', False):
 237:         result = get_auto_sync_status()
 238:         if args.json:
 239:             print(format_output(result, True))
 240:         else:
 241:             status_emoji = "âœ…" if result["enabled"] else "âŒ"
 242:             print(f"{status_emoji} {result['message']}")
 243:         return 0
 244: 
 245:     print("âŒ auto-sync requires --enable, --disable, or --status", file=sys.stderr)
 246:     return 1
 247: 
 248: 
 249: def cmd_context(args) -> int:
 250:     """Execute context command with delegation."""
 251:     handlers = {
 252:         "sync": _handle_context_sync,
 253:         "health": _handle_context_health,
 254:         "prune": _handle_context_prune,
 255:         "pre-sync": _handle_context_pre_sync,
 256:         "post-sync": _handle_context_post_sync,
 257:         "auto-sync": _handle_context_auto_sync,
 258:     }
 259: 
 260:     handler = handlers.get(args.action)
 261:     if not handler:
 262:         print(f"Unknown context action: {args.action}", file=sys.stderr)
 263:         return 1
 264: 
 265:     try:
 266:         return handler(args)
 267:     except Exception as e:
 268:         print(f"âŒ Context operation failed: {str(e)}", file=sys.stderr)
 269:         import traceback
 270:         traceback.print_exc()
 271:         return 1
 272: 
 273: 
 274: # === DRIFT COMMAND (delegated) ===
 275: 
 276: def _handle_drift_history(args) -> int:
 277:     """Handle drift history action."""
 278:     history = get_drift_history(
 279:         last_n=args.last,
 280:         prp_id=args.prp_id,
 281:         action_filter=args.action_filter
 282:     )
 283: 
 284:     if args.json:
 285:         print(format_output({"history": history}, True))
 286:         return 0
 287: 
 288:     if not history:
 289:         print("No drift decisions found")
 290:         return 0
 291: 
 292:     print("\nðŸ“Š DRIFT DECISION HISTORY\n")
 293:     print("â”" * 80)
 294:     print(f"{'PRP ID':<12} {'Score':<8} {'Action':<18} {'Reviewer':<12} {'Date':<20}")
 295:     print("â”" * 80)
 296: 
 297:     for h in history:
 298:         decision = h["drift_decision"]
 299:         prp_id = h["prp_id"]
 300:         score = decision["score"]
 301:         action = decision["action"]
 302:         reviewer = decision.get("reviewer", "unknown")
 303:         timestamp = decision.get("timestamp", "N/A")[:10]
 304: 
 305:         print(f"{prp_id:<12} {score:<8.2f} {action:<18} {reviewer:<12} {timestamp:<20}")
 306: 
 307:     print("â”" * 80)
 308:     print(f"\nTotal: {len(history)} decisions\n")
 309:     return 0
 310: 
 311: 
 312: def _handle_drift_show(args) -> int:
 313:     """Handle drift show action."""
 314:     if not args.prp_id:
 315:         print("âŒ show requires PRP ID argument", file=sys.stderr)
 316:         return 1
 317: 
 318:     decision = show_drift_decision(args.prp_id)
 319: 
 320:     if args.json:
 321:         print(format_output(decision, True))
 322:         return 0
 323: 
 324:     dd = decision["drift_decision"]
 325:     print(f"\nðŸ“‹ DRIFT DECISION: {decision['prp_id']}")
 326:     print(f"PRP: {decision['prp_name']}\n")
 327:     print(f"Score: {dd['score']:.2f}%")
 328:     print(f"Action: {dd['action']}")
 329:     print(f"Reviewer: {dd.get('reviewer', 'unknown')}")
 330:     print(f"Timestamp: {dd.get('timestamp', 'N/A')}\n")
 331: 
 332:     print("Justification:")
 333:     print(f"  {dd['justification']}\n")
 334: 
 335:     if "category_breakdown" in dd:
 336:         print("Category Breakdown:")
 337:         for cat, score in dd["category_breakdown"].items():
 338:             print(f"  â€¢ {cat}: {score:.2f}%")
 339:         print()
 340: 
 341:     return 0
 342: 
 343: 
 344: def _handle_drift_summary(args) -> int:
 345:     """Handle drift summary action."""
 346:     summary = drift_summary()
 347: 
 348:     if args.json:
 349:         print(format_output(summary, True))
 350:         return 0
 351: 
 352:     print("\nðŸ“Š DRIFT SUMMARY\n")
 353:     print("â”" * 60)
 354:     print(f"Total PRPs: {summary['total_prps']}")
 355:     print(f"PRPs with Drift: {summary['prps_with_drift']}")
 356:     print(f"Average Drift Score: {summary['avg_drift_score']:.2f}%\n")
 357: 
 358:     print("Decisions:")
 359:     for action, count in summary.get("decisions", {}).items():
 360:         print(f"  â€¢ {action}: {count}")
 361:     print()
 362: 
 363:     print("Score Distribution:")
 364:     dist = summary.get("score_distribution", {})
 365:     print(f"  â€¢ Low (0-10%): {dist.get('low', 0)}")
 366:     print(f"  â€¢ Medium (10-30%): {dist.get('medium', 0)}")
 367:     print(f"  â€¢ High (30%+): {dist.get('high', 0)}")
 368:     print()
 369: 
 370:     if summary.get("category_breakdown"):
 371:         print("Category Breakdown:")
 372:         for cat, data in summary["category_breakdown"].items():
 373:             print(f"  â€¢ {cat}: {data['avg']:.2f}% avg ({data['count']} PRPs)")
 374:         print()
 375: 
 376:     if summary.get("reviewer_breakdown"):
 377:         print("Reviewer Breakdown:")
 378:         for reviewer, count in summary["reviewer_breakdown"].items():
 379:             print(f"  â€¢ {reviewer}: {count}")
 380:         print()
 381: 
 382:     return 0
 383: 
 384: 
 385: def _handle_drift_compare(args) -> int:
 386:     """Handle drift compare action."""
 387:     if not args.prp_id or not args.prp_id2:
 388:         print("âŒ compare requires two PRP IDs", file=sys.stderr)
 389:         return 1
 390: 
 391:     comparison = compare_drift_decisions(args.prp_id, args.prp_id2)
 392: 
 393:     if args.json:
 394:         print(format_output(comparison, True))
 395:         return 0
 396: 
 397:     comp = comparison["comparison"]
 398:     prp1 = comparison["prp_1"]
 399:     prp2 = comparison["prp_2"]
 400: 
 401:     print(f"\nðŸ” DRIFT COMPARISON: {args.prp_id} vs {args.prp_id2}\n")
 402:     print("â”" * 60)
 403: 
 404:     print(f"\n{args.prp_id}:")
 405:     print(f"  Score: {prp1['drift_decision']['score']:.2f}%")
 406:     print(f"  Action: {prp1['drift_decision']['action']}")
 407: 
 408:     print(f"\n{args.prp_id2}:")
 409:     print(f"  Score: {prp2['drift_decision']['score']:.2f}%")
 410:     print(f"  Action: {prp2['drift_decision']['action']}")
 411: 
 412:     print(f"\nDifferences:")
 413:     print(f"  Score Difference: {comp['score_diff']:.2f}%")
 414:     print(f"  Same Action: {'Yes' if comp['same_action'] else 'No'}")
 415: 
 416:     if comp.get("common_categories"):
 417:         print(f"\nCommon Categories:")
 418:         for cat in comp["common_categories"]:
 419:             print(f"  â€¢ {cat}")
 420: 
 421:     if comp.get("divergent_categories"):
 422:         print(f"\nDivergent Categories:")
 423:         for cat in comp["divergent_categories"]:
 424:             print(f"  â€¢ {cat}")
 425: 
 426:     print()
 427:     return 0
 428: 
 429: 
 430: def cmd_drift(args) -> int:
 431:     """Execute drift history command with delegation."""
 432:     handlers = {
 433:         "history": _handle_drift_history,
 434:         "show": _handle_drift_show,
 435:         "summary": _handle_drift_summary,
 436:         "compare": _handle_drift_compare,
 437:     }
 438: 
 439:     handler = handlers.get(args.action)
 440:     if not handler:
 441:         print(f"Unknown drift action: {args.action}", file=sys.stderr)
 442:         return 1
 443: 
 444:     try:
 445:         return handler(args)
 446:     except ValueError as e:
 447:         print(f"âŒ {str(e)}", file=sys.stderr)
 448:         return 1
 449:     except Exception as e:
 450:         print(f"âŒ Drift operation failed: {str(e)}", file=sys.stderr)
 451:         return 1
 452: 
 453: 
 454: # === RUN_PY COMMAND ===
 455: 
 456: def cmd_run_py(args) -> int:
 457:     """Execute run_py command."""
 458:     try:
 459:         auto_input = getattr(args, 'input', None)
 460: 
 461:         result = run_py(
 462:             code=args.code if hasattr(args, 'code') else None,
 463:             file=args.file if hasattr(args, 'file') else None,
 464:             auto=auto_input,
 465:             args=args.script_args or ""
 466:         )
 467: 
 468:         if result["stdout"]:
 469:             print(result["stdout"], end="")
 470: 
 471:         if result["stderr"]:
 472:             print(result["stderr"], end="", file=sys.stderr)
 473: 
 474:         if args.json:
 475:             summary = {
 476:                 "exit_code": result["exit_code"],
 477:                 "success": result["success"],
 478:                 "duration": result["duration"]
 479:             }
 480:             print(json.dumps(summary, indent=2))
 481: 
 482:         return result["exit_code"]
 483: 
 484:     except Exception as e:
 485:         print(f"âŒ Python execution failed: {str(e)}", file=sys.stderr)
 486:         return 1
 487: 
 488: 
 489: # === PRP COMMANDS ===
 490: 
 491: def cmd_prp_validate(args) -> int:
 492:     """Execute prp validate command."""
 493:     from ce.prp import validate_prp_yaml, format_validation_result
 494: 
 495:     try:
 496:         result = validate_prp_yaml(args.file)
 497: 
 498:         if args.json:
 499:             print(format_output(result, True))
 500:         else:
 501:             print(format_validation_result(result))
 502: 
 503:         return 0 if result["success"] else 1
 504:     except FileNotFoundError as e:
 505:         print(f"âŒ {str(e)}", file=sys.stderr)
 506:         return 1
 507:     except Exception as e:
 508:         print(f"âŒ PRP validation failed: {str(e)}", file=sys.stderr)
 509:         return 1
 510: 
 511: 
 512: def cmd_prp_generate(args) -> int:
 513:     """Execute prp generate command."""
 514:     try:
 515:         # Set environment variable for sequential thinking
 516:         import os
 517:         if hasattr(args, 'use_thinking'):
 518:             os.environ['CE_USE_SEQUENTIAL_THINKING'] = 'true' if args.use_thinking else 'false'
 519: 
 520:         output_dir = args.output or "PRPs/feature-requests"
 521:         join_prp = getattr(args, 'join_prp', None)
 522:         prp_path = generate_prp(args.initial_md, output_dir, join_prp=join_prp)
 523: 
 524:         result = {
 525:             "success": True,
 526:             "prp_path": prp_path,
 527:             "message": f"PRP generated: {prp_path}"
 528:         }
 529: 
 530:         if args.json:
 531:             print(format_output(result, True))
 532:         else:
 533:             print(f"âœ… PRP generated: {prp_path}")
 534: 
 535:         return 0
 536: 
 537:     except FileNotFoundError as e:
 538:         print(f"âŒ {str(e)}", file=sys.stderr)
 539:         return 1
 540:     except ValueError as e:
 541:         print(f"âŒ Invalid INITIAL.md: {str(e)}", file=sys.stderr)
 542:         return 1
 543:     except Exception as e:
 544:         print(f"âŒ PRP generation failed: {str(e)}", file=sys.stderr)
 545:         return 1
 546: 
 547: 
 548: def cmd_prp_execute(args) -> int:
 549:     """Execute prp execute command."""
 550:     from .execute import execute_prp
 551:     from .exceptions import EscalationRequired
 552: 
 553:     try:
 554:         result = execute_prp(
 555:             prp_id=args.prp_id,
 556:             start_phase=args.start_phase,
 557:             end_phase=args.end_phase,
 558:             skip_validation=args.skip_validation,
 559:             dry_run=args.dry_run
 560:         )
 561: 
 562:         if args.json:
 563:             print(format_output(result, True))
 564:             return 0 if result["success"] else 1
 565: 
 566:         if result.get("dry_run"):
 567:             print(f"\nâœ… Dry run: {len(result['phases'])} phases parsed")
 568:             for phase in result['phases']:
 569:                 print(f"  Phase {phase['phase_number']}: {phase['phase_name']} ({phase['hours']}h)")
 570:         else:
 571:             print(f"\n{'='*80}")
 572:             print(f"âœ… PRP-{args.prp_id} execution complete")
 573:             print(f"{'='*80}")
 574:             print(f"Phases completed: {result['phases_completed']}")
 575:             print(f"Confidence score: {result['confidence_score']}")
 576:             print(f"Execution time: {result['execution_time']}")
 577:             print(f"Checkpoints created: {len(result['checkpoints_created'])}")
 578: 
 579:         return 0 if result["success"] else 1
 580: 
 581:     except EscalationRequired as e:
 582:         print(f"\n{'='*80}", file=sys.stderr)
 583:         print(f"ðŸš¨ ESCALATION REQUIRED", file=sys.stderr)
 584:         print(f"{'='*80}", file=sys.stderr)
 585:         print(f"Reason: {e.reason}", file=sys.stderr)
 586:         print(f"\nError Details:", file=sys.stderr)
 587:         print(f"  Type: {e.error.get('type', 'unknown')}", file=sys.stderr)
 588:         print(f"  Location: {e.error.get('file', 'unknown')}:{e.error.get('line', '?')}", file=sys.stderr)
 589:         print(f"  Message: {e.error.get('message', 'No message')}", file=sys.stderr)
 590:         print(f"\nðŸ”§ Troubleshooting:", file=sys.stderr)
 591:         print(e.troubleshooting, file=sys.stderr)
 592:         return 2
 593: 
 594:     except FileNotFoundError as e:
 595:         print(f"âŒ {str(e)}", file=sys.stderr)
 596:         return 1
 597:     except RuntimeError as e:
 598:         print(f"âŒ Execution failed: {str(e)}", file=sys.stderr)
 599:         return 1
 600:     except Exception as e:
 601:         print(f"âŒ Unexpected error: {str(e)}", file=sys.stderr)
 602:         import traceback
 603:         traceback.print_exc()
 604:         return 1
 605: 
 606: 
 607: def cmd_prp_analyze(args) -> int:
 608:     """Execute prp analyze command."""
 609:     from pathlib import Path
 610:     from .prp_analyzer import analyze_prp, format_analysis_report
 611: 
 612:     try:
 613:         prp_path = Path(args.file)
 614:         analysis = analyze_prp(prp_path)
 615:         print(format_analysis_report(analysis, json_output=args.json))
 616: 
 617:         # Return exit code based on size category
 618:         if analysis.size_category.value == "RED":
 619:             return 2
 620:         elif analysis.size_category.value == "YELLOW":
 621:             return 1
 622:         else:
 623:             return 0
 624: 
 625:     except FileNotFoundError as e:
 626:         print(f"âŒ {str(e)}", file=sys.stderr)
 627:         return 1
 628:     except Exception as e:
 629:         print(f"âŒ PRP analysis failed: {str(e)}", file=sys.stderr)
 630:         import traceback
 631:         traceback.print_exc()
 632:         return 1
 633: 
 634: 
 635: # === PIPELINE COMMANDS ===
 636: 
 637: def cmd_pipeline_validate(args) -> int:
 638:     """Execute pipeline validate command."""
 639:     try:
 640:         pipeline = load_abstract_pipeline(args.pipeline_file)
 641:         result = validate_pipeline(pipeline)
 642: 
 643:         if result["success"]:
 644:             print("âœ… Pipeline validation passed")
 645:             return 0
 646:         else:
 647:             print("âŒ Pipeline validation failed:")
 648:             for error in result["errors"]:
 649:                 print(f"  - {error}")
 650:             return 1
 651: 
 652:     except Exception as e:
 653:         print(f"âŒ Validation error: {str(e)}", file=sys.stderr)
 654:         return 1
 655: 
 656: 
 657: def cmd_pipeline_render(args) -> int:
 658:     """Execute pipeline render command."""
 659:     from pathlib import Path
 660: 
 661:     try:
 662:         pipeline = load_abstract_pipeline(args.pipeline_file)
 663: 
 664:         if args.executor == "github-actions":
 665:             executor = GitHubActionsExecutor()
 666:         else:
 667:             executor = MockExecutor()
 668: 
 669:         rendered = executor.render(pipeline)
 670: 
 671:         if args.output:
 672:             Path(args.output).write_text(rendered)
 673:             print(f"âœ… Rendered to {args.output}")
 674:         else:
 675:             print(rendered)
 676: 
 677:         return 0
 678: 
 679:     except Exception as e:
 680:         print(f"âŒ Render error: {str(e)}", file=sys.stderr)
 681:         return 1
 682: 
 683: 
 684: # === METRICS COMMAND ===
 685: 
 686: def cmd_metrics(args) -> int:
 687:     """Display system metrics and success rates."""
 688:     try:
 689:         collector = MetricsCollector(metrics_file=args.file)
 690:         summary = collector.get_summary()
 691: 
 692:         if args.format == "json":
 693:             print(json.dumps(summary, indent=2))
 694:             return 0
 695: 
 696:         # Human-readable format
 697:         print("\nðŸ“Š Context Engineering Metrics")
 698:         print("=" * 60)
 699: 
 700:         # Success rates
 701:         rates = summary["success_rates"]
 702:         print("\nðŸŽ¯ Success Rates:")
 703:         print(f"  First-pass:  {rates['first_pass_rate']:.1f}%")
 704:         print(f"  Second-pass: {rates['second_pass_rate']:.1f}%")
 705:         print(f"  Overall:     {rates['overall_rate']:.1f}%")
 706:         print(f"  Total PRPs:  {rates['total_executions']}")
 707: 
 708:         # Validation stats
 709:         val_stats = summary["validation_stats"]
 710:         if val_stats:
 711:             print("\nâœ… Validation Pass Rates:")
 712:             for key, value in sorted(val_stats.items()):
 713:                 if key.endswith("_pass_rate"):
 714:                     level = key.replace("_pass_rate", "")
 715:                     total_key = f"{level}_total"
 716:                     total = val_stats.get(total_key, 0)
 717:                     print(f"  {level.upper()}: {value:.1f}% ({total} executions)")
 718: 
 719:         # Performance
 720:         perf = summary["performance"]
 721:         print("\nâš¡ Performance:")
 722:         print(f"  Avg duration: {perf['avg_duration']:.1f}s")
 723:         print(f"  Total PRPs:   {perf['total_prps']}")
 724:         print(f"  Total validations: {perf['total_validations']}")
 725: 
 726:         print("=" * 60)
 727:         return 0
 728: 
 729:     except FileNotFoundError:
 730:         print(f"âŒ Metrics file not found: {args.file}", file=sys.stderr)
 731:         print(f"ðŸ”§ Troubleshooting: Run PRP executions to collect metrics", file=sys.stderr)
 732:         return 1
 733:     except Exception as e:
 734:         print(f"âŒ Metrics error: {str(e)}", file=sys.stderr)
 735:         return 1
 736: 
 737: 
 738: # === ANALYZE-CONTEXT COMMAND ===
 739: 
 740: def _get_analysis_result(args, cache_ttl: int):
 741:     """Get analysis result from cache or fresh analysis.
 742:     
 743:     Args:
 744:         args: Command arguments
 745:         cache_ttl: Cache TTL in minutes
 746:     
 747:     Returns:
 748:         Analysis result dict
 749:     """
 750:     from .update_context import (
 751:         analyze_context_drift,
 752:         get_cached_analysis,
 753:         is_cache_valid,
 754:     )
 755:     
 756:     # Skip cache if forced
 757:     if getattr(args, 'force', False):
 758:         return analyze_context_drift()
 759:     
 760:     # Try to use cache
 761:     cached = get_cached_analysis()
 762:     if cached and is_cache_valid(cached, ttl_minutes=cache_ttl):
 763:         return cached
 764:     
 765:     # Cache miss or invalid - run fresh analysis
 766:     return analyze_context_drift()
 767: 
 768: 
 769: def _print_analysis_output(result, args, cache_ttl: int) -> None:
 770:     """Print analysis output in human or JSON format.
 771:     
 772:     Args:
 773:         result: Analysis result
 774:         args: Command arguments
 775:         cache_ttl: Cache TTL in minutes
 776:     """
 777:     if args.json:
 778:         print(format_output(result, True))
 779:         return
 780:     
 781:     # Calculate cache age for display
 782:     from datetime import datetime, timezone
 783:     cache_age_str = ""
 784:     if not getattr(args, 'force', False):
 785:         try:
 786:             generated_at = datetime.fromisoformat(
 787:                 result["generated_at"].replace("+00:00", "+00:00")
 788:             )
 789:             if generated_at.tzinfo is None:
 790:                 generated_at = generated_at.replace(tzinfo=timezone.utc)
 791:             now = datetime.now(timezone.utc)
 792:             age_minutes = int((now - generated_at).total_seconds() / 60)
 793:             cache_age_str = f" ({age_minutes}m old, TTL: {cache_ttl}m)"
 794:         except Exception:
 795:             cache_age_str = f" (TTL: {cache_ttl}m)"
 796:     
 797:     # Human-readable output
 798:     drift_score = result["drift_score"]
 799:     drift_level = result["drift_level"]
 800:     violations = result.get("violation_count", 0)
 801:     missing = len(result.get("missing_examples", []))
 802:     duration = result.get("duration_seconds", 0)
 803:     
 804:     # Emoji indicators
 805:     if drift_level == "ok":
 806:         indicator = "âœ…"
 807:     elif drift_level == "warning":
 808:         indicator = "âš ï¸ "
 809:     else:  # critical
 810:         indicator = "ðŸš¨"
 811:     
 812:     print("ðŸ” Analyzing context drift...")
 813:     if duration > 0:
 814:         print("   ðŸ“Š Pattern conformance: scan complete")
 815:         print("   ðŸ“š Documentation gaps: check complete")
 816:         print()
 817:     
 818:     if cache_age_str and not getattr(args, 'force', False):
 819:         print(f"âœ… Using cached analysis{cache_age_str}")
 820:         print(f"   Use --force to re-analyze")
 821:     
 822:     print(f"{indicator} Analysis complete ({duration}s)")
 823:     print(f"   Drift Score: {drift_score:.1f}% ({drift_level.upper()})")
 824:     print(f"   Violations: {violations}")
 825:     if missing > 0:
 826:         print(f"   Missing Examples: {missing}")
 827:     print(f"   Report: {result['report_path']}")
 828: 
 829: 
 830: def cmd_analyze_context(args) -> int:
 831:     """Execute analyze-context command.
 832:     
 833:     Fast drift check without metadata updates - optimized for CI/CD.
 834:     
 835:     Returns:
 836:         Exit code: 0 (ok), 1 (warning), 2 (critical)
 837:     """
 838:     from .update_context import get_cache_ttl
 839:     
 840:     try:
 841:         # Get cache TTL (CLI flag > config > default)
 842:         cache_ttl = get_cache_ttl(getattr(args, 'cache_ttl', None))
 843:         
 844:         # Get analysis result (cached or fresh)
 845:         result = _get_analysis_result(args, cache_ttl)
 846:         
 847:         # Print output
 848:         _print_analysis_output(result, args, cache_ttl)
 849:         
 850:         # Return exit code based on drift level
 851:         exit_codes = {"ok": 0, "warning": 1, "critical": 2}
 852:         return exit_codes[result["drift_level"]]
 853:     
 854:     except Exception as e:
 855:         print(f"âŒ Analysis failed: {str(e)}", file=sys.stderr)
 856:         import traceback
 857:         traceback.print_exc()
 858:         return 1
 859: 
 860: 
 861: # === UPDATE-CONTEXT COMMAND ===
 862: 
 863: def cmd_update_context(args) -> int:
 864:     """Execute update-context command.
 865: 
 866:     Workflow:
 867:         1. Standard context sync (always runs)
 868:         2. Drift remediation workflow (always runs)
 869:            - Vanilla mode (no --remediate): asks approval before PRP generation
 870:            - YOLO mode (--remediate): skips approval, auto-generates PRP
 871:     """
 872:     try:
 873:         # Step 1: ALWAYS run standard context sync first
 874:         target_prp = args.prp if hasattr(args, 'prp') and args.prp else None
 875:         result = sync_context(target_prp=target_prp)
 876: 
 877:         if args.json:
 878:             print(format_output(result, True))
 879:         else:
 880:             print("âœ… Context sync completed")
 881:             print(f"   PRPs scanned: {result['prps_scanned']}")
 882:             print(f"   PRPs updated: {result['prps_updated']}")
 883:             print(f"   PRPs moved: {result['prps_moved']}")
 884:             print(f"   CE updated: {result['ce_updated_count']}")
 885:             print(f"   Serena updated: {result['serena_updated_count']}")
 886: 
 887:             if result['errors']:
 888:                 print(f"\nâš ï¸  Errors encountered:")
 889:                 for error in result['errors']:
 890:                     print(f"   - {error}")
 891: 
 892:         # Step 2: ALWAYS run drift remediation workflow after sync
 893:         from .update_context import remediate_drift_workflow
 894: 
 895:         yolo_mode = hasattr(args, 'remediate') and args.remediate
 896:         remediate_result = remediate_drift_workflow(yolo_mode=yolo_mode)
 897: 
 898:         if args.json:
 899:             # Combine both results in JSON output
 900:             combined = {
 901:                 "sync": result,
 902:                 "remediation": remediate_result
 903:             }
 904:             print(format_output(combined, True))
 905: 
 906:         # Combine success status from both workflows
 907:         success = result['success'] and remediate_result['success']
 908:         return 0 if success else 1
 909: 
 910:     except Exception as e:
 911:         print(f"âŒ Update context failed: {str(e)}", file=sys.stderr)
 912:         import traceback
 913:         traceback.print_exc()
 914:         return 1
 915: 
 916: 
 917: def cmd_vacuum(args):
 918:     """Handle vacuum command.
 919: 
 920:     Args:
 921:         args: Parsed command-line arguments
 922: 
 923:     Returns:
 924:         Exit code (0 = success, 1 = candidates found, 2 = error)
 925:     """
 926:     from pathlib import Path
 927:     from .vacuum import VacuumCommand
 928: 
 929:     try:
 930:         # Find project root (where .ce/ directory exists)
 931:         current = Path.cwd()
 932:         project_root = None
 933: 
 934:         for parent in [current] + list(current.parents):
 935:             if (parent / ".ce").exists():
 936:                 project_root = parent
 937:                 break
 938: 
 939:         if not project_root:
 940:             print("âŒ Error: Not in a Context Engineering project (.ce/ not found)", file=sys.stderr)
 941:             return 2
 942: 
 943:         # Run vacuum command
 944:         vacuum = VacuumCommand(project_root)
 945:         return vacuum.run(
 946:             dry_run=not (args.execute or args.force or args.auto or args.nuclear),
 947:             min_confidence=args.min_confidence,
 948:             exclude_strategies=args.exclude_strategies or [],
 949:             execute=args.execute,
 950:             force=args.force,
 951:             auto=args.auto,
 952:             nuclear=args.nuclear,
 953:         )
 954: 
 955:     except KeyboardInterrupt:
 956:         print("\nâŒ Vacuum cancelled by user", file=sys.stderr)
 957:         return 2
 958:     except Exception as e:
 959:         print(f"âŒ Vacuum failed: {str(e)}", file=sys.stderr)
 960:         import traceback
 961:         traceback.print_exc()
 962:         return 2
 963: 
 964: 
 965: # === BLEND COMMAND ===
 966: 
 967: def cmd_blend(args) -> int:
 968:     """Execute blend command."""
 969:     return blend_run_blend(args)
 970: 
 971: 
 972: # === CLEANUP COMMAND ===
 973: 
 974: def cmd_cleanup(args) -> int:
 975:     """Execute cleanup command."""
 976:     from pathlib import Path
 977:     from .blending.cleanup import cleanup_legacy_dirs
 978: 
 979:     try:
 980:         # Determine dry_run mode
 981:         dry_run = not args.execute
 982: 
 983:         target_project = Path.cwd()
 984: 
 985:         status = cleanup_legacy_dirs(
 986:             target_project=target_project,
 987:             dry_run=dry_run
 988:         )
 989: 
 990:         # Exit with success if all cleanups succeeded
 991:         if all(status.values()):
 992:             return 0
 993:         else:
 994:             return 1
 995: 
 996:     except ValueError as e:
 997:         # Migration incomplete
 998:         print(f"âŒ {e}", file=sys.stderr)
 999:         return 2
1000:     except Exception as e:
1001:         print(f"âŒ Cleanup failed: {e}", file=sys.stderr)
1002:         import traceback
1003:         traceback.print_exc()
1004:         return 1
1005: 
1006: 
1007: # === INIT-PROJECT COMMAND ===
1008: 
1009: def cmd_init_project(args) -> int:
1010:     """Execute init-project command.
1011: 
1012:     Orchestrates CE framework installation on target projects using ProjectInitializer.
1013: 
1014:     Args:
1015:         args: Parsed command-line arguments with:
1016:             - target_dir: Path to target project
1017:             - dry_run: If True, show actions without executing
1018:             - blend_only: If True, run only blend phase
1019:             - phase: Which phase to run (extract, blend, initialize, verify, all)
1020: 
1021:     Returns:
1022:         Exit code: 0 (success), 1 (user error), 2 (initialization error)
1023:     """
1024:     from pathlib import Path
1025: 
1026:     try:
1027:         # Parse and resolve target directory to absolute path
1028:         target_dir = Path(args.target_dir).resolve()
1029: 
1030:         # Validate target directory exists
1031:         if not target_dir.exists():
1032:             print(f"âŒ Target directory not found: {target_dir}", file=sys.stderr)
1033:             print(f"ðŸ”§ Troubleshooting: Verify path and ensure directory exists", file=sys.stderr)
1034:             return 1
1035: 
1036:         # Create ProjectInitializer instance
1037:         dry_run = getattr(args, 'dry_run', False)
1038:         initializer = ProjectInitializer(target_dir, dry_run=dry_run)
1039: 
1040:         # Handle --blend-only flag (takes precedence over --phase)
1041:         if getattr(args, 'blend_only', False):
1042:             result = initializer.blend()
1043: 
1044:             if args.json:
1045:                 print(format_output({"blend": result}, True))
1046:             else:
1047:                 print(result.get("message", "Blend phase completed"))
1048:                 if result.get("stdout"):
1049:                     print(result["stdout"])
1050:                 if result.get("stderr"):
1051:                     print(result["stderr"], file=sys.stderr)
1052: 
1053:             return 0 if result.get("success", False) else 2
1054: 
1055:         # Handle --phase flag or run all phases
1056:         phase = getattr(args, 'phase', 'all')
1057:         results = initializer.run(phase=phase)
1058: 
1059:         # Output results
1060:         if getattr(args, 'json', False):
1061:             print(format_output(results, True))
1062:         else:
1063:             for phase_name, result in results.items():
1064:                 print(f"\n{'='*60}")
1065:                 print(f"Phase: {phase_name}")
1066:                 print(f"{'='*60}")
1067:                 print(result.get("message", "No message"))
1068: 
1069:                 # Print stdout/stderr if present
1070:                 if result.get("stdout"):
1071:                     print(result["stdout"])
1072:                 if result.get("stderr"):
1073:                     print(result["stderr"], file=sys.stderr)
1074: 
1075:         # Determine exit code based on all phases
1076:         all_success = all(r.get("success", True) for r in results.values())
1077:         return 0 if all_success else 2
1078: 
1079:     except ValueError as e:
1080:         # Invalid phase or other user errors
1081:         print(f"âŒ {str(e)}", file=sys.stderr)
1082:         print(f"ðŸ”§ Troubleshooting: Check command arguments and try again", file=sys.stderr)
1083:         return 1
1084:     except Exception as e:
1085:         # Initialization errors from ProjectInitializer
1086:         print(f"âŒ Initialization failed: {str(e)}", file=sys.stderr)
1087:         print(f"ðŸ”§ Troubleshooting: Check error details above and verify framework files exist", file=sys.stderr)
1088:         import traceback
1089:         traceback.print_exc()
1090:         return 2
</file>

<file path=".claude/commands/generate-prp.md">
  1: # /generate-prp - Generate PRP from INITIAL.md
  2: 
  3: Automates PRP (Product Requirements Prompt) generation from INITIAL.md with comprehensive codebase research, documentation fetching, and context synthesis.
  4: 
  5: ## Usage
  6: 
  7: ```
  8: /generate-prp <initial-md-path>                        # Creates new PRP + Linear issue
  9: /generate-prp <initial-md-path> --join-prp <prp-ref>  # Joins existing PRP's Linear issue
 10: ```
 11: 
 12: **PRP Reference Formats**:
 13: - Number: `--join-prp 12` (searches for PRP-12)
 14: - ID: `--join-prp PRP-12`
 15: - File path: `--join-prp PRPs/executed/PRP-12-feature.md`
 16: 
 17: ## What It Does
 18: 
 19: 1. **Parses INITIAL.md structure**:
 20:    - Extracts FEATURE, EXAMPLES, DOCUMENTATION, OTHER CONSIDERATIONS sections
 21:    - Validates required sections present (FEATURE and EXAMPLES are mandatory)
 22: 
 23: 2. **Proposes clean code**:
 24:    - Follows project code quality standards (50-line functions, 500-line files)
 25:    - Applies KISS principle (simple solutions, minimal dependencies)
 26:    - Ensures no fishy fallbacks or silent error masking
 27:    - All mocks marked with FIXME comments in production code
 28:    - Includes actionable error messages with troubleshooting guidance
 29: 
 30: 3. **Researches codebase** (via Serena MCP):
 31:    - Searches for similar patterns using keywords
 32:    - Analyzes symbol structure and relationships
 33:    - Infers test framework (pytest/unittest/jest)
 34:    - Identifies architectural patterns
 35: 
 36: 3. **Fetches documentation** (via Context7 MCP):
 37:    - Resolves library names to Context7 IDs
 38:    - Fetches relevant library documentation
 39:    - Extracts topics from feature description
 40:    - Includes external documentation links
 41: 
 42: 4. **Generates complete PRP**:
 43:    - Synthesizes 6-section PRP structure (TL;DR, Context, Implementation Steps, Validation Gates, Testing Strategy, Rollout Plan)
 44:    - Creates YAML header with metadata
 45:    - Auto-generates next PRP ID (PRP-N+1)
 46:    - Validates completeness (ensures all required sections present)
 47: 
 48: 5. **Creates/Updates Linear issue**:
 49:    - **Without --join-prp**: Creates new Linear issue with project defaults (from `.ce/linear-defaults.yml`)
 50:    - **With --join-prp**: Updates existing PRP's Linear issue with new PRP information
 51:    - Updates PRP YAML header with `issue: {ISSUE-ID}`
 52: 
 53: 6. **Outputs to**: `PRPs/feature-requests/PRP-{id}-{feature-slug}.md`
 54: 
 55: ## INITIAL.md Structure
 56: 
 57: Your INITIAL.md must follow this structure:
 58: 
 59: ```markdown
 60: # Feature: <Feature Name>
 61: 
 62: ## FEATURE
 63: <What to build - user story, acceptance criteria>
 64: 
 65: ## EXAMPLES
 66: <Similar code patterns from codebase, inline code blocks, or file references>
 67: 
 68: ## DOCUMENTATION
 69: <Library docs, API references, external resources>
 70: 
 71: ## OTHER CONSIDERATIONS
 72: <Gotchas, constraints, security concerns, edge cases>
 73: ```
 74: 
 75: **Required sections**: FEATURE, EXAMPLES
 76: **Optional sections**: DOCUMENTATION, OTHER CONSIDERATIONS
 77: 
 78: ## Example
 79: 
 80: ```bash
 81: # Create INITIAL.md
 82: cat > feature-requests/user-auth/INITIAL.md << 'EOF'
 83: # Feature: User Authentication System
 84: 
 85: ## FEATURE
 86: Build JWT-based user authentication with:
 87: - User registration with email/password
 88: - Login with JWT token generation
 89: - Token refresh mechanism
 90: 
 91: **Acceptance Criteria:**
 92: 1. Users can register with valid email and password
 93: 2. Login returns JWT access token and refresh token
 94: 3. Protected endpoints validate JWT tokens
 95: 
 96: ## EXAMPLES
 97: ```python
 98: async def authenticate_user(email: str, password: str) -> dict:
 99:     user = await db.users.find_one({"email": email})
100:     if not user or not verify_password(password, user["password_hash"]):
101:         raise AuthenticationError("Invalid credentials")
102: 
103:     access_token = create_jwt(user["id"], expires_in=3600)
104:     return {"access_token": access_token}
105: ```
106: 
107: See src/oauth.py:42-67 for similar async authentication pattern
108: 
109: ## DOCUMENTATION
110: - [JWT Best Practices](https://jwt.io/introduction)
111: - [FastAPI Security](https://fastapi.tiangolo.com/tutorial/security/)
112: - "pytest" for testing
113: - "bcrypt" for password hashing
114: 
115: ## OTHER CONSIDERATIONS
116: **Security:**
117: - Hash passwords with bcrypt (cost factor 12)
118: - Rate limiting on login endpoint (5 attempts per 15 min)
119: EOF
120: 
121: # Generate PRP
122: cd tools
123: uv run ce prp generate feature-requests/user-auth/INITIAL.md
124: 
125: # Output: PRPs/feature-requests/PRP-6-user-authentication-system.md
126: ```
127: 
128: ## CLI Command
129: 
130: ```bash
131: # Basic usage (creates new PRP + Linear issue)
132: cd tools
133: uv run ce prp generate <path-to-initial.md>
134: 
135: # Join existing PRP's Linear issue
136: uv run ce prp generate <path-to-initial.md> --join-prp 12
137: uv run ce prp generate <path-to-initial.md> --join-prp PRP-12
138: uv run ce prp generate <path-to-initial.md> --join-prp PRPs/executed/PRP-12-feature.md
139: 
140: # Custom output directory
141: uv run ce prp generate <path-to-initial.md> -o /custom/path
142: 
143: # JSON output (for scripting)
144: uv run ce prp generate <path-to-initial.md> --json
145: 
146: # Combined options
147: uv run ce prp generate <path-to-initial.md> --join-prp 12 --json
148: ```
149: 
150: **Use Cases for --join-prp**:
151: - **Related features**: Multiple PRPs implementing parts of same initiative
152: - **Incremental work**: Breaking large PRP into smaller chunks
153: - **Follow-up work**: Additional PRP for same feature area
154: 
155: **Example workflow**:
156: ```bash
157: # Create first PRP for auth system
158: uv run ce prp generate auth-part1.md
159: # Output: PRP-10 created, Linear issue BLA-25 created
160: 
161: # Create second PRP, join same issue
162: uv run ce prp generate auth-part2.md --join-prp 10
163: # Output: PRP-11 created, BLA-25 updated with PRP-11 info
164: ```
165: 
166: ## Output Structure
167: 
168: The generated PRP will have:
169: 
170: ```markdown
171: ---
172: prp_id: TBD
173: feature_name: User Authentication System
174: status: pending
175: created: 2025-10-13T00:00:00Z
176: updated: 2025-10-13T00:00:00Z
177: complexity: medium
178: estimated_hours: 3-5
179: dependencies: JWT Best Practices, FastAPI Security, pytest
180: ---
181: 
182: # User Authentication System
183: 
184: ## 1. TL;DR
185: **Objective**: ...
186: **What**: ...
187: **Why**: ...
188: **Effort**: ...
189: **Dependencies**: ...
190: 
191: ## 2. Context
192: ### Background
193: ...
194: ### Constraints and Considerations
195: ...
196: ### Documentation References
197: ...
198: 
199: ## 3. Implementation Steps
200: ### Phase 1: Setup and Research (30 min)
201: ...
202: ### Phase 2: Core Implementation (2-3 hours)
203: ...
204: ### Phase 3: Testing and Validation (1-2 hours)
205: ...
206: 
207: ## 4. Validation Gates
208: ### Gate 1: Unit Tests Pass
209: **Command**: `uv run pytest tests/unit/ -v`
210: ...
211: 
212: ## 5. Testing Strategy
213: ### Test Framework
214: pytest
215: ### Test Command
216: ```bash
217: uv run pytest tests/ -v
218: ```
219: ...
220: 
221: ## 6. Rollout Plan
222: ### Phase 1: Development
223: ...
224: ### Phase 2: Review
225: ...
226: ### Phase 3: Deployment
227: ...
228: 
229: ---
230: 
231: ## Research Findings
232: ### Serena Codebase Analysis
233: ...
234: ### Documentation Sources
235: ...
236: ```
237: 
238: ## Graceful Degradation
239: 
240: The tool works even if MCP servers are unavailable:
241: - **Without Serena**: No codebase research, but PRP still generated with user-provided examples
242: - **Without Context7**: No library documentation fetched, but external links preserved
243: - **Without Sequential Thinking**: Heuristic-based topic extraction used
244: 
245: ## Code Quality Standards Applied
246: 
247: All generated implementations follow:
248: - **Function limits**: Target 50 lines max per function
249: - **File limits**: Target 500 lines max per file
250: - **KISS principle**: Simple solutions first, clear code over clever code
251: - **Error handling**: Fast failure with actionable troubleshooting messages
252: - **No silent failures**: Exceptions bubble up, never swallowed
253: - **Real functionality**: No hardcoded success messages or fake results
254: - **Naming conventions**: Business-focused (no version numbers or placeholders)
255: 
256: ## Tips
257: 
258: 1. **Be specific in FEATURE section**: Include clear acceptance criteria
259: 2. **Provide relevant EXAMPLES**: Reference similar code in your codebase
260: 3. **Link to DOCUMENTATION**: Include library docs and external resources
261: 4. **Note OTHER CONSIDERATIONS**: Security concerns, edge cases, constraints
262: 5. **Code quality alignment**: Generated code will follow project standards - review for consistency
263: 
264: ## Haiku-Ready PRP Checklist
265: 
266: Before executing a generated PRP, verify it's optimized for Claude 4.5 Haiku execution:
267: 
268: - [ ] **Goal**: Exact end state described, not vague improvement
269: - [ ] **Output**: File paths and line numbers specified
270: - [ ] **Limits**: Scope boundaries explicit (what's IN/OUT)
271: - [ ] **Data**: All required context inline in PRP (no external references)
272: - [ ] **Evaluation**: Validation gates are copy-paste bash commands
273: - [ ] **Decisions**: All architectural choices made (Haiku executes, doesn't decide)
274: - [ ] **Code Snippets**: Before/after code provided for major changes
275: - [ ] **No Vague Language**: Check for "appropriate", "suitable", "handle appropriately"
276: 
277: **Reference**: See [PRP-23: Haiku-Optimized PRP Guidelines](../../PRPs/feature-requests/PRP-23-haiku-optimized-prp-guidelines.md) for detailed patterns.
278: 
279: ## Next Steps After Generation
280: 
281: 1. Review generated PRP for completeness
282: 2. Fill in TBD fields (prp_id will be auto-assigned on execution)
283: 3. Adjust estimated hours if needed
284: 4. **Check Haiku-Ready checklist above**
285: 5. Execute PRP using `/execute-prp <prp-file>`
286: 
287: ## Implementation Details
288: 
289: - **Module**: `tools/ce/generate.py`
290: - **Tests**: `tools/tests/test_generate.py` (24 tests)
291: - **PRP Reference**: `PRPs/feature-requests/PRP-3-command-automation.md`
</file>

<file path=".claude/settings.local.json">
  1: {
  2:   "permissions": {
  3:     "allow": [
  4:       "Bash(git:*)",
  5:       "Bash(uv:*)",
  6:       "Bash(uvx:*)",
  7:       "Bash(pytest:*)",
  8:       "Bash(python:*)",
  9:       "Bash(python3:*)",
 10:       "Bash(gh:*)",
 11:       "Bash(rg:*)",
 12:       "Bash(env:*)",
 13:       "Bash(head:*)",
 14:       "Bash(tail:*)",
 15:       "Bash(cat:*)",
 16:       "Bash(grep:*)",
 17:       "Bash(wc:*)",
 18:       "Bash(echo:*)",
 19:       "Bash(ps:*)",
 20:       "Bash(rm -rf ~/.mcp-auth)",
 21:       "Bash(ls:*)",
 22:       "Bash(cd:*)",
 23:       "Bash(pwd:*)",
 24:       "Bash(find:*)",
 25:       "Bash(tree:*)",
 26:       "Bash(which:*)",
 27:       "Bash(whereis:*)",
 28:       "Bash(file:*)",
 29:       "Bash(stat:*)",
 30:       "Bash(less:*)",
 31:       "Bash(more:*)",
 32:       "Bash(sed:*)",
 33:       "Bash(awk:*)",
 34:       "Bash(sort:*)",
 35:       "Bash(uniq:*)",
 36:       "Bash(cut:*)",
 37:       "Bash(diff:*)",
 38:       "Bash(comm:*)",
 39:       "Read(//Users/bprzybysz/nc-src/**)",
 40:       "Read(//Users/bprzybysz/.claude/**)",
 41:       "WebFetch(domain:github.com)",
 42:       "SlashCommand(/generate-prp:*)",
 43:       "SlashCommand(/execute-prp:*)",
 44:       "SlashCommand(/peer-review:*)",
 45:       "SlashCommand(/mcp:*)",
 46:       "WebSearch",
 47:       "mcp__syntropy__context7_get_library_docs",
 48:       "mcp__syntropy__context7_resolve_library_id",
 49:       "mcp__syntropy__linear_create_issue",
 50:       "mcp__syntropy__linear_get_issue",
 51:       "mcp__syntropy__linear_list_issues",
 52:       "mcp__syntropy__linear_list_projects",
 53:       "mcp__syntropy__linear_update_issue",
 54:       "mcp__syntropy__linear_list_teams",
 55:       "mcp__syntropy__linear_get_team",
 56:       "mcp__syntropy__linear_list_users",
 57:       "mcp__syntropy__linear_create_project",
 58:       "mcp__syntropy__serena_activate_project",
 59:       "mcp__syntropy__serena_create_text_file",
 60:       "mcp__syntropy__serena_find_referencing_symbols",
 61:       "mcp__syntropy__serena_find_symbol",
 62:       "mcp__syntropy__serena_get_symbols_overview",
 63:       "mcp__syntropy__serena_list_dir",
 64:       "mcp__syntropy__serena_read_file",
 65:       "mcp__syntropy__serena_replace_symbol_body",
 66:       "mcp__syntropy__serena_search_for_pattern",
 67:       "mcp__syntropy__serena_write_memory",
 68:       "mcp__syntropy__serena_read_memory",
 69:       "mcp__syntropy__serena_list_memories",
 70:       "mcp__syntropy__serena_delete_memory",
 71:       "mcp__syntropy__thinking_sequentialthinking",
 72:       "mcp__syntropy__healthcheck",
 73:       "mcp__syntropy__enable_tools",
 74:       "mcp__syntropy__list_all_tools"
 75:     ],
 76:     "deny": [
 77:       "mcp__syntropy__serena_think_about_collected_information",
 78:       "mcp__syntropy__serena_think_about_task_adherence",
 79:       "mcp__syntropy__serena_think_about_whether_you_are_done",
 80:       "mcp__syntropy__filesystem_read_media_file",
 81:       "mcp__syntropy__filesystem_read_multiple_files",
 82:       "mcp__syntropy__filesystem_create_directory",
 83:       "mcp__syntropy__filesystem_move_file",
 84:       "mcp__syntropy__filesystem_list_directory_with_sizes",
 85:       "mcp__syntropy__git_git_branch",
 86:       "mcp__syntropy__git_git_checkout",
 87:       "mcp__syntropy__git_git_show",
 88:       "mcp__syntropy__git_git_create_branch",
 89:       "mcp__syntropy__git_git_reset",
 90:       "mcp__syntropy__git_git_diff_staged",
 91:       "mcp__syntropy__git_git_diff_unstaged",
 92:       "mcp__syntropy__filesystem_read_file",
 93:       "mcp__syntropy__filesystem_read_text_file",
 94:       "mcp__syntropy__filesystem_write_file",
 95:       "mcp__syntropy__filesystem_edit_file",
 96:       "mcp__syntropy__filesystem_list_directory",
 97:       "mcp__syntropy__filesystem_search_files",
 98:       "mcp__syntropy__filesystem_directory_tree",
 99:       "mcp__syntropy__filesystem_get_file_info",
100:       "mcp__syntropy__git_git_status",
101:       "mcp__syntropy__git_git_diff",
102:       "mcp__syntropy__git_git_log",
103:       "mcp__syntropy__git_git_add",
104:       "mcp__syntropy__git_git_commit",
105:       "mcp__syntropy__github_create_or_update_file",
106:       "mcp__syntropy__github_search_repositories",
107:       "mcp__syntropy__github_create_repository",
108:       "mcp__syntropy__github_get_file_contents",
109:       "mcp__syntropy__github_push_files",
110:       "mcp__syntropy__github_create_issue",
111:       "mcp__syntropy__github_create_pull_request",
112:       "mcp__syntropy__github_fork_repository",
113:       "mcp__syntropy__github_create_branch",
114:       "mcp__syntropy__github_list_commits",
115:       "mcp__syntropy__github_list_issues",
116:       "mcp__syntropy__github_update_issue",
117:       "mcp__syntropy__github_add_issue_comment",
118:       "mcp__syntropy__github_search_code",
119:       "mcp__syntropy__github_search_issues",
120:       "mcp__syntropy__github_search_users",
121:       "mcp__syntropy__github_get_issue",
122:       "mcp__syntropy__github_get_pull_request",
123:       "mcp__syntropy__github_list_pull_requests",
124:       "mcp__syntropy__github_create_pull_request_review",
125:       "mcp__syntropy__github_merge_pull_request",
126:       "mcp__syntropy__github_get_pull_request_files",
127:       "mcp__syntropy__github_get_pull_request_status",
128:       "mcp__syntropy__github_update_pull_request_branch",
129:       "mcp__syntropy__github_get_pull_request_comments",
130:       "mcp__syntropy__github_get_pull_request_reviews",
131:       "mcp__syntropy__repomix_pack_codebase",
132:       "mcp__syntropy__repomix_grep_repomix_output",
133:       "mcp__syntropy__repomix_read_repomix_output",
134:       "mcp__syntropy__repomix_pack_remote_repository",
135:       "mcp__syntropy__playwright_navigate",
136:       "mcp__syntropy__playwright_screenshot",
137:       "mcp__syntropy__playwright_click",
138:       "mcp__syntropy__playwright_fill",
139:       "mcp__syntropy__playwright_evaluate",
140:       "mcp__syntropy__playwright_get_visible_text",
141:       "mcp__syntropy__perplexity_perplexity_ask",
142:       "mcp__syntropy__init_project",
143:       "mcp__syntropy__get_system_doc",
144:       "mcp__syntropy__get_user_doc",
145:       "mcp__syntropy__get_summary",
146:       "mcp__syntropy__denoise"
147:     ],
148:     "ask": [
149:       "Bash(rm:*)",
150:       "Bash(mv:*)",
151:       "Bash(cp:*)",
152:       "Bash(curl:*)",
153:       "Bash(wget:*)",
154:       "Bash(nc:*)",
155:       "Bash(telnet:*)",
156:       "Bash(ssh:*)",
157:       "Bash(scp:*)",
158:       "Bash(rsync:*)",
159:       "Bash(sudo:*)",
160:       "Bash(brew install:*)",
161:       "Bash(npm install:*)",
162:       "Bash(pip install:*)",
163:       "Bash(gem install:*)"
164:     ],
165:     "additionalDirectories": [
166:       "/Users/bprzybyszi"
167:     ]
168:   },
169:   "hooks": {
170:     "SessionStart": [
171:       {
172:         "hooks": [
173:           {
174:             "type": "command",
175:             "command": "PROJECT_ROOT=$(git rev-parse --show-toplevel 2>/dev/null || pwd) && cd \"$PROJECT_ROOT/tools\" && bash scripts/session-startup.sh",
176:             "timeout": 12
177:           }
178:         ]
179:       }
180:     ]
181:   },
182:   "enabledPlugins": {}
183: }
</file>

<file path=".claude/commands/peer-review.md">
  1: # /peer-review - Context-Naive Peer Review Command
  2: 
  3: Perform context-naive peer review of specified PRP work with optional execution review.
  4: 
  5: ## Usage
  6: 
  7: ```bash
  8: /peer-review [prp-reference] [exe|execution]
  9: ```
 10: 
 11: ## Parameters
 12: 
 13: ### 1. prp-reference (optional, default: latest)
 14: Specify which PRP to review using one of these formats:
 15: 
 16: - **PRP ID**: `PRP-8.8` or `8.8` or `34.2.1` (batch notation)
 17: - **File path**: `context-engineering/PRPs/PRP-8.8-web-ui-ux-improvements.md`
 18: - **Natural language**: `"shift pattern logic"` or `"web ui ux improvements"`
 19: - **Keyword**: `latest` (most recent PRP from conversation)
 20: - **Batch context**: If batch active, searches within batch PRPs first
 21: 
 22: ### 2. exe|execution (optional)
 23: Control review mode:
 24: 
 25: - **Absent**: Document review only (default mode)
 26: - **exe**: Review PRP execution results (assumes PRP already executed)
 27: - **execution**: Alias for `exe` (same behavior, clearer intent)
 28: 
 29: ## Batch Context Integration
 30: 
 31: When working within a batch workflow, `/peer-review` automatically understands batch context:
 32: 
 33: ```bash
 34: # After /batch-gen-prp or during /batch-exe-prp
 35: /peer-review 34.2.1              # Review specific PRP from batch 34
 36: /peer-review latest              # Review latest PRP in batch context
 37: /peer-review "classification"    # Search within batch 34 PRPs first
 38: 
 39: # Execution review within batch
 40: /peer-review 34.2.1 exe          # Review execution of one PRP in batch
 41: /peer-review latest execution    # Review execution of latest PRP in batch
 42: ```
 43: 
 44: **Batch Context Detection**:
 45: - Checks for active batch in conversation (from `/batch-gen-prp` or `/batch-exe-prp`)
 46: - Prioritizes PRPs within active batch when searching
 47: - Falls back to all PRPs if not found in batch
 48: - **For full batch review**: Use `/batch-peer-review` for systematic review of all PRPs with inter-PRP consistency checks
 49: 
 50: **When to Use Single vs Batch Review**:
 51: - **Use `/peer-review`** (this command): Deep dive into one specific PRP, detailed analysis, spot checks
 52: - **Use `/batch-peer-review`**: Systematic review of all PRPs, inter-PRP consistency, parallel efficiency
 53: 
 54: ## Examples
 55: 
 56: ```bash
 57: # Review latest PRP document (no execution review)
 58: /peer-review
 59: 
 60: # Review specific PRP by ID
 61: /peer-review PRP-8.8
 62: 
 63: # Review specific PRP in batch (batch notation)
 64: /peer-review 34.2.1
 65: 
 66: # Find PRP by natural language description
 67: /peer-review "shift pattern logic"
 68: 
 69: # Review execution of already-executed PRP
 70: /peer-review PRP-8.8 exe
 71: 
 72: # Review execution of batch PRP
 73: /peer-review 34.2.1 execution
 74: 
 75: # Review execution of most recent PRP
 76: /peer-review latest execution
 77: 
 78: # Batch workflow examples
 79: /peer-review 34.2.1              # Document review (before execution)
 80: /peer-review 34.2.1 exe          # Execution review (after execution)
 81: /peer-review latest              # Latest PRP in active batch
 82: ```
 83: 
 84: ## Review Process
 85: 
 86: ### Phase 1: Document Review (Always Performed)
 87: 
 88: 1. **Locate PRP**: Find PRP file from reference parameter
 89: 2. **Read Fresh**: Read PRP as standalone artifact, ignoring generation conversation
 90: 3. **Evaluate Quality**:
 91:    - âœ… Completeness: All sections present and detailed?
 92:    - âœ… Clarity: Technical requirements unambiguous?
 93:    - âœ… Feasibility: Implementation approach sound?
 94:    - âœ… Testability: Acceptance criteria measurable?
 95:    - âœ… Edge Cases: Potential issues identified?
 96:    - âœ… Alignment with CLAUDE.md guidelines.
 97:    - âœ… Existing patterns (ce examples) and architecture respectation
 98:    - âœ… Existing code reuse
 99:    - âœ… Check also serena memories for more guidelines
100: 
101: 4. **Provide Recommendations**: Actionable improvements
102: 5. **Apply Improvements**: Update PRP unless profound questions arise
103: 6. **Document Review**: Add review notes to PRP appendix
104: 
105: ### Phase 2: Execution Review (Only if exe|execution flag present)
106: 
107: **Prerequisite**: PRP must already be executed via `/execute-prp` or manual implementation
108: 
109: 1. **Read PRP Requirements**: Review what was supposed to be implemented
110: 2. **Read Changed Files Fresh**: Read implementation as standalone artifacts, ignoring implementation conversation
111: 3. **Evaluate Execution**:
112:    - âœ… Implementation matches PRP requirements?
113:    - âœ… Code quality meets project standards?
114:    - âœ… Acceptance criteria satisfied?
115:    - âœ… Unintended side effects detected?
116:    - âœ… Edge cases handled?
117:    - âœ… No implementation violating guidelines specified in Document Review (CLAUDE.md)
118:    - âœ… No implementation violating existing patterns (ce examples) and architecture respectation
119:    - âœ… No implementation duplicating existing code (should extend existing code)
120:    - âœ… Check also serena memories for more guidelines not to violate
121: 
122: 4. **Provide Recommendations**: Actionable fixes
123: 5. **Apply Fixes**: Update code unless profound questions arise
124: 6. **Document Execution Review**: Add notes to PRP execution section
125: 
126: ## Output Format
127: 
128: ### Document Review Output
129: 
130: ```markdown
131: ## Context-Naive Peer Review: Document
132: 
133: **PRP**: PRP-8.8-web-ui-ux-improvements.md
134: **Reviewed**: 2025-10-02T19:55:00Z
135: 
136: ### Findings
137: - âœ… Strength 1: Clear structure with before/after code examples
138: - âœ… Strength 2: Specific line numbers for all changes
139: - âš ï¸ Issue 1: React Hooks violation in Change #4
140: - âš ï¸ Issue 2: className inconsistency in documentation
141: 
142: ### Recommendations Applied
143: 1. Fixed React Hooks pattern: Moved to Set-based state management
144: 2. Removed gap-2 from button className
145: 3. Added explicit useState import statement
146: 4. Fixed target state diagram to show flat ZIP structure
147: 
148: ### Questions for User
149: (None - all issues resolved)
150: ```
151: 
152: ### Execution Review Output (if exe|execution flag used)
153: 
154: ```markdown
155: ## Context-Naive Peer Review: Execution
156: 
157: **PRP**: PRP-8.8-web-ui-ux-improvements.md
158: **Execution Reviewed**: 2025-10-02T20:00:00Z
159: 
160: ### Implementation Findings
161: - âœ… Change #1: ZIP structure simplified correctly (main.py:609)
162: - âœ… Change #2: Section numbering removed (App.tsx:91,100,146)
163: - âœ… Change #3: Download button text updated (App.tsx:202)
164: - âš ï¸ Issue: useState import missing from App.tsx
165: - âŒ Critical: expandedJobs state not initialized at component level
166: 
167: ### Fixes Applied
168: 1. Added useState import to App.tsx line 2
169: 2. Initialized expandedJobs Set state at component level
170: 3. Tested expanded state persistence during polling
171: 
172: ### Questions for User
173: (None - all issues resolved)
174: ```
175: 
176: ## Context-Naive Definition
177: 
178: **What It Means**:
179: - **IGNORE**: Conversation that generated PRP or implementation code
180: - **USE**: Project context (CLAUDE.md, codebase structure, ce examples, existing PRPs, serena memories)
181: - **GOAL**: Fresh perspective as if first time reading the artifact
182: 
183: **Why It Matters**:
184: - Catches inconsistencies between plan and code
185: - Identifies assumptions made during rapid development
186: - Validates documentation matches implementation
187: - Ensures artifacts are self-documenting
188: 
189: ## Error Handling
190: 
191: ### PRP Not Found
192: ```
193: âŒ PRP not found: "shift pattern logic"
194: 
195: Available PRPs matching search:
196: - PRP-5.2: Shift Patterns Logic Implementation
197: - PRP-5.3: Critical Validation Enum Fixes
198: 
199: Please specify: /peer-review PRP-5.2
200: ```
201: 
202: ### Multiple Matches
203: ```
204: âš ï¸ Multiple PRPs match "shift pattern":
205: 1. PRP-5.2: Shift Patterns Logic Implementation
206: 2. PRP-6.9.3: Shift Pattern Hour Ranges
207: 
208: Please clarify which PRP to review.
209: ```
210: 
211: ### Execution Review Without Execution
212: ```
213: âš ï¸ Execution review requested but PRP not executed: PRP-8.8
214: 
215: Please execute PRP first:
216: /execute-prp PRP-8.8
217: 
218: Then review execution:
219: /peer-review PRP-8.8 exe
220: ```
221: 
222: ## Integration with Workflow
223: 
224: ### Single PRP Workflow
225: 
226: #### After Generate PRP (Document Review)
227: ```bash
228: # Generate PRP
229: /generate-prp "Web UI UX improvements"
230: 
231: # Immediately review document quality
232: /peer-review latest
233: 
234: # Result: PRP improved before any coding starts
235: ```
236: 
237: #### After Execute PRP (Execution Review)
238: ```bash
239: # Execute PRP implementation
240: /execute-prp PRP-8.8
241: 
242: # Review execution results with fresh eyes
243: /peer-review PRP-8.8 execution
244: 
245: # Result: Catches implementation issues vs spec
246: ```
247: 
248: #### Complete Single PRP Workflow
249: ```bash
250: # Step 1: Generate and review PRP document
251: /generate-prp "simplify ZIP structure"
252: /peer-review latest
253: 
254: # Step 2: Execute PRP
255: /execute-prp latest
256: 
257: # Step 3: Review execution
258: /peer-review latest exe
259: 
260: # Result: High-quality PRP + implementation
261: ```
262: 
263: ### Batch PRP Workflow
264: 
265: #### During Batch Execution (Individual PRP Review)
266: ```bash
267: # Generate batch
268: /batch-gen-prp PRP-34-INITIAL.md
269: 
270: # Review individual PRP in batch (if needed)
271: /peer-review 34.2.1              # Review doc before batch execution
272: /peer-review "classification"    # Search by keyword in batch
273: 
274: # Execute batch
275: /batch-exe-prp --batch 34
276: 
277: # Review individual PRP execution (if issues found)
278: /peer-review 34.2.1 exe          # Review execution of one PRP
279: /peer-review latest execution    # Review latest executed PRP in batch
280: ```
281: 
282: #### When to Use Single vs Batch Review
283: 
284: **Use `/peer-review` (single) when**:
285: - Reviewing one specific PRP in detail
286: - Fixing issues in one PRP during batch workflow
287: - Iterating on one PRP before batch execution
288: - Deep dive into one PRP's execution
289: 
290: **Use `/batch-peer-review` (batch) when**:
291: - Reviewing all PRPs in batch systematically
292: - Checking inter-PRP consistency (deps, terminology, file conflicts)
293: - Quality gate before/after batch execution
294: - Parallel review of multiple PRPs
295: 
296: ### Quality Gate Before Merge
297: ```bash
298: # Review completed PRP execution (single)
299: /peer-review PRP-8.8 execution
300: 
301: # Review completed batch execution (batch)
302: /batch-peer-review --batch 34 --exe
303: 
304: # Validates:
305: # - All changes implemented correctly
306: # - No unintended side effects
307: # - Acceptance criteria met
308: # - (Batch only) Inter-PRP integration correct
309: ```
310: 
311: ## Command Implementation
312: 
313: This command should:
314: 1. **Detect Batch Context**:
315:    - Search conversation for recent `/batch-gen-prp` or `/batch-exe-prp` calls (last 10 messages)
316:    - Extract batch ID from command: `/batch-gen-prp PRP-34-INITIAL.md` â†’ batch 34
317:    - OR detect batch notation in PRP reference: `34.2.1` â†’ batch 34
318:    - OR scan for batch PRP files in recent context (e.g., `PRP-34.2.1.md`)
319:    - Use detected batch ID to prioritize batch PRPs when searching (search `PRPs/feature-requests/PRP-{batch_id}.*` first)
320: 2. **Parse prp-reference parameter**:
321:    - ID formats: `PRP-8.8`, `8.8`, `34.2.1` (batch notation)
322:    - Filepath: `context-engineering/PRPs/PRP-8.8.md` or `PRPs/feature-requests/PRP-34.2.1.md`
323:    - Natural language: Search within batch PRPs first (if batch active), then all PRPs
324:    - "latest": Most recent PRP from conversation (prioritize batch context)
325: 3. **Locate PRP file**:
326:    - If batch notation (e.g., `34.2.1`): Search `PRPs/feature-requests/PRP-34.2.1*.md`
327:    - If standard notation: Search `.ce/PRPs/` or `PRPs/` or `context-engineering/PRPs/`
328:    - If NL search: Prioritize batch PRPs (if batch active), then search all PRPs
329: 4. **Phase 1 - Document Review**:
330:    - Read PRP document (ignoring generation conversation)
331:    - Perform systematic quality review (9 checks)
332:    - Apply recommendations to PRP file
333: 5. **Phase 2 - Execution Review** (if exe|execution flag):
334:    - Check PRP has been executed (look for changed files from PRP specs)
335:    - Read implementation files (ignoring implementation conversation)
336:    - Validate implementation vs PRP requirements (9 checks)
337:    - Apply fixes to code
338: 6. **Output review summary** with findings + fixes
339: 
340: ## Best Practices
341: 
342: **When to Use Document Review Only**:
343: - After generating new PRP (validate quality before execution)
344: - Reviewing PRPs created by others
345: - Planning phase - want to improve docs before coding
346: 
347: **When to Use Execution Review**:
348: - After `/execute-prp` completes (validate implementation matches spec)
349: - Before marking PRP as complete
350: - Quality gate before merging to main branch
351: - Troubleshooting implementation issues
352: 
353: **Command Efficiency**:
354: - Use natural language for quick PRP lookup
355: - Use `latest` to review most recent work
356: - Separate document and execution reviews for focused feedback
357: 
358: ## Related Commands
359: 
360: **Single PRP Commands**:
361: - `/generate-prp` - Generate new PRP from natural language
362: - `/execute-prp` - Execute PRP implementation
363: - `/peer-review` - Review single PRP (document or execution)
364: 
365: **Batch PRP Commands**:
366: - `/batch-gen-prp` - Generate multiple PRPs from master plan
367: - `/batch-exe-prp` - Execute batch of PRPs in parallel
368: - `/batch-peer-review` - Review entire batch (document or execution)
369: 
370: **Context Management**:
371: - `/update-context` - Update project context after PRP execution
372: 
373: ## Workflow Integration
374: 
375: ### Single PRP Workflow
376: 
377: ```
378: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
379: â”‚  /generate-prp  â”‚  Create PRP
380: â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
381:          â”‚
382:          â†“
383: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
384: â”‚  /peer-review   â”‚  Review document quality
385: â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
386:          â”‚
387:          â†“
388: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
389: â”‚  /execute-prp   â”‚  Implement changes
390: â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
391:          â”‚
392:          â†“
393: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
394: â”‚ /peer-review exeâ”‚  Review execution results
395: â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
396: ```
397: 
398: ### Batch PRP Workflow (with single peer-review for spot checks)
399: 
400: ```
401: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
402: â”‚  /batch-gen-prp      â”‚  Create batch of PRPs
403: â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
404:            â”‚
405:            â†“
406: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
407: â”‚ /batch-peer-review   â”‚  Review all PRPs (document)
408: â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
409:            â”‚
410:            â†“  (optional spot checks)
411:     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
412:     â”‚             â”‚
413:     â†“             â†“
414: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” Continue
415: â”‚ /peer-review â”‚ (individual PRP fixes)
416: â”‚   34.2.1     â”‚
417: â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
418:            â”‚
419:            â†“
420: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
421: â”‚  /batch-exe-prp      â”‚  Execute batch (parallel)
422: â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
423:            â”‚
424:            â†“
425: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
426: â”‚ /batch-peer-review   â”‚  Review all executions
427: â”‚       --exe          â”‚
428: â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
429:            â”‚
430:            â†“  (optional spot checks)
431:     â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
432:     â”‚             â”‚
433:     â†“             â†“
434: â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” Continue
435: â”‚ /peer-review â”‚ (individual execution fixes)
436: â”‚  34.2.1 exe  â”‚
437: â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
438:            â”‚
439:            â†“
440:       âœ… Done
441: ```
</file>

<file path="CLAUDE.md">
  1: # Context Engineering Tools - Project Guide
  2: 
  3: **Project**: CLI tooling for Context Engineering framework operations
  4: 
  5: ## Communication
  6: 
  7: Direct, token-efficient. No fluff. Call out problems directly.
  8: 
  9: ## Core Principles
 10: 
 11: ### Syntropy MCP First
 12: - Use `mcp__syntropy__<server>_<tool>` format
 13: - Prefer Syntropy tools over bash/cmdline
 14: 
 15: ### No Fishy Fallbacks
 16: - Fast failure: Let exceptions bubble up
 17: - Actionable errors: Include ðŸ”§ troubleshooting
 18: - No silent corruption
 19: 
 20: ### KISS
 21: - Simple solutions first
 22: - Clear code over clever code
 23: - Minimal dependencies (stdlib only)
 24: - Single responsibility per function
 25: 
 26: ### UV Package Management - STRICT
 27: ```bash
 28: uv add package-name              # Production
 29: uv add --dev package-name        # Development
 30: uv sync                          # Install
 31: 
 32: # âŒ FORBIDDEN: Manual pyproject.toml editing
 33: ```
 34: 
 35: ### Ad-Hoc Code Policy
 36: - Max 3 LOC inline
 37: - Longer code â†’ tmp/ file and execute
 38: - Must execute via run_py
 39: 
 40: ## Quick Commands
 41: 
 42: ```bash
 43: cd tools
 44: 
 45: # Validation & health
 46: uv run ce validate --level all
 47: uv run ce context health
 48: uv run ce git status
 49: 
 50: # Cleanup
 51: uv run ce vacuum                  # Dry-run (report only)
 52: uv run ce vacuum --execute        # Delete temp files only
 53: uv run ce vacuum --auto           # Delete temp files + obsolete docs/dead links
 54: 
 55: # Testing
 56: uv run pytest tests/ -v
 57: 
 58: # Run Python (3 LOC max ad-hoc)
 59: uv run ce run_py "print('hello')"
 60: uv run ce run_py ../tmp/script.py
 61: ```
 62: 
 63: ## Framework Initialization
 64: 
 65: **Quick Start** (Automated - RECOMMENDED):
 66: ```bash
 67: npx syntropy-mcp init ce-framework
 68: ```
 69: Time: <5 minutes | Automatically extracts 50 files, reorganizes tools, blends settings
 70: 
 71: **Manual Setup**: See [examples/INITIALIZATION.md](examples/INITIALIZATION.md) for complete CE 1.1 initialization guide.
 72: 
 73: **Manual Key Steps** (5-phase workflow):
 74: 1. **Bucket Collection**: Extract existing Serena memories, examples, PRPs, CLAUDE.md, .claude directory
 75: 2. **User Files Migration**: Copy validated user files with `type: user` YAML headers
 76: 3. **Repomix Package Handling**: Extract ce-infrastructure.xml to /system/ subfolders
 77: 4. **Blending**: Merge framework + user files (CLAUDE.md sections, settings.local.json, commands)
 78: 5. **Cleanup**: Remove initialization artifacts, verify structure
 79: 
 80: **Repomix Usage** (manual context loading):
 81: 
 82: ```bash
 83: # Load workflow docs (commands, validation, PRP patterns)
 84: # Reference package - stored in .ce/, not extracted during initialization
 85: cat .ce/ce-workflow-docs.xml
 86: 
 87: # Load infrastructure docs (memories, rules, system architecture)
 88: # Extracted to /system/ subfolders during Phase 3 of initialization
 89: npx repomix --unpack .ce/ce-infrastructure.xml --target tmp/extraction/
 90: ```
 91: 
 92: **Repomix Package Structure** (CE 1.1):
 93: - **ce-workflow-docs.xml**: 85KB (reference package, not extracted)
 94: - **ce-infrastructure.xml**: 206KB (all framework files with /system/ organization)
 95: - **Combined**: 291KB total
 96: 
 97: **Build and Distribution**:
 98: ```bash
 99: # Regenerate packages and distribute to syntropy-mcp
100: .ce/build-and-distribute.sh
101: ```
102: This script regenerates both packages and copies them to `syntropy-mcp/boilerplate/ce-framework/` for automated init.
103: 
104: **Migration Scenarios**:
105: 
106: All scenarios documented in [INITIALIZATION.md](examples/INITIALIZATION.md) with scenario-specific variations within each phase:
107: - **Greenfield**: New project setup (10 min)
108: - **Mature Project**: Add CE to existing codebase (45 min)
109: - **CE 1.0 Upgrade**: Upgrade CE 1.0 â†’ CE 1.1 (40 min)
110: - **Partial Install**: Complete partial CE installation (15 min)
111: 
112: **Memory Type System** (CE 1.1):
113: 
114: Framework memories (23 files) use `type: regular` by default:
115: ```yaml
116: ---
117: type: regular
118: category: documentation
119: tags: [tag1, tag2, tag3]
120: created: "2025-11-04T17:30:00Z"
121: updated: "2025-11-04T17:30:00Z"
122: ---
123: ```
124: 
125: **Critical Memory Candidates** (upgrade during initialization):
126: - code-style-conventions.md
127: - suggested-commands.md
128: - task-completion-checklist.md
129: - testing-standards.md
130: - tool-usage-syntropy.md
131: - use-syntropy-tools-not-bash.md
132: 
133: **User File Headers** (added during Phase 2 of initialization):
134: 
135: User memories:
136: ```yaml
137: ---
138: type: user
139: source: target-project
140: created: "2025-11-04T00:00:00Z"
141: updated: "2025-11-04T00:00:00Z"
142: ---
143: ```
144: 
145: User PRPs:
146: ```yaml
147: ---
148: prp_id: USER-001
149: title: User Feature Implementation
150: status: completed
151: created: "2025-11-04"
152: source: target-project
153: type: user
154: ---
155: ```
156: 
157: **See Also**:
158: - [examples/INITIALIZATION.md](examples/INITIALIZATION.md) - Complete initialization guide
159: - [.serena/memories/README.md](.serena/memories/README.md) - Memory type system documentation
160: - [examples/templates/PRP-0-CONTEXT-ENGINEERING.md](examples/templates/PRP-0-CONTEXT-ENGINEERING.md) - Document framework installation
161: 
162: ## Working Directory
163: 
164: **Default**: `/Users/bprzybysz/nc-src/ctx-eng-plus`
165: 
166: **For tools/ commands**: Use `cd tools &&` or `uv run -C tools`
167: 
168: **Note**: In THIS repo (ctx-eng-plus development), tools are at `tools/`. In TARGET projects that extract the infrastructure package, tools are installed to `.ce/tools/`.
169: 
170: ## Hooks
171: 
172: **Pre-Commit**: Runs `ce validate --level 4` before commit (skip: `--no-verify`)
173: 
174: **Session Start**: Auto drift score check
175: 
176: **Shell Functions** (optional): Source `.ce/shell-functions.sh` for `cet` alias
177: 
178: ## Tool Naming Convention
179: 
180: Format: `mcp__syntropy__<server>_<tool>`
181: - `mcp__` - MCP prefix (double underscore)
182: - `syntropy__` - Syntropy server (double underscore)
183: - `<server>_` - Server name + single underscore
184: - `<tool>` - Tool name
185: 
186: Example: `mcp__syntropy__serena_find_symbol`
187: 
188: ## Allowed Tools Summary
189: 
190: **Post-Lockdown State** (after PRP-A & PRP-D):
191: - **Before**: 87 MCP tools (via Syntropy aggregator)
192: - **After**: 28 MCP tools (59 denied for native tool preference)
193: - **Token reduction**: ~44k tokens (96% reduction from 46kâ†’2k)
194: 
195: ### Kept Tools by Category
196: 
197: **Serena** (13 tools): Code symbol navigation
198: - activate_project, find_symbol, get_symbols_overview, search_for_pattern
199: - find_referencing_symbols, replace_symbol_body, write_memory, read_memory
200: - list_memories, create_text_file, read_file, list_dir, delete_memory
201: 
202: **Linear** (9 tools): Project management integration
203: - create_issue, get_issue, list_issues, update_issue
204: - list_projects, list_teams, list_users, get_team, create_project
205: 
206: **Context7** (2 tools): Library documentation
207: - resolve_library_id, get_library_docs
208: 
209: **Thinking** (1 tool): Complex reasoning
210: - sequentialthinking
211: 
212: **Syntropy System** (3 tools): System utilities
213: - healthcheck (MCP diagnostics)
214: - enable_tools (dynamic tool management)
215: - list_all_tools (list all available tools with states)
216: 
217: **Bash Commands** (~50 patterns): See "Command Permissions" section below
218: **Native Tools**: Read, Write, Edit, Glob, Grep, WebSearch, WebFetch
219: 
220: ### Denied Tools (55 total)
221: 
222: **Rationale**: Native Claude Code tools provide equivalent or better functionality
223: 
224: **Categories**:
225: - Filesystem (8): Use Read, Write, Edit, Glob instead
226: - Git (5): Use Bash(git:*) instead
227: - GitHub (26): Use Bash(gh:*) instead
228: - Repomix (4): Use incremental Glob/Grep/Read instead
229: - Playwright (6): Use WebFetch or Bash(playwright CLI) instead
230: - Perplexity (1): Use WebSearch instead
231: - Syntropy (5): Use Read for docs, rare-use tools
232: 
233: **Full details**: See [TOOL-USAGE-GUIDE.md](TOOL-USAGE-GUIDE.md)
234: 
235: ## Command Permissions
236: 
237: **Permission Model**: Auto-allow safe commands, ask-first for potentially destructive operations.
238: 
239: ### Auto-Allow Patterns (~35 bash patterns)
240: 
241: Commands that never prompt:
242: 
243: **File Inspection**:
244: - `ls`, `cat`, `head`, `tail`, `less`, `more`, `file`, `stat`
245: 
246: **Navigation**:
247: - `cd`, `pwd`, `which`, `whereis`
248: 
249: **Search**:
250: - `find`, `grep`, `rg`, `tree`
251: 
252: **Text Processing**:
253: - `sed`, `awk`, `sort`, `uniq`, `cut`, `diff`, `comm`, `wc`
254: 
255: **Environment**:
256: - `env`, `ps`, `echo`
257: 
258: **Development**:
259: - `git` (all operations), `gh` (GitHub CLI)
260: - `uv`, `uvx`, `pytest`
261: - `python`, `python3`
262: 
263: **Special Cases**:
264: - `rm -rf ~/.mcp-auth` (MCP troubleshooting)
265: 
266: **Full list**: See `.claude/settings.local.json` "allow" array
267: 
268: ### Ask-First Patterns (15 patterns)
269: 
270: Commands that require confirmation:
271: 
272: **File Operations** (potentially destructive):
273: - `rm`, `mv`, `cp`
274: 
275: **Network Operations**:
276: - `curl`, `wget`, `nc`, `telnet`, `ssh`, `scp`, `rsync`
277: 
278: **Package Management**:
279: - `brew install`, `npm install`, `pip install`, `gem install`
280: 
281: **System Operations**:
282: - `sudo` (any sudo command)
283: 
284: **Rationale**: Safety gate for operations that modify files, access network, or require elevated privileges.
285: 
286: **Full list**: See `.claude/settings.local.json` "ask" array
287: 
288: ### Permission Behavior
289: 
290: **Unlisted commands**: Prompt by default (ask before execution)
291: **Workaround**: Add to allow list in `.claude/settings.local.json` if frequently used
292: 
293: ## Quick Tool Selection
294: 
295: **ðŸ”— Comprehensive Guide**: See [examples/TOOL-USAGE-GUIDE.md](examples/TOOL-USAGE-GUIDE.md) for:
296: - Decision tree (flowchart for tool selection)
297: - Common tasks with right/wrong examples
298: - Anti-patterns to avoid
299: - Migration table (55 denied tools â†’ alternatives)
300: 
301: **Quick Reference**:
302: 
303: **Analyze code**:
304: - Know symbol â†’ `serena_find_symbol`
305: - Explore file â†’ `serena_get_symbols_overview`
306: - Search patterns â†’ `Grep` (native, not serena_search_for_pattern)
307: - Find usages â†’ `serena_find_referencing_symbols`
308: 
309: **Modify files**:
310: - New â†’ `Write` (native)
311: - Existing (surgical) â†’ `Edit` (native)
312: - Config/text â†’ `Read` (native)
313: 
314: **Version control**:
315: - Use `Bash(git:*)` (native git commands)
316: - NOT `mcp__syntropy__git_git_status` (denied)
317: 
318: **GitHub operations**:
319: - Use `Bash(gh:*)` (native gh CLI)
320: - NOT `mcp__syntropy__github_*` (denied)
321: 
322: **External knowledge**:
323: - Documentation â†’ `context7_get_library_docs`
324: - Web search â†’ `WebSearch` (native)
325: - Web content â†’ `WebFetch` (native)
326: 
327: **Complex reasoning**: `sequentialthinking`
328: 
329: **Project management**: Linear tools (all 9 kept)
330: 
331: **System health**: `healthcheck` (detailed diagnostics with `detailed=true`)
332: 
333: ## Project Structure
334: 
335: ```
336: tools/
337: â”œâ”€â”€ ce/                 # Source code
338: â”‚   â”œâ”€â”€ core.py         # File, git, shell ops
339: â”‚   â”œâ”€â”€ validate.py     # 3-level validation
340: â”‚   â””â”€â”€ context.py      # Context management
341: â”œâ”€â”€ tests/              # Test suite
342: â”œâ”€â”€ pyproject.toml      # UV config (don't edit!)
343: â””â”€â”€ bootstrap.sh        # Setup script
344: ```
345: 
346: ## Testing Standards
347: 
348: **TDD**: Test first â†’ fail â†’ implement â†’ refactor
349: 
350: **Real functionality**: No fake results, no mocks in tests
351: 
352: **Test before critical changes** (tool naming, API changes, refactoring)
353: 
354: ## Code Quality
355: 
356: - Functions: 50 lines (single responsibility)
357: - Files: 300-500 lines (logical modules)
358: - Classes: 100 lines (single concept)
359: - Mark mocks with FIXME in production code
360: 
361: ## Context Commands
362: 
363: ```bash
364: # Sync all PRPs with codebase
365: cd tools && uv run ce update-context
366: 
367: # Sync specific PRP
368: cd tools && uv run ce update-context --prp PRPs/executed/PRP-6.md
369: 
370: # Fast drift check (2-3s vs 10-15s)
371: cd tools && uv run ce analyze-context
372: 
373: # Force re-analysis
374: cd tools && uv run ce analyze-context --force
375: ```
376: 
377: **Drift Exit Codes**:
378: - 0: <5% (healthy)
379: - 1: 5-15% (warning)
380: - 2: â‰¥15% (critical)
381: 
382: ## Syntropy MCP Tool Sync
383: 
384: **Dynamic tool management** - Enable/disable tools at runtime without restart
385: 
386: ```bash
387: # Sync settings with Syntropy MCP tool state
388: /sync-with-syntropy
389: 
390: # Workflow example:
391: # 1. Enable/disable tools via Syntropy
392: mcp__syntropy__enable_tools(
393:   enable=["serena_find_symbol", "context7_get_library_docs"],
394:   disable=["filesystem_read_file", "git_git_status"]
395: )
396: 
397: # 2. Sync settings to .claude/settings.local.json
398: /sync-with-syntropy
399: 
400: # 3. Verify changes
401: cat .claude/settings.local.json
402: ```
403: 
404: **How it works**:
405: 1. Call `mcp__syntropy__list_all_tools` to get current tool states
406: 2. Update `.claude/settings.local.json` to match
407: 3. Backup original settings to `.claude/settings.local.json.backup`
408: 4. Output clear summary of changes made
409: 
410: **Benefits**:
411: - Real-time tool control (no MCP restart needed)
412: - Persistent state across sessions (`~/.syntropy/tool-state.json`)
413: - Context-aware tool sets (enable 10 tools for quick tasks, all 87 for deep analysis)
414: 
415: ## Linear Integration
416: 
417: **Config**: `.ce/linear-defaults.yml`
418: - Project: "Context Engineering"
419: - Assignee: "blazej.przybyszewski@gmail.com"
420: - Team: "Blaise78"
421: 
422: **Auto-create issues**: `/generate-prp` uses defaults
423: 
424: **Join existing issue**: `/generate-prp --join-prp 12`
425: 
426: **Troubleshooting**: `rm -rf ~/.mcp-auth` (pre-approved)
427: 
428: ## Batch PRP Generation
429: 
430: **Decompose large plans into staged, parallelizable PRPs with automatic dependency analysis**
431: 
432: ```bash
433: # Create plan document
434: vim FEATURE-PLAN.md
435: 
436: # Generate all PRPs with parallel subagents
437: /batch-gen-prp FEATURE-PLAN.md
438: 
439: # Output: Multiple PRPs with format PRP-X.Y.Z
440: #   X = Batch ID (next free number)
441: #   Y = Stage number
442: #   Z = Order within stage
443: ```
444: 
445: **Plan Format**:
446: ```markdown
447: # Plan Title
448: 
449: ## Phases
450: 
451: ### Phase 1: Name
452: 
453: **Goal**: One-sentence objective
454: **Estimated Hours**: 0.5
455: **Complexity**: low
456: **Files Modified**: path/to/file
457: **Dependencies**: None
458: **Implementation Steps**: [steps]
459: **Validation Gates**: [gates]
460: ```
461: 
462: **What It Does**:
463: 1. Parses plan document â†’ Extracts phases
464: 2. Builds dependency graph â†’ Analyzes deps + file conflicts
465: 3. Assigns stages â†’ Groups independent PRPs for parallel execution
466: 4. Spawns Sonnet subagents â†’ Parallel generation per stage
467: 5. Monitors via heartbeat files â†’ 30s polling, kills after 2 failed polls
468: 6. Creates Linear issues â†’ One per PRP
469: 7. Outputs summary â†’ All generated PRPs grouped by stage
470: 
471: **Example Output**:
472: ```
473: Batch 43:
474:   Stage 1: PRP-43.1.1
475:   Stage 2: PRP-43.2.1, PRP-43.2.2, PRP-43.2.3 (parallel)
476:   Stage 3: PRP-43.3.1
477: ```
478: 
479: **Integration with Execution**:
480: ```bash
481: # Generate PRPs from plan
482: /batch-gen-prp BIG-FEATURE-PLAN.md
483: 
484: # Execute entire batch
485: /batch-exe-prp --batch 43
486: 
487: # Or stage-by-stage
488: /batch-exe-prp --batch 43 --stage 1
489: /batch-exe-prp --batch 43 --stage 2
490: ```
491: 
492: **Time Savings**: 8 PRPs sequential (30 min) â†’ parallel (10-12 min) = **60% faster**
493: 
494: **See**: `.claude/commands/batch-gen-prp.md` for complete documentation
495: 
496: ## PRP Sizing
497: 
498: ```bash
499: cd tools && uv run ce prp analyze <path-to-prp.md>
500: ```
501: 
502: **Size Categories**:
503: - GREEN: â‰¤700 lines, â‰¤8h, LOW-MEDIUM risk
504: - YELLOW: 700-1000 lines, 8-12h, MEDIUM risk
505: - RED: >1000 lines, >12h, HIGH risk
506: 
507: **Exit Codes**: 0 (GREEN), 1 (YELLOW), 2 (RED)
508: 
509: ## Testing Patterns
510: 
511: **Strategy pattern** for composable testing:
512: - **Unit**: Test single strategy in isolation
513: - **Integration**: Test subgraph with real + mock
514: - **E2E**: Full pipeline, all external deps mocked
515: 
516: **Mock Strategies**: MockSerenaStrategy, MockContext7Strategy, MockLLMStrategy
517: 
518: **Real Strategies**: RealParserStrategy, RealCommandStrategy
519: 
520: ## Documentation Standards
521: 
522: **Mermaid Diagrams**: Always specify text color
523: - Light backgrounds â†’ `color:#000`
524: - Dark backgrounds â†’ `color:#fff`
525: - Format: `style X fill:#bgcolor,color:#textcolor`
526: 
527: ## Efficient Doc Review
528: 
529: **Grep-first validation** (90% token reduction):
530: 1. Structural validation (Grep patterns, 1-2k tokens)
531: 2. Code quality checks (Grep anti-patterns, 500 tokens)
532: 3. Targeted reads (2-3 files only, 3-5k tokens)
533: 
534: **Total**: ~5-7k tokens vs 200k+ for read-all
535: 
536: ## Resources
537: 
538: - `.ce/` - System boilerplate (don't modify)
539: - `.ce/RULES.md` - Framework rules
540: - `PRPs/[executed,feature-requests]` - Feature requests
541: - `examples/` - Framework patterns and user code
542: 
543: ## Keyboard Shortcuts
544: 
545: ### Image Pasting (macOS)
546: 
547: **cmd+v**: Paste screenshot images into Claude Code
548: - Requires Karabiner-Elements (configured via PRP-30)
549: - Remaps cmd+v â†’ ctrl+v in terminals only
550: - Config: `~/.config/karabiner/assets/complex_modifications/claude-code-cmd-v.json`
551: - Toggle: Karabiner-Elements â†’ Complex Modifications
552: 
553: **Setup** (one-time):
554: ```bash
555: brew install --cask karabiner-elements
556: # Enable rule in Karabiner-Elements UI â†’ Complex Modifications
557: ```
558: 
559: ## Git Worktree - Parallel PRP Development
560: 
561: **Native git solution for working on multiple PRPs simultaneously**
562: 
563: ### Quick Start
564: 
565: ```bash
566: # Create worktree for PRP-A (creates ../ctx-eng-plus-prp-a)
567: git worktree add ../ctx-eng-plus-prp-a -b prp-a-feature
568: 
569: # Work in worktree
570: cd ../ctx-eng-plus-prp-a
571: # Make changes...
572: git add .
573: git commit -m "Implement feature"
574: 
575: # List all worktrees
576: git worktree list
577: 
578: # Remove worktree after merging
579: git worktree remove ../ctx-eng-plus-prp-a
580: ```
581: 
582: ### Commands
583: 
584: **Create**:
585: ```bash
586: git worktree add <path> -b <branch-name>
587: # Example: git worktree add ../ctx-eng-plus-prp-12 -b prp-12-validation
588: ```
589: 
590: **List**:
591: ```bash
592: git worktree list
593: # Shows: path, commit hash, branch name
594: ```
595: 
596: **Remove**:
597: ```bash
598: git worktree remove <path>
599: # or: git worktree remove --force <path>  # if uncommitted changes
600: ```
601: 
602: **Prune** (clean stale references):
603: ```bash
604: git worktree prune
605: ```
606: 
607: ### Workflow for Parallel PRPs
608: 
609: **Stage 1: Create Worktrees**
610: ```bash
611: # From main repo: /Users/bprzybysz/nc-src/ctx-eng-plus
612: git worktree add ../ctx-eng-plus-prp-a -b prp-a-tool-deny
613: git worktree add ../ctx-eng-plus-prp-b -b prp-b-usage-guide
614: git worktree add ../ctx-eng-plus-prp-c -b prp-c-worktree-docs
615: ```
616: 
617: **Stage 2: Execute in Parallel**
618: ```bash
619: # Terminal 1
620: cd ../ctx-eng-plus-prp-a
621: # Edit .claude/settings.local.json
622: git add .
623: git commit -m "PRP-A: Add tools to deny list"
624: 
625: # Terminal 2
626: cd ../ctx-eng-plus-prp-b
627: # Create TOOL-USAGE-GUIDE.md
628: git add .
629: git commit -m "PRP-B: Create tool usage guide"
630: 
631: # Terminal 3
632: cd ../ctx-eng-plus-prp-c
633: # Update CLAUDE.md
634: git add .
635: git commit -m "PRP-C: Migrate to worktree docs"
636: ```
637: 
638: **Stage 3: Merge in Order**
639: ```bash
640: cd /Users/bprzybysz/nc-src/ctx-eng-plus
641: git checkout main
642: 
643: # Merge PRP-A first
644: git merge prp-a-tool-deny --no-ff
645: git push origin main
646: 
647: # Merge PRP-B
648: git merge prp-b-usage-guide --no-ff
649: git push origin main
650: 
651: # Merge PRP-C (may conflict with PRP-A on settings.local.json)
652: git merge prp-c-worktree-docs --no-ff
653: # If conflicts, resolve manually (see Conflict Resolution below)
654: git push origin main
655: ```
656: 
657: **Stage 4: Cleanup**
658: ```bash
659: git worktree remove ../ctx-eng-plus-prp-a
660: git worktree remove ../ctx-eng-plus-prp-b
661: git worktree remove ../ctx-eng-plus-prp-c
662: git worktree prune
663: ```
664: 
665: ### Critical Constraints
666: 
667: **âš ï¸ Same Branch Limitation**
668: 
669: **CANNOT** check out the same branch in multiple worktrees simultaneously.
670: 
671: **Example of ERROR**:
672: ```bash
673: # Main repo on `main` branch
674: cd /Users/bprzybysz/nc-src/ctx-eng-plus
675: git branch
676: # * main
677: 
678: # Try to create worktree on `main`
679: git worktree add ../ctx-eng-plus-test -b main
680: # ERROR: fatal: 'main' is already checked out at '/Users/bprzybysz/nc-src/ctx-eng-plus'
681: ```
682: 
683: **Solution**: Each worktree must use a **unique branch**.
684: 
685: ```bash
686: # Main repo stays on gitbutler/workspace or main
687: # Each PRP worktree uses dedicated branch
688: git worktree add ../ctx-eng-plus-prp-a -b prp-a-unique  # âœ“
689: git worktree add ../ctx-eng-plus-prp-b -b prp-b-unique  # âœ“
690: ```
691: 
692: ### Conflict Resolution
693: 
694: When merging parallel PRPs, conflicts may occur if they modify the same file sections.
695: 
696: **Scenario 1: No Conflicts** (PRP-A + PRP-B)
697: ```bash
698: git merge prp-a-tool-deny --no-ff  # âœ“ Success
699: git merge prp-b-usage-guide --no-ff  # âœ“ Success (different files)
700: ```
701: 
702: **Scenario 2: Merge Conflict** (PRP-A + PRP-D both edit settings.local.json)
703: 
704: **Step 1: Attempt Merge**
705: ```bash
706: git merge prp-d-command-perms --no-ff
707: # Auto-merging .claude/settings.local.json
708: # CONFLICT (content): Merge conflict in .claude/settings.local.json
709: # Automatic merge failed; fix conflicts and then commit the result.
710: ```
711: 
712: **Step 2: Check Conflict Markers**
713: ```bash
714: git status
715: # Unmerged paths:
716: #   both modified:   .claude/settings.local.json
717: ```
718: 
719: **Step 3: Read File to See Conflicts**
720: ```python
721: Read(file_path="/Users/bprzybysz/nc-src/ctx-eng-plus/.claude/settings.local.json")
722: # Look for:
723: # <<<<<<< HEAD
724: # ... current branch content ...
725: # =======
726: # ... incoming branch content ...
727: # >>>>>>> prp-d-command-perms
728: ```
729: 
730: **Step 4: Resolve with Edit Tool**
731: ```python
732: # Remove conflict markers, keep desired changes from both branches
733: Edit(
734:   file_path="/Users/bprzybysz/nc-src/ctx-eng-plus/.claude/settings.local.json",
735:   old_string="""<<<<<<< HEAD
736:   "deny": [existing tools...]
737: =======
738:   "deny": [incoming tools...]
739: >>>>>>> prp-d-command-perms""",
740:   new_string="""  "deny": [merged tools from both branches...]"""
741: )
742: ```
743: 
744: **Step 5: Stage and Commit**
745: ```bash
746: git add .claude/settings.local.json
747: git commit -m "Merge prp-d-command-perms: Resolve settings conflict"
748: ```
749: 
750: **Scenario 3: Conflicting Logic** (PRP-A denies tool, PRP-D allows same tool)
751: 
752: **Resolution**: Apply **last-merged wins** or **manual decision**.
753: 
754: ```json
755: // PRP-A (merged first): Denies "mcp__syntropy__git_git_status"
756: "deny": ["mcp__syntropy__git_git_status"]
757: 
758: // PRP-D (merging now): Allows "git" commands implicitly
759: "allow": ["Bash(git:*)"]
760: 
761: // Decision: Keep Bash(git:*) in allow, keep git_git_status in deny
762: // Rationale: Native bash git preferred over MCP wrapper
763: ```
764: 
765: ### Comparison: GitButler vs Worktree
766: 
767: | Feature | GitButler | Git Worktree |
768: |---------|-----------|--------------|
769: | **Parallel Development** | âœ“ Virtual branches | âœ“ Physical worktrees |
770: | **Branch Switching** | âœ— Not needed | âœ— Not needed |
771: | **Conflict Detection** | âœ“ Real-time ðŸ”’ icon | âš ï¸ At merge time |
772: | **Native Git** | âœ— Proprietary layer | âœ“ Built-in since Git 2.5 |
773: | **Learning Curve** | Medium (new concepts) | Low (standard git) |
774: | **Merge Strategy** | UI-based | CLI-based (standard) |
775: | **Same Branch Limit** | âœ“ Can work on same "virtual" branch | âœ— Must use unique branches |
776: | **Tool Requirement** | Requires GitButler app + CLI | âœ“ Native git (no install) |
777: | **Workspace Branch** | Auto-merges to `gitbutler/workspace` | Manual merge to `main` |
778: 
779: ### Benefits of Worktree Approach
780: 
781: 1. **Native Git**: No external dependencies, works everywhere
782: 2. **Explicit Branches**: Clear separation, standard git workflow
783: 3. **Merge Control**: Full control over merge order and conflict resolution
784: 4. **Universal**: Works on any git version â‰¥2.5 (2015)
785: 5. **Simple Cleanup**: `git worktree remove` + `git worktree prune`
786: 
787: ### Example: 3-PRP Parallel Execution
788: 
789: ```bash
790: # Stage 1: Create worktrees (30 seconds)
791: git worktree add ../ctx-eng-plus-prp-a -b prp-a-tool-deny
792: git worktree add ../ctx-eng-plus-prp-b -b prp-b-usage-guide
793: git worktree add ../ctx-eng-plus-prp-c -b prp-c-worktree-docs
794: 
795: # Stage 2: Execute in parallel (15 minutes total, vs 45 sequential)
796: # Each PRP executes independently in its worktree
797: 
798: # Stage 3: Merge in dependency order (5 minutes)
799: git merge prp-a-tool-deny --no-ff     # Merge order: 1
800: git merge prp-b-usage-guide --no-ff   # Merge order: 2
801: git merge prp-c-worktree-docs --no-ff # Merge order: 3
802: 
803: # Stage 4: Cleanup (30 seconds)
804: git worktree remove ../ctx-eng-plus-prp-a
805: git worktree remove ../ctx-eng-plus-prp-b
806: git worktree remove ../ctx-eng-plus-prp-c
807: git worktree prune
808: ```
809: 
810: **Time Savings**: 45 min sequential â†’ 20 min parallel (55% reduction)
811: 
812: ---
813: 
814: ## Troubleshooting
815: 
816: ```bash
817: # Tool not found
818: cd tools && uv pip install -e .
819: 
820: # Tests failing
821: uv sync
822: uv run pytest tests/ -v
823: 
824: # Linear "Not connected"
825: rm -rf ~/.mcp-auth
826: 
827: # Check PRP's Linear issue ID
828: grep "^issue:" PRPs/executed/PRP-12-feature.md
829: ```
830: 
831: **New Issues** (added after lockdown):
832: 
833: ### Issue: "Permission prompt for safe command"
834: 
835: **Symptom**: Commands like `ls` or `cat` prompt for permission
836: 
837: **Cause**: Command not in auto-allow list
838: 
839: **Solution**:
840: 1. Check if command matches pattern: `grep 'Bash(ls' .claude/settings.local.json`
841: 2. If missing, add pattern to allow list
842: 3. Or approve once (permission remembered for session)
843: 
844: ### Issue: "Command denied" or "tool not found"
845: 
846: **Symptom**: MCP tool like `mcp__syntropy__filesystem_read_file` fails
847: 
848: **Cause**: Tool in deny list (post-lockdown)
849: 
850: **Solution**:
851: 1. Check TOOL-USAGE-GUIDE.md for alternative
852: 2. Example: `filesystem_read_file` â†’ Use `Read` (native) instead
853: 3. If tool should be allowed, remove from deny list (rare)
854: 
855: ### Issue: "MCP tools context too large"
856: 
857: **Symptom**: Token usage warning for MCP tools
858: 
859: **Cause**: Deny list not applied (MCP not reconnected)
860: 
861: **Solution**:
862: ```bash
863: # Reconnect MCP servers
864: /mcp
865: 
866: # Verify token reduction
867: # Expected: ~2k tokens for MCP tools (was ~46k)
868: ```
869: 
870: ## Permissions
871: 
872: **âŒ NEVER** replace all permissions with one entry in `.claude/settings.local.json`
873: 
874: **âœ… ALLOWED**: Surgical edits to individual permissions
875: 
876: ## Special Notes
877: 
878: - Linear MCP context: "linear( mcp)" = linear-server mcp
879: - Compact conversation: Use claude-3-haiku-20240307
880: - Activate Serena: Use project's root full path
881: - Ad-hoc code strict: 3 LOC max, no exceptions
</file>

</files>
