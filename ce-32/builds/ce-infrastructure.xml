This file is a merged representation of a subset of the codebase, containing specifically included files and files not matching ignore patterns, combined into a single document by Repomix.
The content has been processed where line numbers have been added.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of a subset of the repository's contents that is considered the most important context.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Only files matching these patterns are included: .ce/PRPs/executed/system/PRP-0-CONTEXT-ENGINEERING.md, .serena/memories/**, .claude/commands/batch-exe-prp.md, .claude/commands/batch-gen-prp.md, .claude/commands/denoise.md, .claude/commands/execute-prp.md, .claude/commands/generate-prp.md, .claude/commands/peer-review.md, .claude/commands/sync-with-syntropy.md, .claude/commands/syntropy-health.md, .claude/commands/tools-misuse-scan.md, .claude/commands/update-context.md, .claude/commands/vacuum.md, tools/ce/**/*.py, tools/pyproject.toml, tools/bootstrap.sh, .claude/settings.local.json, CLAUDE.md, .ce/blend-config.yml, tools/README.md, examples/ce-blend-usage.md, examples/ce-init-project-usage.md, examples/example.setting.local.md, examples/INDEX.md, examples/INITIALIZATION.md, examples/linear-integration-example.md, examples/mermaid-color-palette.md, examples/prp-decomposition-patterns.md, examples/README.md, examples/tmp-directory-convention.md, examples/TOOL-USAGE-GUIDE.md, examples/patterns/dedrifting-lessons.md, examples/patterns/error-recovery.py, examples/patterns/mocks-marking.md, examples/patterns/pipeline-testing.py, examples/patterns/strategy-testing.py, examples/model/SystemModel.md
- Files matching these patterns are excluded: examples/patterns/example-simple-feature.md, examples/patterns/git-message-rules.md, examples/l4-validation-example.md, examples/syntropy-status-hook-system.md, tools/tests/**, .git/**, .tmp/**, __pycache__/**, *.pyc
- Files matching default ignore patterns are excluded
- Line numbers have been added to the beginning of each line
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.ce/
  PRPs/
    executed/
      system/
        PRP-0-CONTEXT-ENGINEERING.md
  blend-config.yml
.claude/
  commands/
    batch-exe-prp.md
    batch-gen-prp.md
    denoise.md
    execute-prp.md
    generate-prp.md
    peer-review.md
    sync-with-syntropy.md
    syntropy-health.md
    tools-misuse-scan.md
    update-context.md
    vacuum.md
  settings.local.json
.serena/
  memories/
    code-style-conventions.md
    codebase-structure.md
    cwe78-prp22-newline-escape-issue.md
    l4-validation-usage.md
    linear-issue-creation-pattern.md
    linear-issue-tracking-integration.md
    linear-mcp-integration-example.md
    linear-mcp-integration.md
    project-overview.md
    PRP-15-remediation-workflow-implementation.md
    prp-2-implementation-patterns.md
    prp-backlog-system.md
    prp-structure-initialized.md
    README.md
    serena-implementation-verification-pattern.md
    serena-mcp-tool-restrictions.md
    suggested-commands.md
    syntropy-status-hook-pattern.md
    system-model-specification.md
    task-completion-checklist.md
    testing-standards.md
    tool-config-optimization-completed.md
    tool-usage-syntropy.md
    use-syntropy-tools-not-bash.md
examples/
  model/
    SystemModel.md
  patterns/
    dedrifting-lessons.md
    error-recovery.py
    mocks-marking.md
    pipeline-testing.py
    strategy-testing.py
  ce-blend-usage.md
  ce-init-project-usage.md
  example.setting.local.md
  INDEX.md
  INITIALIZATION.md
  linear-integration-example.md
  mermaid-color-palette.md
  prp-decomposition-patterns.md
  README.md
  tmp-directory-convention.md
  TOOL-USAGE-GUIDE.md
tools/
  ce/
    blending/
      strategies/
        __init__.py
        base.py
        claude_md.py
        examples.py
        memories.py
        settings.py
        simple.py
      __init__.py
      classification.py
      cleanup.py
      core.py
      detection.py
      llm_client.py
      validation.py
    examples/
      syntropy/
        context7_patterns.py
        filesystem_patterns.py
        git_patterns.py
        linear_patterns.py
        serena_patterns.py
    executors/
      __init__.py
      base.py
      github_actions.py
      mock.py
    testing/
      __init__.py
      builder.py
      mocks.py
      real_strategies.py
      strategy.py
    toml_formats/
      __init__.py
      pep621_handler.py
      poetry_handler.py
      setuptools_handler.py
      version_resolver.py
    vacuum_strategies/
      __init__.py
      backup_files.py
      base.py
      commented_code.py
      obsolete_docs.py
      orphan_tests.py
      temp_files.py
      unreferenced_code.py
    __init__.py
    __main__.py
    blend.py
    blueprint_parser.py
    cli_handlers.py
    code_analyzer.py
    config_loader.py
    context.py
    core.py
    drift_analyzer.py
    drift.py
    exceptions.py
    execute.py
    generate.py
    init_project.py
    linear_mcp_resilience.py
    linear_utils.py
    logging_config.py
    markdown_lint.py
    mcp_adapter.py
    mcp_utils.py
    mermaid_validator.py
    metrics.py
    pattern_detectors.py
    pattern_extractor.py
    pipeline.py
    profiling.py
    prp_analyzer.py
    prp.py
    repomix_unpack.py
    resilience.py
    shell_utils.py
    toml_merger.py
    update_context.py
    vacuum.py
    validate_permissions.py
    validate.py
    validation_loop.py
  bootstrap.sh
  pyproject.toml
  README.md
CLAUDE.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/commands/batch-gen-prp.md">
  1: # /batch-gen-prp - Batch PRP Generation with Parallel Subagents
  2: 
  3: Decomposes large plan documents into staged, parallelizable PRPs with automatic dependency analysis and concurrent generation using subagents.
  4: 
  5: **Architecture**: Coordinator spawns parallel subagents running `/generate-prp` in batch mode
  6: 
  7: ## Usage
  8: 
  9: ```bash
 10: /batch-gen-prp <plan-file-path>
 11: 
 12: # Examples:
 13: /batch-gen-prp TOOL-PERMISSION-LOCKDOWN-PLAN.md
 14: /batch-gen-prp feature-requests/AUTH-SYSTEM-PLAN.md
 15: /batch-gen-prp PRPs/plans/BIG-FEATURE-PLAN.md
 16: ```
 17: 
 18: ## What It Does
 19: 
 20: 1. **Parses plan document** ‚Üí Extracts phases with metadata
 21: 2. **Builds dependency graph** ‚Üí Analyzes explicit dependencies + file conflicts
 22: 3. **Assigns stages** ‚Üí Groups independent PRPs for parallel execution
 23: 4. **Shows plan** ‚Üí User confirms generation strategy
 24: 5. **Spawns subagents** ‚Üí Parallel generation per stage (Sonnet model)
 25: 6. **Monitors progress** ‚Üí Health checks via file timestamp polling (30s intervals)
 26: 7. **Aggregates results** ‚Üí Collects generated PRPs + Linear issues
 27: 8. **Outputs summary** ‚Üí Shows all generated PRPs grouped by stage
 28: 
 29: **Time Savings**: 8 PRPs sequential (30 min) ‚Üí parallel (10-12 min) = **60% faster**
 30: 
 31: ---
 32: 
 33: ## Plan Document Format
 34: 
 35: ### Structure
 36: 
 37: ```markdown
 38: # [Plan Title]
 39: 
 40: ## Overview
 41: [High-level description of what this plan achieves]
 42: 
 43: ## Success Criteria
 44: - [ ] Criterion 1
 45: - [ ] Criterion 2
 46: 
 47: ## Phases
 48: 
 49: ### Phase 1: [Name]
 50: 
 51: **Goal**: [One-sentence objective]
 52: 
 53: **Estimated Hours**: [X.X]
 54: 
 55: **Complexity**: [low|medium|high]
 56: 
 57: **Files Modified**:
 58: - path/to/file1.ext
 59: - path/to/file2.ext
 60: 
 61: **Dependencies**: [None | Phase 1, Phase 2]
 62: 
 63: **Implementation Steps**:
 64: 1. Step 1
 65: 2. Step 2
 66: 
 67: **Validation Gates**:
 68: - [ ] Validation 1
 69: - [ ] Validation 2
 70: 
 71: **Conflict Notes**: [Optional - explicit conflict warnings]
 72: 
 73: ---
 74: 
 75: ### Phase 2: [Name]
 76: [Same structure repeated]
 77: ```
 78: 
 79: ### Example: TOOL-PERMISSION-LOCKDOWN-PLAN.md
 80: 
 81: ```markdown
 82: # Tool & Permission Lockdown
 83: 
 84: ## Overview
 85: Implement comprehensive tool deny list and command permission system to reduce token usage and improve security.
 86: 
 87: ## Success Criteria
 88: - [ ] 55 MCP tools denied
 89: - [ ] Command permissions documented
 90: - [ ] Token usage reduced by 44k (96%)
 91: 
 92: ## Phases
 93: 
 94: ### Phase 1: Tool Deny List
 95: 
 96: **Goal**: Add 55 denied tools to settings.local.json
 97: 
 98: **Estimated Hours**: 0.42
 99: 
100: **Complexity**: low
101: 
102: **Files Modified**:
103: - .claude/settings.local.json
104: 
105: **Dependencies**: None
106: 
107: **Implementation Steps**:
108: 1. Read existing settings.local.json
109: 2. Add 55 tools to deny array (Filesystem: 8, Git: 5, GitHub: 26, Repomix: 4, Playwright: 6, Perplexity: 1, Syntropy: 5)
110: 3. Validate JSON syntax
111: 
112: **Validation Gates**:
113: - [ ] JSON validates
114: - [ ] 55 tools in deny list
115: - [ ] No duplicates
116: 
117: ---
118: 
119: ### Phase 2: Usage Guide
120: 
121: **Goal**: Create comprehensive tool usage guide
122: 
123: **Estimated Hours**: 0.50
124: 
125: **Complexity**: low
126: 
127: **Files Modified**:
128: - TOOL-USAGE-GUIDE.md
129: 
130: **Dependencies**: Phase 1
131: 
132: **Implementation Steps**:
133: 1. Create TOOL-USAGE-GUIDE.md
134: 2. Add decision tree flowchart
135: 3. Document all 55 denied tools with native alternatives
136: 4. Add common tasks with examples
137: 
138: **Validation Gates**:
139: - [ ] All 55 denied tools documented
140: - [ ] Decision tree included
141: - [ ] 9 common task examples
142: ```
143: 
144: ---
145: 
146: ## Workflow
147: 
148: ### Step 1: Parse Plan Document
149: 
150: ```python
151: # Read plan file
152: plan_content = Read(file_path=plan_path)
153: 
154: # Extract phases
155: phases = []
156: for section in parse_markdown(plan_content):
157:     if section.heading.startswith("### Phase"):
158:         phase = {
159:             "name": extract_after_colon(section.heading),
160:             "goal": extract_field(section, "Goal"),
161:             "estimated_hours": float(extract_field(section, "Estimated Hours")),
162:             "complexity": extract_field(section, "Complexity"),
163:             "files_modified": extract_list(section, "Files Modified"),
164:             "dependencies": extract_field(section, "Dependencies"),
165:             "implementation_steps": extract_list(section, "Implementation Steps"),
166:             "validation_gates": extract_list(section, "Validation Gates"),
167:             "conflict_notes": extract_field(section, "Conflict Notes", optional=True)
168:         }
169:         phases.append(phase)
170: ```
171: 
172: **Output**:
173: ```
174: Found 5 phases:
175:   Phase 1: Tool Deny List (0.42h, low)
176:   Phase 2: Usage Guide (0.50h, low)
177:   Phase 3: Worktree Docs (0.58h, low)
178:   Phase 4: Command Permissions (0.42h, low)
179:   Phase 5: Doc Updates (0.50h, low)
180: ```
181: 
182: ### Step 2: Build Dependency Graph
183: 
184: **Explicit Dependencies**:
185: ```python
186: dep_graph = {}
187: for i, phase in enumerate(phases):
188:     phase_num = i + 1
189:     phase_letter = chr(64 + phase_num)  # A, B, C, ...
190: 
191:     dep_graph[phase_num] = {
192:         "name": phase.name,
193:         "letter": phase_letter,
194:         "phase": phase,
195:         "dependencies": parse_dependency_string(phase.dependencies),
196:         "files": phase.files_modified
197:     }
198: ```
199: 
200: **Implicit Dependencies (File Conflicts)**:
201: ```python
202: file_map = defaultdict(list)
203: for phase_num, data in dep_graph.items():
204:     for file in data.files:
205:         file_map[file].append(phase_num)
206: 
207: # Detect conflicts
208: conflicts = {}
209: for file, phases in file_map.items():
210:     if len(phases) > 1:
211:         conflicts[file] = phases
212: ```
213: 
214: **Output**:
215: ```
216: Dependency graph:
217:   Phase 1 (A): Tool Deny List
218:     Dependencies: None
219:     Files: .claude/settings.local.json
220: 
221:   Phase 2 (B): Usage Guide
222:     Dependencies: Phase 1
223:     Files: TOOL-USAGE-GUIDE.md
224: 
225:   Phase 3 (C): Worktree Docs
226:     Dependencies: Phase 1
227:     Files: CLAUDE.md
228: 
229:   Phase 4 (D): Command Permissions
230:     Dependencies: Phase 2, Phase 3
231:     Files: .claude/settings.local.json
232: 
233:   Phase 5 (E): Doc Updates
234:     Dependencies: Phase 1
235:     Files: CLAUDE.md
236: 
237: ‚ö† File conflicts detected:
238:   - .claude/settings.local.json: Phase 1, Phase 4
239:     ‚Üí Phase 4 will merge after Phase 1 (MEDIUM conflict)
240: 
241:   - CLAUDE.md: Phase 3, Phase 5
242:     ‚Üí Phase 5 will merge after Phase 3 (LOW conflict - different sections)
243: ```
244: 
245: ### Step 3: Assign Stages
246: 
247: **Algorithm**: Topological sort + conflict resolution
248: 
249: ```python
250: def detect_circular_dependency(dep_graph, assigned):
251:     """Detect circular dependency and return cycle path
252: 
253:     H3: Show actual cycle path in error message
254: 
255:     Returns: list of phase numbers forming cycle, or None if no cycle
256:     """
257:     def dfs(node, visited, stack):
258:         visited.add(node)
259:         stack.append(node)
260: 
261:         for dep in dep_graph[node].dependencies:
262:             if dep not in assigned:  # Only check unassigned deps
263:                 if dep not in visited:
264:                     cycle = dfs(dep, visited, stack)
265:                     if cycle:
266:                         return cycle
267:                 elif dep in stack:
268:                     # Found cycle
269:                     cycle_start = stack.index(dep)
270:                     return stack[cycle_start:] + [dep]
271: 
272:         stack.pop()
273:         return None
274: 
275:     # Check all unassigned nodes
276:     for node in dep_graph:
277:         if node not in assigned:
278:             cycle = dfs(node, set(), [])
279:             if cycle:
280:                 return cycle
281: 
282:     return None
283: 
284: def assign_stages(dep_graph, file_map):
285:     """Group phases into stages maximizing parallelism"""
286:     stages = []
287:     assigned = set()
288: 
289:     while len(assigned) < len(dep_graph):
290:         # Find phases with all dependencies satisfied
291:         ready = [
292:             phase_num for phase_num in dep_graph
293:             if phase_num not in assigned
294:             and all(dep in assigned for dep in dep_graph[phase_num].dependencies)
295:         ]
296: 
297:         if not ready:
298:             # H3: Detect and show circular dependency path
299:             cycle_path = detect_circular_dependency(dep_graph, assigned)
300:             if cycle_path:
301:                 cycle_str = " ‚Üí ".join([f"Phase {p}" for p in cycle_path])
302:                 raise CircularDependencyError(f"Circular dependency detected: {cycle_str}")
303:             else:
304:                 raise CircularDependencyError("Circular dependency detected (unable to determine path)")
305: 
306:         # Group by file conflicts
307:         parallel_groups = []
308:         for phase_num in ready:
309:             # Check if phase conflicts with any group
310:             placed = False
311:             for group in parallel_groups:
312:                 if not has_file_conflict(phase_num, group, file_map):
313:                     group.append(phase_num)
314:                     placed = True
315:                     break
316: 
317:             if not placed:
318:                 parallel_groups.append([phase_num])
319: 
320:         # Create stage
321:         stage_type = "parallel" if all(len(g) == 1 for g in parallel_groups) or len(ready) > 1 else "sequential"
322:         stages.append({
323:             "stage_num": len(stages) + 1,
324:             "type": stage_type,
325:             "phases": ready
326:         })
327: 
328:         assigned.update(ready)
329: 
330:     return stages
331: ```
332: 
333: **Output**:
334: ```
335: Stage assignment:
336:   Stage 1 (parallel): Phase 1 (A)
337:   Stage 2 (parallel): Phase 2 (B), Phase 3 (C), Phase 5 (E)
338:   Stage 3 (sequential): Phase 4 (D)
339: 
340: Estimated execution time:
341:   Sequential: 2.42 hours
342:   Parallel: 1.42 hours
343:   Savings: 41% (1.0 hours)
344: ```
345: 
346: ### Step 4: Calculate PRP IDs
347: 
348: **Format**: `PRP-X.Y.Z`
349: - X = Batch ID (next free PRP number)
350: - Y = Stage number
351: - Z = Order within stage
352: 
353: ```python
354: import re
355: from glob import glob
356: 
357: def extract_prp_number(filename):
358:     """Extract root batch number from PRP filename
359: 
360:     C2: Handles both sequential and batch PRP formats
361: 
362:     Examples:
363:         PRP-42-feature.md ‚Üí 42
364:         PRP-43.2.1-feature.md ‚Üí 43 (ignore stage/order)
365:         PRP-100.5.3-complex.md ‚Üí 100
366: 
367:     Returns: int (batch number) or 0 if no match
368:     """
369:     match = re.search(r'PRP-(\d+)', filename)
370:     if match:
371:         return int(match.group(1))
372:     return 0
373: 
374: # Find next batch ID
375: existing_prps = glob("PRPs/feature-requests/PRP-*.md")
376: if existing_prps:
377:     max_id = max([extract_prp_number(p) for p in existing_prps])
378:     batch_id = max_id + 1
379: else:
380:     batch_id = 1  # First batch
381: 
382: # Assign PRP IDs
383: prp_ids = {}
384: execution_order = 1
385: for stage in stages:
386:     for i, phase_num in enumerate(stage.phases):
387:         prp_id = f"{batch_id}.{stage.stage_num}.{i+1}"
388:         prp_ids[phase_num] = {
389:             "prp_id": prp_id,
390:             "stage": stage.stage_num,
391:             "execution_order": execution_order,
392:             "merge_order": execution_order
393:         }
394:         execution_order += 1
395: ```
396: 
397: **Output**:
398: ```
399: PRP ID Assignment (Batch 43):
400:   PRP-43.1.1: Phase 1 - Tool Deny List
401:   PRP-43.2.1: Phase 2 - Usage Guide
402:   PRP-43.2.2: Phase 3 - Worktree Docs
403:   PRP-43.2.3: Phase 5 - Doc Updates
404:   PRP-43.3.1: Phase 4 - Command Permissions
405: ```
406: 
407: ### Step 5: Show Plan to User
408: 
409: ```
410: üìã Batch PRP Generation Plan
411: ============================================================
412: 
413: Input: TOOL-PERMISSION-LOCKDOWN-PLAN.md
414: Phases detected: 5
415: 
416: Dependency graph:
417:   Phase 1 (A): Tool Deny List (no deps)
418:   Phase 2 (B): Usage Guide (depends: Phase 1)
419:   Phase 3 (C): Worktree Docs (depends: Phase 1)
420:   Phase 4 (D): Command Permissions (depends: Phase 2, Phase 3)
421:   Phase 5 (E): Doc Updates (depends: Phase 1)
422: 
423: Stage assignment:
424:   Stage 1 (parallel): PRP-43.1.1
425:   Stage 2 (parallel): PRP-43.2.1, PRP-43.2.2, PRP-43.2.3
426:   Stage 3 (sequential): PRP-43.3.1
427: 
428: ‚ö† File conflicts detected:
429:   - .claude/settings.local.json: PRP-43.1.1, PRP-43.3.1 (MEDIUM)
430:     ‚Üí PRP-43.3.1 will merge after PRP-43.1.1
431: 
432:   - CLAUDE.md: PRP-43.2.2, PRP-43.2.3 (LOW)
433:     ‚Üí Different sections, conflicts unlikely
434: 
435: Estimated execution time:
436:   Sequential: 2.42h
437:   Parallel: 1.42h
438:   Savings: 41% (1.0h)
439: 
440: Generated PRPs will be created at:
441:   PRPs/feature-requests/PRP-43.1.1-tool-deny-list.md
442:   PRPs/feature-requests/PRP-43.2.1-usage-guide.md
443:   PRPs/feature-requests/PRP-43.2.2-worktree-docs.md
444:   PRPs/feature-requests/PRP-43.2.3-doc-updates.md
445:   PRPs/feature-requests/PRP-43.3.1-command-permissions.md
446: 
447: Proceed with generation? [y/N]:
448: ```
449: 
450: ### Step 6: Spawn Subagents (Stage by Stage)
451: 
452: **Create monitoring directory**:
453: ```bash
454: mkdir -p .tmp/batch-gen
455: ```
456: 
457: **For each stage**:
458: ```python
459: for stage in stages:
460:     print(f"\nüîß Stage {stage.stage_num} ({stage.type})")
461:     print("=" * 60)
462: 
463:     if stage.type == "parallel" and len(stage.phases) > 1:
464:         # Spawn parallel subagents
465:         agents = []
466:         for phase_num in stage.phases:
467:             prp_info = prp_ids[phase_num]
468:             phase_data = dep_graph[phase_num]
469: 
470:             # Build JSON input for subagent
471:             batch_input = {
472:                 "batch_mode": True,
473:                 "prp_id": prp_info.prp_id,
474:                 "feature_name": phase_data.name,
475:                 "goal": phase_data.phase.goal,
476:                 "estimated_hours": phase_data.phase.estimated_hours,
477:                 "complexity": phase_data.phase.complexity,
478:                 "files_modified": phase_data.phase.files_modified,
479:                 "dependencies": [prp_ids[dep].prp_id for dep in phase_data.dependencies],
480:                 "implementation_steps": phase_data.phase.implementation_steps,
481:                 "validation_gates": phase_data.phase.validation_gates,
482:                 "stage": f"stage-{stage.stage_num}-parallel",
483:                 "execution_order": prp_info.execution_order,
484:                 "merge_order": prp_info.merge_order,
485:                 "conflict_potential": calculate_conflict(phase_num, file_map),
486:                 "conflict_notes": phase_data.phase.conflict_notes or "",
487:                 "worktree_path": f"../ctx-eng-plus-prp-{prp_info.prp_id.replace('.', '-')}",
488:                 "branch_name": f"prp-{prp_info.prp_id.replace('.', '-')}-{slugify(phase_data.name)}",
489:                 "create_linear_issue": True,
490:                 "plan_context": f"Part of {plan_title} initiative"
491:             }
492: 
493:             # Spawn subagent
494:             agent = Task(
495:                 description=f"Generate PRP-{prp_info.prp_id}",
496:                 prompt=f"""
497: You are generating a PRP in batch mode for the /batch-gen-prp coordinator.
498: 
499: Use the /generate-prp command with this structured JSON input:
500: 
501: ```json
502: {json.dumps(batch_input, indent=2)}
503: ```
504: 
505: Follow the "Batch Mode Workflow" section of /generate-prp:
506: 1. Parse JSON input
507: 2. Write heartbeat to .tmp/batch-gen/PRP-{prp_info.prp_id}.status
508: 3. Generate PRP file with all metadata
509: 4. Create Linear issue
510: 5. Return JSON report
511: 
512: **IMPORTANT**:
513: - Write heartbeat every 10-15 seconds
514: - Return JSON report at end (coordinator needs it)
515: - On error, still return JSON with status: FAILED
516: """,
517:                 subagent_type="general-purpose",
518:                 model="sonnet"  # User specified: use Sonnet, not haiku
519:             )
520:             agents.append((phase_num, agent))
521: 
522:             print(f"  Spawned agent for PRP-{prp_info.prp_id}: {phase_data.name}")
523: 
524:         # Monitor agents
525:         results = monitor_parallel_agents(agents, prp_ids, timeout=300)  # 5 min timeout
526:     else:
527:         # Sequential execution
528:         results = []
529:         for phase_num in stage.phases:
530:             result = generate_prp_sequential(phase_num, dep_graph, prp_ids)
531:             results.append(result)
532: 
533:     # Show stage results
534:     show_stage_results(stage, results)
535: ```
536: 
537: ### Step 7: Monitor Agents (Health Check Protocol)
538: 
539: **Monitoring function** (runs every 30 seconds):
540: ```python
541: def monitor_parallel_agents(agents, prp_ids, timeout=300):
542:     """Monitor subagents via file timestamp polling"""
543:     start_time = time.now()
544:     results = {}
545:     agent_status = {}
546:     failed_polls = defaultdict(int)  # Track consecutive failures
547: 
548:     # Initialize monitoring
549:     for phase_num, agent in agents:
550:         prp_id = prp_ids[phase_num].prp_id
551:         agent_status[prp_id] = {
552:             "agent": agent,
553:             "phase_num": phase_num,
554:             "status": "STARTING",
555:             "progress": 0,
556:             "last_heartbeat": start_time,
557:             "stalled_warnings": 0
558:         }
559: 
560:     # Poll every 30 seconds
561:     while len(results) < len(agents):
562:         time.sleep(30)
563: 
564:         # Clear screen and show header
565:         print("\n" + "=" * 60)
566:         print(f"üìä Monitoring {len(agents)} Agents")
567:         print("=" * 60 + "\n")
568: 
569:         for prp_id, status_data in agent_status.items():
570:             if prp_id in results:
571:                 continue  # Already completed
572: 
573:             # Check heartbeat file
574:             heartbeat_file = f".tmp/batch-gen/PRP-{prp_id}.status"
575:             prp_file_pattern = f"PRPs/feature-requests/PRP-{prp_id}-*.md"
576: 
577:             # Check if PRP file exists (completion signal)
578:             prp_files = glob(prp_file_pattern)
579:             if prp_files:
580:                 # Agent completed successfully
581:                 results[prp_id] = {
582:                     "status": "SUCCESS",
583:                     "file_path": prp_files[0]
584:                 }
585:                 print(f"PRP-{prp_id}: {dep_graph[status_data.phase_num].name}")
586:                 print(f"  Status: [COMPLETED] ‚úì DONE")
587:                 continue
588: 
589:             # Check heartbeat file
590:             if os.path.exists(heartbeat_file):
591:                 # H5: Handle corrupted heartbeat files
592:                 try:
593:                     with open(heartbeat_file, 'r') as f:
594:                         heartbeat = json.load(f)
595:                     age = time.now() - heartbeat.timestamp
596:                 except (json.JSONDecodeError, IOError, KeyError) as e:
597:                     # Treat corrupted heartbeat as missing
598:                     failed_polls[prp_id] += 1
599:                     age = time.now() - start_time
600:                     print(f"PRP-{prp_id}: {dep_graph[status_data.phase_num].name}")
601:                     print(f"  Status: [CORRUPTED_HEARTBEAT] ‚ö† WARNING (poll {failed_polls[prp_id]}/2)")
602:                     print(f"  Error: {str(e)}")
603:                     print()
604:                     continue
605: 
606:                 age = time.now() - heartbeat.timestamp
607: 
608:                 status_data.status = heartbeat.status
609:                 status_data.progress = heartbeat.progress
610:                 status_data.last_heartbeat = heartbeat.timestamp
611:                 failed_polls[prp_id] = 0  # Reset failure counter
612: 
613:                 # Determine health
614:                 if heartbeat.status == "FAILED":
615:                     results[prp_id] = {
616:                         "status": "FAILED",
617:                         "error": heartbeat.get("error", "Unknown error")
618:                     }
619:                     health = "‚ùå FAILED"
620:                 elif age < 120:  # 2 minutes
621:                     health = "‚úì HEALTHY"
622:                 elif age < 300:  # 5 minutes
623:                     health = "‚ö† WARNING"
624:                     status_data.stalled_warnings += 1
625:                 else:
626:                     health = "‚ùå STALLED"
627: 
628:                 # Display
629:                 print(f"PRP-{prp_id}: {dep_graph[status_data.phase_num].name}")
630:                 print(f"  Status: [{heartbeat.status}{'.' * (15 - len(heartbeat.status))}] {health} ({int(age)}s ago)")
631:                 if "current_step" in heartbeat:
632:                     print(f"  Step: {heartbeat.current_step}")
633:                 print(f"  Progress: {heartbeat.progress}%")
634:             else:
635:                 # No heartbeat file yet
636:                 failed_polls[prp_id] += 1
637:                 age = time.now() - start_time
638: 
639:                 if failed_polls[prp_id] >= 2:
640:                     # Kill agent after 2 consecutive failed polls (user requirement)
641:                     print(f"PRP-{prp_id}: {dep_graph[status_data.phase_num].name}")
642:                     print(f"  Status: [NO_HEARTBEAT] ‚ùå KILLED (2 failed polls)")
643: 
644:                     # Kill agent
645:                     # Note: Task API doesn't support kill, so mark as failed
646:                     results[prp_id] = {
647:                         "status": "FAILED",
648:                         "error": "Agent killed: No heartbeat after 2 polls (60s)"
649:                     }
650:                 elif age < 60:  # 1 minute grace period
651:                     print(f"PRP-{prp_id}: {dep_graph[status_data.phase_num].name}")
652:                     print(f"  Status: [STARTING....] ‚è≥ INITIALIZING ({int(age)}s)")
653:                 else:
654:                     print(f"PRP-{prp_id}: {dep_graph[status_data.phase_num].name}")
655:                     print(f"  Status: [NO_HEARTBEAT] ‚ö† WARNING ({int(age)}s, poll {failed_polls[prp_id]}/2)")
656: 
657:             print()  # Blank line between PRPs
658: 
659:         # Check timeout
660:         if time.now() - start_time > timeout:
661:             print("\n‚ö† Timeout reached. Killing stalled agents...")
662:             for prp_id, status_data in agent_status.items():
663:                 if prp_id not in results:
664:                     results[prp_id] = {
665:                         "status": "FAILED",
666:                         "error": f"Timeout after {timeout}s"
667:                     }
668:             break
669: 
670:     # M3: Cleanup heartbeat files
671:     for prp_id in agent_status.keys():
672:         heartbeat_file = f".tmp/batch-gen/PRP-{prp_id}.status"
673:         if os.path.exists(heartbeat_file):
674:             try:
675:                 os.remove(heartbeat_file)
676:             except OSError:
677:                 pass  # Ignore cleanup errors
678: 
679:     return results
680: ```
681: 
682: **Display Example** (during monitoring):
683: ```
684: ============================================================
685: üìä Monitoring 3 Agents
686: ============================================================
687: 
688: PRP-43.2.1: Usage Guide
689:   Status: [WRITING........] ‚úì HEALTHY (15s ago)
690:   Step: Generating Implementation Steps section
691:   Progress: 65%
692: 
693: PRP-43.2.2: Worktree Docs
694:   Status: [COMPLETED] ‚úì DONE
695: 
696: PRP-43.2.3: Doc Updates
697:   Status: [RESEARCHING....] ‚ö† WARNING (3m 20s ago)
698:   Progress: 30%
699: 
700: Health Summary: 1 HEALTHY, 1 WARNING, 0 STALLED
701: Next poll in 30s...
702: ```
703: 
704: ### Step 8: Aggregate Results
705: 
706: ```python
707: # Collect all results across stages
708: all_results = {
709:     "batch_id": batch_id,
710:     "plan_file": plan_path,
711:     "total_prps": len(phases),
712:     "successful": 0,
713:     "failed": 0,
714:     "stages": []
715: }
716: 
717: for stage in stages:
718:     stage_results = {
719:         "stage_num": stage.stage_num,
720:         "type": stage.type,
721:         "prps": []
722:     }
723: 
724:     for phase_num in stage.phases:
725:         prp_id = prp_ids[phase_num].prp_id
726:         result = results.get(prp_id, {"status": "UNKNOWN"})
727: 
728:         stage_results.prps.append({
729:             "prp_id": prp_id,
730:             "phase_name": dep_graph[phase_num].name,
731:             "status": result.status,
732:             "file_path": result.get("file_path"),
733:             "linear_issue": result.get("linear_issue"),
734:             "linear_url": result.get("linear_url"),
735:             "error": result.get("error")
736:         })
737: 
738:         if result.status == "SUCCESS":
739:             all_results.successful += 1
740:         else:
741:             all_results.failed += 1
742: 
743:     all_results.stages.append(stage_results)
744: ```
745: 
746: ### Step 9: Output Summary
747: 
748: ```
749: ‚úÖ Batch PRP Generation Complete
750: ============================================================
751: 
752: Batch ID: 43
753: Plan: TOOL-PERMISSION-LOCKDOWN-PLAN.md
754: Generated: 5/5 PRPs (100% success rate)
755: 
756: Stage 1 (parallel):
757:   ‚úì PRP-43.1.1: Tool Deny List
758:     ‚Üí PRPs/feature-requests/PRP-43.1.1-tool-deny-list.md
759:     ‚Üí Linear: CTX-45 (https://linear.app/...)
760: 
761: Stage 2 (parallel - 3 agents):
762:   ‚úì PRP-43.2.1: Usage Guide
763:     ‚Üí PRPs/feature-requests/PRP-43.2.1-usage-guide.md
764:     ‚Üí Linear: CTX-46 (https://linear.app/...)
765: 
766:   ‚úì PRP-43.2.2: Worktree Docs
767:     ‚Üí PRPs/feature-requests/PRP-43.2.2-worktree-docs.md
768:     ‚Üí Linear: CTX-47 (https://linear.app/...)
769: 
770:   ‚úì PRP-43.2.3: Doc Updates
771:     ‚Üí PRPs/feature-requests/PRP-43.2.3-doc-updates.md
772:     ‚Üí Linear: CTX-48 (https://linear.app/...)
773: 
774: Stage 3 (sequential):
775:   ‚úì PRP-43.3.1: Command Permissions
776:     ‚Üí PRPs/feature-requests/PRP-43.3.1-command-permissions.md
777:     ‚Üí Linear: CTX-49 (https://linear.app/...)
778: 
779: Execution time: 12m 34s
780: Time saved: 41% vs sequential (17m 45s)
781: 
782: Next steps:
783:   1. Review generated PRPs in PRPs/feature-requests/
784:   2. Execute with:
785:      /batch-exe-prp --batch 43
786:      or stage-by-stage:
787:      /batch-exe-prp --batch 43 --stage 1
788:      /batch-exe-prp --batch 43 --stage 2
789:      /batch-exe-prp --batch 43 --stage 3
790: ```
791: 
792: **If failures occurred**:
793: ```
794: ‚ö† Partial Success: 4/5 PRPs generated
795: ============================================================
796: 
797: [... successful PRPs listed ...]
798: 
799: Failed:
800:   ‚ùå PRP-43.2.3: Doc Updates
801:     Error: Agent killed: No heartbeat after 2 polls (60s)
802: 
803:     Retry with:
804:     /generate-prp --prp-id 43.2.3 --retry
805: 
806: Or regenerate entire stage:
807:     /batch-gen-prp TOOL-PERMISSION-LOCKDOWN-PLAN.md --stage 2 --retry-failed
808: ```
809: 
810: ---
811: 
812: ## Integration with `/batch-exe-prp`
813: 
814: **Full workflow**:
815: ```bash
816: # Step 1: Plan decomposition + generation
817: /batch-gen-prp BIG-FEATURE-PLAN.md
818: # Output: 8 PRPs in PRPs/feature-requests/PRP-43.*.md
819: 
820: # Step 2: Execute by batch (all stages)
821: /batch-exe-prp --batch 43
822: # Executes Stage 1 ‚Üí Stage 2 (parallel) ‚Üí Stage 3
823: 
824: # OR: Execute stage-by-stage
825: /batch-exe-prp --batch 43 --stage 1
826: /batch-exe-prp --batch 43 --stage 2
827: /batch-exe-prp --batch 43 --stage 3
828: ```
829: 
830: **H6: Batch Filtering Integration**
831: 
832: ‚ö†Ô∏è **NOTE**: The `--batch` flag syntax shown above assumes `/batch-exe-prp` supports batch ID filtering. This feature needs verification:
833: 
834: 1. **Check if `/batch-exe-prp` supports `--batch` flag**
835: 2. **If not**: Update `/batch-exe-prp` to add batch filtering by parsing PRP-X.Y.Z format
836: 3. **Or**: Update this doc to show manual PRP selection:
837:    ```bash
838:    # Manual approach (if --batch not supported)
839:    /execute-prp PRPs/feature-requests/PRP-43.1.1-*.md
840:    /execute-prp PRPs/feature-requests/PRP-43.2.1-*.md
841:    /execute-prp PRPs/feature-requests/PRP-43.2.2-*.md
842:    # ... etc
843:    ```
844: 
845: **Recommended**: Add `--batch` support to `/batch-exe-prp` for seamless workflow
846: 
847: **Batch metadata**: PRPs contain all necessary metadata for execution
848: - `stage`: Which stage the PRP belongs to
849: - `execution_order`: Order within batch
850: - `merge_order`: Global merge sequence
851: - `dependencies`: Other PRPs that must complete first
852: - `conflict_potential`: Merge conflict warning
853: 
854: ---
855: 
856: ## Error Handling
857: 
858: ### 1. Circular Dependencies
859: 
860: **Detection**: Topological sort fails
861: **Error**:
862: ```
863: ‚ùå Circular dependency detected:
864:   Phase 2 ‚Üí Phase 3 ‚Üí Phase 4 ‚Üí Phase 2
865: 
866: Please revise plan to break the cycle.
867: ```
868: 
869: ### 2. Agent Failures
870: 
871: **Behavior**: Continue with other agents (user requirement: "Continue with other 2? Yes")
872: 
873: **Example**:
874: ```
875: Stage 2 (parallel): 3 agents
876:   ‚úì PRP-43.2.1: SUCCESS
877:   ‚ùå PRP-43.2.2: FAILED (timeout)
878:   ‚úì PRP-43.2.3: SUCCESS
879: 
880: Result: 2/3 PRPs generated, proceed to Stage 3
881: Failed PRPs can be retried later
882: ```
883: 
884: ### 3. No Heartbeat (2 Failed Polls)
885: 
886: **User requirement**: "second polling status failed = kill"
887: 
888: **Behavior**:
889: - Poll 1 (30s): No heartbeat ‚Üí ‚ö† WARNING
890: - Poll 2 (60s): Still no heartbeat ‚Üí ‚ùå KILL
891: - Mark as FAILED, continue with other agents
892: 
893: **H4: 60-Second Kill Timeout**
894: 
895: ‚ö†Ô∏è **WARNING**: Agents are killed if no heartbeat for 60 seconds (2 polls √ó 30s).
896: 
897: This is **intentionally aggressive** for PRP generation (typical time: 30-90s). Reasons:
898: - Prevents hung agents from blocking stage completion
899: - Faster feedback for failures
900: - PRP generation should be quick (< 60s for most)
901: 
902: **If your PRPs require >60s**:
903: - Research-heavy PRPs may timeout if Serena queries are slow
904: - Complex PRPs with deep codebase analysis may need more time
905: 
906: **Solutions**:
907: 1. **Adjust phase granularity**: Break large phases into smaller ones
908: 2. **Skip research**: Set research to optional in batch input
909: 3. **Increase timeout**: Modify monitoring code (not recommended)
910: 
911: **Comparison with /batch-exe-prp**: Execution timeout is 10 minutes (much longer because code execution takes longer than generation)
912: 
913: ### 4. Invalid Plan Format
914: 
915: **Validation errors**:
916: ```
917: ‚ùå Plan validation failed:
918:   - Phase 3: Missing "Estimated Hours" field
919:   - Phase 5: "Dependencies" field references non-existent "Phase 9"
920: 
921: Please fix plan format and retry.
922: ```
923: 
924: ---
925: 
926: ## Symmetry with `/batch-exe-prp`
927: 
928: ### Shared Patterns
929: 
930: | Aspect | `/batch-gen-prp` | `/batch-exe-prp` |
931: |--------|------------------|------------------|
932: | **Coordinator Role** | Parse plan ‚Üí spawn agents | Parse PRPs ‚Üí spawn agents |
933: | **Subagent Type** | `general-purpose` | `general-purpose` |
934: | **Model** | Sonnet (generation) | Auto (execution) |
935: | **Parallelism** | Stage-based | Stage-based |
936: | **Polling Interval** | 30 seconds | 30 seconds |
937: | **Health Signals** | File timestamps | Git commit timestamps |
938: | **Status Levels** | HEALTHY/WARNING/STALLED | HEALTHY/WARNING/STALLED |
939: | **Kill Policy** | 2 failed polls | 10 minutes no commits |
940: | **Error Handling** | Continue on partial failure | Continue on partial failure |
941: | **Output Format** | JSON aggregation | JSON aggregation |
942: 
943: ### Key Differences
944: 
945: | Aspect | `/batch-gen-prp` | `/batch-exe-prp` |
946: |--------|------------------|------------------|
947: | **Input** | Plan markdown file | Generated PRP files |
948: | **Output** | PRP files + Linear issues | Code changes in worktrees |
949: | **Work Location** | Single directory | Git worktrees |
950: | **Health Signal** | `.tmp/batch-gen/*.status` | Git log timestamps |
951: | **Cleanup** | Remove .tmp/ files | Remove worktrees |
952: 
953: ---
954: 
955: ## Example Plan Document
956: 
957: See `examples/TOOL-PERMISSION-LOCKDOWN-PLAN.md` for complete example.
958: 
959: ---
960: 
961: ## Tips
962: 
963: 1. **Clear goals**: Each phase should have one specific, measurable objective
964: 2. **Explicit dependencies**: Always declare "Dependencies: Phase N" if needed
965: 3. **File accuracy**: List all files that will be modified (used for conflict detection)
966: 4. **Validation gates**: Make them copy-pasteable bash commands
967: 5. **Reasonable hours**: 0.25-2h per phase (larger tasks = multiple phases)
968: 6. **Review before execution**: Generated PRPs may need minor adjustments
969: 
970: ---
971: 
972: ## Next Steps
973: 
974: After batch generation:
975: 1. Review all generated PRPs in `PRPs/feature-requests/PRP-{batch-id}.*`
976: 2. Adjust any PRP details if needed
977: 3. Execute entire batch: `/batch-exe-prp --batch {batch-id}`
978: 4. Or execute stage-by-stage for more control
</file>

<file path=".claude/commands/denoise.md">
  1: # /denoise - Boil Out Document Noise
  2: 
  3: Boil out noise from documents‚Äîremove verbosity while strictly guaranteeing complete retention of all essential information.
  4: 
  5: ## Usage
  6: 
  7: ```bash
  8: # Denoise single document
  9: /denoise path/to/document.md
 10: 
 11: # Denoise with custom compression target (default: 60-75% reduction)
 12: /denoise path/to/document.md --target 70
 13: 
 14: # Preview changes without writing (dry-run)
 15: /denoise path/to/document.md --dry-run
 16: 
 17: # Denoise multiple documents
 18: /denoise docs/*.md
 19: 
 20: # Denoise and show statistics
 21: /denoise path/to/document.md --verbose
 22: ```
 23: 
 24: ## What It Does
 25: 
 26: **Removes**:
 27: - Verbose explanations (keeps essential meaning)
 28: - Redundant examples (keeps one representative)
 29: - Long code blocks (keeps signature + key logic)
 30: - Repetitive sections
 31: - Wordy transitions
 32: - Multiple ways of saying same thing
 33: 
 34: **Preserves**:
 35: - All essential information
 36: - Technical accuracy
 37: - Commands and references
 38: - Critical warnings and errors
 39: - Quick reference data
 40: - Structure and organization
 41: 
 42: **Guaranteed**: Zero information loss, only noise removal
 43: 
 44: ## Algorithm
 45: 
 46: ### Phase 1: Structural Analysis
 47: - Parse markdown sections
 48: - Identify redundant patterns
 49: - Detect verbose vs concise sections
 50: - Map cross-references
 51: 
 52: ### Phase 2: Content Classification
 53: - **Keep**: Commands, references, warnings, key facts
 54: - **Compress**: Long explanations ‚Üí bullet points
 55: - **Deduplicate**: Multiple examples ‚Üí single best
 56: - **Condense**: Multi-paragraph ‚Üí 2-3 lines
 57: 
 58: ### Phase 3: Optimization
 59: - Convert verbose text to scannable format
 60: - Preserve all unique information
 61: - Maintain readability
 62: - Keep structure intact
 63: 
 64: ### Phase 4: Validation
 65: - Verify no information loss
 66: - Check all references intact
 67: - Ensure commands preserved
 68: - Validate structure
 69: 
 70: ## Output
 71: 
 72: ```
 73: üìÑ Denoising: CLAUDE.md
 74: 
 75: Before: 1040 lines
 76: After:  259 lines
 77: Reduction: 75% (781 lines)
 78: 
 79: ‚úÖ Preserved:
 80:   - All 5 core principles
 81:   - 48 allowed tools summary
 82:   - 15 quick commands
 83:   - 12 troubleshooting entries
 84:   - All resource paths
 85: 
 86: ‚ùå Removed:
 87:   - 6 verbose sections (150+ lines)
 88:   - 12 redundant code examples
 89:   - 8 long explanations
 90:   - 4 duplicate references
 91: 
 92: Validation: PASS ‚úÖ
 93: All essential information preserved.
 94: 
 95: Written to: CLAUDE.md
 96: ```
 97: 
 98: ## Examples
 99: 
100: ### Before (Verbose)
101: ```markdown
102: ## Working Directory
103: 
104: **Default Context:** Project root (`/Users/bprzybysz/nc-src/ctx-eng-plus`)
105: 
106: **For tools/ commands:** Always prefix with `cd tools &&` or use full paths from root.
107: 
108: **Note:** Claude Code doesn't have a persistent working directory setting per project.
109: Always specify context explicitly:
110: 
111: ```bash
112: # Correct patterns
113: cd tools && uv run ce --help
114: cd tools && uv run pytest tests/ -v
115: uv run -C tools ce validate --level all  # Using uv -C flag
116: 
117: # Avoid (relative paths from wrong location)
118: uv run ce --help  # Will fail if not in tools/
119: ```
120: ```
121: 
122: ### After (Denoised)
123: ```markdown
124: ## Working Directory
125: 
126: **Default**: `/Users/bprzybysz/nc-src/ctx-eng-plus`
127: 
128: **For tools/ commands**: Use `cd tools &&` or `uv run -C tools`
129: ```
130: 
131: ## Configuration
132: 
133: **Location**: `.ce/denoise-config.yml` (optional)
134: 
135: ```yaml
136: denoise:
137:   # Target compression ratio (default: 60-75%)
138:   target_reduction: 70
139: 
140:   # Preserve these sections verbatim
141:   preserve_sections:
142:     - "Quick Commands"
143:     - "Troubleshooting"
144: 
145:   # Maximum examples to keep per section
146:   max_examples: 1
147: 
148:   # Condense code blocks longer than N lines
149:   condense_code_threshold: 20
150: ```
151: 
152: ## Rules
153: 
154: ### MUST Preserve
155: - Commands and CLI references
156: - Configuration examples
157: - Error messages and troubleshooting
158: - Technical specifications
159: - Quick reference tables
160: - Resource paths
161: 
162: ### MAY Compress
163: - Long explanations (‚Üí bullet points)
164: - Multiple examples (‚Üí best one)
165: - Verbose prose (‚Üí concise)
166: - Repetitive sections (‚Üí single instance)
167: 
168: ### MUST NOT Change
169: - Technical accuracy
170: - Command syntax
171: - File paths
172: - URLs and references
173: - Code logic (only formatting/length)
174: 
175: ## When to Use
176: 
177: **Good for**:
178: - Documentation that grew organically
179: - Multiple authors with varying styles
180: - Copy-paste accumulation
181: - Tutorial-style docs needing quick reference format
182: - Token-heavy files
183: 
184: **Not for**:
185: - Tutorial content (intentional verbosity)
186: - Step-by-step guides with explanations
187: - API documentation with full examples
188: - Already concise documents
189: 
190: ## Implementation
191: 
192: This command uses:
193: 1. **LLM-based analysis** (via Syntropy thinking tool)
194:    - Semantic understanding of content
195:    - Intelligent redundancy detection
196:    - Context-aware compression
197: 
198: 2. **Rule-based optimization**
199:    - Markdown structure preservation
200:    - Code block condensing
201:    - Bullet point conversion
202: 
203: 3. **Validation layer**
204:    - Information extraction (before/after)
205:    - Completeness check
206:    - Cross-reference validation
207: 
208: ## Integration
209: 
210: **Pre-commit hook** (optional):
211: ```yaml
212: # .claude/settings.local.json
213: {
214:   "hooks": {
215:     "preCommit": {
216:       "command": "/denoise docs/*.md --dry-run --verbose",
217:       "description": "Check for document bloat before commit"
218:     }
219:   }
220: }
221: ```
222: 
223: **Weekly maintenance**:
224: ```bash
225: # Add to crontab or CI/CD
226: /denoise CLAUDE.md syntropy-mcp/CLAUDE.md --verbose
227: ```
228: 
229: ## Related
230: 
231: - `/peer-review` - Review document quality
232: - `/update-context` - Sync context with codebase
233: - `ce validate --level 1` - Markdown linting
</file>

<file path=".claude/commands/execute-prp.md">
  1: # /execute-prp - Execute PRP with Self-Healing
  2: 
  3: Automates PRP execution with phase-by-phase implementation, L1-L4 validation loops, self-healing error recovery, and automatic escalation triggers.
  4: 
  5: **Note**: This command defines the PRP execution protocol. `/batch-exe-prp` launches parallel agents that follow this protocol independently in separate git worktrees.
  6: 
  7: ## Usage
  8: 
  9: ```
 10: /execute-prp <prp-file-or-id>
 11: ```
 12: 
 13: **Examples:**
 14: ```
 15: /execute-prp PRP-5
 16: /execute-prp PRPs/feature-requests/PRP-5-context-sync-integration.md
 17: ```
 18: 
 19: ## What It Does
 20: 
 21: 1. **Parses PRP Blueprint**:
 22:    - Extracts phases from `## üîß Implementation Blueprint`
 23:    - Validates phase structure (number, name, hours, goal, approach)
 24:    - Identifies files to create/modify and functions to implement
 25:    - Extracts validation commands and checkpoint instructions
 26: 
 27: 2. **Creates Git Branch** (in worktree if specified):
 28:    - If executing in git worktree: Uses existing branch from worktree setup
 29:    - If executing in main repo: Creates branch `prp-{prp_id}-{sanitized-name}`
 30:    - Format example: `prp-6-user-authentication`
 31:    - Enables parallel PRP execution (see CLAUDE.md "Git Worktree" section)
 32:    - **Pattern**: Branch created/verified BEFORE any file modifications
 33: 
 34: 3. **Initializes PRP Context**:
 35:    - Creates active PRP session in `.ce/active_prp_session`
 36:    - Initializes state tracking for phases
 37:    - Sets up checkpoint namespace: `checkpoint-{prp_id}-{phase}`
 38:    - Stores branch name and worktree path if applicable
 39: 
 40: 4. **Executes Each Phase**:
 41:    - Creates/modifies files using Serena MCP
 42:    - Implements functions from blueprint
 43:    - Logs progress to console
 44:    - Updates phase state
 45: 
 46: 4. **Runs Validation Loop** (L1-L4 with self-healing):
 47:    - **L1 (Syntax & Style)**: `validate_level_1()` - Linting, formatting, type checks
 48:    - **L2 (Unit Tests)**: Runs phase validation command (e.g., `pytest tests/test_auth.py`)
 49:    - **L3 (Integration)**: `validate_level_3()` - Integration tests
 50:    - **L4 (Pattern Conformance)**: `validate_level_4(prp_path)` - Drift detection (<30%)
 51: 
 52:    **Self-Healing** (L1-L2 only, max 3 attempts):
 53:    - Parses error output (type, file, line, message)
 54:    - Checks escalation triggers
 55:    - Applies automatic fixes (e.g., add missing imports)
 56:    - Re-runs validation
 57:    - Escalates to human if persistent/ambiguous
 58: 
 59:    **Error Escalation Triggers**:
 60:    - **Persistent**: Same error after 3 attempts
 61:    - **Ambiguous**: Generic error with no file/line info
 62:    - **Architectural**: Keywords: refactor, redesign, circular import
 63:    - **External**: Network errors, package issues
 64:    - **Security**: CVE, credentials, permissions
 65: 
 66: 5. **Creates Checkpoints**:
 67:    - Git commit after each phase: `git commit -m "Phase N: {goal}"`
 68:    - Git tag after validation gate: `checkpoint-{prp_id}-phase{N}-{timestamp}`
 69:    - Preserves rollback points for easy recovery
 70: 
 71: 6. **Post-Execution Sync** (if auto-sync enabled via PRP-5):
 72:    - Runs cleanup protocol (archives memories, deletes ephemeral state)
 73:    - Syncs context (reindexes new code)
 74:    - Creates final checkpoint
 75:    - Verifies drift < 10%
 76: 
 77: 7. **Calculates Confidence Score**:
 78:    - **10/10**: All L1-L4 passed on first attempt
 79:    - **9/10**: All passed, 1-2 self-heals
 80:    - **8/10**: All passed, 3+ self-heals
 81:    - **7/10**: L1-L3 passed, L4 skipped
 82:    - **5/10**: Validation failures
 83: 
 84: 8. **Ends PRP Context**:
 85:    - Removes active session
 86:    - Returns execution summary
 87: 
 88: ## Execution Protocol Specification
 89: 
 90: **This section defines the PRP execution protocol that `/batch-exe-prp` agents follow independently.**
 91: 
 92: ### Protocol Steps (Sequential per Phase)
 93: 
 94: ```
 95: For each phase in PRP blueprint:
 96:   1. Parse phase metadata (number, name, goal, hours)
 97:   2. Execute implementation steps:
 98:      - Read files (if modifying existing)
 99:      - Apply changes (Edit/Write tools)
100:      - Create new files if needed
101:   3. Run validation loop:
102:      a. L1 (Syntax & Style):
103:         - Run linting (ruff/pylint/eslint)
104:         - Run formatting (black/prettier)
105:         - Run type checking (mypy/tsc)
106:         - Self-heal if failed (max 3 attempts)
107:      b. L2 (Unit Tests):
108:         - Run phase validation command
109:         - Parse test failures
110:         - Self-heal if failed (max 3 attempts)
111:      c. L3 (Integration Tests):
112:         - Run integration test suite
113:         - NO self-healing (escalate on failure)
114:      d. L4 (Pattern Conformance):
115:         - Calculate drift score
116:         - ABORT if drift >30%
117:         - NO self-healing (escalate on failure)
118:   4. Create checkpoint:
119:      - Git commit: "Phase {N}: {goal}"
120:      - Git tag: "checkpoint-{prp_id}-phase{N}-{timestamp}"
121:   5. Output progress signal:
122:      - "STATUS:PHASE_COMPLETE:{N}/{total}"
123: 
124: Return JSON report:
125: {
126:   "prp_id": "PRP-X",
127:   "status": "SUCCESS|FAILED|PARTIAL",
128:   "phases_completed": N,
129:   "phases_total": M,
130:   "confidence_score": 1-10,
131:   "validation_results": {...},
132:   "self_heals": N,
133:   "commit_hash": "abc123",
134:   "execution_time": "Xm Ys",
135:   "files_modified": ["file1", "file2"],
136:   "errors": [...]
137: }
138: ```
139: 
140: ### Self-Healing Rules
141: 
142: **L1-L2 ONLY** (max 3 attempts per error):
143: 
144: **Auto-fixable**:
145: - Import errors ‚Üí Add missing import
146: - Formatting errors ‚Üí Apply formatter
147: - Simple type errors ‚Üí Add type hints
148: 
149: **DO NOT auto-fix**:
150: - Persistent errors (same error 3x) ‚Üí Escalate
151: - Ambiguous errors (no file/line) ‚Üí Escalate
152: - Architectural errors (circular import, refactor needed) ‚Üí Escalate
153: - External errors (network, package not found) ‚Üí Escalate
154: - Security errors (CVE, credentials) ‚Üí Escalate immediately
155: 
156: ### Health Check Protocol (for batch agents)
157: 
158: **Output every 5 minutes**:
159: ```
160: HEALTH:OK                          # All systems normal
161: HEALTH:ERROR:timeout               # Validation timeout
162: HEALTH:ERROR:import_error:file.py  # Specific error detected
163: ```
164: 
165: **Completion signals**:
166: ```
167: STATUS:COMPLETE:10/10              # Success (confidence score)
168: STATUS:FAILED:L3_timeout           # Failure reason
169: STATUS:PARTIAL:2/3                 # Partial (N of M phases done)
170: ```
171: 
172: ## CLI Command
173: 
174: ```bash
175: # Basic usage
176: cd tools
177: uv run ce execute <prp-id-or-path>
178: 
179: # Execute specific phase range
180: uv run ce execute PRP-5 --start-phase 2 --end-phase 3
181: 
182: # Dry run (parse blueprint without execution)
183: uv run ce execute PRP-5 --dry-run
184: 
185: # Skip validation (dangerous - for debugging only)
186: uv run ce execute PRP-5 --skip-validation
187: 
188: # JSON output (for scripting)
189: uv run ce execute PRP-5 --json
190: ```
191: 
192: ## Example Workflow
193: 
194: ```bash
195: # 1. Generate PRP from INITIAL.md
196: /generate-prp feature-requests/auth/INITIAL.md
197: # Output: PRPs/feature-requests/PRP-6-user-authentication-system.md
198: 
199: # 2. Review generated PRP
200: # - Check implementation blueprint phases
201: # - Verify validation commands
202: # - Adjust if needed
203: 
204: # 3. Execute PRP with auto-sync
205: cd tools
206: uv run ce context auto-sync --enable  # Enable auto-sync (PRP-5)
207: /execute-prp PRP-6
208: 
209: # Expected output:
210: # ================================================================================
211: # Phase 1: Core Logic Implementation
212: # Goal: Implement main authentication flow
213: # ================================================================================
214: #
215: #   üìù Create: src/auth.py - Authentication logic
216: #   üîß Implement: authenticate_user
217: #   üîß Implement: validate_token
218: #
219: #   üß™ Running validation...
220: #     L1: Syntax & Style...
221: #     ‚úÖ L1 passed (0.45s)
222: #     L2: Running pytest tests/test_auth.py -v...
223: #     ‚úÖ L2 passed (1.23s)
224: #     L3: Integration Tests...
225: #     ‚úÖ L3 passed (2.15s)
226: #     L4: Pattern Conformance...
227: #     ‚úÖ L4 passed (drift: 5.2%)
228: #   ‚úÖ Validation complete
229: #
230: # ‚úÖ Phase 1 complete
231: #
232: # ... (phases 2-3)
233: #
234: # ================================================================================
235: # Running post-execution sync...
236: # ================================================================================
237: #
238: # ‚úÖ Post-sync complete: drift=3.1%
239: #    Cleanup: True
240: #    Memories archived: 2
241: #    Final checkpoint: checkpoint-PRP-6-final-20251013-120000
242: #
243: # ‚úÖ Execution complete: 10/10 confidence (45m 23s)
244: ```
245: 
246: ## Self-Healing Capabilities
247: 
248: ### Auto-Fixable Errors (L1-L2)
249: 
250: **Import Errors**:
251: ```python
252: # Error: ImportError: No module named 'jwt'
253: # Fix: Adds "import jwt" to file at appropriate location
254: ```
255: 
256: ### Escalation Triggers (Human Intervention Required)
257: 
258: The system escalates to human when:
259: 
260: 1. **Persistent Error** (same error after 3 attempts):
261:    ```
262:    ‚ùå L2 failed after 3 attempts - escalating
263:    üîß Troubleshooting:
264:       1. Review error details - same error occurred 3 times
265:       2. Check if fix logic matches error type
266:       3. Consider if architectural change needed
267:    ```
268: 
269: 2. **Ambiguous Error** (generic error with no file/line info):
270:    ```
271:    Error: "something went wrong"
272:    üîß Troubleshooting:
273:       1. Run validation command manually for full context
274:       2. Check logs for additional error details
275:    ```
276: 
277: 3. **Architectural Changes** (keywords: refactor, redesign, circular import):
278:    ```
279:    Error: "circular import detected between auth.py and models.py"
280:    üîß Troubleshooting:
281:       1. Review error for architectural keywords
282:       2. Consider if code structure needs reorganization
283:    ```
284: 
285: 4. **External Dependencies** (network errors, package issues):
286:    ```
287:    Error: "connection refused" or "package not found"
288:    üîß Troubleshooting:
289:       1. Check network connectivity
290:       2. Verify package repository access (PyPI, npm)
291:    ```
292: 
293: 5. **Security Concerns** (CVE, credentials, permissions):
294:    ```
295:    Error: "vulnerability detected" or "secret exposed"
296:    üîß Troubleshooting:
297:       1. DO NOT auto-fix security-related errors
298:       2. Review error for exposed secrets/credentials
299:    ```
300: 
301: ## Options
302: 
303: | Flag | Description | Example |
304: |------|-------------|---------|
305: | `--start-phase N` | Start execution from phase N | `--start-phase 2` |
306: | `--end-phase N` | Stop execution at phase N | `--end-phase 3` |
307: | `--skip-validation` | Skip validation loops (debugging only) | `--skip-validation` |
308: | `--dry-run` | Parse blueprint without execution | `--dry-run` |
309: | `--json` | Output results as JSON | `--json` |
310: 
311: ## Validation Gates
312: 
313: Each phase runs through 4 validation levels:
314: 
315: ### L1: Syntax & Style (Auto-healing: Yes)
316: - Linting (ruff, pylint, eslint)
317: - Formatting (black, prettier)
318: - Type checking (mypy, tsc)
319: - **Max 3 self-healing attempts**
320: 
321: ### L2: Unit Tests (Auto-healing: Yes)
322: - Runs phase validation command
323: - Parses test failures
324: - Attempts automatic fixes (import errors, simple logic)
325: - **Max 3 self-healing attempts**
326: 
327: ### L3: Integration Tests (Auto-healing: No)
328: - End-to-end functionality tests
329: - Manual intervention on failure
330: - Escalates architectural issues
331: 
332: ### L4: Pattern Conformance (Auto-healing: No)
333: - Compares implementation to INITIAL.md EXAMPLES
334: - Calculates drift score (0-100%)
335: - **Aborts if drift > 30%** (requires explicit user acceptance)
336: - User decision: accept/reject/update EXAMPLES
337: 
338: ## PRP State Isolation (via PRP-2)
339: 
340: Each execution maintains isolated state:
341: 
342: - **Checkpoints**: `checkpoint-{prp_id}-phase{N}-{timestamp}`
343: - **Memories**: `{prp_id}-checkpoint-*`, `{prp_id}-learnings-*`
344: - **Active Session**: `.ce/active_prp_session` (JSON with prp_id, phase, validation_attempts)
345: - **Cleanup**: Automatic ephemeral state removal after execution (if auto-sync enabled)
346: 
347: ## Context Sync Integration (via PRP-5)
348: 
349: With auto-sync enabled (`ce context auto-sync --enable`):
350: 
351: **Before execution**: N/A (pre-sync happens in generation phase)
352: 
353: **After execution** (Step 6.5):
354: 1. Runs cleanup protocol (PRP-2)
355: 2. Syncs context (reindexes new code)
356: 3. Verifies drift < 10%
357: 4. Creates final checkpoint
358: 5. Removes active PRP session
359: 
360: ## Claude Code Hooks
361: 
362: **Integrated Context Monitoring** (configured in `.claude/settings.local.json`):
363: 
364: **Active Hooks**:
365: 
366: - **SessionStart**: Context health check - Warns about drift on session start (>10%)
367: 
368: **Additional hooks available** (add to settings.local.json as needed):
369: 
370: - **UserPromptSubmit**: Auto-sync reminder (checks if auto-sync disabled)
371: - **PostToolUse**: Drift spike detector (alerts after Edit/Write if drift >40%)
372: 
373: **Current hook configuration**:
374: 
375: ```json
376: {
377:   "hooks": {
378:     "SessionStart": [
379:       {
380:         "matcher": "*",
381:         "hooks": [
382:           {
383:             "type": "command",
384:             "command": "cd tools && uv run ce context health --json | jq -r 'if .drift_score > 30 then \"‚ö†Ô∏è HIGH DRIFT: \" + (.drift_score | tostring) + \"% - Run: ce context sync\" elif .drift_score > 10 then \"‚ö†Ô∏è Moderate drift: \" + (.drift_score | tostring) + \"%\" else \"‚úÖ Context healthy: \" + (.drift_score | tostring) + \"%\" end'",
385:             "timeout": 5
386:           }
387:         ]
388:       }
389:     ]
390:   }
391: }
392: ```
393: 
394: **Use cases**:
395: 
396: - Daily development: SessionStart health check (drift warning)
397: - Long sessions: PostToolUse drift spike detector (alerts >40% drift)
398: - Exploration: Complements auto-sync for non-PRP work
399: 
400: **Note**: Hooks are optional. Auto-sync mode handles PRP workflow automatically.
401: 
402: **More info**: See official docs at <https://docs.claude.com/en/docs/claude-code/hooks>
403: 
404: ## Common Issues
405: 
406: ### Issue: "PRP file not found: PRP-5"
407: 
408: ```bash
409: # Solution: Use full path or ensure PRP is in PRPs/feature-requests/
410: /execute-prp PRPs/feature-requests/PRP-5-context-sync-integration.md
411: ```
412: 
413: ### Issue: Validation fails with "command not found"
414: 
415: ```bash
416: # Solution: Ensure validation command is correct in PRP blueprint
417: # Example: Use "uv run pytest tests/" not just "pytest tests/"
418: ```
419: 
420: ### Issue: Self-healing stuck in loop
421: 
422: ```bash
423: # Solution: Same error 3 times triggers escalation
424: # Review escalation message for troubleshooting guidance
425: ```
426: 
427: ### Issue: "Auto-sync failed"
428: 
429: ```bash
430: # Solution: Post-sync is non-blocking, execution still completes
431: # Run manual sync: cd tools && uv run ce context post-sync PRP-5
432: ```
433: 
434: ## Output Structure
435: 
436: ```json
437: {
438:   "success": true,
439:   "prp_id": "PRP-6",
440:   "phases_completed": 3,
441:   "validation_results": {
442:     "Phase1": {
443:       "success": true,
444:       "validation_levels": {
445:         "L1": {"passed": true, "attempts": 1, "errors": []},
446:         "L2": {"passed": true, "attempts": 2, "errors": ["import error"]},
447:         "L3": {"passed": true, "attempts": 1, "errors": []},
448:         "L4": {"passed": true, "attempts": 1, "drift_score": 5.2}
449:       },
450:       "self_healed": ["L2: Fixed after 2 attempts"]
451:     }
452:   },
453:   "checkpoints_created": [
454:     "checkpoint-PRP-6-phase1-20251013-100000",
455:     "checkpoint-PRP-6-phase2-20251013-110000",
456:     "checkpoint-PRP-6-phase3-20251013-120000"
457:   ],
458:   "confidence_score": "10/10",
459:   "execution_time": "45m 23s"
460: }
461: ```
462: 
463: ## Next Steps After Execution
464: 
465: 1. **Review Execution Summary**:
466:    - Check confidence score (target: 10/10)
467:    - Review self-healing actions taken
468:    - Verify all validation gates passed
469: 
470: 2. **Test Manually** (if confidence < 10/10):
471:    - Run validation commands manually
472:    - Review self-healing fixes
473:    - Address any escalated errors
474: 
475: 3. **Rollback if Needed** (via PRP-2):
476: 
477:    ```bash
478:    cd tools
479:    uv run ce prp restore PRP-6 phase2
480:    ```
481: 
482: 4. **Context Sync** (if auto-sync disabled):
483: 
484:    ```bash
485:    cd tools
486:    uv run ce context post-sync PRP-6
487:    ```
488: 
489: 5. **Peer Review**:
490: 
491:    ```bash
492:    /peer-review exe PRPs/feature-requests/PRP-6-user-authentication-system.md
493:    ```
494: 
495: ## Tips
496: 
497: 1. **Enable auto-sync** before execution: `ce context auto-sync --enable`
498: 2. **Use dry-run** to preview phases: `ce execute PRP-6 --dry-run`
499: 3. **Test incrementally**: Use `--start-phase` and `--end-phase` for partial execution
500: 4. **Trust self-healing**: Let L1-L2 auto-fixes run before manual intervention
501: 5. **Review escalations**: Escalation messages include specific troubleshooting guidance
502: 6. **Check confidence score**: 10/10 means production-ready, <8/10 needs review
503: 
504: ## Implementation Details
505: 
506: - **Module**: `tools/ce/execute.py`
507: - **Tests**: `tools/tests/test_execute.py` (20+ tests)
508: - **PRP Reference**: `PRPs/feature-requests/PRP-4-execute-prp-orchestration.md`
509: - **Self-Healing**: 90%+ success rate on L1-L2 errors
510: - **Speed Improvement**: 10-24x faster than manual implementation
511: 
512: ## Related Commands
513: 
514: - `/generate-prp` - Generate PRP from INITIAL.md
515: - `/peer-review exe <prp-file>` - Review execution quality
516: - `ce prp restore <prp-id> [phase]` - Rollback to checkpoint (PRP-2)
517: - `ce context post-sync <prp-id>` - Manual post-execution sync (PRP-5)
518: - `ce validate --level 4 <prp-path>` - Run L4 pattern conformance (PRP-1)
</file>

<file path=".claude/commands/generate-prp.md">
  1: # /generate-prp - Generate PRP from INITIAL.md
  2: 
  3: Automates PRP (Product Requirements Prompt) generation from INITIAL.md with comprehensive codebase research, documentation fetching, and context synthesis.
  4: 
  5: ## Usage
  6: 
  7: ```
  8: /generate-prp <initial-md-path>                        # Creates new PRP + Linear issue
  9: /generate-prp <initial-md-path> --join-prp <prp-ref>  # Joins existing PRP's Linear issue
 10: ```
 11: 
 12: **PRP Reference Formats**:
 13: - Number: `--join-prp 12` (searches for PRP-12)
 14: - ID: `--join-prp PRP-12`
 15: - File path: `--join-prp PRPs/executed/PRP-12-feature.md`
 16: 
 17: ## What It Does
 18: 
 19: 1. **Parses INITIAL.md structure**:
 20:    - Extracts FEATURE, EXAMPLES, DOCUMENTATION, OTHER CONSIDERATIONS sections
 21:    - Validates required sections present (FEATURE and EXAMPLES are mandatory)
 22: 
 23: 2. **Proposes clean code**:
 24:    - Follows project code quality standards (50-line functions, 500-line files)
 25:    - Applies KISS principle (simple solutions, minimal dependencies)
 26:    - Ensures no fishy fallbacks or silent error masking
 27:    - All mocks marked with FIXME comments in production code
 28:    - Includes actionable error messages with troubleshooting guidance
 29: 
 30: 3. **Researches codebase** (via Serena MCP):
 31:    - Searches for similar patterns using keywords
 32:    - Analyzes symbol structure and relationships
 33:    - Infers test framework (pytest/unittest/jest)
 34:    - Identifies architectural patterns
 35: 
 36: 3. **Fetches documentation** (via Context7 MCP):
 37:    - Resolves library names to Context7 IDs
 38:    - Fetches relevant library documentation
 39:    - Extracts topics from feature description
 40:    - Includes external documentation links
 41: 
 42: 4. **Generates complete PRP**:
 43:    - Synthesizes 6-section PRP structure (TL;DR, Context, Implementation Steps, Validation Gates, Testing Strategy, Rollout Plan)
 44:    - Creates YAML header with metadata
 45:    - Auto-generates next PRP ID (PRP-N+1)
 46:    - Validates completeness (ensures all required sections present)
 47: 
 48: 5. **Creates/Updates Linear issue**:
 49:    - **Without --join-prp**: Creates new Linear issue with project defaults (from `.ce/linear-defaults.yml`)
 50:    - **With --join-prp**: Updates existing PRP's Linear issue with new PRP information
 51:    - Updates PRP YAML header with `issue: {ISSUE-ID}`
 52: 
 53: 6. **Outputs to**: `PRPs/feature-requests/PRP-{id}-{feature-slug}.md`
 54: 
 55: ## INITIAL.md Structure
 56: 
 57: Your INITIAL.md must follow this structure:
 58: 
 59: ```markdown
 60: # Feature: <Feature Name>
 61: 
 62: ## FEATURE
 63: <What to build - user story, acceptance criteria>
 64: 
 65: ## EXAMPLES
 66: <Similar code patterns from codebase, inline code blocks, or file references>
 67: 
 68: ## DOCUMENTATION
 69: <Library docs, API references, external resources>
 70: 
 71: ## OTHER CONSIDERATIONS
 72: <Gotchas, constraints, security concerns, edge cases>
 73: ```
 74: 
 75: **Required sections**: FEATURE, EXAMPLES
 76: **Optional sections**: DOCUMENTATION, OTHER CONSIDERATIONS
 77: 
 78: ## Example
 79: 
 80: ```bash
 81: # Create INITIAL.md
 82: cat > feature-requests/user-auth/INITIAL.md << 'EOF'
 83: # Feature: User Authentication System
 84: 
 85: ## FEATURE
 86: Build JWT-based user authentication with:
 87: - User registration with email/password
 88: - Login with JWT token generation
 89: - Token refresh mechanism
 90: 
 91: **Acceptance Criteria:**
 92: 1. Users can register with valid email and password
 93: 2. Login returns JWT access token and refresh token
 94: 3. Protected endpoints validate JWT tokens
 95: 
 96: ## EXAMPLES
 97: ```python
 98: async def authenticate_user(email: str, password: str) -> dict:
 99:     user = await db.users.find_one({"email": email})
100:     if not user or not verify_password(password, user["password_hash"]):
101:         raise AuthenticationError("Invalid credentials")
102: 
103:     access_token = create_jwt(user["id"], expires_in=3600)
104:     return {"access_token": access_token}
105: ```
106: 
107: See src/oauth.py:42-67 for similar async authentication pattern
108: 
109: ## DOCUMENTATION
110: - [JWT Best Practices](https://jwt.io/introduction)
111: - [FastAPI Security](https://fastapi.tiangolo.com/tutorial/security/)
112: - "pytest" for testing
113: - "bcrypt" for password hashing
114: 
115: ## OTHER CONSIDERATIONS
116: **Security:**
117: - Hash passwords with bcrypt (cost factor 12)
118: - Rate limiting on login endpoint (5 attempts per 15 min)
119: EOF
120: 
121: # Generate PRP
122: cd tools
123: uv run ce prp generate feature-requests/user-auth/INITIAL.md
124: 
125: # Output: PRPs/feature-requests/PRP-6-user-authentication-system.md
126: ```
127: 
128: ## CLI Command
129: 
130: ```bash
131: # Basic usage (creates new PRP + Linear issue)
132: cd tools
133: uv run ce prp generate <path-to-initial.md>
134: 
135: # Join existing PRP's Linear issue
136: uv run ce prp generate <path-to-initial.md> --join-prp 12
137: uv run ce prp generate <path-to-initial.md> --join-prp PRP-12
138: uv run ce prp generate <path-to-initial.md> --join-prp PRPs/executed/PRP-12-feature.md
139: 
140: # Custom output directory
141: uv run ce prp generate <path-to-initial.md> -o /custom/path
142: 
143: # JSON output (for scripting)
144: uv run ce prp generate <path-to-initial.md> --json
145: 
146: # Combined options
147: uv run ce prp generate <path-to-initial.md> --join-prp 12 --json
148: ```
149: 
150: **Use Cases for --join-prp**:
151: - **Related features**: Multiple PRPs implementing parts of same initiative
152: - **Incremental work**: Breaking large PRP into smaller chunks
153: - **Follow-up work**: Additional PRP for same feature area
154: 
155: **Example workflow**:
156: ```bash
157: # Create first PRP for auth system
158: uv run ce prp generate auth-part1.md
159: # Output: PRP-10 created, Linear issue BLA-25 created
160: 
161: # Create second PRP, join same issue
162: uv run ce prp generate auth-part2.md --join-prp 10
163: # Output: PRP-11 created, BLA-25 updated with PRP-11 info
164: ```
165: 
166: ## Output Structure
167: 
168: The generated PRP will have:
169: 
170: ```markdown
171: ---
172: prp_id: TBD
173: feature_name: User Authentication System
174: status: pending
175: created: 2025-10-13T00:00:00Z
176: updated: 2025-10-13T00:00:00Z
177: complexity: medium
178: estimated_hours: 3-5
179: dependencies: JWT Best Practices, FastAPI Security, pytest
180: ---
181: 
182: # User Authentication System
183: 
184: ## 1. TL;DR
185: **Objective**: ...
186: **What**: ...
187: **Why**: ...
188: **Effort**: ...
189: **Dependencies**: ...
190: 
191: ## 2. Context
192: ### Background
193: ...
194: ### Constraints and Considerations
195: ...
196: ### Documentation References
197: ...
198: 
199: ## 3. Implementation Steps
200: ### Phase 1: Setup and Research (30 min)
201: ...
202: ### Phase 2: Core Implementation (2-3 hours)
203: ...
204: ### Phase 3: Testing and Validation (1-2 hours)
205: ...
206: 
207: ## 4. Validation Gates
208: ### Gate 1: Unit Tests Pass
209: **Command**: `uv run pytest tests/unit/ -v`
210: ...
211: 
212: ## 5. Testing Strategy
213: ### Test Framework
214: pytest
215: ### Test Command
216: ```bash
217: uv run pytest tests/ -v
218: ```
219: ...
220: 
221: ## 6. Rollout Plan
222: ### Phase 1: Development
223: ...
224: ### Phase 2: Review
225: ...
226: ### Phase 3: Deployment
227: ...
228: 
229: ---
230: 
231: ## Research Findings
232: ### Serena Codebase Analysis
233: ...
234: ### Documentation Sources
235: ...
236: ```
237: 
238: ## Graceful Degradation
239: 
240: The tool works even if MCP servers are unavailable:
241: - **Without Serena**: No codebase research, but PRP still generated with user-provided examples
242: - **Without Context7**: No library documentation fetched, but external links preserved
243: - **Without Sequential Thinking**: Heuristic-based topic extraction used
244: 
245: ## Code Quality Standards Applied
246: 
247: All generated implementations follow:
248: - **Function limits**: Target 50 lines max per function
249: - **File limits**: Target 500 lines max per file
250: - **KISS principle**: Simple solutions first, clear code over clever code
251: - **Error handling**: Fast failure with actionable troubleshooting messages
252: - **No silent failures**: Exceptions bubble up, never swallowed
253: - **Real functionality**: No hardcoded success messages or fake results
254: - **Naming conventions**: Business-focused (no version numbers or placeholders)
255: 
256: ## Tips
257: 
258: 1. **Be specific in FEATURE section**: Include clear acceptance criteria
259: 2. **Provide relevant EXAMPLES**: Reference similar code in your codebase
260: 3. **Link to DOCUMENTATION**: Include library docs and external resources
261: 4. **Note OTHER CONSIDERATIONS**: Security concerns, edge cases, constraints
262: 5. **Code quality alignment**: Generated code will follow project standards - review for consistency
263: 
264: ## Haiku-Ready PRP Checklist
265: 
266: Before executing a generated PRP, verify it's optimized for Claude 4.5 Haiku execution:
267: 
268: - [ ] **Goal**: Exact end state described, not vague improvement
269: - [ ] **Output**: File paths and line numbers specified
270: - [ ] **Limits**: Scope boundaries explicit (what's IN/OUT)
271: - [ ] **Data**: All required context inline in PRP (no external references)
272: - [ ] **Evaluation**: Validation gates are copy-paste bash commands
273: - [ ] **Decisions**: All architectural choices made (Haiku executes, doesn't decide)
274: - [ ] **Code Snippets**: Before/after code provided for major changes
275: - [ ] **No Vague Language**: Check for "appropriate", "suitable", "handle appropriately"
276: 
277: **Reference**: See [PRP-23: Haiku-Optimized PRP Guidelines](../../PRPs/feature-requests/PRP-23-haiku-optimized-prp-guidelines.md) for detailed patterns.
278: 
279: ## Next Steps After Generation
280: 
281: 1. Review generated PRP for completeness
282: 2. Fill in TBD fields (prp_id will be auto-assigned on execution)
283: 3. Adjust estimated hours if needed
284: 4. **Check Haiku-Ready checklist above**
285: 5. Execute PRP using `/execute-prp <prp-file>`
286: 
287: ## Implementation Details
288: 
289: - **Module**: `tools/ce/generate.py`
290: - **Tests**: `tools/tests/test_generate.py` (24 tests)
291: - **PRP Reference**: `PRPs/feature-requests/PRP-3-command-automation.md`
</file>

<file path=".claude/commands/sync-with-syntropy.md">
 1: # /sync-with-syntropy - Sync Settings with Syntropy MCP Tool State
 2: 
 3: Updates `.claude/settings.local.json` to match Syntropy MCP's current tool enable/disable state.
 4: 
 5: ## Workflow
 6: 
 7: 1. **Call Syntropy MCP**:
 8:    ```
 9:    mcp__syntropy__list_all_tools()
10:    ```
11: 
12:    Expected response:
13:    ```json
14:    {
15:      "total_tools": 87,
16:      "enabled_tools": 45,
17:      "disabled_tools": 42,
18:      "tools": [
19:        {"name": "mcp__syntropy__serena_find_symbol", "description": "...", "status": "enabled"},
20:        {"name": "mcp__syntropy__filesystem_read_file", "description": "...", "status": "disabled"}
21:      ]
22:    }
23:    ```
24: 
25: 2. **Load Settings**:
26:    - Read `.claude/settings.local.json`
27:    - If missing, create with structure: `{"allow": [], "deny": [], "ask": []}`
28: 
29: 3. **Process Disabled Tools**:
30:    For each tool with `status: "disabled"`:
31:    - Remove from `allow` list if present
32:    - Remove from `deny` list if present
33:    - Remove from `ask` list if present
34: 
35: 4. **Process Enabled Tools**:
36:    For each tool with `status: "enabled"`:
37:    - If NOT in `allow`, `deny`, OR `ask` lists ‚Üí Add to `allow` list
38:    - If already in any list ‚Üí No change
39: 
40: 5. **Backup and Validate**:
41:    - Backup: Copy settings to `.claude/settings.local.json.backup`
42:    - Validate JSON syntax: Parse and re-stringify to ensure validity
43:    - If validation fails ‚Üí Abort, restore backup, show error
44: 
45: 6. **Write Settings**:
46:    - Write updated settings with `indent=2`
47:    - Preserve all other settings (hooks, etc.)
48: 
49: 7. **Output Summary**:
50:    ```
51:    ‚úì Synced settings with Syntropy MCP tool state
52: 
53:    Removed 2 disabled tools:
54:      - mcp__syntropy__filesystem_read_file
55:      - mcp__syntropy__git_git_status
56: 
57:    Added 1 enabled tool to allow list:
58:      - mcp__syntropy__serena_find_symbol
59: 
60:    No changes: 45 tools already correct
61:    ```
62: 
63: ## Error Handling
64: 
65: **Syntropy MCP Not Connected**:
66: ```
67: ‚ùå Error: Syntropy MCP not connected
68: 
69:    Please ensure Syntropy MCP server is running:
70:    - Check MCP status with /mcp
71:    - Restart MCP if needed
72: ```
73: 
74: **Invalid JSON**:
75: ```
76: ‚ùå Error: Settings file invalid after update
77: 
78:    Validation error: {error details}
79: 
80:    Settings restored from backup.
81:    üîß Check .claude/settings.local.json.backup for last good state
82: ```
83: 
84: **Empty Tool List**:
85: ```
86: ‚ö† No tools returned from Syntropy MCP
87: 
88:   Skipping settings update (nothing to sync)
89: ```
</file>

<file path=".claude/commands/syntropy-health.md">
 1: # /syntropy-health - Syntropy MCP Health Check
 2: 
 3: Check health status of Syntropy MCP server and all underlying servers.
 4: 
 5: ## Usage
 6: 
 7: ```bash
 8: # Quick check (default - fast, essential servers only)
 9: /syntropy-health
10: 
11: # Full diagnostic check (all servers, detailed info)
12: /syntropy-health full
13: ```
14: 
15: ## What It Checks
16: 
17: **Quick Mode** (default):
18: - Essential servers: Serena, Filesystem, Git, Linear, Thinking
19: - Response times for each server
20: - Connection status
21: - Tool call counts (if available)
22: 
23: **Full Mode**:
24: - All 9 MCP servers (including lazy-loaded: Context7, Repomix, GitHub, Perplexity)
25: - Detailed diagnostics
26: - Last errors (if any)
27: - Call statistics
28: 
29: ## Output Format
30: 
31: ```
32: üè• Syntropy MCP Health Check
33: 
34: ‚úÖ serena: 124ms (15 calls)
35: ‚úÖ filesystem: 45ms (8 calls)
36: ‚úÖ git: 67ms (3 calls)
37: ‚ö†Ô∏è  linear: 1205ms (2 calls) - SLOW
38: ‚ùå github: Not connected
39: 
40: Overall: 4/5 servers healthy
41: ```
42: 
43: ## Troubleshooting
44: 
45: ### "Not connected" errors
46: 
47: Clear MCP auth cache and restart:
48: ```bash
49: rm -rf ~/.mcp-auth
50: ```
51: 
52: ### Slow response times (>1s)
53: 
54: Server may be starting up or overloaded. Wait 30s and retry.
55: 
56: ### All servers failing
57: 
58: Check if MCP servers are running:
59: ```bash
60: ps aux | grep mcp
61: ```
62: 
63: ## When to Use
64: 
65: **Recommended on session start** to verify MCP infrastructure:
66: - After Claude Code restart
67: - When seeing unexpected tool failures
68: - Before starting major work (PRPs, refactoring)
69: - After clearing auth cache
70: 
71: ## Implementation
72: 
73: This command calls the `healthcheck` tool with appropriate parameters:
74: - Default: `{"detailed": false, "timeout_ms": 2000}`
75: - Full: `{"detailed": true, "timeout_ms": 5000}`
76: 
77: ## Related
78: 
79: - `/update-context` - Updates project context (requires healthy Serena)
80: - `/generate-prp` - Requires healthy Serena + Filesystem
81: - `/execute-prp` - Requires multiple healthy servers
</file>

<file path=".claude/commands/update-context.md">
  1: # Update Context
  2: 
  3: Sync Context Engineering (CE) and Serena knowledge systems with actual codebase implementation state.
  4: 
  5: ## Usage
  6: ```bash
  7: /update-context [--prp <prp-file>]
  8: ```
  9: 
 10: ## Parameters
 11: - `--prp <prp-file>`: Optional. Target specific PRP file for sync (path relative to project root)
 12: 
 13: ## What it does
 14: 
 15: Universal system hygiene command that maintains bidirectional sync between knowledge systems and codebase:
 16: 
 17: 1. **Scans PRPs**
 18:    - Universal mode: All PRPs in `PRPs/feature-requests/` and `PRPs/executed/`
 19:    - Targeted mode: Single PRP specified with `--prp` flag
 20: 
 21: 2. **Updates YAML Headers**
 22:    - Sets `context_sync.ce_updated` flag (based on implementation verification)
 23:    - Sets `context_sync.serena_updated` flag (if Serena MCP available)
 24:    - Adds `last_sync` timestamp
 25:    - Updates `updated_by` attribution
 26: 
 27: 3. **Verifies Implementations**
 28:    - Extracts expected functions/classes from PRP content
 29:    - Cross-references with actual codebase via Serena MCP
 30:    - Marks `ce_updated=true` only if ALL expected implementations found
 31: 
 32: 4. **Manages PRP Status**
 33:    - Auto-transitions PRPs from `status: new` ‚Üí `status: executed` when verified
 34:    - Moves files from `PRPs/feature-requests/` ‚Üí `PRPs/executed/` atomically
 35:    - Identifies deprecated PRPs for archival
 36: 
 37: 5. **Detects Pattern Drift**
 38:    - **Code Violations**: Scans codebase for violations of documented patterns (examples/)
 39:    - **Missing Examples**: Identifies critical PRPs without corresponding pattern documentation
 40:    - Generates structured drift report with solution proposals
 41:    - Saves report to `.ce/drift-report.md`
 42: 
 43: 6. **Reports Results**
 44:    - Summary statistics (PRPs scanned/updated/moved)
 45:    - Drift score and violation count
 46:    - Clear logging of all operations
 47: 
 48: ## Examples
 49: 
 50: ```bash
 51: # Sync all PRPs with codebase
 52: /update-context
 53: 
 54: # Sync specific PRP only
 55: /update-context --prp PRPs/executed/PRP-13-production-hardening.md
 56: 
 57: # Typical workflow
 58: # 1. Implement feature
 59: # 2. Run /update-context to verify and sync
 60: # 3. Review drift report if generated
 61: # 4. Fix violations or create missing examples
 62: ```
 63: 
 64: ## When to Use
 65: 
 66: **Run /update-context after:**
 67: - Completing PRP implementation
 68: - Significant codebase refactoring
 69: - Adding new examples/ patterns
 70: - Weekly system hygiene (prevent drift accumulation)
 71: 
 72: **Run with --prp flag when:**
 73: - Testing single PRP verification
 74: - Debugging context sync issues
 75: - Quick spot-check after small change
 76: 
 77: ## Drift Detection
 78: 
 79: When drift is detected, generates `.ce/drift-report.md` with:
 80: 
 81: **Part 1: Code Violating Documented Patterns**
 82: - Error handling violations (bare except, missing troubleshooting)
 83: - Naming convention violations (versioned suffixes)
 84: - KISS violations (overcomplicated implementations)
 85: 
 86: **Part 2: Missing Pattern Documentation**
 87: - Critical PRPs (complexity ‚â• medium) without examples/
 88: - Suggested example paths
 89: - Rationale for documentation need
 90: 
 91: **Each violation includes:**
 92: - File location
 93: - Specific issue
 94: - Pattern reference
 95: - Proposed solution
 96: 
 97: ## Graceful Degradation
 98: 
 99: - **Serena MCP unavailable**: Continues with warnings, sets `serena_updated=false`
100: - **examples/ missing**: Skips drift detection with info log
101: - **Invalid YAML**: Skips file with warning, continues with others
102: - **Permission errors**: Raises error with troubleshooting guidance (no silent failures)
103: 
104: ## YAML Header Updates
105: 
106: Example before:
107: ```yaml
108: status: new
109: context_sync:
110:   ce_updated: false
111:   serena_updated: false
112: ```
113: 
114: Example after:
115: ```yaml
116: status: executed
117: context_sync:
118:   ce_updated: true
119:   serena_updated: true
120:   last_sync: 2025-10-14T17:00:00Z
121: updated_by: update-context-command
122: ```
123: 
124: ## Success Criteria
125: 
126: ‚úÖ All PRPs scanned successfully
127: ‚úÖ YAML headers updated accurately
128: ‚úÖ Status transitions executed correctly
129: ‚úÖ Files moved atomically (no data loss)
130: ‚úÖ Drift detection identifies real violations
131: ‚úÖ No false positives in pattern checks
132: 
133: ## Related Commands
134: 
135: - `/generate-prp` - Create new PRP blueprint
136: - `/execute-prp` - Implement PRP feature
137: - `/peer-review` - Review PRP or execution
138: - `/validate-prp-system` - Comprehensive system validation
139: 
140: **Goal:** Prevent documentation rot through automated sync verification and drift detection.
</file>

<file path=".serena/memories/code-style-conventions.md">
  1: ---
  2: type: regular
  3: category: documentation
  4: tags: [code-style, conventions, standards]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # Code Style and Conventions
 10: 
 11: ## Communication Style
 12: **Direct, token-efficient. No fluff.**
 13: - Short sentences, maximum clarity
 14: - Call out problems directly
 15: - Real talk, zero BS
 16: 
 17: ## Core Principles
 18: 
 19: ### No Fishy Fallbacks - MANDATORY
 20: - ‚úÖ Fast Failure: Let exceptions bubble up
 21: - ‚úÖ Actionable Errors: Include üîß troubleshooting guidance
 22: - ‚úÖ No Silent Corruption: Make failures visible
 23: - ‚ùå No hardcoded default values that bypass business logic
 24: - ‚ùå No silent exception catches that hide data corruption
 25: - ‚ùå No broad exception catches that mask root causes
 26: 
 27: ### KISS (Keep It Simple, Stupid)
 28: - Simple solutions first
 29: - Clear code over clever code
 30: - Minimal dependencies (stdlib only for this project)
 31: - Single responsibility per function
 32: - Direct implementation
 33: - Fast failure
 34: 
 35: ## File and Function Limits (Guidelines)
 36: - **Functions**: ~50 lines max - single, clear responsibility
 37: - **Files**: ~300-500 lines max - break into logical modules if approaching
 38: - **Classes**: ~100 lines max - represent single concept
 39: 
 40: ## Naming Conventions
 41: ```python
 42: # AVOID: Version-specific naming
 43: def get_v2_processor()
 44: 
 45: # PREFER: Business-focused naming
 46: def get_processor()
 47: ```
 48: 
 49: ## Type Hints
 50: Use Python type hints throughout:
 51: ```python
 52: def run_cmd(cmd: str, timeout: int = 60) -> Dict[str, Any]:
 53:     """Execute shell command with timeout."""
 54:     pass
 55: ```
 56: 
 57: ## Docstrings (Google Style)
 58: ```python
 59: def run_cmd(cmd: str, timeout: int = 60) -> Dict[str, Any]:
 60:     """Execute shell command with timeout.
 61: 
 62:     Args:
 63:         cmd: Shell command to execute
 64:         timeout: Command timeout in seconds
 65: 
 66:     Returns:
 67:         Dict with: success, stdout, stderr, exit_code, duration
 68: 
 69:     Raises:
 70:         TimeoutError: If command exceeds timeout
 71:         RuntimeError: If command execution fails
 72: 
 73:     Note: No fishy fallbacks - exceptions thrown for troubleshooting.
 74:     """
 75: ```
 76: 
 77: ## Exception Handling
 78: ```python
 79: # ‚úÖ GOOD - Clear troubleshooting
 80: def git_checkpoint(message: str) -> str:
 81:     result = run_cmd(f'git tag -a "{tag}" -m "{message}"')
 82:     if not result["success"]:
 83:         raise RuntimeError(
 84:             f"Failed to create checkpoint: {result['stderr']}\n"
 85:             f"üîß Troubleshooting: Ensure you have commits to tag"
 86:         )
 87:     return tag
 88: 
 89: # ‚ùå BAD - Silent failure
 90: def git_checkpoint(message: str) -> str:
 91:     try:
 92:         result = run_cmd(f'git tag -a "{tag}" -m "{message}"')
 93:         return tag
 94:     except:
 95:         return "checkpoint-failed"  # FISHY FALLBACK!
 96: ```
 97: 
 98: ## Function Design
 99: ```python
100: # ‚úÖ GOOD - Single responsibility, clear purpose
101: def git_status() -> Dict[str, Any]:
102:     """Get git repository status."""
103:     # ... implementation
104: 
105: # ‚ùå BAD - Multiple responsibilities
106: def git_stuff(action: str) -> Any:
107:     """Do various git things."""
108:     if action == "status": ...
109:     elif action == "diff": ...
110:     elif action == "commit": ...
111: ```
112: 
113: ## Mock/Placeholder Policy - MANDATORY
114: **Always mark non-test mocks with FIXME comments.**
115: 
116: ```python
117: # ‚ùå FORBIDDEN - Unmarked placeholder
118: def process_data(params):
119:     return {"success": True}  # Hidden fake implementation
120: 
121: # ‚úÖ REQUIRED - Clearly marked placeholder
122: def process_data(params):
123:     # FIXME: Placeholder implementation - returns fake values
124:     # TODO: Implement real processing
125:     return {"success": True}  # FIXME: Hardcoded value
126: ```
127: 
128: **Rules**:
129: - Production code: ALWAYS add `# FIXME:` comment
130: - Test files: Mocks are intentional, no FIXME needed
131: - Temporary implementations: Must have FIXME + TODO
132: 
133: ## File Management
134: - **Update Existing Files**: Always modify rather than create new versions
135: - **No Variants**: Never create `enhanced_`, `optimized_`, `v2_` variants
136: - **Minimal File Count**: Smallest number that maintains best practices
137: - **Single Source of Truth**: Each concept in exactly one location
</file>

<file path=".serena/memories/codebase-structure.md">
  1: ---
  2: type: regular
  3: category: architecture
  4: tags: [structure, organization, modules]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # Codebase Structure
 10: 
 11: ## Top-Level Layout
 12: ```
 13: ctx-eng-plus/
 14: ‚îú‚îÄ‚îÄ tools/                  # Main CLI package (primary workspace)
 15: ‚îú‚îÄ‚îÄ docs/                   # Documentation and research
 16: ‚îú‚îÄ‚îÄ PRPs/                   # PRP documents and planning
 17: ‚îú‚îÄ‚îÄ .serena/                # Serena MCP configuration
 18: ‚îú‚îÄ‚îÄ .claude/                # Claude Code configuration
 19: ‚îú‚îÄ‚îÄ CLAUDE.md               # Project-specific guidance
 20: ‚îú‚îÄ‚îÄ README.md               # Main documentation
 21: ‚îî‚îÄ‚îÄ .gitignore              # Git ignore rules
 22: ```
 23: 
 24: ## Tools Package (Main Component)
 25: ```
 26: tools/
 27: ‚îú‚îÄ‚îÄ ce/                     # Source code package
 28: ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py         # Package metadata and version
 29: ‚îÇ   ‚îú‚îÄ‚îÄ __main__.py         # CLI entry point (argparse)
 30: ‚îÇ   ‚îú‚îÄ‚îÄ core.py             # Core functionality (shell, git, file ops)
 31: ‚îÇ   ‚îú‚îÄ‚îÄ validate.py         # 3-level validation gates
 32: ‚îÇ   ‚îú‚îÄ‚îÄ context.py          # Context management & drift detection
 33: ‚îÇ   ‚îú‚îÄ‚îÄ generate.py         # PRP generation (PRP-3)
 34: ‚îÇ   ‚îú‚îÄ‚îÄ execute.py          # PRP execution orchestration (PRP-4)
 35: ‚îÇ   ‚îú‚îÄ‚îÄ prp.py              # PRP state management (PRP-2)
 36: ‚îÇ   ‚îú‚îÄ‚îÄ drift_analyzer.py   # Implementation drift analysis
 37: ‚îÇ   ‚îú‚îÄ‚îÄ code_analyzer.py    # Code pattern extraction
 38: ‚îÇ   ‚îú‚îÄ‚îÄ mermaid_validator.py # Mermaid diagram validation
 39: ‚îÇ   ‚îî‚îÄ‚îÄ pattern_extractor.py # Pattern extraction utilities
 40: ‚îú‚îÄ‚îÄ tests/                  # Test suite
 41: ‚îÇ   ‚îú‚îÄ‚îÄ test_cli.py         # CLI interface tests
 42: ‚îÇ   ‚îú‚îÄ‚îÄ test_core.py        # Core function tests
 43: ‚îÇ   ‚îú‚îÄ‚îÄ test_validate.py    # Validation tests
 44: ‚îÇ   ‚îú‚îÄ‚îÄ test_context.py     # Context management tests
 45: ‚îÇ   ‚îú‚îÄ‚îÄ test_generate.py    # PRP generation tests
 46: ‚îÇ   ‚îî‚îÄ‚îÄ test_execute.py     # PRP execution tests
 47: ‚îú‚îÄ‚îÄ pyproject.toml          # UV package configuration
 48: ‚îú‚îÄ‚îÄ uv.lock                 # UV lock file (auto-generated)
 49: ‚îú‚îÄ‚îÄ README.md               # User documentation
 50: ‚îî‚îÄ‚îÄ bootstrap.sh            # Setup script
 51: ```
 52: 
 53: ## Core Modules Overview
 54: 
 55: ### ce/core.py
 56: **Functions**: `run_cmd`, `read_file`, `write_file`, `git_status`, `git_diff`, `git_checkpoint`, `run_py`
 57: - Shell command execution with timeout
 58: - File operations (read/write)
 59: - Git operations (status, diff, checkpoints)
 60: - Python code execution (auto-detect mode)
 61: 
 62: ### ce/validate.py
 63: **Functions**: `validate_level_1`, `validate_level_2`, `validate_level_3`, `validate_level_4`, `validate_all`
 64: - L1: Syntax validation (py_compile, mypy, ruff)
 65: - L2: Unit tests
 66: - L3: Integration tests
 67: - L4: Pattern conformance (drift analysis)
 68: 
 69: ### ce/context.py
 70: **Functions**: `sync`, `health`, `prune`, `pre_generation_sync`, `post_execution_sync`, `calculate_drift_score`, `enable_auto_sync`, `disable_auto_sync`, `is_auto_sync_enabled`
 71: - Context synchronization with codebase
 72: - Health checks for context drift
 73: - Pruning stale context data
 74: - Pre/post workflow sync automation (Steps 2.5 and 6.5)
 75: - Auto-sync mode management
 76: 
 77: ### ce/generate.py (PRP-3)
 78: **Functions**: `parse_initial_md`, `research_codebase`, `fetch_documentation`, `generate_prp`, `synthesize_tldr`, `synthesize_implementation`
 79: - INITIAL.md parsing
 80: - Codebase research orchestration (Serena MCP)
 81: - Documentation fetching (Context7 MCP)
 82: - PRP template synthesis
 83: - All 6 PRP sections generation
 84: 
 85: ### ce/execute.py (PRP-4)
 86: **Functions**: `parse_blueprint`, `execute_prp`, `execute_phase`, `run_validation_loop`, `apply_self_healing_fix`, `check_escalation_triggers`
 87: - PRP blueprint parsing
 88: - Phase-by-phase execution
 89: - L1-L4 validation loops
 90: - Self-healing mechanisms (L1-L2 retries)
 91: - Escalation flow (5 trigger types)
 92: 
 93: ### ce/prp.py (PRP-2)
 94: **Functions**: State management, checkpoint creation, cleanup protocol
 95: - PRP session tracking
 96: - Checkpoint management
 97: - Ephemeral state cleanup
 98: 
 99: ### ce/drift_analyzer.py
100: **Functions**: `analyze_implementation`, `calculate_drift_score`, `get_auto_fix_suggestions`
101: - Implementation vs specification drift analysis
102: - Drift scoring for L4 validation
103: - Auto-fix recommendations
104: 
105: ### ce/code_analyzer.py
106: **Functions**: `analyze_code_patterns`, `_analyze_python`, `_analyze_typescript`, `determine_language`, `count_code_symbols`
107: - Language-specific pattern extraction
108: - Code symbol counting
109: - Generic pattern analysis
110: 
111: ### ce/mermaid_validator.py
112: - Mermaid diagram validation
113: - Theme compatibility checking
114: 
115: ### ce/pattern_extractor.py
116: - Pattern extraction utilities
117: - Code pattern analysis
118: 
119: ### ce/__main__.py
120: **CLI Commands**:
121: - `ce validate` - Run validation gates (L1-L4)
122: - `ce git` - Git operations (status, diff, checkpoint)
123: - `ce context` - Context management (sync, health, prune, auto-sync)
124: - `ce run_py` - Execute Python code (auto-detect or explicit mode)
125: - `ce prp validate` - PRP YAML validation
126: - `ce prp generate` - PRP generation from INITIAL.md
127: - `ce prp execute` - PRP execution with validation loops
128: 
129: ## Documentation Structure
130: ```
131: docs/
132: ‚îî‚îÄ‚îÄ research/               # Research and design documentation
133:     ‚îú‚îÄ‚îÄ 00-index.md         # Documentation index
134:     ‚îú‚îÄ‚îÄ 01-prp-system.md    # PRP system design
135:     ‚îú‚îÄ‚îÄ 02-context-engineering-foundations.md
136:     ‚îú‚îÄ‚îÄ 03-mcp-orchestration.md
137:     ‚îú‚îÄ‚îÄ 04-self-healing-framework.md
138:     ‚îú‚îÄ‚îÄ 05-persistence-layers.md
139:     ‚îú‚îÄ‚îÄ 06-workflow-patterns.md
140:     ‚îú‚îÄ‚îÄ 07-commands-reference.md
141:     ‚îú‚îÄ‚îÄ 08-validation-testing.md
142:     ‚îú‚îÄ‚îÄ 09-best-practices-antipatterns.md
143:     ‚îú‚îÄ‚îÄ 10-tooling-configuration.md
144:     ‚îî‚îÄ‚îÄ 11-claude-code-features.md
145: ```
146: 
147: ## PRP Structure
148: ```
149: PRPs/
150: ‚îú‚îÄ‚îÄ executed/               # Completed PRPs
151: ‚îÇ   ‚îú‚îÄ‚îÄ PRP-1-*.md         # Init & L4 validation
152: ‚îÇ   ‚îú‚îÄ‚îÄ PRP-1.2-*.md       # YAML validation
153: ‚îÇ   ‚îú‚îÄ‚îÄ PRP-2-*.md         # State management
154: ‚îÇ   ‚îú‚îÄ‚îÄ PRP-3-*.md         # Generate command
155: ‚îÇ   ‚îú‚îÄ‚îÄ PRP-4-*.md         # Execute command
156: ‚îÇ   ‚îî‚îÄ‚îÄ PRP-5-*.md         # Context sync
157: ‚îú‚îÄ‚îÄ feature-requests/       # Future PRPs
158: ‚îÇ   ‚îú‚îÄ‚îÄ PRP-6-*.md         # Markdown linting
159: ‚îÇ   ‚îú‚îÄ‚îÄ PRP-7-*.md         # Validation loop tests
160: ‚îÇ   ‚îú‚îÄ‚îÄ PRP-8-*.md         # PRP sizing analysis
161: ‚îÇ   ‚îî‚îÄ‚îÄ PRP-9-*.md         # Serena MCP file ops
162: ‚îî‚îÄ‚îÄ templates/              # PRP templates
163:     ‚îú‚îÄ‚îÄ prp-base-template.md
164:     ‚îú‚îÄ‚îÄ kiss.md
165:     ‚îî‚îÄ‚îÄ self-healing.md
166: ```
167: 
168: ## Key Design Patterns
169: 
170: ### Minimal Dependencies
171: - Stdlib-only approach for core functionality
172: - No external runtime dependencies
173: - pytest for testing only (dev dependency)
174: 
175: ### Single Responsibility
176: - Each module handles one aspect
177: - core.py: system operations
178: - validate.py: validation logic
179: - context.py: context management
180: - generate.py: PRP generation
181: - execute.py: PRP execution
182: - prp.py: state management
183: 
184: ### CLI Architecture
185: - Subcommand-based interface
186: - JSON output support for scripting
187: - Clear error messages with troubleshooting guidance
188: - Workflow automation (auto-sync mode)
189: 
190: ## Implementation Status (2025-10-13)
191: 
192: ### Executed PRPs
193: - ‚úÖ PRP-1: L4 pattern conformance validation
194: - ‚úÖ PRP-1.2: YAML validation command
195: - ‚úÖ PRP-2: PRP state management & cleanup
196: - ‚úÖ PRP-3: PRP generation automation (all 6 phases)
197: - ‚úÖ PRP-4: PRP execution orchestration (all 5 phases)
198: - ‚úÖ PRP-5: Context sync integration (all 6 phases)
199: 
200: ### Feature Requests
201: - ‚è∏Ô∏è PRP-6: Markdown linting
202: - ‚è∏Ô∏è PRP-7: Comprehensive validation loop tests (80% coverage)
203: - ‚è∏Ô∏è PRP-8: PRP sizing constraint analysis
204: - ‚è∏Ô∏è PRP-9: Serena MCP file operations integration
</file>

<file path=".serena/memories/cwe78-prp22-newline-escape-issue.md">
  1: ---
  2: type: regular
  3: category: troubleshooting
  4: tags: [security, cwe78, fix]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # CWE-78 PRP-22: Critical Newline Escape Sequence Issue & Solution
 10: 
 11: ## Problem Encountered
 12: 
 13: When using `mcp__serena__replace_regex` tool to insert f-strings with newline escape sequences, the tool was interpreting `\n` as actual newline characters in the source code, creating syntax errors.
 14: 
 15: ### Symptoms
 16: - **Error**: `SyntaxError: unterminated f-string literal (detected at line 76)`
 17: - **Root Cause**: F-strings with literal newlines instead of `\n` escape sequences
 18: - **Example Failure**:
 19: ```python
 20: # WRONG - tool replaced with literal newline
 21: raise ValueError(
 22:     f"Empty command provided
 23: "  # <-- LITERAL NEWLINE, breaks syntax
 24:     "üîß Troubleshooting: ..."
 25: )
 26: ```
 27: 
 28: ## Solution: Write Direct > Regex Replace
 29: 
 30: ### When to Use Each Approach
 31: 
 32: **‚úÖ USE `write_file()` FOR**:
 33: - Multi-line functions with f-strings
 34: - Complex error messages with escape sequences
 35: - Any code with newlines in string literals
 36: - Functions over ~30 lines
 37: 
 38: **‚ö†Ô∏è AVOID `replace_regex()` FOR**:
 39: - F-strings with `\n` sequences (tool misinterprets them)
 40: - Complex multi-line replacements
 41: - Code with escape sequences or special characters
 42: - When small tweaks to larger functions needed
 43: 
 44: ### Workaround Pattern Used in PRP-22
 45: 
 46: 1. **Identify problem**: Multiple f-strings with `\n` needed in core.py
 47: 2. **Create clean file**: Made `core_fixed.py` with correct syntax
 48: 3. **Replace entire file**: Used `write_file()` to replace core.py with corrected version
 49: 4. **Verify syntax**: Ran imports to verify no syntax errors
 50: 
 51: **Result**: Successfully eliminated syntax issues
 52: 
 53: ## Prevention for Future PRPs
 54: 
 55: ### For Serena Tool Users
 56: 
 57: When editing Python files with f-strings containing escape sequences:
 58: 
 59: ```python
 60: # ‚ùå DON'T use replace_regex for this
 61: raise TimeoutError(
 62:     f"Command timed out after {timeout}s\n"  # Tool breaks this
 63:     f"üîß Troubleshooting: Check for hanging process"
 64: )
 65: 
 66: # ‚úÖ DO use write_file() or read file completely, edit, write back
 67: # Or use simpler find_symbol + replace_symbol_body approach
 68: ```
 69: 
 70: ### Alternative: Symbol-Based Editing
 71: 
 72: For functions with complex f-strings:
 73: 1. Use `find_symbol(name_path, include_body=True)`
 74: 2. Read full function body
 75: 3. Edit the complete function
 76: 4. Use `replace_symbol_body(name_path, new_body)` 
 77: 
 78: **Advantage**: Tool handles escape sequences correctly within symbol replacement
 79: 
 80: ### Quick Reference
 81: 
 82: | Task | Best Tool | Why |
 83: |------|-----------|-----|
 84: | F-string with `\n` | `write_file()` | Preserves escape sequences |
 85: | Single line replacement | `replace_regex()` | Fast, precise |
 86: | Function with escapes | `replace_symbol_body()` | Handles context properly |
 87: | Multi-line with escapes | `write_file()` | Full control |
 88: 
 89: ## Files Affected in PRP-22
 90: 
 91: - `tools/ce/core.py`: 5 functions with f-strings containing `\n`
 92: - Issue occurred with runs of `replace_regex()` attempting to fix newline issues
 93: - **Resolution**: Created `core_fixed.py` with proper syntax, then `write_file()` to replace
 94: 
 95: ## For Next Similar PRP
 96: 
 97: If replacing code with many f-strings:
 98: 1. Prepare clean version in memory/file first
 99: 2. Use `write_file()` for entire modules
100: 3. Or use `replace_symbol_body()` for individual functions
101: 4. Avoid regex approach for escape-heavy code
102: 
103: ## Test Coverage for This Issue
104: 
105: - `tests/test_security.py::test_run_cmd_rejects_command_chaining`: Tests f-string error messages
106: - All 38 security tests verify proper error message formatting
107: - No test failures related to syntax after using `write_file()` approach
</file>

<file path=".serena/memories/l4-validation-usage.md">
  1: ---
  2: type: regular
  3: category: documentation
  4: tags: [validation, testing, l4]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # Level 4 Pattern Conformance Validation - Usage Guide
 10: 
 11: ## Overview
 12: 
 13: L4 validation detects architectural drift between PRP EXAMPLES and implementation code using semantic pattern analysis.
 14: 
 15: ## Key Modules
 16: 
 17: ### 1. code_analyzer.py (Shared Module)
 18: **Single source of truth for pattern detection**
 19: 
 20: ```python
 21: from ce.code_analyzer import (
 22:     analyze_code_patterns,
 23:     determine_language,
 24:     count_code_symbols
 25: )
 26: 
 27: # Analyze code patterns
 28: patterns = analyze_code_patterns(code="def foo(): pass", language="python")
 29: # Returns: {"code_structure": ["functional"], "naming_conventions": ["snake_case"], ...}
 30: 
 31: # Determine language from file extension
 32: lang = determine_language(".py")  # Returns: "python"
 33: 
 34: # Count code symbols (functions, classes, methods)
 35: count = count_code_symbols(code="def foo(): pass\nclass Bar: pass", language="python")
 36: # Returns: 2
 37: ```
 38: 
 39: ### 2. pattern_extractor.py
 40: **Extracts patterns from PRP EXAMPLES section**
 41: 
 42: ```python
 43: from ce.pattern_extractor import extract_patterns_from_prp
 44: 
 45: patterns = extract_patterns_from_prp("PRPs/PRP-X.md")
 46: # Returns: {
 47: #   "code_structure": ["async/await", "class-based"],
 48: #   "error_handling": ["try-except"],
 49: #   "naming_conventions": ["snake_case", "PascalCase"],
 50: #   "test_patterns": ["pytest"],
 51: #   "raw_examples": [{"language": "python", "code": "..."}]
 52: # }
 53: ```
 54: 
 55: ### 3. drift_analyzer.py
 56: **Calculates drift between expected and actual patterns**
 57: 
 58: ```python
 59: from ce.drift_analyzer import (
 60:     analyze_implementation,
 61:     calculate_drift_score,
 62:     get_auto_fix_suggestions
 63: )
 64: 
 65: # Analyze implementation files
 66: analysis = analyze_implementation(
 67:     prp_path="PRPs/PRP-X.md",
 68:     implementation_paths=["src/feature.py"]
 69: )
 70: 
 71: # Calculate drift score
 72: drift = calculate_drift_score(expected_patterns, detected_patterns)
 73: # Returns: {
 74: #   "drift_score": 15.0,  # 0-100%
 75: #   "threshold_action": "auto_fix",  # auto_accept | auto_fix | escalate
 76: #   "category_scores": {"code_structure": 10.0, ...},
 77: #   "mismatches": [...]
 78: # }
 79: 
 80: # Get auto-fix suggestions
 81: suggestions = get_auto_fix_suggestions(drift["mismatches"])
 82: ```
 83: 
 84: ### 4. validate.py
 85: **L4 validation orchestration**
 86: 
 87: ```python
 88: from ce.validate import validate_level_4, calculate_confidence
 89: 
 90: # Run L4 validation
 91: result = validate_level_4(
 92:     prp_path="PRPs/PRP-X.md",
 93:     implementation_paths=["src/feature.py"]  # Optional - auto-detects if None
 94: )
 95: 
 96: # Calculate confidence score (includes L4 result)
 97: results = {1: {...}, 2: {...}, 3: {...}, 4: result}
 98: confidence = calculate_confidence(results)  # Returns: 1-10
 99: ```
100: 
101: ## CLI Usage
102: 
103: ```bash
104: # Run L4 validation on a PRP
105: ce validate --level 4 --prp PRPs/PRP-X.md --files src/feature.py
106: 
107: # Auto-detect implementation files from PRP or git
108: ce validate --level 4 --prp PRPs/PRP-X.md
109: 
110: # Run all validation levels
111: ce validate --level all
112: ```
113: 
114: ## Drift Thresholds
115: 
116: - **0-10% drift**: Auto-accept (pattern variations acceptable)
117: - **10-30% drift**: Auto-fix (display suggestions, continue)
118: - **30%+ drift**: Escalate (interactive user decision required)
119: 
120: ## Pattern Categories
121: 
122: 1. **code_structure**: async/await, class-based, functional, decorators
123: 2. **error_handling**: try-except, early-return, null-checks
124: 3. **naming_conventions**: snake_case, camelCase, PascalCase, _private
125: 4. **data_flow**: props, state, context, closure
126: 5. **test_patterns**: pytest, jest, unittest, fixtures
127: 6. **import_patterns**: relative, absolute
128: 
129: ## User Escalation (30%+ drift)
130: 
131: When drift ‚â• 30%, L4 prompts user:
132: 
133: ```
134: üö® HIGH DRIFT DETECTED: 45.2%
135: 
136: OPTIONS:
137: [A] Accept drift (add DRIFT_JUSTIFICATION to PRP)
138: [R] Reject and halt (requires manual refactoring)
139: [U] Update EXAMPLES in PRP (update specification)
140: [Q] Quit without saving
141: ```
142: 
143: Decisions are persisted to PRP YAML header for audit trail.
144: 
145: ## Confidence Scoring
146: 
147: L4 validation adds +1 to confidence score when:
148: - Drift < 10% (auto-accept), OR
149: - Drift < 30% AND user accepted with justification
150: 
151: Maximum achievable: 10/10 (production-ready)
152: 
153: ## Code Consolidation
154: 
155: **Before**: 780 LOC across pattern_extractor.py + drift_analyzer.py  
156: **After**: 697 LOC with shared code_analyzer.py module  
157: **Savings**: 83 LOC (11% reduction), single source of truth
</file>

<file path=".serena/memories/linear-issue-creation-pattern.md">
 1: ---
 2: type: regular
 3: category: pattern
 4: tags: [linear, issues, automation]
 5: created: "2025-11-04T17:30:00Z"
 6: updated: "2025-11-04T17:30:00Z"
 7: ---
 8: 
 9: # Linear Issue Creation Pattern
10: 
11: ## Working Example
12: 
13: Successfully created 5 MVP PRP issues (BLA-7 through BLA-11) in Linear.
14: 
15: ## Key Parameters
16: 
17: ```python
18: mcp__linear__create_issue(
19:     team="Blaise78",  # Team name, NOT "Context Engineering"
20:     title="PRP-X: Feature Name",
21:     description="# Markdown description...",
22:     priority=1,  # 1=Urgent, 2=High, 3=Medium, 4=Low
23:     labels=["prp", "mvp"],  # Array of label strings
24:     project="Context Engineering"  # Project name as string
25: )
26: ```
27: 
28: ## Description Format (From BLA-6 Reference)
29: 
30: ```markdown
31: # PRP-X.Y: Title
32: 
33: **Status**: Ready/Executed  
34: **PRP File**: PRPs/PRP-X-filename.md
35: 
36: ## Summary
37: 
38: Brief description
39: 
40: ## Key Deliverables
41: 
42: ‚úÖ Item 1  
43: ‚úÖ Item 2
44: 
45: ## Effort
46: 
47: * **Estimated**: Xh
48: * **Phases**: Phase1 (Xh), Phase2 (Yh)
49: 
50: ## Files
51: 
52: * `path/to/file.py`
53: 
54: ## Dependencies
55: 
56: PRP-X, PRP-Y
57: ```
58: 
59: ## PRP YAML Header Integration
60: 
61: After creating issue, update PRP file:
62: 
63: ```yaml
64: issue: "BLA-X"  # Linear issue identifier
65: project: "Context Engineering"  # Linear project name
66: ```
67: 
68: ## Created Issues
69: 
70: - BLA-7: PRP-1 Level 4 Pattern Conformance (25h)
71: - BLA-8: PRP-2 State Management (17.5h)
72: - BLA-9: PRP-3 Command Automation (15h)
73: - BLA-10: PRP-4 Execute-PRP Orchestration (18h, URGENT)
74: - BLA-11: PRP-5 Context Sync Integration (12h)
75: 
76: All issues created in "Context Engineering" project with proper metadata.
</file>

<file path=".serena/memories/linear-issue-tracking-integration.md">
  1: ---
  2: type: regular
  3: category: documentation
  4: tags: [linear, tracking, workflow]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # Linear Issue Tracking Integration Pattern
 10: 
 11: ## Overview
 12: 
 13: Pattern for integrating Linear issue tracking with PRP (Product Requirements Proposal) YAML headers, enabling bi-directional sync between PRPs and Linear project management.
 14: 
 15: ## Implementation Pattern
 16: 
 17: ### 1. PRP Discovery and Analysis
 18: 
 19: Use Python to parse PRP YAML headers and identify which PRPs lack Linear issue tracking:
 20: 
 21: ```python
 22: import re
 23: from pathlib import Path
 24: 
 25: def extract_yaml_header(file_path):
 26:     """Extract YAML header from markdown file."""
 27:     with open(file_path, 'r') as f:
 28:         content = f.read()
 29:     match = re.search(r'^---\n(.*?)\n---', content, re.DOTALL | re.MULTILINE)
 30:     return match.group(1) if match else None
 31: 
 32: def has_linear_tracking(yaml_content):
 33:     """Check if PRP has Linear issue tracking."""
 34:     if not yaml_content:
 35:         return False
 36:     # Check for issue, task_id, or linear_issue fields
 37:     return bool(re.search(r'(issue|task_id|linear_issue):\s*\S+', yaml_content))
 38: ```
 39: 
 40: ### 2. Linear Issue Creation
 41: 
 42: Use Linear MCP server to create issues with PRP metadata:
 43: 
 44: ```python
 45: # Create Linear issue
 46: issue_result = mcp__linear-server__create_issue(
 47:     team="team-key",  # e.g., "Blaise78"
 48:     title=prp_title,
 49:     description=prp_description,
 50:     # Optional: labels, priority, assignee
 51: )
 52: 
 53: # Extract issue details
 54: issue_id = issue_result["id"]  # UUID for updates
 55: issue_number = issue_result["number"]  # Display number (e.g., 17)
 56: issue_key = issue_result["identifier"]  # Full key (e.g., "BLA-17")
 57: ```
 58: 
 59: ### 3. YAML Header Update
 60: 
 61: Update PRP files with issue tracking information:
 62: 
 63: ```python
 64: # Add issue field to YAML header
 65: Edit(
 66:     file_path=prp_path,
 67:     old_string="prp_id: PRP-X\nfeature_name: ...",
 68:     new_string="prp_id: PRP-X\nfeature_name: ...\nissue: BLA-17"
 69: )
 70: ```
 71: 
 72: ### 4. Issue Assignment and Project Management
 73: 
 74: ```python
 75: # Assign to user
 76: mcp__linear-server__update_issue(
 77:     id=issue_uuid,  # IMPORTANT: Use UUID, not issue_number
 78:     assignee="user@example.com"
 79: )
 80: 
 81: # Add to project
 82: mcp__linear-server__update_issue(
 83:     id=issue_uuid,
 84:     project="project-uuid"  # Get from list_projects
 85: )
 86: ```
 87: 
 88: ### 5. Bulk Updates
 89: 
 90: For updating multiple issues, use parallel tool calls:
 91: 
 92: ```python
 93: # Get project ID first
 94: projects = mcp__linear-server__list_projects()
 95: project_id = next(p["id"] for p in projects if p["name"] == "Context Engineering")
 96: 
 97: # Batch update all issues in parallel
 98: for issue in issues:
 99:     mcp__linear-server__update_issue(
100:         id=issue["id"],
101:         assignee="user@example.com",
102:         project=project_id
103:     )
104: ```
105: 
106: ## Key Learnings
107: 
108: ### Linear API Behavior
109: 
110: 1. **UUID vs Issue Number**: The `update_issue` tool requires the UUID `id` parameter, NOT `issue_number`. Always use the UUID from `create_issue` response.
111: 
112: 2. **Project and User Assignment**: Both can be done in same update call or separately.
113: 
114: 3. **Issue Identifiers**: 
115:    - `id`: UUID for API calls (e.g., "fd201036-0eb4-4822-a9d8-c03cd56ab9e6")
116:    - `number`: Display number (e.g., 17)
117:    - `identifier`: Team key + number (e.g., "BLA-17")
118: 
119: ### PRP YAML Structure
120: 
121: Standard PRP YAML header with Linear tracking:
122: 
123: ```yaml
124: ---
125: prp_id: PRP-13
126: feature_name: Production Hardening & Comprehensive Documentation
127: status: partial
128: issue: BLA-23
129: created: 2025-10-13
130: updated: 2025-10-14T15:30:00Z
131: context_sync:
132:   ce_updated: true
133:   serena_updated: true
134:   last_sync: 2025-10-14T15:30:00Z
135: updated_by: context-sync
136: ---
137: ```
138: 
139: ## Error Handling
140: 
141: ### Common Errors
142: 
143: 1. **Invalid Arguments - Missing 'id' field**:
144:    ```
145:    MCP error -32602: Invalid arguments for tool update_issue
146:    ```
147:    **Fix**: Use UUID `id` parameter instead of `issue_number`
148: 
149: 2. **Team Not Found**:
150:    Ensure team key matches exactly (case-sensitive)
151: 
152: 3. **Project Not Found**:
153:    Use `list_projects` to get exact project UUID
154: 
155: ## Workflow Example
156: 
157: Complete workflow for creating Linear issues for untracked PRPs:
158: 
159: ```python
160: # 1. Discover PRPs without issues
161: prp_files = Path("PRPs").rglob("*.md")
162: untracked_prps = []
163: 
164: for prp_file in prp_files:
165:     yaml_header = extract_yaml_header(prp_file)
166:     if not has_linear_tracking(yaml_header):
167:         untracked_prps.append(prp_file)
168: 
169: # 2. Extract titles using bash/awk
170: for prp_file in untracked_prps:
171:     title = extract_h1_title(prp_file)
172:     # awk '/^---$/{p++; next} p==2 && /^#[^#]/{print; exit}' file
173: 
174: # 3. Create Linear issues
175: created_issues = []
176: for prp_file, title in zip(untracked_prps, titles):
177:     issue = mcp__linear-server__create_issue(
178:         team="Blaise78",
179:         title=title
180:     )
181:     created_issues.append((prp_file, issue))
182: 
183: # 4. Update PRP YAML headers
184: for prp_file, issue in created_issues:
185:     Edit(prp_file, add_issue_field(issue["identifier"]))
186: 
187: # 5. Assign and organize
188: project_id = get_project_id("Context Engineering")
189: for _, issue in created_issues:
190:     mcp__linear-server__update_issue(
191:         id=issue["id"],  # UUID
192:         assignee="user@example.com",
193:         project=project_id
194:     )
195: ```
196: 
197: ## Context Sync Integration
198: 
199: After updating PRPs with Linear tracking, update context_sync flags:
200: 
201: ```yaml
202: context_sync:
203:   ce_updated: true
204:   serena_updated: true
205:   last_sync: 2025-10-14T15:30:00Z
206: updated_by: context-sync
207: ```
208: 
209: This ensures drift tracking recognizes the changes as intentional.
210: 
211: ## Related Documentation
212: 
213: - Linear MCP Server: Uses `mcp__linear-server__*` tools
214: - PRP Model: See PRPs/Model.md for YAML header specification
215: - Context Sync: See /update-context command for drift management
216: 
217: ---
218: 
219: Last updated: 2025-10-14
220: Pattern validated: 7 PRPs successfully integrated with Linear tracking
</file>

<file path=".serena/memories/linear-mcp-integration-example.md">
  1: ---
  2: type: regular
  3: category: documentation
  4: tags: [linear, mcp, integration]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # Linear MCP Integration - Pattern Example
 10: 
 11: ## Quick Reference
 12: 
 13: **Config**: `.ce/linear-defaults.yml` - Project defaults (team, assignee, labels)
 14: **Utilities**: `tools/ce/linear_utils.py` - Helper functions
 15: **MCP Tools**: `mcp__linear__*` - Direct Linear API access
 16: 
 17: ## Core Patterns
 18: 
 19: ### 1. Configuration-Driven
 20: 
 21: ```yaml
 22: # .ce/linear-defaults.yml
 23: project: "Context Engineering"
 24: assignee: "blazej.przybyszewski@gmail.com"
 25: team: "Blaise78"
 26: default_labels: ["feature"]
 27: ```
 28: 
 29: ### 2. Python Utilities (Recommended)
 30: 
 31: ```python
 32: from ce.linear_utils import create_issue_with_defaults
 33: 
 34: # Auto-applies config defaults
 35: issue_data = create_issue_with_defaults(
 36:     title="PRP-15: Feature",
 37:     description="...",
 38:     state="todo"
 39: )
 40: issue = mcp__linear__create_issue(**issue_data)
 41: ```
 42: 
 43: ### 3. Direct MCP (Full Control)
 44: 
 45: ```python
 46: # Override defaults when needed
 47: issue = mcp__linear__create_issue(
 48:     team="Blaise78",
 49:     title="...",
 50:     priority=1,  # Urgent
 51:     labels=["bug", "security"],
 52:     project="...",
 53:     assignee="...",
 54:     state="in_progress"
 55: )
 56: ```
 57: 
 58: ## Troubleshooting Flow
 59: 
 60: **L1**: `/mcp` (check status)
 61: **L2**: `/mcp restart linear-server`
 62: **L3**: `rm -rf ~/.mcp-auth && mcp-remote https://mcp.linear.app/sse`
 63:   - **Expected**: HTTP 404 error during connection (normal!)
 64:   - `mcp-remote` tries HTTP first, fails with 404, falls back to SSE
 65:   - Final message: "Proxy established successfully" = ‚úÖ working
 66: **L4**: Reinstall MCP tools
 67: 
 68: ## Anti-Patterns
 69: 
 70: ‚ùå Hardcoded team/project/assignee values
 71: ‚ùå Silent exception handling
 72: ‚ùå Manual YAML editing without parsing
 73: 
 74: ## Best Practices
 75: 
 76: ‚úÖ Use `create_issue_with_defaults()` helper
 77: ‚úÖ Explicit error handling with troubleshooting steps
 78: ‚úÖ Proper YAML parsing for header updates
 79: ‚úÖ Test MCP connection before batch operations
 80: 
 81: ## PRP Integration
 82: 
 83: **Auto-creation**: `/generate-prp` creates Linear issues automatically
 84: **Joining**: `--join-prp 12` appends to existing issue
 85: **Tracking**: YAML header `issue: "BLA-18"` field
 86: 
 87: ## Available Tools
 88: 
 89: **Issues**: create, list, get, update
 90: **Comments**: create, list
 91: **Projects/Teams/Labels**: list, get, create
 92: **Documentation**: search_documentation
 93: 
 94: 20+ MCP tools available with `mcp__linear__` prefix.
 95: 
 96: ## MCP Connection Notes
 97: 
 98: **Transport Strategy**: Linear MCP uses SSE (Server-Sent Events)
 99: **HTTP 404 on connect**: Normal - `mcp-remote` tries HTTP first, then falls back to SSE
100: **OAuth Flow**: Browser-based authentication, stored in `~/.mcp-auth`
101: **Connection Stability**: Early tech - may need multiple attempts
102: 
103: ## File Locations
104: 
105: - Config: `.ce/linear-defaults.yml`
106: - Utilities: `tools/ce/linear_utils.py`
107: - Example: `examples/linear-integration-example.md`
108: - Docs: CLAUDE.md (Linear Integration section)
</file>

<file path=".serena/memories/linear-mcp-integration.md">
  1: ---
  2: type: regular
  3: category: documentation
  4: tags: [linear, mcp, setup]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # Linear MCP Integration Guide
 10: 
 11: ## Overview
 12: Claude Code integrates with Linear via MCP (Model Context Protocol) for issue tracking automation.
 13: 
 14: ## Setup (Already Configured)
 15: Linear MCP is centrally hosted and uses OAuth 2.1 authentication.
 16: 
 17: **Configuration**:
 18: ```json
 19: {
 20:   "mcpServers": {
 21:     "linear": {
 22:       "command": "npx",
 23:       "args": ["-y", "mcp-remote", "https://mcp.linear.app/sse"]
 24:     }
 25:   }
 26: }
 27: ```
 28: 
 29: **Authentication**: Run `/mcp` in Claude Code session to authenticate via OAuth flow.
 30: 
 31: ## Available Tools (Prefix: mcp__linear__)
 32: 
 33: ### Issues
 34: - `list_issues` - List/filter issues (assignee, status, project, team, labels, etc.)
 35: - `get_issue` - Get detailed issue info by ID
 36: - `create_issue` - Create new issue (title, description, team, assignee, labels, etc.)
 37: - `update_issue` - Update existing issue fields
 38: 
 39: ### Projects
 40: - `list_projects` - List/filter projects (team, member, state, etc.)
 41: - `get_project` - Get project details
 42: - `create_project` - Create new project
 43: - `update_project` - Update project fields
 44: - `list_project_labels` - List project labels
 45: 
 46: ### Teams
 47: - `list_teams` - List workspace teams
 48: - `get_team` - Get team details by ID/key/name
 49: 
 50: ### Comments
 51: - `list_comments` - List issue comments
 52: - `create_comment` - Add comment to issue
 53: 
 54: ### Cycles
 55: - `list_cycles` - Get team cycles (current/previous/next)
 56: 
 57: ### Documents
 58: - `list_documents` - List workspace documents
 59: - `get_document` - Get document by ID/slug
 60: 
 61: ### Issue Management
 62: - `list_issue_statuses` - List available issue statuses for team
 63: - `get_issue_status` - Get status details by name/ID
 64: - `list_issue_labels` - List workspace/team labels
 65: - `create_issue_label` - Create new label
 66: 
 67: ### Users
 68: - `list_users` - List workspace users
 69: - `get_user` - Get user details (supports "me" for current user)
 70: 
 71: ### Documentation
 72: - `search_documentation` - Search Linear docs for features/usage
 73: 
 74: ## Common Patterns
 75: 
 76: ### Create Issue
 77: ```python
 78: mcp__linear__create_issue(
 79:     title="Feature title",
 80:     description="Markdown description",
 81:     team="team-key-or-id",
 82:     assignee="me",  # or user ID/name/email
 83:     priority=2,  # 0=None, 1=Urgent, 2=High, 3=Normal, 4=Low
 84:     labels=["label-name-or-id"],
 85:     project="project-name-or-id"
 86: )
 87: ```
 88: 
 89: ### List My Issues
 90: ```python
 91: mcp__linear__list_issues(
 92:     assignee="me",
 93:     state="In Progress",
 94:     orderBy="updatedAt",
 95:     limit=50
 96: )
 97: ```
 98: 
 99: ### Update Issue
100: ```python
101: mcp__linear__update_issue(
102:     id="issue-id",
103:     state="Done",
104:     description="Updated description"
105: )
106: ```
107: 
108: ## Troubleshooting
109: - **"Not connected" error**: Run `/mcp` to authenticate
110: - **Connection failures**: Try restarting Claude Code or disable/re-enable Linear MCP
111: - **Early tech**: Remote MCP connections may require multiple attempts
112: 
113: ## Permission Configuration
114: All Linear tools require explicit permission in Claude Code settings:
115: - Pattern: `mcp__linear__*` for all tools
116: - Or individually: `mcp__linear__create_issue`, `mcp__linear__list_issues`, etc.
117: 
118: ## References
119: - Linear MCP Docs: https://linear.app/docs/mcp
120: - MCP Integration: https://docs.claude.com/en/docs/claude-code/mcp
121: - OAuth Flow: Server-Sent Events (SSE) transport with OAuth 2.1
</file>

<file path=".serena/memories/project-overview.md">
  1: ---
  2: type: regular
  3: category: documentation
  4: tags: [overview, project, context]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # Project Overview
 10: 
 11: ## Purpose
 12: Context Engineering Plus (ctx-eng-plus) - Minimal CLI tooling for Context Engineering framework operations with full PRP workflow automation.
 13: 
 14: ## Project Type
 15: Python CLI toolkit with minimal dependencies (stdlib-focused).
 16: 
 17: ## Key Features
 18: 
 19: ### Core Capabilities
 20: - **ce tool**: Command-line interface for context engineering operations
 21:   - **Validation** (3-level gate system: L1-L4)
 22:   - **Git operations** (status, diff, checkpoints)
 23:   - **Context management** (sync, health, prune, auto-sync mode)
 24:   - **File operations** (read, write)
 25:   - **Python execution** (auto-detect code vs file mode)
 26: 
 27: ### PRP Workflow Automation (NEW - PRP-3, PRP-4, PRP-5)
 28: - **PRP Generation** (`ce prp generate`):
 29:   - Parses INITIAL.md structure
 30:   - Orchestrates Serena MCP for codebase research
 31:   - Fetches documentation via Context7 MCP
 32:   - Synthesizes complete 6-section PRPs (TL;DR, Context, Implementation, Success Criteria, Validation Gates, References)
 33:   - Auto-generates validation commands and checkpoints
 34:   - 3-4x speedup vs manual creation (10-15 min vs 30-60 min)
 35: 
 36: - **PRP Execution** (`ce prp execute`):
 37:   - Parses PRP Implementation Blueprint into executable phases
 38:   - Phase-by-phase orchestrated execution
 39:   - L1-L4 validation loops after each phase
 40:   - Self-healing mechanisms (L1-L2 auto-fix with 3-attempt limit)
 41:   - 5 escalation triggers (persistent errors, ambiguity, architecture, dependencies, security)
 42:   - Auto-checkpoint creation at validation gates
 43:   - 10/10 confidence scoring system
 44:   - 3-6x speedup for simple PRPs (20-60 min vs 60-180 min)
 45: 
 46: - **Context Sync Integration** (`ce context`):
 47:   - **Step 2.5 (Pre-Generation Sync)**: Auto-sync before PRP generation with drift abort >30%
 48:   - **Step 6.5 (Post-Execution Sync)**: Auto-cleanup + sync after PRP execution
 49:   - Auto-sync mode: `ce context auto-sync --enable`
 50:   - Drift detection with 3 thresholds (0-10% healthy, 10-30% warn, 30%+ abort)
 51:   - Memory pruning (stale Serena memories cleanup)
 52:   - Git state verification (clean working tree enforcement)
 53: 
 54: ### Claude Code Integration
 55: - Slash commands: `/generate-prp`, `/execute-prp`
 56: - Session hooks in `.claude/settings.local.json` (SessionStart health check)
 57: - Workflow integration at Steps 2.5 and 6.5
 58: 
 59: ## Tech Stack
 60: - **Language**: Python 3.10+ (tested on 3.13.7)
 61: - **Package Manager**: UV 0.7.19+ (STRICT - never edit pyproject.toml directly)
 62: - **Testing**: pytest 8.4.2+
 63: - **Build System**: Hatchling
 64: - **Dependencies**: Minimal - stdlib only for core functionality
 65: - **MCP Integration**: Serena (code operations), Context7 (docs), Sequential Thinking (synthesis)
 66: 
 67: ## Project Structure
 68: ```
 69: ctx-eng-plus/
 70: ‚îú‚îÄ‚îÄ tools/                  # Main CLI package
 71: ‚îÇ   ‚îú‚îÄ‚îÄ ce/                 # Source code (11 modules)
 72: ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py     # Package metadata
 73: ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __main__.py     # CLI entry point (8 commands)
 74: ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ core.py         # File, git, shell operations
 75: ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ validate.py     # L1-L4 validation gates
 76: ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ context.py      # Context management & drift detection
 77: ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ generate.py     # PRP generation (PRP-3)
 78: ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ execute.py      # PRP execution orchestration (PRP-4)
 79: ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prp.py          # PRP state management (PRP-2)
 80: ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ drift_analyzer.py   # Implementation drift analysis
 81: ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ code_analyzer.py    # Code pattern extraction
 82: ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mermaid_validator.py # Mermaid validation
 83: ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pattern_extractor.py # Pattern extraction
 84: ‚îÇ   ‚îú‚îÄ‚îÄ tests/              # Test suite (8 test modules)
 85: ‚îÇ   ‚îú‚îÄ‚îÄ pyproject.toml      # UV package config
 86: ‚îÇ   ‚îî‚îÄ‚îÄ bootstrap.sh        # Setup script
 87: ‚îú‚îÄ‚îÄ PRPs/                   # PRP documents
 88: ‚îÇ   ‚îú‚îÄ‚îÄ executed/           # 6 completed PRPs (PRP-1 through PRP-5)
 89: ‚îÇ   ‚îú‚îÄ‚îÄ feature-requests/   # 4 future PRPs (PRP-6 through PRP-9)
 90: ‚îÇ   ‚îî‚îÄ‚îÄ templates/          # PRP templates (base, KISS, self-healing)
 91: ‚îú‚îÄ‚îÄ docs/                   # Documentation & research (11 docs)
 92: ‚îú‚îÄ‚îÄ .claude/                # Claude Code config (settings, commands, hooks)
 93: ‚îú‚îÄ‚îÄ CLAUDE.md               # Project-specific guidance
 94: ‚îî‚îÄ‚îÄ README.md               # Main documentation
 95: ```
 96: 
 97: ## CLI Commands Reference
 98: 
 99: ### Validation
100: ```bash
101: ce validate --level 1          # L1: Syntax (py_compile, mypy, ruff)
102: ce validate --level 2          # L2: Unit tests
103: ce validate --level 3          # L3: Integration tests
104: ce validate --level 4          # L4: Pattern conformance (drift analysis)
105: ce validate --level all        # All levels
106: ```
107: 
108: ### Git Operations
109: ```bash
110: ce git status                  # Git status with staging info
111: ce git diff                    # Recent file changes
112: ce git checkpoint "message"    # Create checkpoint (tag + commit)
113: ```
114: 
115: ### Context Management
116: ```bash
117: ce context sync                # Sync context with codebase
118: ce context health              # Context drift health check
119: ce context health --verbose    # Detailed drift breakdown
120: ce context prune               # Prune stale memories (7 days default)
121: ce context auto-sync --enable  # Enable auto-sync mode
122: ce context auto-sync --disable # Disable auto-sync mode
123: ce context auto-sync --status  # Check auto-sync status
124: ce context pre-sync [--force]  # Manual pre-generation sync
125: ce context post-sync <prp-id>  # Manual post-execution sync
126: ```
127: 
128: ### Python Execution
129: ```bash
130: ce run_py "print('hello')"                 # Auto-detect: inline code
131: ce run_py tmp/script.py                    # Auto-detect: file path
132: ce run_py --code "x = [1,2,3]; print(sum(x))"  # Explicit: code
133: ce run_py --file tmp/script.py             # Explicit: file
134: ```
135: 
136: ### PRP Management
137: ```bash
138: ce prp validate <prp-file>         # Validate PRP YAML header
139: ce prp generate <initial-md-path>  # Generate PRP from INITIAL.md
140: ce prp execute <prp-id>            # Execute PRP with validation loops
141: ce prp execute <prp-id> --start-phase 2 --end-phase 3  # Partial execution
142: ce prp execute <prp-id> --dry-run  # Parse blueprint only (no execution)
143: ```
144: 
145: ## Implementation Status (2025-10-13)
146: 
147: ### Completed Features (PRPs 1-5)
148: - ‚úÖ L1-L4 validation gates (PRP-1)
149: - ‚úÖ YAML validation command (PRP-1.2)
150: - ‚úÖ PRP state management & cleanup protocol (PRP-2)
151: - ‚úÖ PRP generation automation with MCP orchestration (PRP-3)
152: - ‚úÖ PRP execution with self-healing & validation loops (PRP-4)
153: - ‚úÖ Context sync integration at workflow Steps 2.5 & 6.5 (PRP-5)
154: - ‚úÖ Auto-sync mode for seamless workflow automation
155: - ‚úÖ Claude Code hooks integration
156: 
157: ### Known Limitations
158: - Test coverage: 40% (target: 80%) - PRP-7 will address
159: - File operations: Filesystem stubs (Serena MCP integration pending) - PRP-9 will address
160: - Self-healing: L1-L2 only (L3-L4 escalate to human)
161: 
162: ### Feature Roadmap (PRPs 6-9)
163: - ‚è∏Ô∏è PRP-6: Markdown linting for documentation quality
164: - ‚è∏Ô∏è PRP-7: Comprehensive validation loop tests (increase coverage to 80%)
165: - ‚è∏Ô∏è PRP-8: PRP sizing constraint analysis & optimal breakdown strategy
166: - ‚è∏Ô∏è PRP-9: Serena MCP integration for file operations (replace stubs)
167: 
168: ## Workflow Integration
169: 
170: ### Context-Aware Development Cycle
171: 1. **Step 1**: Define feature in INITIAL.md (manual)
172: 2. **Step 2**: `/generate-prp` creates comprehensive PRP
173: 3. **Step 2.5**: **Auto-sync** (pre-generation health check, drift abort >30%)
174: 4. **Step 3**: Peer review PRP document
175: 5. **Step 4-5**: `/execute-prp` implements feature with validation
176: 6. **Step 6.5**: **Auto-sync** (post-execution cleanup + sync)
177: 7. **Step 7**: Final validation & PR creation
178: 
179: ### Auto-Sync Mode Benefits
180: - Eliminates stale context errors (15-40% error rate reduction)
181: - Ensures <10% drift for all PRP operations
182: - Automates 4-6 manual steps per PRP (2-5 min saved)
183: - Prevents context pollution through systematic cleanup
184: - Enables reliable autonomous development workflow
185: 
186: ## Platform
187: Darwin (macOS) - system-specific commands may differ from Linux.
188: 
189: ## Design Philosophy
190: - **KISS**: Simple solutions first, avoid over-engineering
191: - **No Fishy Fallbacks**: Fast failure with actionable error messages
192: - **Real Functionality Testing**: Test real functions with real values
193: - **UV Package Management**: Strict - never edit pyproject.toml directly
194: - **Minimal Dependencies**: Stdlib-only for core, pytest for dev only
195: - **Single Responsibility**: Each module handles one concern
196: - **Token Efficiency**: Direct editing over Read ‚Üí Edit cycles
</file>

<file path=".serena/memories/PRP-15-remediation-workflow-implementation.md">
  1: ---
  2: type: regular
  3: category: pattern
  4: tags: [prp, remediation, workflow]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # PRP-15: Integrated PRP Execution - Implementation Complete
 10: 
 11: ## Implementation Status: ‚úÖ COMPLETED
 12: 
 13: ### What Was Changed
 14: 
 15: Modified `remediate_drift_workflow()` in `tools/ce/update_context.py` to integrate full PRP execution cycle (lines 526-772).
 16: 
 17: ### Key Changes
 18: 
 19: #### Before (Old Workflow)
 20: ```
 21: Detect Drift ‚Üí Generate Blueprint ‚Üí Ask Approval ‚Üí Generate PRP ‚Üí Done (manual execution)
 22: ```
 23: 
 24: #### After (New Workflow - Both Modes)
 25: ```
 26: Detect Drift ‚Üí Generate Blueprint ‚Üí [Approval] ‚Üí Generate PRP ‚Üí [Approval] ‚Üí Execute PRP ‚Üí Sync Context ‚Üí "‚úÖ Context Updated"
 27: ```
 28: 
 29: ### Execution Flow
 30: 
 31: #### Vanilla Mode (yolo_mode=False)
 32: 1. **Step 4**: "Proceed with remediation? (yes/no):" - Interactive approval
 33: 2. **Step 5**: Generate PRP (if approved)
 34: 3. **Step 6**: "Execute remediation PRP now? (yes/no):" - Second approval gate
 35: 4. **Step 7-8**: Execute PRP + Sync context (if approved)
 36: 5. **Step 9**: Display "‚úÖ Context updated successfully"
 37: 
 38: #### Remediate Mode (yolo_mode=True, --remediate flag)
 39: 1. **Step 4**: SKIPPED - No approval prompt
 40: 2. **Step 5**: Auto-generate PRP
 41: 3. **Step 6**: SKIPPED - No execution approval
 42: 4. **Step 7-8**: Auto-execute PRP + Sync context
 43: 5. **Step 9**: Display "‚úÖ Context updated successfully"
 44: 
 45: ### New Steps Added
 46: 
 47: **Step 6: Execution Approval Gate (Vanilla Only)**
 48: ```python
 49: if not yolo_mode:
 50:     print(f"\n{'='*60}")
 51:     print("üîß Generated PRP: Execute remediation?")
 52:     execution_approval = input("Execute remediation PRP now? (yes/no): ").strip().lower()
 53:     if execution_approval not in ["yes", "y"]:
 54:         # Return early - PRP generated but not executed
 55:         return {...}
 56: ```
 57: 
 58: **Step 7: PRP Execution**
 59: ```python
 60: # Extract PRP ID from filename (DEDRIFT_PRP-{timestamp}.md)
 61: prp_id = prp_path.stem  # e.g., "DEDRIFT_PRP-20251017-130000"
 62: from .execute import execute_prp
 63: execution_result = execute_prp(prp_id)
 64: # Returns success/failure with metrics: phases_completed, execution_time, confidence_score
 65: ```
 66: 
 67: **Step 8: Re-sync Context**
 68: ```python
 69: sync_result = sync_context()
 70: # Re-runs sync to capture changes from executed PRP
 71: # Updates PRP metadata with execution results
 72: ```
 73: 
 74: **Step 9: Final Completion Message**
 75: ```python
 76: print("=" * 60)
 77: print("‚úÖ Context updated successfully")
 78: print("=" * 60)
 79: print(f"Remediation PRP: {prp_path}")
 80: print(f"Blueprint: {blueprint_path}")
 81: ```
 82: 
 83: ### Return Value Evolution
 84: 
 85: **Old Return**:
 86: ```python
 87: {
 88:     "success": True,
 89:     "prp_path": prp_path,
 90:     "blueprint_path": blueprint_path,
 91:     "errors": []
 92: }
 93: ```
 94: 
 95: **New Return**:
 96: ```python
 97: {
 98:     "success": True,
 99:     "prp_path": prp_path,
100:     "blueprint_path": blueprint_path,
101:     "execution_result": execution_result or None,  # NEW FIELD
102:     "errors": errors
103: }
104: ```
105: 
106: ### Error Handling
107: 
108: **Partial Success**: If PRP generation succeeds but execution fails:
109: - Return `"success": False` with errors list
110: - Allows recovery (user can manually execute PRP)
111: - Non-blocking for context sync (doesn't fail entirely)
112: 
113: **Execution Failure**: If execution fails after approval:
114: - Logs error with context
115: - Captures execution_result = None
116: - Attempts context sync anyway (non-critical)
117: - Returns errors list with troubleshooting guidance
118: 
119: ### Non-Interactive Mode Handling
120: 
121: Already implemented in Step 4 (from previous fixes):
122: - Detects TTY using `is_interactive()` 
123: - Gracefully skips remediation if no TTY in vanilla mode
124: - Suggests `--remediate` flag for automation
125: 
126: ## Testing Plan
127: 
128: ### Test Case 1: Vanilla Mode with User "Yes" to Both
129: ```bash
130: cd tools
131: # Simulate: User types "yes" twice (remediation + execution)
132: echo -e "yes\nyes" | uv run ce update-context
133: ```
134: **Expected Output**:
135: - "Proceed with remediation? (yes/no):" ‚Üí accepts "yes"
136: - Generates DEDRIFT_PRP-*.md
137: - "Execute remediation PRP now? (yes/no):" ‚Üí accepts "yes"
138: - Executes PRP (phases_completed, execution_time, confidence_score)
139: - Re-syncs context
140: - "‚úÖ Context updated successfully"
141: 
142: ### Test Case 2: Vanilla Mode with "No" to Execution
143: ```bash
144: cd tools
145: # Simulate: User types "yes" then "no"
146: echo -e "yes\nno" | uv run ce update-context
147: ```
148: **Expected Output**:
149: - "Proceed with remediation? (yes/no):" ‚Üí accepts "yes"
150: - Generates PRP
151: - "Execute remediation PRP now? (yes/no):" ‚Üí rejects "no"
152: - "‚ö†Ô∏è Execution skipped by user"
153: - "üí° Run manually: /execute-prp {prp_path}"
154: - Returns with success=True but execution_result=None
155: 
156: ### Test Case 3: Remediate Mode (Auto-Execute)
157: ```bash
158: cd tools
159: uv run ce update-context --remediate
160: ```
161: **Expected Output**:
162: - NO approval prompts
163: - Auto-generates DEDRIFT_PRP-*.md
164: - Auto-executes (no approval)
165: - Re-syncs context
166: - "‚úÖ Context updated successfully"
167: 
168: ### Test Case 4: No Drift Detected
169: ```bash
170: cd tools
171: uv run ce update-context
172: ```
173: **Expected Output**:
174: - "‚úÖ No drift detected (score: X.X%)"
175: - "Context is healthy - no remediation needed."
176: - Returns success=True, prp_path=None
177: 
178: ### Test Case 5: Non-Interactive Mode (Vanilla)
179: ```bash
180: cd tools
181: # Pipe empty input (no TTY)
182: echo "" | uv run ce update-context
183: ```
184: **Expected Output**:
185: - "‚è≠Ô∏è Non-interactive mode detected (no TTY)"
186: - "üìÑ Blueprint saved: {path}"
187: - "üí° For automated remediation, use: ce update-context --remediate"
188: - Returns success=True, prp_path=None (graceful exit, no crash)
189: 
190: ## Implementation Verification Checklist
191: 
192: - ‚úÖ Vanilla mode asks for approval twice (remediation + execution)
193: - ‚úÖ Remediate mode skips both approval gates
194: - ‚úÖ Both modes end with "‚úÖ Context updated successfully"
195: - ‚úÖ Both modes re-sync context after execution
196: - ‚úÖ Execution result captured and included in return value
197: - ‚úÖ Error handling for execution failures (partial success)
198: - ‚úÖ Non-interactive mode gracefully skips remediation
199: - ‚úÖ Return signature updated with execution_result field
200: - ‚úÖ Proper logging at each step for troubleshooting
201: 
202: ## Tool Restrictions Encountered
203: 
204: - **Tool Denied**: `mcp__serena__replace_symbol_body` (permission denied)
205: - **Workaround Used**: `mcp__serena__replace_regex()` with multiline pattern
206: - **Memory Created**: `serena-mcp-tool-restrictions` for future reference
207: 
208: ## Next Steps
209: 
210: 1. Test all 5 test cases to verify both modes work
211: 2. Verify error handling in edge cases
212: 3. Run full context update cycle end-to-end
213: 4. Create initial test setup if drift detected in codebase
</file>

<file path=".serena/memories/prp-2-implementation-patterns.md">
  1: ---
  2: type: regular
  3: category: pattern
  4: tags: [prp, implementation, workflow]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # PRP-2 Implementation Patterns
 10: 
 11: **Source**: PRP-2 execution (2025-10-12)  
 12: **Purpose**: Reference patterns for PRP state management implementation
 13: 
 14: ---
 15: 
 16: ## State File Structure
 17: 
 18: **Location**: `.ce/active_prp_session`
 19: 
 20: **Format** (JSON):
 21: ```json
 22: {
 23:   "prp_id": "PRP-2",
 24:   "prp_name": "PRP State Management & Isolation",
 25:   "started_at": "2025-10-12T14:30:15Z",
 26:   "phase": "implementation",
 27:   "checkpoint_count": 3,
 28:   "last_checkpoint": "checkpoint-PRP-2-implementation-20251012-143215",
 29:   "validation_attempts": {
 30:     "gate_1": 1,
 31:     "gate_2": 2
 32:   },
 33:   "serena_memories": [
 34:     "PRP-2-checkpoint-phase1",
 35:     "PRP-2-learnings-atomic-writes"
 36:   ]
 37: }
 38: ```
 39: 
 40: ---
 41: 
 42: ## Atomic Write Pattern
 43: 
 44: **Problem**: Race conditions and partial writes to state file  
 45: **Solution**: Temp file + rename (atomic at filesystem level)
 46: 
 47: ```python
 48: from pathlib import Path
 49: import json
 50: from typing import Dict, Any
 51: 
 52: STATE_DIR = Path(".ce")
 53: STATE_FILE = STATE_DIR / "active_prp_session"
 54: 
 55: def _write_state(state: Dict[str, Any]) -> None:
 56:     """Atomic write using temp file + rename pattern.
 57:     
 58:     Ensures state file integrity even if process crashes mid-write.
 59:     """
 60:     STATE_DIR.mkdir(exist_ok=True)
 61:     temp_file = STATE_FILE.with_suffix(".tmp")
 62:     temp_file.write_text(json.dumps(state, indent=2))
 63:     temp_file.replace(STATE_FILE)  # Atomic on POSIX
 64: ```
 65: 
 66: **Key Benefits**:
 67: - Atomic operation (no partial writes)
 68: - Process crash-safe
 69: - No lock files needed
 70: - POSIX filesystem guarantee
 71: 
 72: ---
 73: 
 74: ## Checkpoint Naming Convention
 75: 
 76: **Pattern**: `checkpoint-{prp_id}-{phase}-{timestamp}`
 77: 
 78: **Components**:
 79: - `checkpoint-` - Fixed prefix for git tag filtering
 80: - `{prp_id}` - PRP identifier (e.g., "PRP-2")
 81: - `{phase}` - Phase name (planning, implementation, testing, validation, complete)
 82: - `{timestamp}` - Format: `YYYYMMDD-HHMMSS` (e.g., "20251012-143215")
 83: 
 84: **Examples**:
 85: ```
 86: checkpoint-PRP-2-planning-20251012-100530
 87: checkpoint-PRP-2-implementation-20251012-143215
 88: checkpoint-PRP-3-testing-20251012-163045
 89: checkpoint-PRP-2-final-20251012-182130
 90: ```
 91: 
 92: **Implementation**:
 93: ```python
 94: from datetime import datetime
 95: 
 96: def create_checkpoint(phase: str, message: Optional[str] = None) -> Dict[str, Any]:
 97:     state = get_active_prp()
 98:     if not state:
 99:         raise RuntimeError("No active PRP session")
100:     
101:     prp_id = state["prp_id"]
102:     timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
103:     tag_name = f"checkpoint-{prp_id}-{phase}-{timestamp}"
104:     
105:     msg = message or f"Checkpoint: {prp_id} {phase} phase"
106:     result = run_cmd(f'git tag -a "{tag_name}" -m "{msg}"')
107:     
108:     if not result["success"]:
109:         raise RuntimeError(
110:             f"Failed to create checkpoint: {result['stderr']}\n"
111:             f"üîß Troubleshooting: Ensure working tree is clean"
112:         )
113:     
114:     # Update state with checkpoint info
115:     state["checkpoint_count"] = state.get("checkpoint_count", 0) + 1
116:     state["last_checkpoint"] = tag_name
117:     _write_state(state)
118:     
119:     return {"tag_name": tag_name, "message": msg}
120: ```
121: 
122: ---
123: 
124: ## Memory Namespacing
125: 
126: **Pattern**: `{prp_id}-{category}-{name}`
127: 
128: **Categories**:
129: - `checkpoint` - Phase checkpoint memories (ephemeral)
130: - `learnings` - Persistent learnings to archive
131: - `temp` - Temporary scratch data (ephemeral)
132: - `validation` - Validation results (ephemeral)
133: 
134: **Examples**:
135: ```
136: PRP-2-checkpoint-phase1
137: PRP-2-learnings-atomic-writes
138: PRP-2-temp-scratch-notes
139: PRP-3-validation-gate-1-results
140: ```
141: 
142: **Implementation with Graceful Degradation**:
143: ```python
144: def write_prp_memory(category: str, name: str, content: str) -> Dict[str, Any]:
145:     """Write PRP-scoped memory with graceful Serena degradation."""
146:     state = get_active_prp()
147:     if not state:
148:         raise RuntimeError("No active PRP session")
149:     
150:     prp_id = state["prp_id"]
151:     memory_name = f"{prp_id}-{category}-{name}"
152:     
153:     # Try Serena MCP, gracefully degrade if unavailable
154:     try:
155:         from mcp import serena
156:         serena.write_memory(memory_name, content)
157:         serena_available = True
158:     except Exception as e:
159:         logger.warning(f"Serena unavailable: {e}")
160:         serena_available = False
161:     
162:     # Track in state regardless of Serena availability
163:     if "serena_memories" not in state:
164:         state["serena_memories"] = []
165:     if memory_name not in state["serena_memories"]:
166:         state["serena_memories"].append(memory_name)
167:     _write_state(state)
168:     
169:     return {
170:         "success": True,
171:         "memory_name": memory_name,
172:         "serena_available": serena_available
173:     }
174: ```
175: 
176: ---
177: 
178: ## Cleanup Protocol (Model.md Section 5.6)
179: 
180: **7-Step Process**:
181: 
182: 1. **Delete intermediate checkpoints** (keep final)
183: 2. **Archive learnings** to project knowledge
184: 3. **Delete ephemeral memories** (checkpoint-*, temp-*)
185: 4. **Reset validation state**
186: 5. **Run context health check**
187: 6. **Archive validation logs**
188: 7. **Remove active session**
189: 
190: **Implementation**:
191: ```python
192: def cleanup_prp(prp_id: str) -> Dict[str, Any]:
193:     """Execute comprehensive cleanup protocol."""
194:     state = get_active_prp()
195:     if not state or state["prp_id"] != prp_id:
196:         raise RuntimeError(f"No active session for {prp_id}")
197:     
198:     result = {
199:         "success": True,
200:         "checkpoints_deleted": 0,
201:         "checkpoints_kept": [],
202:         "memories_archived": [],
203:         "memories_deleted": [],
204:         "context_health": {}
205:     }
206:     
207:     # Step 1: Delete intermediate checkpoints
208:     checkpoints = list_checkpoints(prp_id)
209:     for cp in checkpoints:
210:         if "final" in cp["phase"]:
211:             result["checkpoints_kept"].append(cp["tag_name"])
212:         else:
213:             run_cmd(f"git tag -d {cp['tag_name']}")
214:             result["checkpoints_deleted"] += 1
215:     
216:     # Step 2-3: Archive learnings, delete ephemeral
217:     for memory_name in state.get("serena_memories", []):
218:         category = memory_name.split("-")[1]  # Extract category
219:         if category == "learnings":
220:             result["memories_archived"].append(memory_name)
221:             # TODO: Copy to project knowledge base
222:         elif category in ["checkpoint", "temp"]:
223:             result["memories_deleted"].append(memory_name)
224:             # TODO: Delete from Serena
225:     
226:     # Step 5: Context health check
227:     from .context import health as context_health
228:     result["context_health"] = context_health()
229:     
230:     # Step 7: Remove session
231:     if STATE_FILE.exists():
232:         STATE_FILE.unlink()
233:     
234:     return result
235: ```
236: 
237: ---
238: 
239: ## Test Patterns
240: 
241: ### State Management Tests
242: ```python
243: def test_start_prp_creates_state_file():
244:     """Verify start_prp creates state file with correct structure."""
245:     result = start_prp("PRP-999", "Test PRP")
246:     
247:     assert STATE_FILE.exists()
248:     state = json.loads(STATE_FILE.read_text())
249:     assert state["prp_id"] == "PRP-999"
250:     assert state["prp_name"] == "Test PRP"
251:     assert "started_at" in state
252:     assert state["phase"] == "planning"
253: ```
254: 
255: ### Checkpoint Tests (Requires Clean Git)
256: ```python
257: def test_create_checkpoint_success():
258:     """Verify checkpoint creation with clean git tree."""
259:     start_prp("PRP-999")
260:     
261:     # Ensure clean git state
262:     result = run_cmd("git status --porcelain")
263:     if result["stdout"]:
264:         pytest.skip("Git tree not clean")
265:     
266:     checkpoint = create_checkpoint("phase1")
267:     
268:     assert "checkpoint-PRP-999-phase1" in checkpoint["tag_name"]
269: ```
270: 
271: ### Cleanup Tests
272: ```python
273: def test_cleanup_prp_archives_learnings():
274:     """Verify cleanup identifies learnings for archiving."""
275:     start_prp("PRP-999")
276:     write_prp_memory("learnings", "auth-patterns", "Content")
277:     write_prp_memory("checkpoint", "phase1", "Temp")
278:     
279:     result = cleanup_prp("PRP-999")
280:     
281:     assert "PRP-999-learnings-auth-patterns" in result["memories_archived"]
282:     assert "PRP-999-checkpoint-phase1" in result["memories_deleted"]
283: ```
284: 
285: ---
286: 
287: ## Common Pitfalls & Solutions
288: 
289: ### Pitfall 1: PRP ID Format
290: **Problem**: Leading zeros in PRP IDs (e.g., "PRP-001")  
291: **Error**: `ValueError: Invalid PRP ID format`  
292: **Solution**: Use "PRP-1", "PRP-2" format (no leading zeros)
293: 
294: ### Pitfall 2: Uncommitted Changes
295: **Problem**: Checkpoint creation fails with dirty git tree  
296: **Error**: `RuntimeError: Working tree has uncommitted changes`  
297: **Solution**: Commit or stash changes before creating checkpoints
298: 
299: ### Pitfall 3: Multiple Active PRPs
300: **Problem**: Starting new PRP while another is active  
301: **Error**: `RuntimeError: Another PRP session is active: PRP-2`  
302: **Solution**: Run `end_prp()` or `cleanup_prp()` before starting new session
303: 
304: ### Pitfall 4: Import Errors
305: **Problem**: `ImportError: cannot import name 'context_health'`  
306: **Root Cause**: Function name mismatch  
307: **Solution**: Import with alias: `from .context import health as context_health`
308: 
309: ---
310: 
311: ## Integration Points
312: 
313: ### With Git
314: - Uses annotated tags for checkpoints
315: - Validates clean working tree before checkpoints
316: - Parses `git tag -l` output for checkpoint listing
317: 
318: ### With Serena MCP
319: - Graceful degradation if Serena unavailable
320: - Tracks memory names in state file regardless
321: - Cleanup protocol aware of Serena unavailability
322: 
323: ### With Validation System
324: - Tracks validation attempts in state
325: - Gate numbers stored as keys in `validation_attempts`
326: - Cleanup protocol resets validation state
327: 
328: ---
329: 
330: ## Files Reference
331: 
332: **Implementation**: `tools/ce/prp.py` (lines 210-931)  
333: **Tests**:
334: - State: `tools/tests/test_prp_state.py` (15 tests)
335: - Checkpoints: `tools/tests/test_prp_checkpoint.py` (13 tests)
336: - Cleanup: `tools/tests/test_prp_cleanup.py` (8 tests)
337: 
338: **Total Test Coverage**: 36 tests, all passing
</file>

<file path=".serena/memories/prp-backlog-system.md">
  1: ---
  2: type: regular
  3: category: configuration
  4: tags: [prp, backlog, tracking]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # PRP Backlog System
 10: 
 11: **Type**: Critical System Documentation
 12: 
 13: ## Purpose
 14: 
 15: The `PRPs/backlog/` directory is a **permanent part of the PRP system** for managing future work that hasn't been prioritized or scheduled yet.
 16: 
 17: ## Directory Structure
 18: 
 19: ```
 20: PRPs/
 21: ‚îú‚îÄ‚îÄ feature-requests/   # Active PRPs ready for execution
 22: ‚îú‚îÄ‚îÄ executed/           # Completed PRPs (archived)
 23: ‚îî‚îÄ‚îÄ backlog/            # INITIAL files and PRPs for future execution
 24: ```
 25: 
 26: ## Backlog Usage
 27: 
 28: ### What Goes in Backlog
 29: 
 30: 1. **INITIAL files** - Feature ideas created explicitly for future consideration
 31: 2. **Draft PRPs** - Full PRPs generated but deferred for future execution
 32: 3. **Future enhancements** - Ideas captured for later consideration
 33: 4. **Dependencies waiting** - PRPs blocked by other work
 34: 
 35: ### Workflow
 36: 
 37: ```mermaid
 38: graph LR
 39:     A[Idea/Feature Request] --> B{Execute now or later?}
 40:     B -->|Later| C[Create INITIAL in backlog/]
 41:     B -->|Later with detail| D[Generate PRP in backlog/]
 42:     B -->|Now| E[Create INITIAL in feature-requests/]
 43:     C --> F{User prioritizes?}
 44:     D --> F
 45:     F -->|Yes - user request| G[Move to feature-requests/]
 46:     F -->|No| H[Keep in backlog/]
 47:     G --> I[Execute PRP]
 48:     I --> J[Move to executed/]
 49: ```
 50: 
 51: ### Moving Items
 52: 
 53: **To backlog** (intentional placement):
 54: - `create initial for future` - INITIAL file explicitly for future work
 55: - `generate prp for future` - Full PRP created but deferred
 56: - Low-priority PRPs moved from feature-requests/ (user decision)
 57: 
 58: **From backlog** (user-driven only):
 59: - **User decides**: All prioritization is user decision or explicit request
 60: - Never autonomously promote backlog items to active work
 61: - When user requests, move INITIAL ‚Üí feature-requests/ (or generate PRP first)
 62: - When user resolves dependencies, move PRP ‚Üí feature-requests/
 63: 
 64: ## Commands
 65: 
 66: ### Creating Backlog Items
 67: 
 68: ```bash
 69: # Create INITIAL for future execution
 70: /generate-prp PRPs/backlog/INITIAL-PRP-XX-feature.md
 71: 
 72: # Explicit intent: "create this for backlog"
 73: # Place INITIAL directly in backlog/ directory
 74: ```
 75: 
 76: ### Promoting from Backlog (User Request Only)
 77: 
 78: ```bash
 79: # ONLY when user explicitly requests
 80: mv PRPs/backlog/INITIAL-PRP-XX-feature.md PRPs/feature-requests/
 81: /generate-prp PRPs/feature-requests/INITIAL-PRP-XX-feature.md
 82: ```
 83: 
 84: ## Critical Rules
 85: 
 86: 1. **Never autonomous prioritization** - Agent does not decide what to promote
 87: 2. **User controls backlog** - All movement in/out requires user request
 88: 3. **Explicit requests only** - "Move PRP-X to active" or "Execute backlog item Y"
 89: 4. **No proactive suggestions** - Don't suggest promoting backlog items unprompted
 90: 
 91: ## Persistence
 92: 
 93: This structure ensures:
 94: - No ideas are lost
 95: - Clear separation between active and future work
 96: - Flexible prioritization without losing context
 97: - Audit trail of all feature considerations
 98: - **User control** - agent never decides priority autonomously
 99: 
100: ## References
101: 
102: - **GRAND-PLAN.md**: May contain planned PRPs (check before creating duplicates)
103: - **Tool inventory**: `.ce/tool-inventory.yml` tracks PRP system tooling
104: - **Context sync**: `/update-context` maintains backlog awareness
105: 
106: ## Notes
107: 
108: - Backlog placement is **intentional** - for future execution, not immediate
109: - Backlog items don't require Linear issues (only when promoted to active)
110: - INITIAL files can stay in backlog indefinitely
111: - Backlog size is not a metric - it's a knowledge base
112: - Use backlog to capture ideas without committing to immediate execution
113: - **Prioritization is always user decision** - never autonomous
</file>

<file path=".serena/memories/prp-structure-initialized.md">
 1: ---
 2: type: regular
 3: category: configuration
 4: tags: [prp, structure, setup]
 5: created: "2025-11-04T17:30:00Z"
 6: updated: "2025-11-04T17:30:00Z"
 7: ---
 8: 
 9: # PRP Structure Initialization - Completed
10: 
11: **Date:** 2025-10-11
12: **PRP:** PRP-001-init-context-engineering.md
13: **Status:** Successfully Executed
14: 
15: ## What Was Created
16: 
17: ### Directory Structure
18: ```
19: PRPs/
20: ‚îú‚îÄ‚îÄ templates/
21: ‚îÇ   ‚îú‚îÄ‚îÄ self-healing.md    # Comprehensive template for complex features
22: ‚îÇ   ‚îî‚îÄ‚îÄ kiss.md            # Minimal template for simple features
23: ‚îú‚îÄ‚îÄ feature-requests/      # Future INITIAL.md files location
24: ‚îú‚îÄ‚îÄ ai_docs/              # Cached library documentation (gitignored)
25: ‚îî‚îÄ‚îÄ PRP-001-init-context-engineering.md
26: 
27: examples/
28: ‚îú‚îÄ‚îÄ patterns/             # Code pattern reference library
29: ‚îî‚îÄ‚îÄ README.md            # Usage documentation
30: ```
31: 
32: ### Files Created
33: 1. **PRPs/templates/self-healing.md** - Full-featured template with:
34:    - Serena pre-flight checks
35:    - 3-level validation loops
36:    - Self-healing gates
37:    - Context synchronization protocol
38:    - Confidence scoring
39: 
40: 2. **PRPs/templates/kiss.md** - Minimal template with:
41:    - Essential sections only
42:    - Quick validation commands
43:    - Simple completion checklist
44: 
45: 3. **examples/README.md** - Documentation for:
46:    - Pattern organization structure
47:    - Usage in PRP CONTEXT sections
48:    - Contributing guidelines
49: 
50: ### Configuration Updates
51: - **.gitignore** updated with:
52:   - PRPs/ai_docs/* (temporary documentation cache)
53:   - PRPs/feature-requests/*.tmp (temporary request files)
54:   - PRPs/.cache/ (build artifacts)
55:   - examples/.tmp/ (temporary pattern files)
56: 
57: ## Validation Results
58: 
59: ‚úÖ **Level 1:** Directory structure - PASSED
60: ‚úÖ **Level 2:** File existence - PASSED  
61: ‚úÖ **Level 3:** Git integration - PASSED
62: 
63: All validation gates passed on first attempt.
64: 
65: ## Usage Going Forward
66: 
67: ### Creating New PRPs
68: 1. Choose template: `PRPs/templates/self-healing.md` or `PRPs/templates/kiss.md`
69: 2. Copy to `PRPs/PRP-XXX-feature-name.md`
70: 3. Fill in sections based on requirements
71: 4. Execute with `/execute-prp PRP-XXX-feature-name.md`
72: 
73: ### Adding Code Patterns
74: 1. Implement feature with good patterns
75: 2. Extract reusable parts to `examples/patterns/`
76: 3. Reference in future PRPs: `examples/patterns/name.py:lines`
77: 
78: ## Next Steps
79: 1. Commit structure to git
80: 2. Create first feature PRP using templates
81: 3. Test full PRP workflow end-to-end
82: 4. Refine templates based on real-world usage
83: 
84: ## Notes
85: - Structure follows KISS principles - minimal but complete
86: - Templates based on docs/research/01-prp-system.md proven patterns
87: - No external dependencies required
88: - Pure filesystem operations, no complex tooling
</file>

<file path=".serena/memories/README.md">
  1: # Serena Memories - Memory Type System
  2: 
  3: **Last Updated**: 2025-11-04
  4: **Version**: CE 1.1
  5: 
  6: ---
  7: 
  8: ## Overview
  9: 
 10: This directory contains 23 framework memory files that provide context for AI agents working with the Context Engineering codebase. All memories use YAML front matter to categorize and classify content.
 11: 
 12: ## Memory Type System
 13: 
 14: ### Type Classification
 15: 
 16: All framework memories have a `type` field in their YAML header:
 17: 
 18: ```yaml
 19: ---
 20: type: regular
 21: category: documentation
 22: tags: [tag1, tag2, tag3]
 23: created: "2025-11-04T17:30:00Z"
 24: updated: "2025-11-04T17:30:00Z"
 25: ---
 26: ```
 27: 
 28: ### Type Values
 29: 
 30: - **`type: regular`**: Standard framework memory (default for all 23 memories in ctx-eng-plus)
 31: - **`type: critical`**: High-priority memory (users manually upgrade during target project initialization)
 32: - **`type: user`**: User-created memory in target projects (added during Phase 2 of INITIALIZATION.md)
 33: 
 34: ### Critical Memories (Upgrade Candidates)
 35: 
 36: The following 6 memories are typically upgraded to `type: critical` during target project initialization:
 37: 
 38: 1. **code-style-conventions.md** - Core coding standards and conventions
 39: 2. **suggested-commands.md** - Essential CLI commands and workflows
 40: 3. **task-completion-checklist.md** - Quality gates and completion criteria
 41: 4. **testing-standards.md** - Testing philosophy and TDD approach
 42: 5. **tool-usage-syntropy.md** - Tool selection reference
 43: 6. **use-syntropy-tools-not-bash.md** - Tool usage guidelines
 44: 
 45: **Note**: In ctx-eng-plus (framework repo), all memories default to `type: regular`. Users manually upgrade to `critical` during target project initialization based on their specific needs.
 46: 
 47: ## Category Taxonomy
 48: 
 49: ### documentation (13 files)
 50: Documentation, guides, how-tos, and reference materials.
 51: 
 52: **Files**:
 53: - code-style-conventions.md
 54: - suggested-commands.md
 55: - testing-standards.md
 56: - task-completion-checklist.md
 57: - tool-usage-syntropy.md
 58: - use-syntropy-tools-not-bash.md
 59: - linear-mcp-integration-example.md
 60: - linear-mcp-integration.md
 61: - linear-issue-tracking-integration.md
 62: - l4-validation-usage.md
 63: - project-overview.md
 64: 
 65: ### pattern (5 files)
 66: Code patterns, best practices, and workflow templates.
 67: 
 68: **Files**:
 69: - prp-2-implementation-patterns.md
 70: - PRP-15-remediation-workflow-implementation.md
 71: - serena-implementation-verification-pattern.md
 72: - syntropy-status-hook-pattern.md
 73: - linear-issue-creation-pattern.md
 74: 
 75: ### architecture (2 files)
 76: System design, structure, and architectural specifications.
 77: 
 78: **Files**:
 79: - codebase-structure.md
 80: - system-model-specification.md
 81: 
 82: ### configuration (4 files)
 83: Setup instructions, configuration management, and system state.
 84: 
 85: **Files**:
 86: - prp-structure-initialized.md
 87: - serena-mcp-tool-restrictions.md
 88: - tool-config-optimization-completed.md
 89: - prp-backlog-system.md
 90: 
 91: ### troubleshooting (1 file)
 92: Error resolution, debugging guides, and fixes.
 93: 
 94: **Files**:
 95: - cwe78-prp22-newline-escape-issue.md
 96: 
 97: ## Upgrade Path
 98: 
 99: ### From Regular to Critical
100: 
101: During target project initialization (see `examples/INITIALIZATION.md`), users can upgrade memories to `critical`:
102: 
103: ```bash
104: # Edit memory file YAML header
105: vim .serena/memories/code-style-conventions.md
106: ```
107: 
108: Change:
109: ```yaml
110: type: regular
111: ```
112: 
113: To:
114: ```yaml
115: type: critical
116: ```
117: 
118: ### User Memory Creation
119: 
120: When creating user-specific memories in target projects, use:
121: 
122: ```yaml
123: ---
124: type: user
125: source: target-project
126: created: "2025-11-04T00:00:00Z"
127: updated: "2025-11-04T00:00:00Z"
128: ---
129: 
130: [User memory content]
131: ```
132: 
133: ## Tags
134: 
135: Tags provide additional context for memory categorization. Common tags include:
136: 
137: - **Tool-related**: `syntropy`, `mcp`, `tools`, `serena`
138: - **Workflow**: `prp`, `workflow`, `automation`, `validation`
139: - **Code quality**: `code-style`, `testing`, `standards`, `tdd`
140: - **Integration**: `linear`, `tracking`, `issues`, `github`
141: - **Architecture**: `structure`, `architecture`, `design`, `system-model`
142: 
143: ## File Count
144: 
145: **Framework memories**: 23 files (all `type: regular` by default)
146: **Symlinks**: 1 (TOOL-USAGE-GUIDE.md ‚Üí examples/TOOL-USAGE-GUIDE.md)
147: 
148: ## Usage
149: 
150: ### Serena MCP Integration
151: 
152: All memories are automatically loaded by Serena MCP when the project root is activated:
153: 
154: ```python
155: # Serena automatically indexes memories on activation
156: serena_activate(project_root="/Users/bprzybysz/nc-src/ctx-eng-plus")
157: ```
158: 
159: ### Memory Queries
160: 
161: Use Serena MCP to search and retrieve memories:
162: 
163: ```python
164: # Read specific memory
165: serena_read_memory(name="code-style-conventions")
166: 
167: # List all memories
168: serena_list_memories()
169: 
170: # Search memories by tag
171: serena_search_memories(tags=["prp", "workflow"])
172: ```
173: 
174: ## Maintenance
175: 
176: ### Adding New Memories
177: 
178: When creating new framework memories:
179: 
180: 1. Create file in `.serena/memories/`
181: 2. Add YAML front matter with `type: regular`
182: 3. Choose appropriate category (documentation/pattern/architecture/configuration/troubleshooting)
183: 4. Add 3-5 relevant tags
184: 5. Update this README.md with file count and category breakdown
185: 
186: ### Updating Existing Memories
187: 
188: When modifying memories:
189: 
190: 1. Update `updated` timestamp in YAML header
191: 2. Preserve `type` and `category` (unless reclassifying)
192: 3. Add/remove tags as needed
193: 4. Maintain consistent formatting
194: 
195: ## See Also
196: 
197: - `examples/INITIALIZATION.md` - Framework initialization guide (includes memory type upgrade workflow)
198: - `examples/TOOL-USAGE-GUIDE.md` - Tool selection reference
199: - `CLAUDE.md` - Project-specific guidance
</file>

<file path=".serena/memories/serena-implementation-verification-pattern.md">
  1: ---
  2: type: regular
  3: category: pattern
  4: tags: [serena, verification, testing]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # Serena Implementation Verification Pattern
 10: 
 11: ## Overview
 12: Pattern for using Serena MCP to verify PRP implementations exist in codebase through semantic code understanding (PRP-16).
 13: 
 14: ## Core Function
 15: **Location**: `tools/ce/update_context.py:798-879`  
 16: **Function**: `verify_implementation_with_serena(expected_functions: List[str]) -> bool`
 17: 
 18: ## Purpose
 19: Replace regex-based verification with semantic symbol lookup:
 20: - Query Serena MCP's `find_symbol` for each expected function/class
 21: - Return `True` only if ALL implementations found
 22: - Graceful degradation when Serena unavailable
 23: 
 24: ## Implementation Pattern
 25: 
 26: ### Direct Serena Import
 27: ```python
 28: import mcp__serena as serena
 29: ```
 30: 
 31: **Why direct import**: 
 32: - `mcp_adapter.py` provides file operations, not symbol queries
 33: - `find_symbol` is Serena MCP tool, not wrapped
 34: - Simpler for read-only queries
 35: 
 36: ### Symbol Query Pattern
 37: ```python
 38: result = serena.find_symbol(
 39:     name_path="function_name",
 40:     relative_path="tools/ce/",
 41:     include_body=False
 42: )
 43: 
 44: # Check result
 45: if result and isinstance(result, list) and len(result) > 0:
 46:     # Implementation found
 47:     verified = True
 48: ```
 49: 
 50: ### Response Structure
 51: ```python
 52: # Success
 53: [{"name_path": "func_name", "kind": "Function", "body_location": {...}}]
 54: 
 55: # Not found
 56: []
 57: ```
 58: 
 59: ### Graceful Degradation
 60: ```python
 61: try:
 62:     import mcp__serena as serena
 63:     # ... verification logic
 64: except (ImportError, ModuleNotFoundError):
 65:     logger.warning("Serena MCP not available - skipping verification")
 66:     return False
 67: ```
 68: 
 69: ## Integration Point
 70: Called in `sync_context()` workflow (~line 613):
 71: ```python
 72: serena_verified = verify_implementation_with_serena(expected_functions)
 73: ```
 74: 
 75: ## Test Coverage
 76: **File**: `tools/tests/test_serena_verification.py`
 77: 
 78: **Unit Tests** (5):
 79: - All functions found ‚Üí returns True
 80: - Some missing ‚Üí returns False  
 81: - Empty list ‚Üí returns True (nothing to verify)
 82: - Serena unavailable ‚Üí returns False
 83: - Query exception ‚Üí returns False
 84: 
 85: **Integration Tests** (2):
 86: - sync_context integration
 87: - Real Serena MCP (if available)
 88: 
 89: **E2E Test** (1):
 90: - Full context sync workflow
 91: 
 92: ## Known Limitations
 93: **Hardcoded path**: `relative_path="tools/ce/"`
 94: - Works for 95% of implementations
 95: - Future enhancement: path inference from PRP content
 96: 
 97: ## Usage in Context Sync
 98: 
 99: ### YAML Header Updates
100: ```yaml
101: context_sync:
102:   ce_updated: true
103:   serena_updated: true    # Set when all implementations found
104:   last_sync: "2025-10-16T14:30:00Z"
105:   verified_implementations:
106:     - function_name_1
107:     - function_name_2
108: ```
109: 
110: ### Workflow
111: 1. Extract function names from PRP content
112: 2. Query Serena for each function
113: 3. Update YAML header with results
114: 4. Log verification status
115: 
116: ## Error Handling
117: All errors include üîß troubleshooting guidance:
118: ```python
119: logger.warning(
120:     "Serena MCP not available - skipping verification\n"
121:     "üîß Troubleshooting: Ensure Serena MCP server is configured and running"
122: )
123: ```
124: 
125: ## Related PRPs
126: - **PRP-9**: Serena MCP Integration (infrastructure)
127: - **PRP-14**: Update-Context Command (workflow)
128: - **PRP-15**: Drift Remediation (uses context sync)
129: 
130: ## Example Output
131: ```bash
132: cd tools && uv run ce update-context --prp PRPs/feature-requests/PRP-16-serena-verification.md
133: 
134: # Output:
135: # ‚úì Verified: verify_implementation_with_serena
136: # Serena verification complete: 1/1 implementations found
137: # ‚úÖ Context sync completed
138: ```
139: 
140: ## Best Practices
141: 1. **Import directly**: Use `import mcp__serena as serena`
142: 2. **Check response type**: Verify `isinstance(result, list)`
143: 3. **Graceful degradation**: Catch ImportError/ModuleNotFoundError
144: 4. **Clear logging**: Debug/info/warning at appropriate levels
145: 5. **All or nothing**: Return True only if ALL functions found
146: 6. **No silent failures**: Log missing implementations with names
</file>

<file path=".serena/memories/serena-mcp-tool-restrictions.md">
  1: ---
  2: type: regular
  3: category: configuration
  4: tags: [serena, mcp, permissions]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # Serena MCP Tool Restrictions & Workarounds
 10: 
 11: **Updated**: 2025-10-20 for Syntropy MCP
 12: **Status**: All tools now route through Syntropy aggregation layer
 13: 
 14: ## CRITICAL: Tool Permissions
 15: 
 16: ### DENIED Tools (Syntropy Format)
 17: - `mcp__syntropy_serena_replace_symbol_body` - ‚ùå DENIED (permission denied error)
 18:   - **Reason**: Symbol-level mutations require elevated permissions
 19:   - **Status**: Not available for this project/context
 20: - `mcp__syntropy_serena_replace_regex` - ‚ùå DENIED (permission denied error)
 21:   - **Reason**: Regex-based mutations require elevated permissions
 22:   - **Status**: Not available for this project/context
 23: 
 24: ### ALLOWED Tools (Verified - Syntropy Format)
 25: - `mcp__syntropy_serena_find_symbol` ‚úÖ (read symbol with body)
 26: - `mcp__syntropy_serena_get_symbols_overview` ‚úÖ (read file structure)
 27: - `mcp__syntropy_serena_search_for_pattern` ‚úÖ (pattern search)
 28: - `mcp__syntropy_serena_read_file` ‚úÖ (read Python files)
 29: - `mcp__syntropy_serena_list_dir` ‚úÖ (directory with symbols)
 30: - `mcp__syntropy_serena_find_referencing_symbols` ‚úÖ (usage analysis)
 31: - `mcp__syntropy_serena_insert_after_symbol` ‚úÖ (add code after symbol) **NOW ALLOWED**
 32: - `mcp__syntropy_serena_insert_before_symbol` ‚úÖ (add code before symbol) **NOW ALLOWED**
 33: - `mcp__syntropy_filesystem_edit_file` ‚úÖ (line-based edits)
 34: - `mcp__syntropy_git_*` ‚úÖ (all git operations)
 35: 
 36: ## Workarounds for Symbol Mutations (Syntropy Format)
 37: 
 38: **Instead of**: `replace_symbol_body()` ‚ùå DENIED
 39: **Use**: `mcp__syntropy_filesystem_edit_file()` ‚úÖ (line-based edits)
 40: 
 41: ### Strategy
 42: 1. Use `mcp__syntropy_serena_find_symbol(include_body=True)` to read current implementation
 43: 2. Use `mcp__syntropy_filesystem_edit_file()` for precise line-level changes
 44: 3. For large changes, use multiple `edit_file()` calls or Read + Edit pattern
 45: 
 46: ### Example: Replace Function Body
 47: ```python
 48: # Instead of replace_symbol_body() which is denied:
 49: # Read the file first, then use Edit tool with old_string/new_string
 50: # OR use filesystem edit_file with line-based edits
 51: ```
 52: 
 53: ## Settings Restrictions
 54: 
 55: **CRITICAL**: `.claude/settings.local.json` cannot be overwritten
 56: - User instruction: "NEVER OVERWRITE permissions or I WILL BE UNHAPPY"
 57: - Deny list is intentional - respect it
 58: - Don't try to modify tool restrictions directly
 59: 
 60: ## Current Project Context
 61: - **Project**: Context Engineering Tools (ctx-eng-plus)
 62: - **Affected File**: tools/ce/update_context.py
 63: - **Task**: Add PRP execution to remediate_drift_workflow()
 64: - **Restriction**: Cannot use symbol_body mutations
 65: - **Solution**: Use regex-based replacement instead
 66: 
 67: ## Action Items
 68: 1. ‚úÖ Identify restriction (replace_symbol_body AND replace_regex both denied)
 69: 2. ‚úÖ Use `filesystem_edit_file()` or Read + Edit pattern instead
 70: 3. ‚úÖ Test the change works
 71: 4. ‚úÖ Verify workflow end-to-end
 72: 
 73: ---
 74: 
 75: ## Current Permission Configuration (Verified 2025-10-20)
 76: 
 77: ### Allowed Tools via Syntropy (30 total)
 78: 
 79: **Serena Tools** (11):
 80: - `mcp__syntropy_serena_find_symbol` - Read symbol with body
 81: - `mcp__syntropy_serena_get_symbols_overview` - File structure mapping
 82: - `mcp__syntropy_serena_search_for_pattern` - LSP-powered pattern search
 83: - `mcp__syntropy_serena_find_referencing_symbols` - Usage/impact analysis
 84: - `mcp__syntropy_serena_write_memory` - Session knowledge persistence
 85: - `mcp__syntropy_serena_create_text_file` - File creation
 86: - `mcp__syntropy_serena_read_file` - Read file contents
 87: - `mcp__syntropy_serena_list_dir` - List directory with symbols
 88: - `mcp__syntropy_serena_insert_after_symbol` - Insert code after symbol **NEW**
 89: - `mcp__syntropy_serena_insert_before_symbol` - Insert code before symbol **NEW**
 90: - `mcp__syntropy_serena_activate_project` - Project switching
 91: 
 92: **Filesystem Tools** (8):
 93: - `mcp__syntropy_filesystem_read_text_file` - Read config/text files
 94: - `mcp__syntropy_filesystem_write_file` - Create/overwrite files
 95: - `mcp__syntropy_filesystem_edit_file` - Line-based edits (workaround for mutations)
 96: - `mcp__syntropy_filesystem_list_directory` - Directory listing
 97: - `mcp__syntropy_filesystem_search_files` - File pattern search
 98: - `mcp__syntropy_filesystem_directory_tree` - Tree view
 99: - `mcp__syntropy_filesystem_get_file_info` - File metadata
100: - `mcp__syntropy_filesystem_list_allowed_directories` - Allowed paths
101: 
102: **Git Tools** (5):
103: - `mcp__syntropy_git_git_status` - Repository status
104: - `mcp__syntropy_git_git_diff` - Show changes
105: - `mcp__syntropy_git_git_log` - Commit history
106: - `mcp__syntropy_git_git_add` - Stage files
107: - `mcp__syntropy_git_git_commit` - Create commits
108: 
109: **Other Tools** (9):
110: - `mcp__syntropy_context7_resolve_library_id` - Resolve library docs
111: - `mcp__syntropy_context7_get_library_docs` - Fetch library documentation
112: - `mcp__syntropy_thinking_sequentialthinking` - Complex reasoning
113: - `mcp__syntropy_linear_create_issue` - Create Linear issues
114: - `mcp__syntropy_linear_get_issue` - Retrieve issues
115: - `mcp__syntropy_linear_list_issues` - List issues
116: - `mcp__syntropy_linear_update_issue` - Update issues
117: - `mcp__syntropy_linear_list_projects` - List projects
118: - `mcp__syntropy_repomix_pack_codebase` - Package codebase
119: 
120: ### Denied Tools (Serena - not available via Syntropy)
121: 
122: - **Symbol mutations**: replace_symbol_body, rename_symbol, replace_regex
123: - **Thinking**: think_about_* (3 tools) - reduced autonomous behavior
124: - **Modes**: switch_modes, prepare_for_new_conversation
125: - **Memory**: read_memory, list_memories, delete_memory, check_onboarding, onboarding
126: - **Config**: get_current_config
127: 
128: **Note**: insert_before_symbol and insert_after_symbol are NOW ALLOWED (moved to allow list)
129: 
130: ### Workaround Strategy (Syntropy Updated)
131: 
132: **For symbol-level edits** (replace_symbol_body AND replace_regex both denied):
133: 1. Use Claude Code's Edit tool (Read + Edit pattern with old_string/new_string)
134: 2. Use `mcp__syntropy_filesystem_edit_file()` for line-level changes
135: 3. Read with `mcp__syntropy_serena_find_symbol(include_body=True)` first
136: 
137: ### Critical Workflow Tools (Preserved in Allow List - Syntropy Forwarded)
138: 
139: #### Linear Integration (5 tools) - PRP Generation Workflow
140: 
141: **Tools** (via Syntropy):
142: - `mcp__syntropy_linear_create_issue` - Create new Linear issues
143: - `mcp__syntropy_linear_get_issue` - Retrieve issue details
144: - `mcp__syntropy_linear_list_issues` - List issues with filtering
145: - `mcp__syntropy_linear_update_issue` - Update issue status/content
146: - `mcp__syntropy_linear_list_projects` - List available projects
147: 
148: **Why Preserved**:
149: - The `/generate-prp` command automatically creates Linear issues to track implementation work
150: - Essential for documented PRP workflow (see CLAUDE.md lines 498-554)
151: - Enables issue tracking for feature implementation blueprints
152: 
153: **Usage Example**:
154: ```python
155: # Create issue from PRP generation
156: mcp__syntropy_linear_create_issue(
157:     team="Blaise78",
158:     title="PRP-25: Feature Implementation",
159:     description="Detailed feature from PRP blueprint",
160:     assignee="blazej.przybyszewski@gmail.com"
161: )
162: 
163: # Update issue status as implementation progresses
164: mcp__syntropy_linear_update_issue(
165:     issue_number="BLA-42",
166:     state="in_progress"
167: )
168: ```
169: 
170: ---
171: 
172: #### Context7 Documentation (2 tools) - Library Integration
173: 
174: **Tools** (via Syntropy):
175: - `mcp__syntropy_context7_resolve_library_id` - Resolve library identifiers
176: - `mcp__syntropy_context7_get_library_docs` - Fetch library documentation
177: 
178: **Why Preserved**:
179: - Essential for integrating external library documentation into PRPs
180: - Enables lookups for API references, framework guides, best practices
181: - Required for knowledge-grounded implementation planning
182: 
183: **Usage Example**:
184: ```python
185: # Step 1: Resolve library ID
186: lib_id = mcp__syntropy_context7_resolve_library_id(
187:     libraryName="FastAPI"
188: )
189: 
190: # Step 2: Get documentation for specific topic
191: docs = mcp__syntropy_context7_get_library_docs(
192:     context7CompatibleLibraryID=lib_id,
193:     topic="dependency_injection"
194: )
195: ```
196: 
197: ---
198: 
199: #### Sequential Thinking (1 tool) - Complex Reasoning
200: 
201: **Tool** (via Syntropy):
202: - `mcp__syntropy_thinking_sequentialthinking` - Multi-step problem decomposition
203: 
204: **Why Preserved**:
205: - Enables structured reasoning for complex PRP generation
206: - Used for planning multi-phase implementations
207: - Essential for breaking down large features into manageable PRPs
208: 
209: **Usage Example**:
210: ```python
211: # Structure complex problem solving
212: mcp__syntropy_thinking_sequentialthinking(
213:     thought="Step 1: Analyze current architecture and identify service boundaries",
214:     thoughtNumber=1,
215:     totalThoughts=5,
216:     nextThoughtNeeded=True
217: )
218: 
219: # ... continue with thoughtNumber 2-5 for full decomposition
220: ```
221: 
222: ---
223: 
224: ### Historical Note
225: 
226: **`PRPs/feature-requests/tools-rationalization-study.md`**:
227: - Study recommended denying Linear, Context7, and Sequential-thinking tools
228: - Empirical testing showed this recommendation was incorrect
229: - These tools are essential for documented workflows (see CLAUDE.md)
230: - Study marked as outdated historical reference
231: - Current configuration is empirically validated and production-tested
232: 
233: ### Why These Tools Matter
234: 
235: **Without these critical tools, the following workflows break**:
236: 1. ‚ùå PRP generation cannot auto-create Linear issues (breaks issue tracking)
237: 2. ‚ùå External library integration impossible (blocks knowledge-grounded PRPs)
238: 3. ‚ùå Complex problem decomposition unavailable (limits reasoning for large features)
239: 
240: **With these tools**:
241: 1. ‚úÖ Complete feature tracking from conception to completion
242: 2. ‚úÖ Knowledge-enriched implementations using real library docs
243: 3. ‚úÖ Structured reasoning for complex architectural decisions
</file>

<file path=".serena/memories/suggested-commands.md">
  1: ---
  2: type: regular
  3: category: documentation
  4: tags: [commands, cli, reference]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # Suggested Commands
 10: 
 11: ## Package Management (UV - MANDATORY)
 12: ```bash
 13: # Add dependencies
 14: uv add package-name              # Production dependency
 15: uv add --dev package-name        # Development dependency
 16: 
 17: # Install dependencies
 18: uv sync                          # Install all dependencies
 19: 
 20: # Run commands in virtual environment
 21: uv run <command>                 # Run any command with venv activated
 22: 
 23: # FORBIDDEN: Never edit pyproject.toml directly!
 24: ```
 25: 
 26: ## Testing
 27: ```bash
 28: # Navigate to tools directory first
 29: cd tools
 30: 
 31: # Run all tests
 32: uv run pytest tests/ -v
 33: 
 34: # Run specific test file
 35: uv run pytest tests/test_core.py -v
 36: 
 37: # Run specific test
 38: uv run pytest tests/test_core.py::test_run_cmd_success -v
 39: 
 40: # Quick test run (fail fast)
 41: uv run pytest tests/ -x --tb=short -q
 42: ```
 43: 
 44: ## CE Tool Usage
 45: ```bash
 46: cd tools
 47: 
 48: # Validation
 49: uv run ce validate --level all    # Run all validation gates
 50: uv run ce validate --level 1      # Run level 1 validation
 51: uv run ce validate --level 2      # Run level 2 validation
 52: uv run ce validate --level 3      # Run level 3 validation
 53: 
 54: # Git operations
 55: uv run ce git status               # Git repository status
 56: uv run ce git diff                 # Show git diff
 57: uv run ce git checkpoint "msg"     # Create git checkpoint (tag)
 58: 
 59: # Context management
 60: uv run ce context sync             # Sync context with codebase
 61: uv run ce context health           # Check context health
 62: uv run ce context prune            # Prune stale context
 63: 
 64: # JSON output (for scripting)
 65: uv run ce git status --json | jq '.clean'
 66: uv run ce context health --json | jq '.drift_score'
 67: ```
 68: 
 69: ## Development Setup
 70: ```bash
 71: # First time setup
 72: cd tools
 73: ./bootstrap.sh                    # Run bootstrap script
 74: 
 75: # Install in editable mode
 76: uv pip install -e .               # If needed for development
 77: ```
 78: 
 79: ## Darwin (macOS) System Commands
 80: ```bash
 81: # File operations
 82: ls -la                            # List files with details
 83: find . -name "*.py"               # Find Python files
 84: grep -r "pattern" .               # Recursive search
 85: 
 86: # Git operations
 87: git status                        # Repository status
 88: git diff                          # Show changes
 89: git log --oneline -n 10           # Recent commits
 90: 
 91: # Process management
 92: ps aux | grep python              # Find Python processes
 93: ```
 94: 
 95: ## Troubleshooting
 96: ```bash
 97: # Tool not found
 98: cd tools && uv pip install -e .
 99: 
100: # Tests failing
101: cd tools && uv sync               # Reinstall dependencies
102: uv run pytest tests/ -v           # Run tests
103: 
104: # Permission errors
105: chmod +x bootstrap.sh             # Make executable
106: ```
</file>

<file path=".serena/memories/syntropy-status-hook-pattern.md">
  1: ---
  2: type: regular
  3: category: pattern
  4: tags: [syntropy, hooks, status]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # Syntropy Status Hook - Cache-Based Architecture
 10: 
 11: ## Pattern: SessionStart Hook with MCP Data via Cache
 12: 
 13: **Location**: `tools/scripts/session-startup.sh`, `tools/scripts/syntropy-status.py`, `tools/scripts/cache-syntropy-health.py`
 14: 
 15: **Problem Solved**: SessionStart hooks can't directly call MCP tools (execution context limitation), but need to display real MCP healthcheck data.
 16: 
 17: **Solution**: Cache-based architecture where Claude Code updates cache, hooks read from cache.
 18: 
 19: ## Architecture
 20: 
 21: ```
 22: Claude Code (MCP access)
 23:   ‚Üí mcp__syntropy__healthcheck
 24:   ‚Üí cache-syntropy-health.py (writes JSON)
 25:   ‚Üí .ce/syntropy-health-cache.json
 26: 
 27: SessionStart Hook (no MCP access)
 28:   ‚Üí session-startup.sh
 29:   ‚Üí syntropy-status.py (reads cache)
 30:   ‚Üí Display real healthcheck data
 31: ```
 32: 
 33: ## Key Files
 34: 
 35: 1. **session-startup.sh**: Unified startup hook
 36:    - Runs context drift check
 37:    - Calls syntropy-status.py for MCP health
 38:    - Lists available MCP tools
 39:    - Timeout: 12s (sufficient for all checks)
 40: 
 41: 2. **syntropy-status.py**: Cache reader + formatter
 42:    - Reads `.ce/syntropy-health-cache.json`
 43:    - Displays server status with emojis
 44:    - Shows stale cache warnings (>5 min)
 45:    - Error guidance if cache missing
 46: 
 47: 3. **cache-syntropy-health.py**: Cache updater
 48:    - Accepts healthcheck JSON via stdin
 49:    - Writes to `.ce/syntropy-health-cache.json`
 50:    - Adds cache metadata (timestamp)
 51: 
 52: ## Benefits
 53: 
 54: 1. **Fast**: ~200ms cache read vs 2-3s MCP call
 55: 2. **Real data**: No hardcoded fallbacks (violates "No Fishy Fallbacks" policy)
 56: 3. **Graceful degradation**: Stale/missing cache shows errors with troubleshooting
 57: 4. **Single hook**: Consolidated from 3 separate hooks (better timeout management)
 58: 
 59: ## Benefits Over Previous Implementation
 60: 
 61: **Before (PRP-27 identified issues)**:
 62: - ‚ùå Hardcoded static data (FIXME: "Static data from recent healthcheck")
 63: - ‚ùå Silent failures on session resume (5s timeout too short)
 64: - ‚ùå No real MCP integration
 65: - ‚ùå Multiple separate hooks (timeout fragility)
 66: 
 67: **After (PRP-27 solution)**:
 68: - ‚úÖ Real MCP healthcheck data via cache
 69: - ‚úÖ 12s timeout (sufficient margin for all checks)
 70: - ‚úÖ Graceful error handling with troubleshooting
 71: - ‚úÖ Single unified hook script
 72: 
 73: ## Usage Pattern
 74: 
 75: **Refresh cache** (from Claude Code session):
 76: ```bash
 77: # The healthcheck is called automatically and cached
 78: # Manual refresh if needed:
 79: mcp__syntropy__healthcheck(detailed=True)
 80: # Results written to .ce/syntropy-health-cache.json
 81: ```
 82: 
 83: **Hook configuration** (`.claude/settings.local.json`):
 84: ```json
 85: {
 86:   "hooks": {
 87:     "SessionStart": [{
 88:       "matcher": "*",
 89:       "hooks": [{
 90:         "type": "command",
 91:         "command": "PROJECT_ROOT=$(git rev-parse --show-toplevel) && cd \"$PROJECT_ROOT/tools\" && bash scripts/session-startup.sh",
 92:         "timeout": 12
 93:       }]
 94:     }]
 95:   }
 96: }
 97: ```
 98: 
 99: ## Cache File Format
100: 
101: `.ce/syntropy-health-cache.json`:
102: ```json
103: {
104:   "cached_at": "2025-10-21T07:20:22.324Z",
105:   "data": {
106:     "syntropy": {"version": "0.1.0", "status": "healthy"},
107:     "servers": [
108:       {"server": "serena", "status": "healthy", "connected": true},
109:       ...
110:     ],
111:     "summary": {"total": 9, "healthy": 9, "degraded": 0, "down": 0}
112:   }
113: }
114: ```
115: 
116: **TTL**: 5 minutes (after which status displays stale warning)
117: 
118: ## Error Handling Examples
119: 
120: **Missing cache**:
121: ```
122: ‚ùå Syntropy health cache not found
123: üîß Run this in Claude Code to refresh:
124:    Call mcp__syntropy__healthcheck and pipe to cache-syntropy-health.py
125: ```
126: 
127: **Stale cache** (>5 minutes):
128: ```
129: ‚ö†Ô∏è Cache is stale (>5 minutes old)
130: üîß Consider refreshing healthcheck
131: [still displays data]
132: ```
133: 
134: ## Anti-Patterns Avoided
135: 
136: ‚ùå **Static hardcoded data**: Old script had FIXME with fake server list
137: ‚ùå **Silent failures**: Hooks timing out without user feedback
138: ‚ùå **Multiple hooks racing**: 3 separate 5s hooks (timeout fragility)
139: ‚ùå **Direct MCP calls in hooks**: Not possible, would fail
140: ‚ùå **Fishy fallbacks**: No fake success when real data unavailable
141: 
142: ## Implementation Notes
143: 
144: **Why cache instead of direct MCP call?**
145: - SessionStart hooks run in shell context (no MCP access)
146: - MCP tools only callable from Claude Code execution context
147: - Cache provides fast reads (~200ms) vs slow MCP calls (2-3s)
148: - Cache survives across sessions (shows last known state)
149: 
150: **Timeout calculation** (12s total):
151: - uv venv initialization: ~2-3s
152: - Context drift check: ~1-2s
153: - Syntropy status (cache read): ~200ms
154: - MCP tools list: ~500ms
155: - Buffer for slow systems: ~6-7s
156: 
157: ## Related Files
158: 
159: - **Policy**: CLAUDE.md "No Fishy Fallbacks" (line 47)
160: - **PRP**: PRPs/executed/PRP-27-syntropy-status-hook.md
161: - **Example**: examples/syntropy-status-hook-system.md
162: - **Tests**: Validated via manual testing (Gates 1-4 passed)
163: 
164: ## Testing Strategy
165: 
166: **Gate 1**: Architecture verification
167: - Single unified hook ‚úÖ
168: - Clear error boundaries ‚úÖ
169: - Timeout margin verified ‚úÖ
170: 
171: **Gate 2**: MCP integration
172: - No hardcoded data ‚úÖ
173: - Real healthcheck via cache ‚úÖ
174: - Error messages with guidance ‚úÖ
175: 
176: **Gate 3**: Functional testing
177: - Fresh session shows all checks ‚úÖ
178: - Cache read displays real data ‚úÖ
179: - Missing cache shows error ‚úÖ
180: 
181: **Gate 4**: Error handling
182: - Missing cache: actionable guidance ‚úÖ
183: - Stale cache: warning displayed ‚úÖ
184: - Graceful degradation ‚úÖ
</file>

<file path=".serena/memories/system-model-specification.md">
  1: ---
  2: type: regular
  3: category: architecture
  4: tags: [system-model, architecture, design]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # System Model Specification
 10: 
 11: **Type**: Critical System Documentation
 12: 
 13: **Location**: `examples/model/SystemModel.md`
 14: 
 15: ## Purpose
 16: 
 17: The SystemModel.md is the **formal specification** and **target architecture** for the Context Engineering Management system. This is the master reference document that defines:
 18: 
 19: - Core principles (Context-as-Compiler mental model)
 20: - Architecture (Four Pillars: WRITE, SELECT, COMPRESS, ISOLATE)
 21: - Complete workflow (6-step + Steps 2.5 & 6.5 for context sync)
 22: - Quality framework (4-level validation: L1-L4)
 23: - Performance metrics and targets
 24: 
 25: ## Critical: Model vs Implementation Status
 26: 
 27: **‚ö†Ô∏è This document describes TARGET ARCHITECTURE**
 28: 
 29: Features marked üîú are planned but not yet implemented.
 30: 
 31: **Implemented (‚úÖ)**:
 32: - L1-L3 validation gates
 33: - Git operations
 34: - Context management (sync, health, drift detection)
 35: - Python execution (3 LOC enforcement)
 36: - PRP generation & execution
 37: - Auto-sync mode (Steps 2.5 & 6.5)
 38: 
 39: **Planned (üîú)**:
 40: - L4 pattern conformance automation
 41: - PRP-aware state management commands
 42: - Drift tracking commands (`ce drift history`, `ce drift show`)
 43: 
 44: ## Key Sections Reference
 45: 
 46: ### 1. System Overview
 47: - **Context-as-Compiler**: Missing context = Hallucination (like compiler errors)
 48: - **10-100x improvement**: 10x baseline via structured prompts, 100x exceptional cases with full MCP
 49: - **Core principle**: Complete context provision necessary and sufficient
 50: 
 51: ### 2. Evolution & Philosophy
 52: - **Three stages**: Vibe Coding (10-20%) ‚Üí Prompt Engineering (40-60%) ‚Üí Context Engineering (85-97%)
 53: - **No Fishy Fallbacks**: Fast failure with actionable errors
 54: - **KISS**: Simple solutions, minimal dependencies
 55: - **Real Functionality Testing**: No mocks, no fake results
 56: - **Strict Enforcement**: 3 LOC limit, UV package management, 10/10 confidence required
 57: 
 58: ### 3. Architecture
 59: 
 60: #### 3.1 Four Pillars
 61: 1. **WRITE**: Persistence (Serena memories, git checkpoints, validation logs)
 62: 2. **SELECT**: Dynamic retrieval (find_symbol, search_for_pattern, Context7 docs)
 63: 3. **COMPRESS**: Efficiency (overview-first, targeted reads, token optimization)
 64: 4. **ISOLATE**: Safety (validation gates, checkpoints, error boundaries)
 65: 
 66: #### 3.2 PRP System
 67: - **6 primary sections**: GOAL, WHY, WHAT, CONTEXT, IMPLEMENTATION BLUEPRINT, VALIDATION LOOPS
 68: - **Optional sections**: SERENA PRE-FLIGHT, SELF-HEALING GATES, DRIFT_JUSTIFICATION
 69: - **Information density**: Specific over vague ("Next.js 14.2.3" not "modern practices")
 70: 
 71: #### 3.3 Validation Framework
 72: - **Level 1**: Syntax & style (10s, auto-fix: Yes)
 73: - **Level 2**: Unit tests (30-60s, auto-fix: Conditional)
 74: - **Level 3**: Integration (1-2min, auto-fix: Manual)
 75: - **Level 4**: Pattern conformance (30-60s, **NEW** - compares vs EXAMPLES in INITIAL.md)
 76:   - 0-10% drift: Auto-accept
 77:   - 10-30% drift: Auto-fix or log warning
 78:   - **30%+ drift: HALT & ESCALATE TO USER** (human decision required)
 79: 
 80: ### 4. Components
 81: 
 82: #### 4.1 Tool Ecosystem
 83: - **run_py**: 3 LOC limit enforcement, auto-detect mode
 84: - **ce CLI**: validate, git, context, prp commands
 85: - **MCP Integration**: Serena (codebase), Context7 (docs), Sequential Thinking (reasoning)
 86: 
 87: #### 4.2 Templates
 88: - **Self-Healing**: Complex features with extensive validation
 89: - **KISS**: Simple features, quick implementations
 90: 
 91: ### 5. Workflow (6-Step + Context Sync)
 92: 
 93: **Step 1**: CLAUDE.md (one-time setup)
 94: **Step 2**: INITIAL.md (2-5 min)
 95: **Step 2.5**: **Context Sync & Health Check** (1-2 min) - NEW
 96: **Step 3**: /generate-prp (10-15 min)
 97: **Step 4**: Human Validation (5-10 min) - CRITICAL CHECKPOINT
 98: **Step 5**: /execute-prp (20-90 min)
 99: **Step 6**: Validation Loop L1-L4 (continuous)
100: **Step 6.5**: **State Cleanup & Context Sync** (2-3 min) - NEW
101: 
102: **Time Distribution**:
103: - Simple: 16-28 min (vs 3-5 hrs manual)
104: - Medium: 33-60 min (vs 8-15 hrs manual)
105: - Complex: 63-120 min (vs 20-40 hrs manual)
106: 
107: ### 6. Implementation Patterns
108: - No Fishy Fallbacks (fast failure pattern)
109: - 3 LOC Rule (strict enforcement)
110: - Real Functionality Testing (no mocks in production)
111: - Auto-Detect Mode (smart file vs code detection)
112: - UV Package Management (never edit pyproject.toml manually)
113: 
114: ### 7. Quality Assurance
115: - Self-healing loop (max 3 attempts, then escalate)
116: - Confidence scoring (1-10, **10/10 required** for production)
117: - Pipeline architecture (strategy pattern for testing)
118: 
119: ### 8. Performance Metrics
120: - **Case study**: PRP Taskmaster (25 min, 36x speedup - EXCEPTIONAL OUTLIER)
121: - **Typical**: 10-24x speedup for production features
122: - **Success rates**: 85% first-pass, 97% second-pass, 92% self-healing
123: - **Productivity**: 3-4x for teams
124: 
125: ### 9. Design Objectives
126: - Reliability: 97% error catch rate (L1-L4), 92% self-healing, 94% production-ready
127: - Performance: 10-40x faster (typically 10-24x)
128: - Security: No secret exposure, validation before commit
129: 
130: ### 10. Operational Model
131: - **Development modes**: Research, Generation, Execution, Validation
132: - **Error handling**: L1 auto-fix, L2 analyze+fix, L3 debug, escalate after 3 attempts
133: 
134: ## Critical References
135: 
136: **Complete Documentation Suite**: `docs/research/00-index.md`
137: **Foundations**: `docs/research/02-context-engineering-foundations.md`
138: **PRP System**: `docs/research/01-prp-system.md`
139: **MCP Orchestration**: `docs/research/03-mcp-orchestration.md`
140: **Validation**: `docs/research/08-validation-testing.md`
141: **Best Practices**: `docs/research/09-best-practices-antipatterns.md`
142: 
143: ## Usage
144: 
145: **When to reference**:
146: - Designing new features (check architecture alignment)
147: - Writing PRPs (follow structure and information density requirements)
148: - Implementing validation (understand 4-level gates)
149: - Debugging issues (check operational model and error handling)
150: - Understanding performance claims (review metrics methodology)
151: 
152: **What NOT to trust blindly**:
153: - üîú Planned features may not be implemented yet
154: - Check `tools/README.md` for current implementation status
155: - Performance metrics (Section 8) mix research-backed claims + internal observations
156: - 36x speedup (Section 8.1) is exceptional outlier, not typical
157: 
158: ## Metadata
159: 
160: **Version**: 1.0
161: **Type**: Model Specification
162: **Status**: Active
163: **Last Updated**: 2025-10-12
164: **Maintainer**: Context Engineering Team
</file>

<file path=".serena/memories/task-completion-checklist.md">
 1: ---
 2: type: regular
 3: category: documentation
 4: tags: [checklist, workflow, quality]
 5: created: "2025-11-04T17:30:00Z"
 6: updated: "2025-11-04T17:30:00Z"
 7: ---
 8: 
 9: # Task Completion Checklist
10: 
11: ## When a Task is Completed
12: 
13: ### 1. Code Quality Checks
14: - ‚úÖ No fishy fallbacks or silent failures
15: - ‚úÖ All mocks marked with FIXME in production code
16: - ‚úÖ Functions under 50 lines, files under 500 lines
17: - ‚úÖ Business-focused naming (no version references)
18: - ‚úÖ Proper docstrings with type hints
19: - ‚úÖ Exception handling with troubleshooting guidance
20: 
21: ### 2. Testing
22: ```bash
23: cd tools
24: uv run pytest tests/ -v           # Run all tests
25: ```
26: - ‚úÖ All tests pass
27: - ‚úÖ Tests use real functionality (no fake results)
28: - ‚úÖ New functionality has corresponding tests
29: - ‚úÖ No regressions introduced
30: 
31: ### 3. Documentation
32: - ‚úÖ Update README.md if user-facing changes
33: - ‚úÖ Update CLAUDE.md if workflow changes
34: - ‚úÖ Docstrings updated for modified functions
35: - ‚úÖ Comments added for complex logic
36: 
37: ### 4. Git Operations (if committing)
38: ```bash
39: cd tools
40: uv run ce git status              # Check status
41: git add <files>                   # Stage changes
42: git commit -m "message"           # Commit with clear message
43: ```
44: 
45: ### 5. Validation Gates (if applicable)
46: ```bash
47: cd tools
48: uv run ce validate --level all    # Run all validation gates
49: ```
50: 
51: ### 6. Quick Smoke Test
52: ```bash
53: cd tools
54: uv run ce --help                  # Verify CLI works
55: uv run ce validate --level 1      # Quick validation
56: ```
57: 
58: ## Adding New Function Workflow
59: 1. Write function with docstring
60: 2. Add exception handling with troubleshooting guidance
61: 3. Write test that calls REAL function (no mocks)
62: 4. Run tests: `uv run pytest tests/ -v`
63: 5. Update README if user-facing
64: 
65: ## Fixing Bug Workflow
66: 1. Write test that reproduces bug (should fail)
67: 2. Fix the bug
68: 3. Run tests (should pass now)
69: 4. Verify no regressions: `uv run pytest tests/ -v`
70: 
71: ## Package Management Reminder
72: ```bash
73: # ‚úÖ REQUIRED
74: uv add package-name              # Add production dependency
75: uv add --dev package-name        # Add development dependency
76: uv sync                          # Install dependencies
77: 
78: # ‚ùå FORBIDDEN
79: # Manual pyproject.toml editing
80: ```
81: 
82: ## Pre-Commit Checklist
83: - [ ] Tests pass (`uv run pytest tests/ -v`)
84: - [ ] No fishy fallbacks introduced
85: - [ ] Exception handling includes troubleshooting
86: - [ ] Documentation updated if needed
87: - [ ] No version-specific naming
88: - [ ] UV package management used (not manual edits)
</file>

<file path=".serena/memories/testing-standards.md">
 1: ---
 2: type: regular
 3: category: documentation
 4: tags: [testing, standards, tdd]
 5: created: "2025-11-04T17:30:00Z"
 6: updated: "2025-11-04T17:30:00Z"
 7: ---
 8: 
 9: # Testing Standards
10: 
11: ## Testing Philosophy
12: **Real Functionality Testing - ZERO TOLERANCE FOR FAKE RESULTS**
13: 
14: The point of test/validation logic is to test REAL functionality using REAL values printing REAL results.
15: 
16: ## Real Testing Required
17: ```python
18: # ‚úÖ GOOD - Tests real function
19: def test_git_status():
20:     status = git_status()  # Real call
21:     assert "clean" in status
22:     assert isinstance(status["staged"], list)
23: 
24: # ‚ùå BAD - Mocked result
25: def test_git_status():
26:     status = {"clean": True}  # FAKE!
27:     assert status["clean"]
28: ```
29: 
30: ## Exception Testing
31: ```python
32: # ‚úÖ GOOD - Real exception handling
33: try:
34:     result = real_function()
35:     print(f"‚úÖ Real result: {result}")
36: except Exception as e:
37:     print(f"‚ùå Function FAILED: {e}")
38:     print("üîß Troubleshooting: Check configuration...")
39:     raise  # MANDATORY - Always throw real exceptions
40: 
41: # ‚ùå FORBIDDEN - Fake success
42: print("‚úÖ Test passed")  # While actual test failed
43: result = {"success": True}  # Fake fallback when real function fails
44: ```
45: 
46: ## Test Running
47: ```bash
48: # Navigate to tools directory
49: cd tools
50: 
51: # Run all tests
52: uv run pytest tests/ -v
53: 
54: # Run specific test file
55: uv run pytest tests/test_core.py -v
56: 
57: # Run specific test
58: uv run pytest tests/test_core.py::test_run_cmd_success -v
59: 
60: # Quick test run (fail fast, minimal output)
61: uv run pytest tests/ -x --tb=short -q
62: ```
63: 
64: ## Test Organization
65: ```
66: tests/
67: ‚îú‚îÄ‚îÄ test_cli.py         # CLI interface tests
68: ‚îú‚îÄ‚îÄ test_core.py        # Core functionality tests
69: ‚îú‚îÄ‚îÄ test_validate.py    # Validation gate tests
70: ‚îî‚îÄ‚îÄ test_context.py     # Context management tests
71: ```
72: 
73: ## Pytest Configuration
74: Located in `tools/pyproject.toml`:
75: ```toml
76: [tool.pytest.ini_options]
77: testpaths = ["tests"]
78: python_files = "test_*.py"
79: python_functions = "test_*"
80: addopts = "-v --tb=short"
81: ```
82: 
83: ## TDD Approach (Pragmatic)
84: 1. **Write test first** - Define expected behavior
85: 2. **Watch it fail** - Ensure test validates logic
86: 3. **Write minimal code** - Just enough to make test pass
87: 4. **Refactor** - Improve while keeping tests green
88: 5. **CRITICAL**: Tests must invoke actual functionality - no mocks unless explicit
89: 
90: ## Key Principles
91: - Tests must use real methods
92: - No hardcoded success messages
93: - No fake metrics or percentages
94: - Let real failures throw with troubleshooting messages
95: - **Exception is exception and MUST be thrown**
</file>

<file path=".serena/memories/tool-config-optimization-completed.md">
 1: ---
 2: type: regular
 3: category: configuration
 4: tags: [tools, optimization, config]
 5: created: "2025-11-04T17:30:00Z"
 6: updated: "2025-11-04T17:30:00Z"
 7: ---
 8: 
 9: # Tool Configuration Optimization - Completion Record
10: 
11: **Completed**: 2025-10-17
12: 
13: ## What Was Done
14: 
15: Addressed all 7 violations from tools-misuse-test-report.md with comprehensive documentation and structured metadata:
16: 
17: ### Files Modified/Created
18: 
19: 1. **examples/tool-usage-patterns.md** - Enhanced with 4 sections:
20:    - Quick Reference: Common Violations (table with 8 denied patterns)
21:    - Troubleshooting: Permission Denied Errors (with immediate fixes)
22:    - Real Production Examples (4 complete before/after examples)
23:    - Performance Benchmarks (measured improvements: 10-50x for most operations)
24: 
25: 2. **.ce/tool-alternatives.yml** - New metadata file:
26:    - 18 denied tools documented
27:    - 32 alternatives provided
28:    - Detection patterns for automation
29:    - Severity levels (block/warn)
30: 
31: ## Violations Covered
32: 
33: **Bash Anti-patterns (6)**:
34: - Bash(cat), head, tail, grep, awk, wc, sed, echo, find, python
35: 
36: **Denied Tools (1)**:
37: - mcp__serena__replace_symbol_body
38: 
39: ## Key Metrics
40: 
41: - All violations have immediate remedies with examples
42: - Performance impact quantified (10-50x faster with alternatives)
43: - 60-80% token savings per operation
44: - 100% automation readiness (detection patterns + alternatives)
45: 
46: ## Integration Points Ready
47: 
48: - ‚úÖ settings.local.json deny list (blocks violations)
49: - ‚úÖ tool-usage-patterns.md (guides agents away from violations)
50: - ‚è≥ tools-misuse-scan --remediate mode (ready to consume alternatives.yml)
51: - ‚è≥ Pre-commit hooks (detection patterns ready)
52: 
53: ## Status
54: 
55: - **Phase 1**: ‚úÖ Policy Enforcement (COMPLETE)
56: - **Phase 2**: ‚úÖ Documentation & Guidance (COMPLETE - THIS SESSION)
57: - **Phase 3**: ‚è≥ Auto-Remediation (PENDING - needs tools-misuse-scan implementation)
58: - **Phase 4**: üîÆ Continuous Monitoring (FUTURE)
59: 
60: ## Usage
61: 
62: For agents: Reference `examples/tool-usage-patterns.md` sections:
63: - Quick Reference table for fast lookup
64: - Troubleshooting section for error resolution
65: - Real Production Examples for patterns
66: 
67: For automation: Load `.ce/tool-alternatives.yml` for:
68: - Detection pattern matching
69: - Alternative suggestions
70: - Performance impact lookup
</file>

<file path=".serena/memories/tool-usage-syntropy.md">
  1: ---
  2: type: regular
  3: category: documentation
  4: tags: [syntropy, mcp, tools]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # Syntropy MCP Tool Usage - Updated Reference
 10: 
 11: **Purpose**: Fast agent tool selection using Syntropy MCP aggregation layer
 12: 
 13: **Target Audience**: AI agents working with Context Engineering codebase
 14: 
 15: **Last Updated**: 2025-10-20
 16: 
 17: **Status**: ‚úÖ ACTIVE - All legacy tools now routed through Syntropy unified interface
 18: 
 19: ---
 20: 
 21: ## Overview: Syntropy Tool Aggregation
 22: 
 23: **What changed**: All MCP tools now route through Syntropy MCP server with unified interface
 24: 
 25: **Permissions Format**: `mcp__syntropy__<server>_<tool>` (for settings.local.json)
 26: **Actual Callable**: `mcp__syntropy___<server>__<tool>` (double underscores between components)
 27: 
 28: **Examples**:
 29: - `mcp__syntropy__serena_find_symbol` (was: `mcp__serena__find_symbol`)
 30: - `mcp__syntropy__filesystem_read_text_file` (was: `mcp__filesystem__read_text_file`)
 31: - `mcp__syntropy__git_git_status` (was: `mcp__git__git_status`)
 32: 
 33: **Benefit**: Single MCP server managing 6 underlying servers with connection pooling and lifecycle management
 34: 
 35: ---
 36: 
 37: ## Code Navigation & Analysis
 38: 
 39: ### Find function/class definition
 40: 
 41: **USE**: `mcp__syntropy__serena_find_symbol`
 42: 
 43: ```python
 44: # Find function by name
 45: find_symbol(name_path="authenticate_user", include_body=True)
 46: 
 47: # Find method in class
 48: find_symbol(name_path="UserAuth/validate", include_body=True)
 49: ```
 50: 
 51: **When**: You know the symbol name and want to see its implementation
 52: 
 53: ### Understand file structure
 54: 
 55: **USE**: `mcp__syntropy__serena_get_symbols_overview`
 56: 
 57: ```python
 58: # Get top-level overview
 59: get_symbols_overview(path="src/auth.py")
 60: ```
 61: 
 62: **When**: First time exploring a file, want to see all classes/functions
 63: 
 64: ### Search for pattern in codebase
 65: 
 66: **USE**: `mcp__syntropy__serena_search_for_pattern`
 67: 
 68: ```python
 69: # Find async functions
 70: search_for_pattern(pattern="async def.*authenticate", path="src/")
 71: 
 72: # Find specific error handling
 73: search_for_pattern(pattern="except.*ValueError", path="src/")
 74: ```
 75: 
 76: **When**: Searching for code patterns, not specific symbol names
 77: 
 78: ### Find all usages of function
 79: 
 80: **USE**: `mcp__syntropy__serena_find_referencing_symbols`
 81: 
 82: ```python
 83: # Find everywhere validate_token is called
 84: find_referencing_symbols(name_path="validate_token", path="src/auth.py")
 85: ```
 86: 
 87: **When**: Understanding dependencies, impact analysis before changes
 88: 
 89: ---
 90: 
 91: ## File Operations
 92: 
 93: ### Read file contents
 94: 
 95: **USE**:
 96: - `mcp__syntropy__filesystem_read_text_file` - For config files, markdown, non-code
 97: - `mcp__syntropy__serena_read_file` - For Python/code files (indexed by LSP)
 98: 
 99: ```python
100: # Read config file
101: read_text_file(path=".ce/tool-inventory.yml")
102: 
103: # Read Python module
104: read_file(relative_path="ce/core.py")
105: ```
106: 
107: **When**: Need to read entire file contents
108: 
109: ### List directory contents
110: 
111: **USE**: `mcp__syntropy__filesystem_list_directory`
112: 
113: ```python
114: list_directory(path="examples/")
115: ```
116: 
117: **When**: Exploring directory structure, finding files
118: 
119: ### Find files by pattern
120: 
121: **USE**: `mcp__syntropy__filesystem_search_files`
122: 
123: ```python
124: # Find all test files
125: search_files(path="tests", pattern="test_*.py")
126: ```
127: 
128: **When**: Finding files matching specific naming pattern
129: 
130: ### Edit file with line-based changes
131: 
132: **USE**: `mcp__syntropy__filesystem_edit_file`
133: 
134: ```python
135: edit_file(
136:     path="ce/config.py",
137:     edits=[{
138:         "oldText": "debug = False",
139:         "newText": "debug = True"
140:     }]
141: )
142: ```
143: 
144: **When**: Making precise line-by-line edits to existing files
145: 
146: ### Write new file
147: 
148: **USE**: `mcp__syntropy__serena_create_text_file`
149: 
150: ```python
151: create_text_file(
152:     relative_path="new_module.py",
153:     body="# New module\ndef hello():\n    pass"
154: )
155: ```
156: 
157: **When**: Creating new files in the project
158: 
159: ---
160: 
161: ## Version Control
162: 
163: ### Check git status
164: 
165: **USE**: `mcp__syntropy__git_git_status`
166: 
167: ```python
168: git_status(repo_path=".")
169: ```
170: 
171: **When**: Check working directory status before commits
172: 
173: ### View recent changes
174: 
175: **USE**: `mcp__syntropy__git_git_diff`
176: 
177: ```python
178: git_diff(repo_path=".", target="HEAD")
179: ```
180: 
181: **When**: Review changes before committing
182: 
183: ### See commit history
184: 
185: **USE**: `mcp__syntropy__git_git_log`
186: 
187: ```python
188: git_log(repo_path=".", max_count=10)
189: ```
190: 
191: **When**: Understanding recent commits, finding commit messages
192: 
193: ### Stage and commit changes
194: 
195: **USE**: `mcp__syntropy__git_git_add` + `mcp__syntropy__git_git_commit`
196: 
197: ```python
198: # Stage files
199: git_add(repo_path=".", files=["ce/core.py", "tests/test_core.py"])
200: 
201: # Commit with message
202: git_commit(repo_path=".", message="feat: add new feature")
203: ```
204: 
205: **When**: Creating commits during implementation
206: 
207: ---
208: 
209: ## Documentation Lookup
210: 
211: ### Get library documentation
212: 
213: **USE**: `mcp__syntropy__context7_resolve_library_id` + `mcp__syntropy__context7_get_library_docs`
214: 
215: ```python
216: # Step 1: Resolve library ID
217: lib_id = mcp__syntropy__context7_resolve_library_id(libraryName="pytest")
218: 
219: # Step 2: Get docs
220: docs = mcp__syntropy__context7_get_library_docs(
221:     context7CompatibleLibraryID=lib_id,
222:     topic="fixtures"
223: )
224: ```
225: 
226: **When**: Need external library documentation, API references
227: 
228: ---
229: 
230: ## Complex Reasoning
231: 
232: ### Multi-step problem decomposition
233: 
234: **USE**: `mcp__syntropy__thinking_sequentialthinking`
235: 
236: ```python
237: sequentialthinking(
238:     thought="First, I need to understand the authentication flow",
239:     thoughtNumber=1,
240:     totalThoughts=5,
241:     nextThoughtNeeded=True
242: )
243: ```
244: 
245: **When**: Complex problems requiring step-by-step reasoning
246: 
247: ---
248: 
249: ## Codebase Packaging
250: 
251: ### Pack entire codebase for AI
252: 
253: **USE**: `mcp__syntropy__repomix_pack_codebase`
254: 
255: ```python
256: pack_codebase(
257:     output="codebase.txt"
258: )
259: ```
260: 
261: **When**: Need to package entire codebase for analysis, sharing with other AI systems
262: 
263: ---
264: 
265: ## Project Management
266: 
267: ### Create/update Linear issue
268: 
269: **USE**: `mcp__syntropy__linear_create_issue` / `mcp__syntropy__linear_update_issue`
270: 
271: ```python
272: # Create issue
273: create_issue(
274:     team="Blaise78",
275:     title="PRP-18: Tool Configuration",
276:     description="Optimize tool usage...",
277:     assignee="blazej.przybyszewski@gmail.com"
278: )
279: 
280: # Update issue
281: update_issue(
282:     issue_number="BLA-123",
283:     state="in_progress"
284: )
285: ```
286: 
287: **When**: Creating tasks, updating issue status
288: 
289: ### List issues
290: 
291: **USE**: `mcp__syntropy__linear_list_issues`
292: 
293: ```python
294: list_issues(
295:     team="Blaise78",
296:     assignee="me",
297:     state="in_progress"
298: )
299: ```
300: 
301: **When**: Finding assigned work, checking project status
302: 
303: ---
304: 
305: ## Syntropy-Specific Benefits
306: 
307: ### Single Connection Point
308: 
309: Instead of managing 6 separate MCP server connections:
310: - ‚úÖ Single Syntropy server manages all connections
311: - ‚úÖ Connection pooling reduces overhead
312: - ‚úÖ Lazy initialization spawns servers on first use
313: - ‚úÖ Automatic cleanup on session end
314: 
315: ### Unified Error Handling
316: 
317: All tool calls include:
318: - ‚úÖ Structured logging with timing
319: - ‚úÖ Clear error messages with troubleshooting
320: - ‚úÖ Consistent exception handling across all servers
321: 
322: ### Extensible Architecture
323: 
324: New MCP servers can be added by:
325: 1. Adding server config to `/syntropy-mcp/servers.json`
326: 2. Updating tool list in `/syntropy-mcp/src/index.ts`
327: 3. No Claude Code configuration changes needed!
328: 
329: ---
330: 
331: ## Tool Selection Decision Tree
332: 
333: ```
334: Need to work with code?
335: ‚îú‚îÄ Know symbol name? ‚Üí mcp__syntropy__serena_find_symbol
336: ‚îú‚îÄ Exploring file? ‚Üí mcp__syntropy__serena_get_symbols_overview
337: ‚îú‚îÄ Pattern search? ‚Üí mcp__syntropy__serena_search_for_pattern
338: ‚îî‚îÄ Find usages? ‚Üí mcp__syntropy__serena_find_referencing_symbols
339: 
340: Need to read file?
341: ‚îú‚îÄ Code file? ‚Üí mcp__syntropy__serena_read_file
342: ‚îî‚îÄ Config/text? ‚Üí mcp__syntropy__filesystem_read_text_file
343: 
344: Need to write/edit?
345: ‚îú‚îÄ Create file? ‚Üí mcp__syntropy__serena_create_text_file
346: ‚îî‚îÄ Edit existing? ‚Üí mcp__syntropy__filesystem_edit_file
347: 
348: Need git operation?
349: ‚îî‚îÄ Use mcp__syntropy__git_* tools (status, diff, log, add, commit)
350: 
351: Need external docs?
352: ‚îî‚îÄ Use mcp__syntropy__context7_* tools
353: 
354: Need complex reasoning?
355: ‚îî‚îÄ Use mcp__syntropy__thinking_sequentialthinking
356: 
357: Need project management?
358: ‚îî‚îÄ Use mcp__syntropy__linear_* tools
359: 
360: Need codebase packaging?
361: ‚îî‚îÄ Use mcp__syntropy__repomix_pack_codebase
362: ```
363: 
364: ---
365: 
366: ## Migration from Legacy Tools
367: 
368: | Old Tool | New Tool (Syntropy) | Notes |
369: |----------|-------------------|-------|
370: | `mcp__serena__find_symbol` | `mcp__syntropy__serena_find_symbol` | Same functionality, unified MCP |
371: | `mcp__filesystem__read_text_file` | `mcp__syntropy__filesystem_read_text_file` | Same functionality, unified MCP |
372: | `mcp__git__git_status` | `mcp__syntropy__git_git_status` | Same functionality, unified MCP |
373: | `mcp__context7__get_library_docs` | `mcp__syntropy__context7_get_library_docs` | Same functionality, unified MCP |
374: | `mcp__sequential-thinking__sequentialthinking` | `mcp__syntropy__thinking_sequentialthinking` | Same functionality, unified MCP |
375: | `mcp__linear-server__create_issue` | `mcp__syntropy__linear_create_issue` | Same functionality, unified MCP |
376: | `mcp__repomix__pack_codebase` | `mcp__syntropy__repomix_pack_codebase` | Same functionality, unified MCP |
377: 
378: **All tools now route through Syntropy MCP server for unified management and connection pooling.**
379: 
380: ---
381: 
382: ## Performance & Reliability
383: 
384: ### Connection Pooling Benefits
385: - First call: ~1-2 seconds (server spawn)
386: - Subsequent calls: <50ms (connection reused)
387: - Automatic cleanup on session end
388: - No resource leaks
389: 
390: ### Error Handling
391: - All errors include troubleshooting guidance
392: - Graceful fallback on connection failure
393: - Structured logging for debugging
394: - Clear error messages with context
395: 
396: ---
397: 
398: ## Current Permission Configuration
399: 
400: **Status**: Updated for Syntropy MCP (2025-10-20)
401: 
402: ### Allow List - Syntropy Tools
403: ```
404: mcp__syntropy__serena_*
405: mcp__syntropy__filesystem_*
406: mcp__syntropy__git_*
407: mcp__syntropy__context7_*
408: mcp__syntropy__thinking_*
409: mcp__syntropy__linear_*
410: mcp__syntropy__repomix_*
411: ```
412: 
413: All legacy `mcp__*` permissions replaced with Syntropy unified interface.
414: 
415: ---
416: 
417: ## Support & Troubleshooting
418: 
419: ### Tool Not Found
420: - Restart Claude Code to refresh tool list
421: - Check Syntropy MCP server is connected (visible in `/mcp` menu)
422: - Verify tool name format: `mcp__syntropy__<server>_<tool>` (permissions)
423: 
424: ### Connection Timeout
425: - First connection may take 1-2 seconds (server spawn)
426: - Subsequent calls should be <50ms
427: - If timeout persists, check server configuration in `/syntropy-mcp/servers.json`
428: 
429: ### Server Configuration
430: - Located at: `/Users/bprzybysz/nc-src/ctx-eng-plus/syntropy-mcp/servers.json`
431: - Spawn command format: `npx` or `uvx`
432: - All servers use lazy initialization (spawn on first use)
</file>

<file path=".serena/memories/use-syntropy-tools-not-bash.md">
  1: ---
  2: type: regular
  3: category: documentation
  4: tags: [syntropy, tools, guidelines]
  5: created: "2025-11-04T17:30:00Z"
  6: updated: "2025-11-04T17:30:00Z"
  7: ---
  8: 
  9: # Use Syntropy MCP Tools Instead of Bash Commands
 10: 
 11: ## Core Principle
 12: 
 13: **ALWAYS prefer Syntropy MCP tools over bash commands** - they provide the same functionality with better error handling, permissions management, and context awareness.
 14: 
 15: ## Common Replacements
 16: 
 17: ### File Operations
 18: 
 19: ‚ùå **DON'T USE BASH:**
 20: ```bash
 21: cat file.txt
 22: ls -la directory/
 23: find . -name "*.py"
 24: mkdir new_dir
 25: mv file1.txt file2.txt
 26: ```
 27: 
 28: ‚úÖ **USE SYNTROPY:**
 29: ```python
 30: # Read file
 31: mcp__syntropy_filesystem_read_text_file(path="file.txt")
 32: 
 33: # List directory
 34: mcp__syntropy_filesystem_list_directory(path="directory/")
 35: 
 36: # Search files
 37: mcp__syntropy_filesystem_search_files(pattern="*.py")
 38: 
 39: # Get directory tree
 40: mcp__syntropy_filesystem_directory_tree(path=".", max_depth=3)
 41: 
 42: # Get file info
 43: mcp__syntropy_filesystem_get_file_info(path="file.txt")
 44: ```
 45: 
 46: ### Git Operations
 47: 
 48: ‚ùå **DON'T USE BASH:**
 49: ```bash
 50: git status
 51: git diff
 52: git log -5
 53: git add file.txt
 54: git commit -m "message"
 55: ```
 56: 
 57: ‚úÖ **USE SYNTROPY:**
 58: ```python
 59: # Git status
 60: mcp__syntropy_git_git_status(repo_path="/path/to/repo")
 61: 
 62: # Git diff
 63: mcp__syntropy_git_git_diff(repo_path="/path/to/repo", staged=False)
 64: 
 65: # Git log
 66: mcp__syntropy_git_git_log(repo_path="/path/to/repo", max_count=5)
 67: 
 68: # Git add
 69: mcp__syntropy_git_git_add(repo_path="/path/to/repo", paths=["file.txt"])
 70: 
 71: # Git commit
 72: mcp__syntropy_git_git_commit(repo_path="/path/to/repo", message="message")
 73: ```
 74: 
 75: ### Code Navigation
 76: 
 77: ‚ùå **DON'T USE BASH:**
 78: ```bash
 79: grep -r "function_name" .
 80: find . -name "class_name"
 81: ```
 82: 
 83: ‚úÖ **USE SYNTROPY:**
 84: ```python
 85: # Find symbol by name
 86: mcp__syntropy_serena_find_symbol(name_path="ClassName.method_name", include_body=True)
 87: 
 88: # Search for pattern
 89: mcp__syntropy_serena_search_for_pattern(pattern="def function_.*", file_glob="**/*.py")
 90: 
 91: # Get file structure
 92: mcp__syntropy_serena_get_symbols_overview(relative_path="module/file.py")
 93: 
 94: # Find references
 95: mcp__syntropy_serena_find_referencing_symbols(name_path="function_name")
 96: ```
 97: 
 98: ## Why Syntropy Tools Are Better
 99: 
100: 1. **Permissions**: Already approved in settings.local.json
101: 2. **Error Handling**: Consistent error responses with context
102: 3. **Type Safety**: Structured JSON responses vs parsing text
103: 4. **Performance**: Optimized for Claude Code integration
104: 5. **Cross-Platform**: Works identically on macOS/Linux/Windows
105: 6. **No Bash Approval**: Bash commands require explicit permission patterns
106: 
107: ## Allowed Bash Commands (Exceptions)
108: 
109: **ONLY use bash for these specific patterns:**
110: 
111: 1. **UV package management**:
112:    - `uv run`, `uv add`, `uvx`, `uv run pytest`
113: 
114: 2. **Environment variables**:
115:    - `env`, `export`
116: 
117: 3. **Git operations covered by permissions**:
118:    - `git diff-tree` (specific case)
119: 
120: 4. **System utilities**:
121:    - `brew install` (package installation)
122:    - `ps` (process listing)
123: 
124: 5. **MCP auth reset**:
125:    - `rm -rf ~/.mcp-auth` (specific permission)
126: 
127: 6. **Text utilities** (when no alternative):
128:    - `head`, `tail`, `cat`, `grep`, `wc`, `echo`
129: 
130: ## Migration Examples
131: 
132: ### Example 1: Check Git Status
133: 
134: **Before:**
135: ```bash
136: cd /path/to/repo && git status
137: ```
138: 
139: **After:**
140: ```python
141: mcp__syntropy_git_git_status(repo_path="/path/to/repo")
142: ```
143: 
144: ### Example 2: List Directory
145: 
146: **Before:**
147: ```bash
148: ls -la /path/to/dir
149: ```
150: 
151: **After:**
152: ```python
153: mcp__syntropy_filesystem_list_directory(path="/path/to/dir")
154: ```
155: 
156: ### Example 3: Find Code Pattern
157: 
158: **Before:**
159: ```bash
160: grep -r "def process_" tools/ce/
161: ```
162: 
163: **After:**
164: ```python
165: mcp__syntropy_serena_search_for_pattern(
166:     pattern="def process_",
167:     file_glob="tools/ce/**/*.py"
168: )
169: ```
170: 
171: ### Example 4: Read File
172: 
173: **Before:**
174: ```bash
175: cat /path/to/file.txt
176: ```
177: 
178: **After:**
179: ```python
180: mcp__syntropy_filesystem_read_text_file(path="/path/to/file.txt")
181: ```
182: 
183: ## Implementation Priority
184: 
185: **For PRP-27 follow-up work:**
186: 
187: 1. Replace all `ls` with `mcp__syntropy_filesystem_list_directory`
188: 2. Replace all `cat` with `mcp__syntropy_filesystem_read_text_file`
189: 3. Replace all `git status/diff/log` with Syntropy git tools
190: 4. Replace all `grep -r` with Serena search tools
191: 5. Keep only approved bash patterns (uv, env, specific git, brew)
192: 
193: ## Benefits Demonstrated in PRP-27
194: 
195: The cache-based architecture for Syntropy status hook **proves this pattern works**:
196: 
197: - Fast execution (~200ms vs 2-3s for bash equivalents)
198: - Structured JSON responses (no parsing needed)
199: - Consistent error handling (no shell exit code guessing)
200: - Works in hook context (bash has permission issues)
201: 
202: ## Related
203: 
204: - **PRP**: PRPs/executed/PRP-27-syntropy-status-hook.md
205: - **Policy**: CLAUDE.md (project guidelines)
206: - **Permissions**: .claude/settings.local.json (lines 31-90)
207: - **Tool Reference**: CLAUDE.md "Syntropy Tools Reference" section
</file>

<file path="examples/model/SystemModel.md">
   1: # Context Engineering Management System
   2: 
   3: **Version:** 1.0
   4: **Type:** System Model Documentation
   5: **Purpose:** Formal specification of autonomous AI-driven development framework
   6: 
   7: > **‚ö†Ô∏è Model Document Notice**
   8: >
   9: > This document describes the **target architecture** and **design specification** for the Context Engineering Management system. Features marked with üîú indicate planned capabilities not yet fully implemented. Refer to individual tool documentation ([tools/README.md](../tools/README.md)) for current implementation status.
  10: >
  11: > **Implementation Status:**
  12: >
  13: > - ‚úÖ **Implemented:** Core validation (L1-L3), git operations, context management, run_py tool
  14: > - üîú **Planned:** PRP-aware state management, L4 pattern conformance automation, drift tracking commands
  15: >
  16: > Performance metrics represent a mix of research-backed claims (cited) and internal observations (marked as such). See [Section 8](#8-performance-metrics) for methodology details.
  17: 
  18: ---
  19: 
  20: ## 1. System Overview
  21: 
  22: ### 1.1 Definition
  23: 
  24: **Context Engineering Management** is a systematic framework for autonomous AI-driven software development that achieves 10-100x improvement over prompt engineering through complete context provision (baseline: 10x via structured prompts, up to 100x with full MCP integration and self-healing). The system eliminates hallucinations by treating missing context as compilation errors, enabling AI agents to deliver production-ready code without human intervention during implementation.
  25: 
  26: **Performance Claims:**
  27: 
  28: - **Research-Backed Baseline:** Traditional AI code generation achieves 35-45% success rate (GitHub Copilot evaluation studies)
  29: - **Internal Observations (n=4 case studies):** Context Engineering framework achieves 85-97% success rate and 10-24x productivity improvement for production features
  30: - **Exceptional Cases:** Up to 100x speedup when combining context engineering + Serena MCP + self-healing (documented in Section 8.1 case study)
  31: 
  32: See [Section 8](#8-performance-metrics) for detailed methodology and case studies.
  33: 
  34: ### 1.2 Core Principle: Context-as-Compiler
  35: 
  36: | Traditional Compiler | Context Engineering |
  37: |---------------------|---------------------|
  38: | Source code ‚Üí Executable | Requirements ‚Üí Production code |
  39: | Missing headers ‚Üí Compile error | Missing context ‚Üí Hallucination |
  40: | Type checking | Validation gates |
  41: | Linker errors | Integration failures |
  42: | Build output | Autonomous implementation |
  43: 
  44: **Key Insight:** Complete context provision is necessary and sufficient for reliable AI code generation.
  45: 
  46: ### 1.3 System Components
  47: 
  48: ```mermaid
  49: graph TB
  50:     A["Context Engineering Management"]
  51: 
  52:     A --> B["PRP System"]
  53:     B --> B1["Structured specifications"]
  54:     B --> B2["Self-healing templates"]
  55:     B --> B3["Validation gates"]
  56: 
  57:     A --> C["Four Pillars Architecture"]
  58:     C --> C1["WRITE - Persistence"]
  59:     C --> C2["SELECT - Retrieval"]
  60:     C --> C3["COMPRESS - Efficiency"]
  61:     C --> C4["ISOLATE - Safety"]
  62: 
  63:     A --> D["Tool Ecosystem"]
  64:     D --> D1["run_py - Execution"]
  65:     D --> D2["ce CLI - Operations"]
  66:     D --> D3["MCP Integration"]
  67: 
  68:     A --> E["Quality Framework"]
  69:     E --> E1["4-level validation (L1-L4)"]
  70:     E --> E2["Self-healing loops"]
  71:     E --> E3["Confidence scoring"]
  72: 
  73:     style A fill:#e3f2fd,color:#000
  74:     style B fill:#fff8e1,color:#000
  75:     style C fill:#f3e5f5,color:#000
  76:     style D fill:#b2ebf2,color:#000
  77:     style E fill:#ffe0b2,color:#000
  78:     style B1 fill:#fff9c4,color:#000
  79:     style B2 fill:#fff9c4,color:#000
  80:     style B3 fill:#fff9c4,color:#000
  81:     style C1 fill:#e1f5fe,color:#000
  82:     style C2 fill:#e1f5fe,color:#000
  83:     style C3 fill:#e1f5fe,color:#000
  84:     style C4 fill:#e1f5fe,color:#000
  85:     style D1 fill:#b2dfdb,color:#000
  86:     style D2 fill:#b2dfdb,color:#000
  87:     style D3 fill:#b2dfdb,color:#000
  88:     style E1 fill:#ffecb3,color:#000
  89:     style E2 fill:#ffecb3,color:#000
  90:     style E3 fill:#ffecb3,color:#000
  91: ```
  92: 
  93: ### See Also
  94: 
  95: - [Context Engineering Framework: Complete Documentation Suite](../docs/research/00-index.md) - Overview of all documentation and framework philosophy
  96: - [Context Engineering Foundations](../docs/research/02-context-engineering-foundations.md) - Deep dive into core principles and context-as-compiler mental model
  97: 
  98: ---
  99: 
 100: ## 2. Evolution & Philosophy
 101: 
 102: ### 2.1 Three-Stage Evolution
 103: 
 104: | Stage | Method | Approach | Success Rate | Bottleneck |
 105: |-------|--------|----------|-------------|------------|
 106: | **Stage 1** | Vibe Coding | Trial-and-error prompting | 10-20% | No structure |
 107: | **Stage 2** | Prompt Engineering | Structured prompts with examples | 40-60% | Context scattered |
 108: | **Stage 3** | Context Engineering | Complete context provision | 85-97% | None (systematic) |
 109: 
 110: **Improvement Factor:** 4-9x success rate improvement from Stage 1 to Stage 3 (85-97% vs 10-20%), with corresponding speed improvements through systematic automation.
 111: 
 112: **Methodology Note:** Success rates are based on internal observations from case studies (n=4 PRPs documented). Research-backed baseline (35-45% for traditional AI code generation) from GitHub Copilot evaluation studies. See [References](#references) for peer-reviewed claims vs internal observations.
 113: 
 114: ### 2.2 Context-as-Compiler Mental Model
 115: 
 116: **Traditional Programming:**
 117: 
 118: ```mermaid
 119: graph LR
 120:     A["Source Code + Headers + Libraries"] --> B["Compiler"]
 121:     B --> C["Executable"]
 122:     D["Missing dependency"] -.-> B
 123:     D -.->|"Error"| E["Compilation Fails"]
 124: 
 125:     style A fill:#e3f2fd,color:#000
 126:     style B fill:#fff8e1,color:#000
 127:     style C fill:#c8e6c9,color:#000
 128:     style D fill:#ffccbc,color:#000
 129:     style E fill:#ef9a9a,color:#000
 130: ```
 131: 
 132: **Context Engineering:**
 133: 
 134: ```mermaid
 135: graph LR
 136:     A["Requirements + Context + Patterns"] --> B["AI Agent"]
 137:     B --> C["Production Code"]
 138:     D["Missing context"] -.-> B
 139:     D -.->|"Hallucination"| E["Incorrect Output"]
 140: 
 141:     style A fill:#e3f2fd,color:#000
 142:     style B fill:#fff8e1,color:#000
 143:     style C fill:#c8e6c9,color:#000
 144:     style D fill:#ffccbc,color:#000
 145:     style E fill:#ef9a9a,color:#000
 146: ```
 147: 
 148: **Implication:** Provide complete context upfront, not iteratively.
 149: 
 150: #### 2.2.1 Concrete Mappings
 151: 
 152: | Traditional Compiler | Context Engineering | Consequence |
 153: |---------------------|---------------------|----|
 154: | Missing header file | Missing validation gate | Compilation fails ‚Üí Systematic validation failure |
 155: | Unresolved symbol | Missing MCP context | Linker error ‚Üí Hallucinated implementation |
 156: | Type mismatch | Schema mismatch | Type error ‚Üí Invalid data structure |
 157: | No optimization flags | No quality gates (L1-L4) | Slow build ‚Üí Low confidence code |
 158: | Runtime crash | No self-healing loop | Debug cycle ‚Üí Automatic fixing |
 159: 
 160: **Application:** When designing PRP context, ask: "What information would a compiler need to ensure compilation succeeds?" Then provide it exhaustively.
 161: 
 162: ### 2.3 Philosophical Principles
 163: 
 164: 1. **No Fishy Fallbacks**
 165:    - Fast failure with actionable error messages
 166:    - No silent error masking
 167:    - Exceptions thrown for troubleshooting
 168: 
 169: 2. **KISS (Keep It Simple, Stupid)**
 170:    - Simple solutions over clever code
 171:    - Minimal dependencies
 172:    - Direct implementation
 173: 
 174: 3. **Real Functionality Testing**
 175:    - No mocks in production code
 176:    - No fake results or hardcoded success messages
 177:    - Real values, real validation
 178: 
 179: 4. **Strict Enforcement**
 180:    - 3 LOC limit for ad-hoc code (non-negotiable)
 181:    - UV package management (no manual edits)
 182:    - All validation gates must pass (10/10 confidence required - includes L4 pattern conformance)
 183: 
 184: ### See Also
 185: 
 186: - [Context Engineering Foundations](../docs/research/02-context-engineering-foundations.md) - Detailed explanation of three-stage evolution and context-as-compiler philosophy
 187: - [Best Practices and Anti-Patterns](../docs/research/09-best-practices-antipatterns.md) - Comprehensive coverage of KISS, No Fishy Fallbacks, and Real Functionality Testing principles
 188: 
 189: ---
 190: 
 191: ## 3. Architecture
 192: 
 193: ### 3.1 Four Pillars
 194: 
 195: ```mermaid
 196: graph LR
 197:     A["Context Engineering"] --> B["WRITE"]
 198:     A --> C["SELECT"]
 199:     A --> D["COMPRESS"]
 200:     A --> E["ISOLATE"]
 201: 
 202:     B --> B1["Serena Memories"]
 203:     B --> B2["Git Checkpoints"]
 204:     B --> B3["Validation Results"]
 205: 
 206:     C --> C1["find_symbol"]
 207:     C --> C2["search_for_pattern"]
 208:     C --> C3["Context7 Docs"]
 209: 
 210:     D --> D1["Overview-first"]
 211:     D --> D2["Targeted Reads"]
 212:     D --> D3["Token Efficiency"]
 213: 
 214:     E --> E1["Validation Gates"]
 215:     E --> E2["Checkpoints"]
 216:     E --> E3["Error Boundaries"]
 217: 
 218:     style A fill:#e3f2fd,color:#000
 219:     style B fill:#fff8e1,color:#000
 220:     style C fill:#f3e5f5,color:#000
 221:     style D fill:#b2ebf2,color:#000
 222:     style E fill:#ffe0b2,color:#000
 223:     style B1 fill:#fff9c4,color:#000
 224:     style B2 fill:#fff9c4,color:#000
 225:     style B3 fill:#fff9c4,color:#000
 226:     style C1 fill:#e1f5fe,color:#000
 227:     style C2 fill:#e1f5fe,color:#000
 228:     style C3 fill:#e1f5fe,color:#000
 229:     style D1 fill:#b2dfdb,color:#000
 230:     style D2 fill:#b2dfdb,color:#000
 231:     style D3 fill:#b2dfdb,color:#000
 232:     style E1 fill:#ffccbc,color:#000
 233:     style E2 fill:#ffccbc,color:#000
 234:     style E3 fill:#ffccbc,color:#000
 235: ```
 236: 
 237: #### 3.1.0 Pillar Interaction Patterns
 238: 
 239: The Four Pillars work together in a continuous cycle:
 240: 
 241: ```mermaid
 242: graph TB
 243:     W["WRITE<br/>Persist insights"] -->|"Enables retrieval from"| S["SELECT<br/>Find context"]
 244:     S -->|"Informs optimization of"| C["COMPRESS<br/>Reduce tokens"]
 245:     C -->|"Respects boundaries from"| I["ISOLATE<br/>Safety gates"]
 246:     I -->|"Validates and stores in"| W
 247: 
 248:     W1["Examples: Memories,<br/>Checkpoints, Logs"] -.-> W
 249:     S1["Examples: find_symbol,<br/>search_for_pattern"] -.-> S
 250:     C1["Examples: Overview-first,<br/>Targeted reads"] -.-> C
 251:     I1["Examples: Validation gates,<br/>Error boundaries"] -.-> I
 252: 
 253:     style W fill:#fff8e1,color:#000
 254:     style S fill:#f3e5f5,color:#000
 255:     style C fill:#b2ebf2,color:#000
 256:     style I fill:#ffe0b2,color:#000
 257:     style W1 fill:#fff9c4,color:#000
 258:     style S1 fill:#e1f5fe,color:#000
 259:     style C1 fill:#b2dfdb,color:#000
 260:     style I1 fill:#ffccbc,color:#000
 261: ```
 262: 
 263: **Usage Patterns:**
 264: 
 265: | Scenario | Pillar Sequence | Outcome |
 266: |----------|-----------------|---------|
 267: | Implementing new feature | WRITE context ‚Üí SELECT similar patterns ‚Üí COMPRESS ‚Üí ISOLATE test | Complete implementation with precedents |
 268: | Debugging failure | SELECT error info ‚Üí ISOLATE root cause ‚Üí WRITE findings ‚Üí COMPRESS learnings | Fast diagnosis with documented patterns |
 269: | Refactoring code | SELECT impact analysis ‚Üí WRITE rollback point ‚Üí COMPRESS changes ‚Üí ISOLATE validation | Safe refactoring with checkpoints |
 270: | Context recovery (crashed session) | SELECT from git ‚Üí WRITE to memory ‚Üí COMPRESS overview ‚Üí ISOLATE validation | Resume from last checkpoint |
 271: 
 272: #### 3.1.1 WRITE: Persistence Layer
 273: 
 274: **Purpose:** Maintain state across sessions and context windows
 275: 
 276: **Mechanisms:**
 277: 
 278: - **Serena Memories:** Project knowledge (structure, conventions, patterns)
 279: - **Git Checkpoints:** Code state at validation gates
 280: - **Validation Logs:** Test results, error history
 281: 
 282: **PRP-Scoped State Management:**
 283: 
 284: To prevent information leakage and desynchronization across multiple PRP executions:
 285: 
 286: 1. **Checkpoint Naming Convention:**
 287: 
 288:    ```
 289:    checkpoint-{prp_id}-{phase}-{timestamp}
 290:    Example: checkpoint-PRP-003-implementation-1728934567
 291:    ```
 292: 
 293: 2. **Memory Namespacing:**
 294: 
 295:    ```python
 296:    # PRP-scoped memory operations (prevents state leakage)
 297:    prp_id = "PRP-003"
 298:    write_memory(f"{prp_id}-checkpoint-phase2", "Type definitions complete, 0 errors")
 299:    checkpoint = read_memory(f"{prp_id}-checkpoint-latest")
 300:    write_memory(f"{prp_id}-learnings", "Pattern: Use transaction wrapper...")
 301:    ```
 302: 
 303: 3. **Checkpoint Lifecycle:**
 304:    - **Create:** At each validation gate during PRP execution
 305:    - **Restore:** `git checkout checkpoint-{prp_id}-{phase}`
 306:    - **Cleanup:** Delete temporary checkpoints after PRP completion (retain final checkpoint only)
 307: 
 308: **Operations:**
 309: 
 310: ```python
 311: # Create PRP-scoped checkpoint
 312: write_memory(f"{prp_id}-checkpoint-types", "Type definitions complete, 0 errors")
 313: 
 314: # Restore PRP context
 315: checkpoint = read_memory(f"{prp_id}-checkpoint-latest")
 316: 
 317: # Track PRP-specific learnings
 318: write_memory(f"{prp_id}-learnings-feature-x", "Pattern: Use transaction wrapper for multi-step DB ops")
 319: 
 320: # Cleanup after PRP completion
 321: delete_memory(f"{prp_id}-checkpoint-*")  # Remove ephemeral checkpoints
 322: delete_memory(f"{prp_id}-learnings-*")   # Archive or remove PRP-specific learnings
 323: ```
 324: 
 325: **PRP ID Tracking Across Sessions:**
 326: 
 327: The `prp_id` is injected and persisted through multiple mechanisms:
 328: 
 329: 1. **Session Initialization:**
 330: 
 331:    ```bash
 332:    # User starts PRP execution with explicit ID
 333:    ce prp start PRP-005
 334:    # Creates session state file: .ce/active_prp_session
 335:    ```
 336: 
 337: 2. **Session State Persistence:**
 338: 
 339:    ```python
 340:    # .ce/active_prp_session (JSON)
 341:    {
 342:      "prp_id": "PRP-005",
 343:      "started_at": "2025-10-12T14:30:00Z",
 344:      "phase": "implementation",
 345:      "checkpoint_count": 3
 346:    }
 347:    ```
 348: 
 349: 3. **Automatic Injection:**
 350:    - All `ce prp` commands read from `.ce/active_prp_session`
 351:    - Memory operations automatically namespace using active PRP ID
 352:    - Git checkpoint creation includes PRP ID from session state
 353: 
 354: 4. **Session Cleanup:**
 355: 
 356:    ```bash
 357:    # Explicit completion
 358:    ce prp cleanup PRP-005
 359:    # Removes .ce/active_prp_session
 360:    # Archives memories to project knowledge
 361:    ```
 362: 
 363: **Cross-Session Continuity:** If session is interrupted, `ce prp status` shows active PRP and last checkpoint, enabling seamless resumption.
 364: 
 365: **State Isolation Guarantee:** Each PRP execution maintains isolated state through namespaced memories and scoped checkpoints, preventing context bleed between PRPs.
 366: 
 367: #### 3.1.2 SELECT: Dynamic Retrieval
 368: 
 369: **Purpose:** Retrieve relevant context on-demand
 370: 
 371: **Mechanisms:**
 372: 
 373: - **Symbol Navigation:** `find_symbol("Class/method", include_body=True)`
 374: - **Pattern Search:** `search_for_pattern("async function.*Error")`
 375: - **Documentation:** Context7 MCP for library-specific docs
 376: 
 377: **Strategy:**
 378: 
 379: 1. Overview first: `get_symbols_overview(file)`
 380: 2. Targeted search: `find_symbol` for specific symbols
 381: 3. Context expansion: `find_referencing_symbols` for relationships
 382: 
 383: #### 3.1.3 COMPRESS: Efficiency Management
 384: 
 385: **Purpose:** Minimize token consumption while maintaining completeness
 386: 
 387: **Techniques:**
 388: 
 389: - **Overview-first:** Structure before implementation details
 390: - **Symbolic editing:** Edit by symbol path, not full file reads
 391: - **Targeted reads:** Read specific lines/symbols, not entire files
 392: - **Batch operations:** Group related changes
 393: 
 394: **Example:**
 395: 
 396: ```
 397: ‚ùå Wasteful: Read(file) ‚Üí Edit(file)  # 10k tokens
 398: ‚úÖ Efficient: Edit(file, old, new)    # 100 tokens
 399: ```
 400: 
 401: #### 3.1.4 ISOLATE: Safety Boundaries
 402: 
 403: **Purpose:** Prevent context interference and ensure reproducibility
 404: 
 405: **Mechanisms:**
 406: 
 407: - **Validation gates:** Checkpoint after each phase
 408: - **Error boundaries:** Self-healing loops with iteration limits
 409: - **Strict rules:** 3 LOC limit, tmp/ folder for scripts
 410: - **Security scans:** Detect sensitive data patterns
 411: 
 412: ### See Also
 413: 
 414: - [Context Engineering Foundations](../docs/research/02-context-engineering-foundations.md) - Four Pillars architecture (WRITE, SELECT, COMPRESS, ISOLATE) in depth
 415: - [Persistence Layers](../docs/research/05-persistence-layers.md) - Ground truth management and persistence strategies
 416: - [MCP Orchestration](../docs/research/03-mcp-orchestration.md) - Strategic MCP integration architecture
 417: 
 418: ---
 419: 
 420: ### 3.2 PRP System Architecture
 421: 
 422: ```mermaid
 423: graph TB
 424:     A["INITIAL.md or PLAN.md"] --> B["PRP Generation<br/>(Manual or /batch-gen-prp)"]
 425:     B --> C["PRP Document(s)"]
 426:     C --> D{"Human Validation"}
 427:     D -->|"Approved"| E["/execute-prp"]
 428:     D -->|"Rejected"| F["Revise PRP"]
 429:     F --> C
 430:     E --> G["Implementation"]
 431:     G --> H["Validation Gate"]
 432:     H --> I{"Pass?"}
 433:     I -->|"Yes"| J["Next Phase"]
 434:     I -->|"No"| K["Self-Heal"]
 435:     K --> G
 436:     J --> L{"More Phases?"}
 437:     L -->|"Yes"| G
 438:     L -->|"No"| M["Production Code"]
 439: 
 440:     style A fill:#fff8e1,color:#000
 441:     style B fill:#f3e5f5,color:#000
 442:     style C fill:#b2ebf2,color:#000
 443:     style D fill:#ff9999,color:#000
 444:     style E fill:#ffe0b2,color:#000
 445:     style F fill:#fff9c4,color:#000
 446:     style G fill:#e1bee7,color:#000
 447:     style H fill:#ffecb3,color:#000
 448:     style I fill:#fff3e0,color:#000
 449:     style J fill:#e1f5fe,color:#000
 450:     style K fill:#ffccbc,color:#000
 451:     style L fill:#f3e5f5,color:#000
 452:     style M fill:#c8e6c9,color:#000
 453: ```
 454: 
 455: #### 3.2.1 PRP Structure
 456: 
 457: **Six Primary Sections:**
 458: 
 459: 1. **GOAL** - Single, clear objective
 460: 2. **WHY** - Business value and user impact
 461: 3. **WHAT** - Measurable success criteria
 462: 4. **CONTEXT** - Complete implementation context
 463:    - Project structure
 464:    - Existing patterns
 465:    - Library documentation
 466:    - Validation commands
 467:    - Gotchas and warnings
 468: 5. **IMPLEMENTATION BLUEPRINT** - Step-by-step pseudocode
 469: 6. **VALIDATION LOOPS** - Four-level testing gates (L1-L4)
 470: 
 471: **Optional Sections:**
 472: 
 473: - SERENA PRE-FLIGHT CHECKS
 474: - SELF-HEALING GATES
 475: - CONFIDENCE SCORING
 476: - COMPLETION CHECKLIST
 477: - **DRIFT_JUSTIFICATION** (Required if pattern drift > 30% accepted)
 478: 
 479: **DRIFT_JUSTIFICATION Section Format:**
 480: 
 481: Required when Level 4 validation detects >30% pattern drift and user accepts it.
 482: 
 483: ```yaml
 484: DRIFT_JUSTIFICATION:
 485:   drift_score: "<percentage>%"
 486:   decision: "accept | reject | update_examples"
 487:   reason: |
 488:     <Multi-line explanation of why drift is justified>
 489:     <Trade-offs considered>
 490:     <Business/technical rationale>
 491:   alternatives_considered:
 492:     - "<Alternative approach>: <Why rejected>"
 493:     - "<Alternative approach>: <Why rejected>"
 494:   approved_by: "user | team_lead | architect"
 495:   date: "YYYY-MM-DD"
 496:   references:
 497:     - "PRP-XXX: <Related drift decision>"
 498:     - "INITIAL.md: <Relevant EXAMPLES section>"
 499: ```
 500: 
 501: **Example:**
 502: 
 503: ```yaml
 504: DRIFT_JUSTIFICATION:
 505:   drift_score: 60%
 506:   decision: accept
 507:   reason: |
 508:     Payment gateway API (Stripe) requires synchronous webhooks.
 509:     Converting to async would break webhook signature validation.
 510:     This is isolated to payment module only.
 511:   alternatives_considered:
 512:     - "Async wrapper with sync bridge: Adds complexity, no performance gain"
 513:     - "Switch to async payment API: Not available from Stripe"
 514:     - "Background job processing: Breaks real-time payment flow"
 515:   approved_by: user
 516:   date: 2025-01-15
 517:   references:
 518:     - "PRP-004: Similar decision for legacy callback API"
 519:     - "INITIAL.md lines 42-56: Async pattern documented"
 520: ```
 521: 
 522: **Purpose:** Creates audit trail of architectural decisions, enables future PRPs to understand why patterns diverged.
 523: 
 524: #### 3.2.2 Information Density Requirements
 525: 
 526: | Anti-Pattern | Best Practice |
 527: |--------------|---------------|
 528: | "Use modern practices" | "Use Next.js 14.2.3 app router (see docs/routing.md:42)" |
 529: | "Handle errors properly" | "Wrap in try-catch, log to Winston, return {error: string}" |
 530: | "Store data efficiently" | "PostgreSQL with pg-pool, max 10 connections" |
 531: 
 532: **Principle:** Provide exactly what's needed‚Äîno more, no less.
 533: 
 534: ---
 535: 
 536: ### 3.3 Validation Framework
 537: 
 538: #### 3.3.1 Four-Level Gate System
 539: 
 540: ```mermaid
 541: graph TB
 542:     A["Code Implementation"] --> B["Level 1: Syntax & Style<br/>10 seconds"]
 543:     B --> B1["‚Ä¢ Linters, formatters, type checkers<br/>‚Ä¢ Auto-fix: Yes<br/>‚Ä¢ Action: Fix and re-run"]
 544:     B1 --> C["Level 2: Unit Tests<br/>30-60 seconds"]
 545:     C --> C1["‚Ä¢ Function-level validation<br/>‚Ä¢ Auto-fix: Conditional<br/>‚Ä¢ Action: Analyze, fix, re-test"]
 546:     C1 --> D["Level 3: Integration<br/>1-2 minutes"]
 547:     D --> D1["‚Ä¢ API endpoints, database, E2E<br/>‚Ä¢ Auto-fix: Manual<br/>‚Ä¢ Action: Debug systematically"]
 548:     D1 --> D2["Level 4: Pattern Conformance<br/>30-60 seconds"]
 549:     D2 --> D3["‚Ä¢ Compare vs EXAMPLES from INITIAL.md<br/>‚Ä¢ Check architectural consistency<br/>‚Ä¢ Detect drift from specification<br/>‚Ä¢ Action: Refactor if drift detected"]
 550:     D3 --> E["Production Ready"]
 551: 
 552:     style A fill:#e3f2fd,color:#000
 553:     style B fill:#fff8e1,color:#000
 554:     style B1 fill:#fff9c4,color:#000
 555:     style C fill:#f3e5f5,color:#000
 556:     style C1 fill:#e1f5fe,color:#000
 557:     style D fill:#b2ebf2,color:#000
 558:     style D1 fill:#b2dfdb,color:#000
 559:     style D2 fill:#e8f5e9,color:#000
 560:     style D3 fill:#c5e1a5,color:#000
 561:     style E fill:#c8e6c9,color:#000
 562: ```
 563: 
 564: #### 3.3.2 Self-Healing Protocol
 565: 
 566: **Standard Loop:**
 567: 
 568: 1. Run validation command
 569: 2. Capture output
 570: 3. If failure:
 571:    - Parse error message
 572:    - Identify root cause
 573:    - Use MCP tools to locate code
 574:    - Apply targeted fix
 575:    - Re-run validation
 576: 4. Repeat until pass OR escalate after 3 attempts
 577: 
 578: **Escalation Triggers:**
 579: 
 580: - Same error after 3 fix attempts
 581: - Ambiguous error messages
 582: - Architectural changes required
 583: - External dependency issues
 584: 
 585: #### 3.3.3 Level 4: Pattern Conformance Validation
 586: 
 587: **Purpose:** Ensure implementation matches architectural patterns defined in INITIAL.md EXAMPLES
 588: 
 589: **Validation Steps:**
 590: 
 591: 1. **Extract patterns from EXAMPLES:**
 592:    - Code structure (async/await vs callbacks)
 593:    - Error handling approach (try-catch, error boundaries)
 594:    - Data flow patterns (props, state, context)
 595:    - Naming conventions (camelCase, PascalCase, snake_case)
 596: 
 597: 2. **Compare implementation:**
 598:    - Use `find_symbol` to analyze new code structure
 599:    - Pattern match against EXAMPLES
 600:    - Calculate drift score (0-100%)
 601: 
 602: 3. **Drift detection thresholds:**
 603:    - **0-10%:** Minor style differences ‚Üí Auto-accept, continue
 604:    - **10-30%:** Moderate drift ‚Üí Auto-fix if possible, log warning
 605:    - **30%+:** Major architectural divergence ‚Üí **HALT & ESCALATE TO USER**
 606: 
 607: 4. **Human Decision Required (30%+ drift):**
 608: 
 609:    When major drift is detected, execution **PAUSES** and presents user with:
 610: 
 611:    ```
 612:    üö® PATTERN DRIFT DETECTED (60% divergence)
 613: 
 614:    üìã EXAMPLES Pattern (from INITIAL.md):
 615:    async def fetch_data():
 616:        try:
 617:            result = await api.get()
 618:            return {"data": result}
 619:        except Exception as e:
 620:            logger.error(f"Fetch failed: {e}")
 621:            raise
 622: 
 623:    üîß Current Implementation:
 624:    def fetch_data():
 625:        result = api.get()
 626:        return result
 627: 
 628:    ‚ùå Differences:
 629:    ‚Ä¢ Missing async/await (architectural)
 630:    ‚Ä¢ No try-catch error handling
 631:    ‚Ä¢ Wrong return format (missing wrapper)
 632: 
 633:    üìö Recent Drift History (last 3 PRPs):
 634:    ‚Ä¢ PRP-004: Accepted 25% drift (added callbacks for legacy API)
 635:    ‚Ä¢ PRP-003: Rejected 45% drift (maintained async consistency)
 636:    ‚Ä¢ PRP-002: Accepted 15% drift (simplified error messages)
 637: 
 638:    ü§î Choose Action:
 639:    [1] Accept drift + document justification in PRP
 640:    [2] Reject drift + refactor to match EXAMPLES
 641:    [3] Update EXAMPLES + accept new pattern
 642: 
 643:    If [1], provide justification:
 644:    > _______________________________________
 645:    ```
 646: 
 647: 5. **User Decision Handling:**
 648: 
 649:    **Option 1: Accept Drift**
 650:    - User provides written justification
 651:    - Justification saved to PRP under `DRIFT_JUSTIFICATION` section
 652:    - Pattern recorded in Serena memory: `drift-{prp_id}-justification`
 653:    - Future PRPs can reference this decision
 654:    - Example: "Legacy callback API requires synchronous interface"
 655: 
 656:    **Option 2: Reject Drift**
 657:    - AI refactors code to match EXAMPLES
 658:    - Re-run L1-L4 validation
 659:    - Continue to Step 6.5 if all gates pass
 660: 
 661:    **Option 3: Update EXAMPLES**
 662:    - User edits INITIAL.md EXAMPLES with new pattern
 663:    - New pattern becomes baseline for future validations
 664:    - Document pattern evolution in INITIAL.md
 665:    - Re-validate current implementation (should now pass)
 666: 
 667: 6. **Drift Justification Format (in PRP):**
 668: 
 669:    ```yaml
 670:    DRIFT_JUSTIFICATION:
 671:      drift_score: 60%
 672:      decision: accept
 673:      reason: |
 674:        Legacy payment API requires synchronous callback interface.
 675:        Async conversion would require major API refactor (out of scope).
 676:        Trade-off: Maintain sync pattern for payment, keep async for data fetching.
 677:      alternatives_considered:
 678:        - Async wrapper: Rejected (adds complexity, no benefit)
 679:        - API upgrade: Rejected (3rd party, no control)
 680:      approved_by: user
 681:      date: 2025-01-15
 682:    ```
 683: 
 684: **Example Check:**
 685: 
 686: ```python
 687: # INITIAL.md EXAMPLES shows:
 688: async def fetch_data():
 689:     try:
 690:         result = await api.get()
 691:         return {"data": result}
 692:     except Exception as e:
 693:         logger.error(f"Fetch failed: {e}")
 694:         raise
 695: 
 696: # Implementation:
 697: def fetch_data():  # ‚ùå Not async
 698:     result = api.get()  # ‚ùå No try-catch
 699:     return result  # ‚ùå Wrong return format
 700: 
 701: # Pattern Conformance: 60% drift ‚Üí Refactor required
 702: ```
 703: 
 704: **Integration:** Runs after Level 3 (Integration tests), before declaring production-ready.
 705: 
 706: **Drift Decision Workflow:**
 707: 
 708: ```mermaid
 709: graph TB
 710:     A["Level 4: Pattern Conformance<br/>Calculate Drift Score"] --> B{Drift Score?}
 711:     B -->|0-10%| C["‚úÖ Accept<br/>Continue to Production"]
 712:     B -->|10-30%| D["‚ö†Ô∏è Auto-fix<br/>Log Warning"]
 713:     D --> E["Re-run L4 Validation"]
 714:     E --> C
 715:     B -->|30%+| F["üö® HALT<br/>Escalate to User"]
 716: 
 717:     F --> G["Display:<br/>‚Ä¢ EXAMPLES pattern<br/>‚Ä¢ Current implementation<br/>‚Ä¢ Differences list<br/>‚Ä¢ Recent drift history"]
 718: 
 719:     G --> H{User Decision}
 720: 
 721:     H -->|1. Accept Drift| I["User provides justification"]
 722:     I --> J["Save DRIFT_JUSTIFICATION to PRP<br/>Record in Serena memory"]
 723:     J --> K["Update drift history<br/>drift-{prp_id}-justification"]
 724:     K --> C
 725: 
 726:     H -->|2. Reject Drift| L["AI refactors code<br/>to match EXAMPLES"]
 727:     L --> M["Re-run L1-L4 validation"]
 728:     M --> N{All gates pass?}
 729:     N -->|Yes| C
 730:     N -->|No| L
 731: 
 732:     H -->|3. Update EXAMPLES| O["User edits INITIAL.md<br/>New pattern becomes baseline"]
 733:     O --> P["Document pattern evolution<br/>in INITIAL.md"]
 734:     P --> E
 735: 
 736:     style A fill:#e3f2fd,color:#000
 737:     style B fill:#fff8e1,color:#000
 738:     style C fill:#c8e6c9,color:#000
 739:     style D fill:#ffe0b2,color:#000
 740:     style F fill:#ffccbc,color:#000
 741:     style G fill:#f3e5f5,color:#000
 742:     style H fill:#ff9999,color:#000
 743:     style I fill:#e1f5fe,color:#000
 744:     style J fill:#e1f5fe,color:#000
 745:     style K fill:#e1f5fe,color:#000
 746:     style L fill:#b2ebf2,color:#000
 747:     style M fill:#b2dfdb,color:#000
 748:     style N fill:#fff8e1,color:#000
 749:     style O fill:#f3e5f5,color:#000
 750:     style P fill:#e8f5e9,color:#000
 751: ```
 752: 
 753: #### 3.3.4 Confidence Scoring
 754: 
 755: | Score | Meaning | Criteria |
 756: |-------|---------|----------|
 757: | 1-3 | Unvalidated | No tests run |
 758: | 4-6 | Partially validated | Syntax checks pass |
 759: | 7-8 | Core validated | Unit tests pass |
 760: | 9 | Integration validated | L1-L3 pass, but pattern drift detected |
 761: | 10 | Production-ready | All 4 gates pass, zero drift from EXAMPLES |
 762: 
 763: **Threshold:** 10/10 required for production deployment (previously 9/10, upgraded to include L4).
 764: 
 765: ---
 766: 
 767: ## 4. Components
 768: 
 769: ### 4.1 Tool Ecosystem
 770: 
 771: **Implementation Status Overview:**
 772: 
 773: Context Engineering framework is **production-ready** with 31+ core PRPs executed (93%+ completion). All critical features implemented, tested, and security-verified.
 774: 
 775: **Core Features Implemented:** ‚úÖ
 776: 
 777: - ‚úÖ 4-level validation gates (L1-L4: syntax, unit tests, integration, pattern conformance + drift)
 778: - ‚úÖ PRP generation & execution (research + synthesis workflow with checkpoint tracking)
 779: - ‚úÖ Batch PRP generation & execution (parallel subagents, git worktrees, dependency analysis)
 780: - ‚úÖ Git operations (status, diff, checkpoints, drift tracking, worktree management)
 781: - ‚úÖ Context management (health monitoring, drift detection, sync, auto-remediation)
 782: - ‚úÖ Error recovery (retry with backoff, circuit breaker, resilience patterns)
 783: - ‚úÖ Metrics & profiling (success rate tracking, performance monitoring)
 784: - ‚úÖ Serena MCP integration (symbol search, pattern analysis, reference tracking)
 785: - ‚úÖ Linear integration (automated issue creation, defaults management)
 786: - ‚úÖ Syntropy MCP aggregation (unified server layer, connection pooling)
 787: - ‚úÖ Security hardening (CWE-78 elimination, command injection prevention)
 788: - ‚úÖ Tool ecosystem optimization (55 MCP tools denied, 96% token reduction)
 789: - ‚úÖ Project maintenance tools (vacuum, denoise, tools-misuse-scan)
 790: - ‚úÖ 9 slash commands for interactive workflows
 791: 
 792: **Post-1.0 Enhancements:** üîú
 793: 
 794: - üîú CLI wrappers for state commands (functions exist, rarely used)
 795: - üîú Alternative CI/CD executors (GitLab CI, Jenkins support)
 796: 
 797: **Architecture:**
 798: 
 799: - **Location:** `tools/ce/` (Python package)
 800: - **Management:** UV package manager
 801: - **CLI:** Single `ce` command with subcommands
 802: - **Testing:** `tools/tests/` with real functionality tests
 803: 
 804: #### 4.1.1 run_py Tool
 805: 
 806: **Purpose:** Execute Python code with strict 3 LOC limit
 807: 
 808: **Rules:**
 809: 
 810: - Ad-hoc code: Max 3 LOC (lines with actual code)
 811: - Longer scripts: Must be in `tmp/` folder
 812: - Auto-detect mode: Smart file vs code detection
 813: 
 814: **Usage:**
 815: 
 816: ```bash
 817: # Ad-hoc (max 3 LOC)
 818: cd tools && uv run ce run_py "import sys; print(sys.version)"
 819: 
 820: # File-based
 821: cd tools && uv run ce run_py tmp/analysis.py
 822: 
 823: # Auto-detect
 824: cd tools && uv run ce run_py "print('hello')"  # Detects code
 825: cd tools && uv run ce run_py tmp/script.py      # Detects file
 826: ```
 827: 
 828: **Implementation:**
 829: 
 830: ```python
 831: def run_py(code: Optional[str] = None,
 832:            file: Optional[str] = None,
 833:            auto: Optional[str] = None) -> Dict[str, Any]:
 834:     # Auto-detect file vs code
 835:     if auto is not None:
 836:         if "/" in auto or auto.endswith(".py"):
 837:             file = auto
 838:         else:
 839:             code = auto
 840: 
 841:     # Enforce 3 LOC limit
 842:     if code is not None:
 843:         lines = [line for line in code.split('\n') if line.strip()]
 844:         if len(lines) > 3:
 845:             raise ValueError(f"Ad-hoc code exceeds 3 LOC limit (found {len(lines)} lines)")
 846: 
 847:     # Execute with uv
 848:     cmd = f"uv run python -c {shlex.quote(code)}" if code else f"uv run python {file}"
 849:     return run_cmd(cmd, timeout=120 if code else 300)
 850: ```
 851: 
 852: #### 4.1.2 ce CLI
 853: 
 854: **Purpose:** Context Engineering operations
 855: 
 856: **Core Commands (Implemented):**
 857: 
 858: - `ce validate --level [1|2|3|all]` - Run validation gates (L1-L4)
 859: - `ce git status` - Git repository status
 860: - `ce git diff [options]` - View git changes
 861: - `ce git checkpoint "message"` - Create git tag checkpoint
 862: - `ce context health` - Context drift analysis
 863: - `ce context sync` - Sync context with codebase changes
 864: - `ce context prune` - Remove stale context entries
 865: - `ce run_py` - Execute Python code (3 LOC limit)
 866: - `ce vacuum [--execute|--auto|--nuclear]` - Clean up project noise (temp files, obsolete docs, unreferenced code)
 867: - `ce drift` - Drift history tracking and analysis
 868: - `ce analyze-context` / `ce analyse-context` - Fast drift check without metadata sync (2-3s vs 10-15s)
 869: 
 870: **PRP Management Commands (Implemented):**
 871: 
 872: - `ce prp validate <prp-file>` - Validate PRP structure and sections
 873: - `ce prp analyze <prp-file>` - Analyze PRP for complexity, sizing, patterns
 874: - `ce update-context [--prp file]` - Sync context, generate drift reports, create remediation PRPs
 875: - `ce metrics [options]` - Collect and display system metrics and success rates
 876: 
 877: **PRP State Management (Functions Implemented, CLI Pending):**
 878: 
 879: - `ce prp start <prp-id>` - ‚úÖ Function exists, CLI wrapper pending
 880: - `ce prp checkpoint <phase>` - ‚úÖ Function exists, CLI wrapper pending
 881: - `ce prp cleanup` - ‚úÖ Function exists, CLI wrapper pending
 882: - `ce prp restore <prp-id> [phase]` - ‚úÖ Function exists, CLI wrapper pending
 883: - `ce prp status` - ‚úÖ Function exists, CLI wrapper pending
 884: - `ce prp list` - ‚úÖ Function exists, CLI wrapper pending
 885: 
 886: **Drift History Commands (Functions Implemented, CLI Pending):**
 887: 
 888: - `ce drift history [--last N]` - ‚úÖ Function exists, CLI wrapper pending
 889: - `ce drift show <prp-id>` - ‚úÖ Function exists, CLI wrapper pending
 890: - `ce drift summary` - ‚úÖ Function exists, CLI wrapper pending
 891: - `ce drift compare <prp-id-1> <prp-id-2>` - ‚úÖ Function exists, CLI wrapper pending
 892: 
 893: **Pipeline Commands (Implemented):**
 894: 
 895: - `ce pipeline validate <yaml>` - Validate abstract pipeline YAML schema
 896: - `ce pipeline render <yaml>` - Render pipeline to concrete format (GitHub Actions, etc.)
 897: 
 898: **Implementation Status:**
 899: 
 900: ```python
 901: # Core operations (fully implemented)
 902: ‚úÖ core.py: run_cmd, git_status, git_diff, git_checkpoint, run_py (CWE-78 secure)
 903: ‚úÖ validate.py: validate_level_1-4 (all 4 levels with L4 drift detection)
 904: ‚úÖ context.py: sync, health, prune (with üîß troubleshooting guidance)
 905: ‚úÖ update_context.py: drift remediation workflow automation (30+ bugs fixed)
 906: 
 907: # Drift analysis (fully implemented)
 908: ‚úÖ drift.py: fast analyze-context command (2-3s vs 10-15s)
 909: ‚úÖ drift_analyzer.py: automated pattern detection + smart caching
 910: 
 911: # PRP system (functions implemented, most CLI exposed)
 912: ‚úÖ prp.py: start_prp, checkpoint, cleanup, restore, status, list (functions)
 913: ‚úÖ generate.py: research + synthesize (via /batch-gen-prp for parallel generation)
 914: ‚úÖ execute.py: phase execution + validation loops (via /execute-prp and /batch-exe-prp)
 915: ‚úÖ prp_analyzer.py: complexity analysis (ce prp analyze)
 916: 
 917: # Tool optimization (fully implemented)
 918: ‚úÖ mcp_adapter.py: MCP tool configuration mapping
 919: ‚úÖ shell_utils.py: Python bash replacements (30-50% context reduction)
 920: ‚úÖ pattern_detectors.py: Tool misuse prevention (6 anti-patterns)
 921: 
 922: # Pipeline & infrastructure (partial)
 923: ‚ö†Ô∏è pipeline.py: schema validation + abstract definition only
 924: ‚úÖ metrics.py: collection and reporting
 925: ‚úÖ linear_utils.py: issue creation + defaults
 926: ‚úÖ testing/: strategy pattern + builder (for PRP validation)
 927: 
 928: # Security (fully verified)
 929: ‚úÖ CWE-78 Command Injection: Eliminated (CVSS 8.1‚Üí0)
 930: ‚úÖ shlex.split() + shell=False: 6 critical locations fixed
 931: ‚úÖ Security Tests: 38/38 pass, 631 regression tests pass
 932: ```
 933: 
 934: **Architecture Note:** Execution driven by slash commands (`/batch-gen-prp`, `/batch-exe-prp`, `/execute-prp`) with state managed internally. CLI commands provide validation, analysis, and utility functions. This differs from model's planned interactive CLI state management but achieves same functionality through delegation.
 935: 
 936: **PRP Context Command Examples:**
 937: 
 938: ```bash
 939: # Start new PRP execution with isolated state
 940: ce prp start PRP-005
 941: 
 942: # Create phase checkpoint (PRP-scoped)
 943: ce prp checkpoint implementation
 944: # Creates: checkpoint-PRP-005-implementation-{timestamp}
 945: 
 946: # Cleanup after PRP completion
 947: ce prp cleanup PRP-005
 948: # - Deletes intermediate checkpoints (keeps final)
 949: # - Archives PRP memories to project knowledge
 950: # - Resets validation state counters
 951: 
 952: # Restore to specific PRP checkpoint
 953: ce prp restore PRP-005 implementation
 954: ```
 955: 
 956: **Drift History Command Examples:**
 957: 
 958: ```bash
 959: # Show last 3 drift decisions
 960: ce drift history --last 3
 961: # Output:
 962: # PRP-005: 45% drift REJECTED (refactored to match async pattern)
 963: # PRP-004: 25% drift ACCEPTED (legacy callback API requirement)
 964: # PRP-003: 15% drift AUTO-FIXED (minor style inconsistency)
 965: 
 966: # Show specific drift justification
 967: ce drift show PRP-004
 968: # Displays full DRIFT_JUSTIFICATION section from PRP-004
 969: 
 970: # Summary of all drift decisions
 971: ce drift summary
 972: # Output:
 973: # Total PRPs analyzed: 10
 974: # Drift decisions:
 975: #   - Accepted: 3 (30%)
 976: #   - Rejected: 5 (50%)
 977: #   - Auto-fixed: 2 (20%)
 978: # Average drift score: 22%
 979: # Common justifications:
 980: #   - Legacy API compatibility: 2 cases
 981: #   - Third-party library constraints: 1 case
 982: 
 983: # Compare drift between two PRPs
 984: ce drift compare PRP-003 PRP-005
 985: # Shows side-by-side drift decisions and reasoning
 986: ```
 987: 
 988: **Design:** Single CLI tool, modular subcommands, UV-managed. PRP state management ensures isolation between executions. Drift tracking creates architectural decision audit trail.
 989: 
 990: ##### 4.1.2.4 Update-Context Reliability Improvements (PRP-21)
 991: 
 992: **Comprehensive Fix** (30+ critical bugs eliminated):
 993: 
 994: **Drift Score Accuracy**:
 995: - ‚ùå **Before**: Used file count (1 file with 30 violations = 3.3% drift - misleading!)
 996: - ‚úÖ **After**: Uses violation count (30 violations / total checks = accurate percentage)
 997: - **Impact**: Drift scores now reflect actual codebase health
 998: 
 999: **Implementation Verification**:
1000: - ‚ùå **Before**: Serena MCP disabled (always False), ce_verified only checked if functions mentioned
1001: - ‚úÖ **After**: AST-based verification (actually checks if functions/classes exist in codebase)
1002: - **Impact**: PRPs auto-transition to executed/ only when implementations verified
1003: 
1004: **Pattern Matching Robustness**:
1005: - ‚ùå **Before**: Regex with `$` anchor missed multiline raises
1006: - ‚úÖ **After**: AST parsing for accurate pattern detection
1007: - **Impact**: Zero false positives/negatives in violation detection
1008: 
1009: **File Operation Safety**:
1010: - ‚ùå **Before**: No atomic writes (corruption risk on mid-write failure)
1011: - ‚úÖ **After**: Temp file + atomic rename pattern
1012: - **Impact**: PRP YAML headers never corrupted
1013: 
1014: **Error Handling**:
1015: - ‚ùå **Before**: Generic exceptions, no troubleshooting guidance
1016: - ‚úÖ **After**: Specific exceptions with üîß troubleshooting steps
1017: - **Impact**: Users can self-resolve issues without escalation
1018: 
1019: **Graceful Degradation**:
1020: - ‚ùå **Before**: Hard failures if Serena MCP unavailable
1021: - ‚úÖ **After**: Works without Serena (sets serena_updated=false with warning)
1022: - **Impact**: System usable even with partial MCP availability
1023: 
1024: **Remediation Workflow**:
1025: - ‚ùå **Before**: --remediate only generated PRP (half-baked)
1026: - ‚úÖ **After**: Full workflow (transform ‚Üí blueprint ‚Üí automated execution)
1027: - **Impact**: PRP-15 drift remediation pipeline complete
1028: 
1029: **Verification** (PRP-21 execution):
1030: - ‚úÖ 30+ bugs fixed across tools/ce/update_context.py
1031: - ‚úÖ Design flaws resolved (state management, error handling)
1032: - ‚úÖ All tests passing post-refactor
1033: - ‚úÖ Drift detection now accurate and reliable
1034: 
1035: **Files Modified**:
1036: - `tools/ce/update_context.py` - Main reliability fixes
1037: - `tools/ce/drift_analyzer.py` - Pattern detection improvements
1038: - `tools/ce/context.py` - Integration updates
1039: 
1040: **Reference**: [PRP-21: update-context Comprehensive Fix](../../PRPs/executed/PRP-21-update-context-comprehensive-fix.md)
1041: 
1042: #### 4.1.3 MCP Integration
1043: 
1044: **Serena MCP** (Codebase Navigation)
1045: 
1046: - `find_symbol(name_path)` - Locate code symbols
1047: - `find_referencing_symbols(name_path, file)` - Find usages
1048: - `search_for_pattern(pattern)` - Regex search
1049: - `get_symbols_overview(file)` - File structure
1050: - `write_memory(name, content)` - Persist knowledge
1051: - `read_memory(name)` - Restore knowledge
1052: 
1053: **Context7 MCP** (Documentation)
1054: 
1055: - `resolve-library-id(name)` - Find library ID
1056: - `get-library-docs(id, topic)` - Fetch docs
1057: 
1058: **Sequential Thinking MCP** (Reasoning)
1059: 
1060: - `sequentialthinking(thought, thought_number, total_thoughts)` - Step-by-step analysis
1061: 
1062: #### 4.1.4 Linear Integration
1063: 
1064: **Purpose:** Automated issue tracking for PRP lifecycle management
1065: 
1066: **Status:** ‚úÖ **IMPLEMENTED** (PRP-24 integration, not in prior model)
1067: 
1068: **Functionality:**
1069: 
1070: - **Auto-issue creation**: `/batch-gen-prp` creates Linear issues automatically during batch generation
1071: - **Default configuration**: `.ce/linear-defaults.yml` stores project, assignee, labels
1072: - **PRP metadata**: YAML header stores `issue: {LINEAR-ISSUE-ID}` for tracking
1073: - **Multi-PRP join**: `--join-prp` flag links multiple PRPs to same issue
1074: - **Status tracking**: Update issue as PRP progresses through phases
1075: 
1076: **Configuration:**
1077: 
1078: ```yaml
1079: # .ce/linear-defaults.yml
1080: project: "Context Engineering"
1081: assignee: "user@example.com"
1082: team: "TeamID"
1083: default_labels:
1084:   - "feature"
1085: ```
1086: 
1087: **Usage:**
1088: 
1089: ```bash
1090: # Auto-create Linear issues during batch PRP generation
1091: /batch-gen-prp BIG-FEATURE-PLAN.md
1092: # ‚Üí Creates PRP-43.1.1, PRP-43.2.1, ... + Linear issues for each
1093: # ‚Üí Updates each PRP YAML with issue ID
1094: 
1095: # Manual issue creation (if needed)
1096: cd tools && uv run python -c "from ce.linear_utils import create_issue; create_issue('PRP-44', 'Feature Name')"
1097: ```
1098: 
1099: #### 4.1.5 Metrics & Performance Monitoring
1100: 
1101: **Purpose:** Collect, analyze, and report execution metrics
1102: 
1103: **Status:** ‚úÖ **IMPLEMENTED** (not in prior model)
1104: 
1105: **Components:**
1106: 
1107: - **metrics.py**: Collection framework (execution time, success rate, quality scores)
1108: - **profiling.py**: Performance profiling (memory, CPU, timing analysis)
1109: - **Linear integration**: Track metrics per issue for productivity analysis
1110: 
1111: **CLI:**
1112: 
1113: ```bash
1114: ce metrics --type prp          # PRP execution metrics
1115: ce metrics --type validation   # Validation gate performance
1116: ce metrics --format json       # JSON output for CI/CD
1117: ```
1118: 
1119: #### 4.1.6 Syntropy MCP Aggregation Layer
1120: 
1121: **Purpose:** Unified interface for all MCP tools via single server
1122: 
1123: **Status:** ‚úÖ **IMPLEMENTED** (PRP-24, transforms tool ecosystem)
1124: 
1125: **Architecture:**
1126: 
1127: - **Single MCP Server**: `syntropy-mcp/` wraps 7 underlying servers
1128: - **Unified namespace**: `mcp__syntropy__<server>__<tool>` format
1129: - **Connection pooling**: Lazy initialization + automatic cleanup
1130: - **Zero breaking changes**: Existing tools preserved, just aggregated
1131: 
1132: **Servers Managed:**
1133: 
1134: 1. Serena (code navigation)
1135: 2. Filesystem (file operations)
1136: 3. Git (version control)
1137: 4. Context7 (documentation)
1138: 5. Sequential Thinking (reasoning)
1139: 6. Linear (issue tracking)
1140: 7. Repomix (codebase packaging)
1141: 
1142: **Benefits:**
1143: 
1144: - Single connection point instead of 8 servers
1145: - Structured logging with timing
1146: - Consistent error handling
1147: - Easy extensibility (new servers via servers.json)
1148: 
1149: **Configuration:**
1150: 
1151: ```json
1152: // syntropy-mcp/servers.json
1153: {
1154:   "servers": {
1155:     "syn-serena": {
1156:       "command": "uvx",
1157:       "args": ["--from", "git+https://...", "serena"]
1158:     },
1159:     // ... other servers
1160:   }
1161: }
1162: ```
1163: 
1164: #### 4.1.7 Quality & Validation Utilities
1165: 
1166: **Purpose:** Markdown linting, Mermaid validation, code quality checks
1167: 
1168: **Status:** ‚úÖ **IMPLEMENTED** (not in prior model)
1169: 
1170: **Components:**
1171: 
1172: - **markdown_lint.py**: Style enforcement for markdown docs
1173: - **mermaid_validator.py**: Diagram validation and color compatibility
1174: - **code_analyzer.py**: Pattern analysis for drift detection
1175: - **pattern_extractor.py**: Example extraction for EXAMPLES sections
1176: 
1177: **Integration:**
1178: 
1179: - **L1 validation**: Linter + formatter auto-fix
1180: - **Pre-commit**: Markdown linting via git hooks
1181: - **PRP validation**: Mermaid color specs in diagrams
1182: 
1183: #### 4.1.8 Security & Command Injection Prevention
1184: 
1185: **Purpose:** CWE-78 vulnerability elimination and command execution safety
1186: 
1187: **Status:** ‚úÖ **IMPLEMENTED & VERIFIED** (PRP-22 executed)
1188: 
1189: **Security Profile:**
1190: 
1191: - **Vulnerability**: CWE-78 (OS Command Injection via `shell=True`)
1192: - **Mitigation**: Replaced `subprocess.run(shell=True)` with `shlex.split() + shell=False`
1193: - **Locations Fixed**: 6 critical locations in core.py and context.py
1194: - **CVSS Score**: 8.1 (HIGH) ‚Üí 0 (Vulnerability eliminated)
1195: 
1196: **Verification:**
1197: 
1198: - ‚úÖ **Security Tests**: 38/38 passed (comprehensive injection prevention)
1199: - ‚úÖ **Regression Tests**: 631 tests passed (no functionality loss)
1200: - ‚úÖ **Validation**: Zero `shell=True` usage in codebase
1201: - ‚úÖ **Backward Compatibility**: All existing callers work unchanged
1202: 
1203: **Implementation Details:**
1204: 
1205: ```python
1206: # BEFORE (VULNERABLE)
1207: result = subprocess.run(cmd, shell=True, ...)  # ‚ùå CWE-78
1208: 
1209: # AFTER (SAFE)
1210: if isinstance(cmd, str):
1211:     cmd_list = shlex.split(cmd)  # Safe parsing
1212: else:
1213:     cmd_list = cmd
1214: 
1215: result = subprocess.run(cmd_list, shell=False, ...)  # ‚úÖ SAFE
1216: ```
1217: 
1218: **Features:**
1219: 
1220: - **Accepts both strings and lists** - Backward compatible
1221: - **Uses shlex.split()** - Properly handles quoted arguments
1222: - **Shell interpretation disabled** - No metacharacter expansion
1223: - **Error handling** - Clear troubleshooting for invalid commands
1224: 
1225: **References:**
1226: 
1227: - [CWE-78: OS Command Injection](https://cwe.mitre.org/data/definitions/78.html) - MITRE/NIST
1228: - [Bandit B602 Security Check](https://bandit.readthedocs.io/en/latest/plugins/b602_subprocess_popen_with_shell_equals_true.html) - Static analysis tool
1229: - [CISA Secure Design Alert](https://www.cisa.gov/resources-tools/resources/secure-design-alert-eliminating-os-command-injection-vulnerabilities) - Federal guidance
1230: 
1231: #### 4.1.9 Slash Commands
1232: 
1233: **Purpose:** High-level workflow commands for interactive Claude Code sessions
1234: 
1235: **Status:** ‚úÖ **IMPLEMENTED** (9 commands active)
1236: 
1237: **Command Overview:**
1238: 
1239: | Command | Purpose | Typical Use Case |
1240: |---------|---------|------------------|
1241: | `/execute-prp` | Execute single PRP | Implement specific feature |
1242: | `/batch-exe-prp` | Execute batch PRPs in parallel | Multi-PRP staged implementation |
1243: | `/batch-gen-prp` | Generate batch PRPs from plan | Decompose large features |
1244: | `/update-context` | Sync context with codebase | After major changes or drift |
1245: | `/vacuum` | Clean up project noise | Remove temp files, obsolete docs |
1246: | `/denoise` | Compress verbose documents | Token optimization for docs |
1247: | `/tools-misuse-scan` | Detect tool anti-patterns | Debug session issues |
1248: | `/syntropy-health` | MCP server health check | Troubleshoot MCP connections |
1249: | `/sync-with-syntropy` | Sync tool permissions | Update settings after tool changes |
1250: 
1251: **Implementation Details:**
1252: 
1253: ```bash
1254: # Location: .claude/commands/*.md
1255: # Format: Markdown files with command documentation
1256: # Invocation: /command-name [args]
1257: # Example: /vacuum --execute
1258: ```
1259: 
1260: **Key Features:**
1261: 
1262: - **Interactive workflow**: Designed for Claude Code conversation flow
1263: - **Context-aware**: Access to full project state and conversation history
1264: - **Integrated with ce CLI**: Many commands wrap ce CLI operations
1265: - **Documentation-driven**: Command behavior defined in markdown files
1266: 
1267: **Command Descriptions:**
1268: 
1269: **1. /execute-prp** - Execute single PRP implementation
1270: - Reads PRP file, executes implementation steps
1271: - Runs validation gates (L1-L4)
1272: - Creates git checkpoints
1273: - Updates PRP metadata (executed timestamp, commit hash)
1274: 
1275: **2. /batch-exe-prp** - Execute batch PRPs with parallel execution
1276: - Parses batch ID from PRP filenames (PRP-X.Y.Z format)
1277: - Groups by stage, executes stages sequentially
1278: - Parallel execution within stage (git worktrees)
1279: - Health monitoring via git commit timestamps
1280: - Automatic merge with conflict detection
1281: 
1282: **3. /batch-gen-prp** - Generate batch PRPs from plan document
1283: - Parses plan markdown ‚Üí extracts phases
1284: - Builds dependency graph (explicit + file conflicts)
1285: - Assigns stages for parallel generation
1286: - Spawns parallel subagents for generation
1287: - Creates Linear issues for each PRP
1288: - Health monitoring via heartbeat files
1289: 
1290: **4. /update-context** - Sync context metadata with codebase
1291: - Updates PRP execution status
1292: - Generates drift reports
1293: - Syncs with Serena memories
1294: - Creates remediation PRPs for high drift
1295: 
1296: **5. /vacuum** - Clean up project noise
1297: - Strategies: temp-files, backup-files, obsolete-docs, unreferenced-code, orphan-tests, commented-code
1298: - Modes: dry-run (report only), --execute (HIGH confidence), --auto (MEDIUM+), --nuclear (ALL)
1299: - Confidence scoring: 30-100%
1300: - Protected paths: .ce/, .claude/, PRPs/, pyproject.toml, etc.
1301: - Report output: .ce/vacuum-report.md
1302: 
1303: **6. /denoise** - Boil out document noise
1304: - Removes verbosity while preserving essential information
1305: - Target: 60-75% reduction in lines
1306: - Preserves: commands, references, warnings, key facts
1307: - Compresses: long explanations, redundant examples, verbose text
1308: - Validation: ensures zero information loss
1309: 
1310: **7. /tools-misuse-scan** - Detect tool anti-patterns
1311: - Scans conversation for denied tool errors
1312: - Categories: Bash anti-patterns, denied tools
1313: - Remediation suggestions with alternatives
1314: - Report format: structured markdown
1315: 
1316: **8. /syntropy-health** - MCP server health diagnostics
1317: - Checks all Syntropy MCP servers (serena, filesystem, git, etc.)
1318: - Connection status, response time, error counts
1319: - Tool availability check
1320: - Detailed diagnostics with `--detailed` flag
1321: 
1322: **9. /sync-with-syntropy** - Sync tool permissions with Syntropy state
1323: - Calls `mcp__syntropy__list_all_tools` for current state
1324: - Updates `.claude/settings.local.json` to match
1325: - Backs up original settings
1326: - Outputs summary of changes
1327: 
1328: **Workflow Integration:**
1329: 
1330: ```bash
1331: # Typical workflow
1332: /syntropy-health              # Check MCP health
1333: /batch-gen-prp PLAN.md        # Generate PRPs from plan
1334: /batch-exe-prp --batch 43     # Execute batch 43
1335: /update-context               # Sync context after execution
1336: /vacuum --execute             # Clean up temp files
1337: ```
1338: 
1339: #### 4.1.10 Batch PRP Generation & Execution
1340: 
1341: **Purpose:** Parallel PRP generation and execution for large features
1342: 
1343: **Status:** ‚úÖ **IMPLEMENTED** (PRP-27-31 era)
1344: 
1345: **Architecture:**
1346: 
1347: ```mermaid
1348: graph TB
1349:     A[Plan Document] --> B[/batch-gen-prp]
1350:     B --> C[Parse Phases]
1351:     C --> D[Build Dependency Graph]
1352:     D --> E[Assign Stages]
1353:     E --> F[Spawn Parallel Subagents]
1354:     F --> G[Monitor via Heartbeats]
1355:     G --> H[Generated PRPs]
1356: 
1357:     H --> I[/batch-exe-prp]
1358:     I --> J[Parse Batch ID]
1359:     J --> K[Group by Stage]
1360:     K --> L[Create Git Worktrees]
1361:     L --> M[Execute Stage in Parallel]
1362:     M --> N[Monitor via Git Commits]
1363:     N --> O[Merge in Order]
1364:     O --> P[Cleanup Worktrees]
1365: 
1366:     style A fill:#e3f2fd,color:#000
1367:     style B fill:#fff8e1,color:#000
1368:     style F fill:#c8e6c9,color:#000
1369:     style H fill:#f3e5f5,color:#000
1370:     style I fill:#fff8e1,color:#000
1371:     style M fill:#c8e6c9,color:#000
1372:     style P fill:#b2ebf2,color:#000
1373: ```
1374: 
1375: **Generation Process:**
1376: 
1377: 1. **Parse Plan**: Extract phases with metadata (goal, hours, complexity, files, dependencies)
1378: 2. **Dependency Analysis**: Build graph with explicit + implicit (file conflict) dependencies
1379: 3. **Stage Assignment**: Topological sort ‚Üí group independent PRPs
1380: 4. **Parallel Generation**: Spawn Sonnet subagents per stage
1381: 5. **Health Monitoring**: 30s polling, heartbeat files, 2-poll kill timeout
1382: 6. **Linear Integration**: Create issue per PRP with defaults
1383: 
1384: **Execution Process:**
1385: 
1386: 1. **Parse Batch**: Extract PRP-X.Y.Z format ‚Üí batch ID, stage, order
1387: 2. **Stage Grouping**: Group PRPs by stage number
1388: 3. **Worktree Creation**: `git worktree add ../ctx-eng-plus-prp-X-Y-Z -b branch-name`
1389: 4. **Parallel Execution**: Execute PRPs in parallel within stage
1390: 5. **Health Monitoring**: 30s polling, git log timestamps, 10min timeout
1391: 6. **Sequential Merge**: Merge branches in order, handle conflicts
1392: 7. **Cleanup**: Remove worktrees, prune references
1393: 
1394: **PRP Naming Convention:**
1395: 
1396: - **Format**: `PRP-X.Y.Z-feature-name.md`
1397: - **X**: Batch ID (next free PRP number)
1398: - **Y**: Stage number (1, 2, 3...)
1399: - **Z**: Order within stage (1, 2, 3...)
1400: - **Example**: `PRP-43.2.3-doc-updates.md` (Batch 43, Stage 2, 3rd PRP in stage)
1401: 
1402: **Metadata in PRP Headers:**
1403: 
1404: ```yaml
1405: stage: stage-2-parallel
1406: execution_order: 4
1407: merge_order: 4
1408: worktree_path: ../ctx-eng-plus-prp-43-2-3
1409: branch_name: prp-43-2-3-doc-updates
1410: conflict_potential: LOW
1411: dependencies: [PRP-43.1.1]
1412: ```
1413: 
1414: **Performance:**
1415: 
1416: - **Generation**: 8 PRPs sequential (30 min) ‚Üí parallel (10-12 min) = **60% faster**
1417: - **Execution**: 3 PRPs sequential (45 min) ‚Üí parallel (20 min) = **55% faster**
1418: - **Monitoring**: 30s polling interval, low overhead
1419: 
1420: **Safety Mechanisms:**
1421: 
1422: - **Dependency validation**: Circular dependency detection
1423: - **File conflict detection**: Implicit dependencies from file overlaps
1424: - **Health monitoring**: Kill stalled agents/executors
1425: - **Merge order**: Enforced sequential merge to handle conflicts
1426: - **Worktree isolation**: No cross-contamination between PRPs
1427: 
1428: **Example Workflow:**
1429: 
1430: ```bash
1431: # 1. Create plan document
1432: vim BIG-FEATURE-PLAN.md
1433: 
1434: # 2. Generate batch PRPs (parallel)
1435: /batch-gen-prp BIG-FEATURE-PLAN.md
1436: # Output: PRPs/feature-requests/PRP-43.*.md (8 PRPs)
1437: #   Stage 1: PRP-43.1.1
1438: #   Stage 2: PRP-43.2.1, PRP-43.2.2, PRP-43.2.3 (parallel)
1439: #   Stage 3: PRP-43.3.1, PRP-43.3.2
1440: 
1441: # 3. Execute batch (parallel within stages)
1442: /batch-exe-prp --batch 43
1443: # Executes Stage 1 ‚Üí Stage 2 (parallel) ‚Üí Stage 3
1444: # Time: ~20 min vs 45 min sequential
1445: 
1446: # 4. Sync context
1447: /update-context
1448: ```
1449: 
1450: **Implementation Status:**
1451: 
1452: - ‚úÖ Batch generation with parallel subagents
1453: - ‚úÖ Batch execution with git worktrees
1454: - ‚úÖ Dependency graph analysis (explicit + file conflicts)
1455: - ‚úÖ Health monitoring (heartbeats for gen, git commits for exe)
1456: - ‚úÖ Linear integration (issue creation per PRP)
1457: - ‚úÖ Conflict detection and merge order enforcement
1458: - ‚úÖ Circular dependency detection with path reporting
1459: - ‚úÖ Worktree creation and cleanup automation
1460: 
1461: ### See Also
1462: 
1463: - [Product Requirements Prompt (PRP) System](../docs/research/01-prp-system.md) - Complete PRP templates, validation gates, and self-healing patterns
1464: - [MCP Orchestration](../docs/research/03-mcp-orchestration.md) - Strategic integration of Serena, Context7, and Sequential Thinking MCPs
1465: - [Command Reference](../docs/research/07-commands-reference.md) - Complete CLI tool documentation and command workflows
1466: - [Tooling and Configuration](../docs/research/10-tooling-configuration.md) - Setup guides for UV, git, validation commands, and MCP servers
1467: - [Syntropy Examples](../tools/ce/examples/syntropy/README.md) - Complete pattern library for Syntropy MCP servers
1468: 
1469: ---
1470: 
1471: ### 4.2 Templates
1472: 
1473: #### 4.2.1 Self-Healing Template
1474: 
1475: **Use Case:** Complex features with extensive validation
1476: 
1477: **Key Sections:**
1478: 
1479: - SERENA PRE-FLIGHT CHECKS
1480: - SELF-HEALING GATES with checkpoint creation
1481: - CONTEXT SYNCHRONIZATION PROTOCOL
1482: - CONFIDENCE SCORING
1483: 
1484: **Characteristics:**
1485: 
1486: - Multiple checkpoints per phase
1487: - Detailed pseudocode
1488: - Comprehensive error handling
1489: - Integration with Serena MCP
1490: 
1491: #### 4.2.2 KISS Template
1492: 
1493: **Use Case:** Simple features, quick implementations
1494: 
1495: **Key Sections:**
1496: 
1497: - Minimal CONTEXT (files, patterns, gotchas)
1498: - Streamlined IMPLEMENTATION (3-4 steps)
1499: - VALIDATION with automatic self-healing note
1500: 
1501: **Characteristics:**
1502: 
1503: - Single checkpoint at end
1504: - High-level pseudocode
1505: - Essential error handling only
1506: - Standard validation commands
1507: 
1508: #### 4.2.3 Template Selection
1509: 
1510: | Factor | Self-Healing | KISS |
1511: |--------|--------------|------|
1512: | Feature complexity | High (multi-component) | Low (single component) |
1513: | Integration points | 3+ systems | 1-2 systems |
1514: | Risk level | Production-critical | Non-critical |
1515: | Team experience | Junior developers | Senior developers |
1516: | Time available | Ample | Limited |
1517: 
1518: ---
1519: 
1520: ### 4.3 Infrastructure
1521: 
1522: ```mermaid
1523: graph LR
1524:     A["project/"]
1525: 
1526:     A --> B[".claude/"]
1527:     B --> B1["commands/<br/>9 Slash commands"]
1528:     B1 --> B1a["batch-gen-prp.md"]
1529:     B1 --> B1b["batch-exe-prp.md"]
1530:     B1 --> B1c["execute-prp.md"]
1531:     B1 --> B1d["vacuum.md, denoise.md, etc."]
1532:     B --> B2["CLAUDE.md<br/>Global rules"]
1533: 
1534:     A --> C["PRPs/"]
1535:     C --> C1["templates/"]
1536:     C1 --> C1a["self-healing.md"]
1537:     C1 --> C1b["kiss.md"]
1538:     C --> C2["ai_docs/<br/>Cached docs"]
1539:     C --> C3["feature-requests/<br/>INITIAL.md files"]
1540:     C --> C4["PRP-*.md<br/>Generated PRPs"]
1541: 
1542:     A --> D["examples/"]
1543:     D --> D1["patterns/<br/>Reusable code"]
1544: 
1545:     A --> E["tools/"]
1546:     E --> E1["ce/<br/>CLI source"]
1547: 
1548:     A --> F["Project source code"]
1549: 
1550:     style A fill:#e3f2fd,color:#000
1551:     style B fill:#fff8e1,color:#000
1552:     style C fill:#f3e5f5,color:#000
1553:     style D fill:#b2ebf2,color:#000
1554:     style E fill:#ffe0b2,color:#000
1555:     style F fill:#ffecb3,color:#000
1556:     style B1 fill:#fff9c4,color:#000
1557:     style B2 fill:#fff9c4,color:#000
1558:     style C1 fill:#e1f5fe,color:#000
1559:     style C2 fill:#e1f5fe,color:#000
1560:     style C3 fill:#e1f5fe,color:#000
1561:     style C4 fill:#e1f5fe,color:#000
1562:     style D1 fill:#b2dfdb,color:#000
1563:     style E1 fill:#ffccbc,color:#000
1564: ```
1565: 
1566: **Purpose:**
1567: 
1568: - `.claude/` - Claude Code configuration
1569: - `PRPs/` - Specification documents
1570: - `examples/` - Reference implementations
1571: - `tools/` - Development utilities
1572: 
1573: ---
1574: 
1575: ## 5. Workflow
1576: 
1577: ### 5.1 Six-Step Process
1578: 
1579: ```mermaid
1580: graph TB
1581:     A["Step 1: CLAUDE.md<br/>Global Rules"] --> B["Step 2: INITIAL.md<br/>Feature Request"]
1582:     B --> B5["Step 2.5: Context Sync<br/>Health Check 1-2 min"]
1583:     B5 --> C["Step 3: PRP Generation<br/>10-15 min Research"]
1584:     C --> D["Step 4: Human Validation<br/>CRITICAL CHECKPOINT"]
1585:     D --> E["Step 5: /execute-prp<br/>20-90 min Implementation"]
1586:     E --> F["Step 6: Validation Loop<br/>L1-L4 + Self-Healing"]
1587:     F --> F5["Step 6.5: State Cleanup<br/>Context Sync 2-3 min"]
1588:     F5 --> G["Production Code<br/>10/10 Confidence"]
1589:     G -.->|Next PRP| B
1590: 
1591:     style A fill:#e3f2fd,color:#000
1592:     style B fill:#fff8e1,color:#000
1593:     style B5 fill:#e1f5fe,color:#000
1594:     style C fill:#f3e5f5,color:#000
1595:     style D fill:#ff9999,color:#000
1596:     style E fill:#b2ebf2,color:#000
1597:     style F fill:#ffe0b2,color:#000
1598:     style F5 fill:#e1f5fe,color:#000
1599:     style G fill:#c8e6c9,color:#000
1600: ```
1601: 
1602: ### 5.2 Step Breakdown
1603: 
1604: **Step 1: CLAUDE.md** (One-time setup)
1605: 
1606: - Establish project-wide rules
1607: - Define code structure limits
1608: - Specify testing requirements
1609: - Document style conventions
1610: 
1611: **Step 2: INITIAL.md** (2-5 minutes)
1612: 
1613: - Write FEATURE section (what to build)
1614: - Add EXAMPLES (similar code)
1615: - Link DOCUMENTATION (library docs)
1616: - List OTHER CONSIDERATIONS (gotchas)
1617: 
1618: **Step 2.5: Context Sync & Health Check** (1-2 minutes)
1619: 
1620: - Run `ce context sync` to refresh context with recent codebase changes
1621: - Run `ce context health` to verify context quality
1622: - Check drift score (abort if > 30% - indicates stale context)
1623: - Verify git clean state (warn if uncommitted changes)
1624: - **Purpose:** Ensure PRP generation uses fresh, accurate context
1625: - **Abort conditions:** High drift, failed sync, context corruption
1626: 
1627: **Step 3: PRP Generation** (10-15 minutes)
1628: 
1629: - **For single PRPs**: Manual PRP writing using templates (.ce/examples/prp-template.md)
1630: - **For batch PRPs**: `/batch-gen-prp PLAN.md` with parallel subagents
1631: - Automated research: codebase patterns, documentation, architecture
1632: - Generate complete PRP with all sections
1633: - Include validation commands and pseudocode
1634: 
1635: **Step 4: Human Validation** (5-10 minutes)
1636: 
1637: - Architecture review
1638: - Security audit
1639: - Requirement coverage check
1640: - Implementation sanity check
1641: 
1642: **Step 5: /execute-prp** (20-90 minutes)
1643: 
1644: - Parse PRP into tasks
1645: - Implement following blueprint
1646: - Run validation gates after each phase
1647: - Self-heal on failures
1648: 
1649: **Step 6: Validation Loop** (Continuous)
1650: 
1651: - Level 1: Syntax checks
1652: - Level 2: Unit tests
1653: - Level 3: Integration tests
1654: - Level 4: Pattern conformance (NEW)
1655:   - Compare implementation vs EXAMPLES from INITIAL.md
1656:   - Verify code follows documented patterns
1657:   - Detect architectural drift from specification
1658: - Self-correct until 10/10 confidence (all 4 gates pass)
1659: 
1660: **Step 6.5: State Cleanup & Context Sync** (2-3 minutes)
1661: 
1662: - Execute cleanup protocol (Section 5.6):
1663:   - Delete intermediate git checkpoints (keep final only)
1664:   - Archive PRP-scoped Serena memories to project knowledge
1665:   - Reset validation state counters
1666: - Run `ce context sync` to index new code
1667: - Run `ce context health` to verify clean state
1668: - Create final checkpoint: `checkpoint-{prp_id}-final`
1669: - **Purpose:** Prevent state leakage into next PRP, maintain context quality
1670: - **Verification:** Clean git tags, drift score stable, no orphaned memories
1671: 
1672: ### 5.3 Time Distribution
1673: 
1674: | Feature Complexity | Context Sync | PRP Gen | Execution | Cleanup | Total | Manual Equiv |
1675: |-------------------|--------------|---------|-----------|---------|-------|--------------|
1676: | Simple | 1-2 min | 5-8 min | 8-15 min | 2-3 min | 16-28 min | 3-5 hrs |
1677: | Medium | 1-2 min | 10-15 min | 20-40 min | 2-3 min | 33-60 min | 8-15 hrs |
1678: | Complex | 1-2 min | 15-25 min | 45-90 min | 2-3 min | 63-120 min | 20-40 hrs |
1679: 
1680: **Notes:**
1681: 
1682: - **Context Sync (Step 2.5):** Health check + drift detection before PRP generation
1683: - **Execution:** Includes L1-L4 validation gates and self-healing
1684: - **Cleanup (Step 6.5):** State cleanup, memory archival, context sync after completion
1685: - **Total:** End-to-end per PRP, including quality gates
1686: 
1687: **Speed Improvement:** 10-40x faster than manual development (typically 10-24x, exceptional cases up to 40x).
1688: 
1689: **Context overhead:** Steps 2.5 and 6.5 add 3-5 min total but prevent state leakage and ensure quality.
1690: 
1691: ### 5.4 Autonomy Levels
1692: 
1693: | Step | Human Involvement | AI Autonomy |
1694: |------|------------------|-------------|
1695: | 1. CLAUDE.md | Manual (one-time) | 0% |
1696: | 2. INITIAL.md | Manual | 0% |
1697: | 2.5. Context Sync | None | 100% |
1698: | 3. PRP Generation | Manual or /batch-gen-prp | 0-100% |
1699: | 4. Validation | Manual (required) | 0% |
1700: | 5. /execute-prp | None | 100% |
1701: | 6. Validation loop (L1-L4) | None | 100% |
1702: | 6.5. State Cleanup | None | 100% |
1703: 
1704: **Key Insight:** Human intervention only at specification (Steps 1-2) and critical checkpoint (Step 4). Context sync and cleanup are fully automated.
1705: 
1706: ### 5.5 Escalation Triggers
1707: 
1708: **When to Intervene During Autonomous Execution (Steps 5-6):**
1709: 
1710: 1. **Persistent Failures**
1711:    - Same error after 3 self-healing attempts
1712:    - Validation failures without clear resolution path
1713:    - Circular dependency or conflicting requirements detected
1714: 
1715: 2. **Architectural Decisions Required**
1716:    - Major refactoring needed beyond PRP scope
1717:    - Design patterns need human judgment
1718:    - Performance trade-offs require business context
1719: 
1720: 3. **External Dependencies**
1721:    - Third-party API failures or breaking changes
1722:    - Database schema conflicts
1723:    - Environment configuration issues
1724: 
1725: 4. **Security Concerns**
1726:    - Potential vulnerability detected during implementation
1727:    - Secret exposure risk identified
1728:    - Permission escalation patterns found
1729: 
1730: 5. **Ambiguous Requirements**
1731:    - PRP specification conflicts with existing code
1732:    - Edge cases not covered in acceptance criteria
1733:    - Business logic interpretation unclear
1734: 
1735: **Escalation Process:**
1736: 
1737: - System logs issue to `PRPs/ISSUES.md`
1738: - Execution pauses at safe checkpoint
1739: - Human reviews context and provides guidance
1740: - Execution resumes with clarified direction
1741: 
1742: ### 5.6 PRP State Cleanup Protocol
1743: 
1744: **Purpose:** Prevent state leakage and desynchronization between PRP executions
1745: 
1746: **When to Execute:** After Step 6 (Validation Loop) completion, before starting next PRP
1747: 
1748: **Cleanup Operations:**
1749: 
1750: 1. **Git Checkpoint Cleanup**
1751: 
1752:    ```bash
1753:    # Keep only final checkpoint for historical reference
1754:    git tag -d checkpoint-{prp_id}-phase1
1755:    git tag -d checkpoint-{prp_id}-phase2
1756:    # Retain: checkpoint-{prp_id}-final
1757:    ```
1758: 
1759: 2. **Serena Memory Archival**
1760: 
1761:    ```python
1762:    # Archive ephemeral PRP memories
1763:    prp_learnings = read_memory(f"{prp_id}-learnings")
1764:    write_memory("project-patterns", prp_learnings)  # Merge to project knowledge
1765:    delete_memory(f"{prp_id}-checkpoint-*")          # Remove ephemeral checkpoints
1766:    delete_memory(f"{prp_id}-temp-*")                # Remove temporary state
1767:    ```
1768: 
1769: 3. **Validation State Reset**
1770:    - Clear self-healing attempt counters
1771:    - Reset error history for next PRP
1772:    - Archive test results to `PRPs/{prp_id}/validation-log.md`
1773: 
1774: 4. **Context Health Check**
1775: 
1776:    ```bash
1777:    ce context health           # Verify clean state
1778:    ce context prune           # Remove stale context entries
1779:    ```
1780: 
1781: **State Boundaries:**
1782: 
1783: | State Type | Scope | Cleanup Strategy |
1784: |------------|-------|------------------|
1785: | Git Checkpoints | Per-PRP | Delete intermediate, keep final |
1786: | Serena Memories | Per-PRP | Archive learnings, delete ephemeral |
1787: | Validation Logs | Per-PRP | Archive to PRP directory |
1788: | Self-healing State | Per-PRP | Reset counters to zero |
1789: | Project Knowledge | Global | Merge PRP learnings, persist patterns |
1790: 
1791: **Verification:**
1792: 
1793: ```bash
1794: # After cleanup, verify no state leakage
1795: git tag | grep checkpoint-{prp_id}  # Should show only *-final tag
1796: ce context health                   # Should report clean state
1797: ```
1798: 
1799: **Critical Rule:** No PRP state should persist into the next PRP execution except:
1800: 
1801: - Final checkpoint (for rollback capability)
1802: - Generalized learnings (merged into project knowledge)
1803: - Persistent project structure knowledge
1804: 
1805: ### See Also
1806: 
1807: - [Workflow Patterns](../docs/research/06-workflow-patterns.md) - Detailed six-step process, timing data, and workflow best practices
1808: - [Product Requirements Prompt (PRP) System](../docs/research/01-prp-system.md) - PRP generation and execution workflows
1809: - [Command Reference](../docs/research/07-commands-reference.md) - Command sequences for workflow automation
1810: 
1811: ---
1812: 
1813: ## 6. Implementation Patterns
1814: 
1815: ### 6.1 No Fishy Fallbacks
1816: 
1817: **Principle:** Fast failure with actionable errors
1818: 
1819: **Anti-Pattern:**
1820: 
1821: ```python
1822: def process_data(params):
1823:     try:
1824:         result = complex_operation(params)
1825:         return result
1826:     except Exception:
1827:         return {"success": True}  # FISHY FALLBACK!
1828: ```
1829: 
1830: **Best Practice:**
1831: 
1832: ```python
1833: def process_data(params):
1834:     try:
1835:         result = complex_operation(params)
1836:         return result
1837:     except ValueError as e:
1838:         raise ValueError(
1839:             f"Invalid parameters: {e}\n"
1840:             f"üîß Troubleshooting: Check param format and ranges"
1841:         ) from e
1842: ```
1843: 
1844: ### 6.2 3 LOC Rule
1845: 
1846: **Principle:** Strict enforcement for ad-hoc code
1847: 
1848: **Rationale:**
1849: 
1850: - Forces code organization
1851: - Prevents unmaintainable inline scripts
1852: - Encourages file-based development
1853: 
1854: **Enforcement:**
1855: 
1856: ```python
1857: # Validate LOC count
1858: lines = [line for line in code.split('\n') if line.strip()]
1859: if len(lines) > 3:
1860:     raise ValueError(
1861:         f"Ad-hoc code exceeds 3 LOC limit (found {len(lines)} lines)\n"
1862:         f"üîß Troubleshooting: Move code to tmp/ file"
1863:     )
1864: ```
1865: 
1866: **Examples:**
1867: 
1868: ```bash
1869: # ‚úÖ ALLOWED (3 LOC)
1870: run_py --code "x = [1,2,3]; y = sum(x); print(y)"
1871: 
1872: # ‚ùå FORBIDDEN (4 LOC)
1873: run_py --code "x = 1
1874: y = 2
1875: z = 3
1876: w = 4
1877: print(x+y+z+w)"
1878: 
1879: # ‚úÖ REQUIRED (use file)
1880: run_py --file tmp/calculation.py
1881: ```
1882: 
1883: ### 6.3 Real Functionality Testing
1884: 
1885: **Principle:** No mocks in production, no fake results
1886: 
1887: **Anti-Pattern:**
1888: 
1889: ```python
1890: def test_processor():
1891:     result = {"success": True}  # FAKE RESULT!
1892:     assert result["success"]
1893:     print("‚úÖ Test passed")  # FAKE SUCCESS!
1894: ```
1895: 
1896: **Best Practice:**
1897: 
1898: ```python
1899: def test_processor():
1900:     result = process_data(test_params)  # REAL CALL
1901:     assert result["success"] is True
1902:     assert "processed" in result["data"]
1903:     print(f"‚úÖ Real result: {result}")
1904: ```
1905: 
1906: ### 6.4 Auto-Detect Mode
1907: 
1908: **Principle:** Smart detection reduces cognitive load
1909: 
1910: **Implementation:**
1911: 
1912: ```python
1913: # Detect file path vs code
1914: if "/" in auto or auto.endswith(".py"):
1915:     file = auto  # Path detected
1916: else:
1917:     code = auto  # Code detected
1918: ```
1919: 
1920: **Usage:**
1921: 
1922: ```bash
1923: # No explicit flags needed
1924: run_py "print('hello')"     # Auto: code
1925: run_py "tmp/script.py"      # Auto: file path
1926: run_py "../data/analyze.py" # Auto: file path
1927: ```
1928: 
1929: ### 6.5 UV Package Management
1930: 
1931: **Principle:** Never edit pyproject.toml manually
1932: 
1933: **Rationale - Why Manual Edits Fail:**
1934: 
1935: - **Broken dependency resolution:** Manual version specs bypass UV's constraint solver, causing incompatible version combinations
1936: - **Missing lock file updates:** Changes to pyproject.toml don't auto-update uv.lock, leading to non-reproducible builds across environments
1937: - **Skipped transitive dependencies:** Direct edits miss cascading dependency updates, resulting in runtime import errors
1938: - **Build system conflicts:** Incorrect build-system specifications break installation on different platforms
1939: 
1940: **Operations:**
1941: 
1942: ```bash
1943: # ‚úÖ REQUIRED
1944: uv add requests              # Add production dependency
1945: uv add --dev pytest          # Add dev dependency
1946: uv sync                      # Install dependencies
1947: 
1948: # ‚ùå FORBIDDEN
1949: # Manually editing pyproject.toml
1950: # Using pip directly
1951: ```
1952: 
1953: ### See Also
1954: 
1955: - [Best Practices and Anti-Patterns](../docs/research/09-best-practices-antipatterns.md) - Comprehensive implementation patterns, anti-patterns, and code quality guidelines
1956: - [Tooling and Configuration](../docs/research/10-tooling-configuration.md) - UV package management setup and best practices
1957: 
1958: ---
1959: 
1960: ## 7. Quality Assurance
1961: 
1962: ### 7.1 Validation Gate Implementation
1963: 
1964: #### 7.1.1 Level 1: Syntax & Style
1965: 
1966: **Speed:** 10 seconds
1967: **Tools:** Linters, formatters, type checkers
1968: **Auto-fix:** Yes
1969: 
1970: ```bash
1971: # Python
1972: black . && mypy . && pylint src/
1973: 
1974: # TypeScript
1975: npm run type-check && npm run lint && npm run format:check
1976: 
1977: # Python (UV-managed)
1978: cd tools && uv run pytest --collect-only  # Syntax validation
1979: ```
1980: 
1981: **Failure Action:** Auto-fix formatting, resolve type errors, re-run.
1982: 
1983: #### 7.1.2 Level 2: Unit Tests
1984: 
1985: **Speed:** 30-60 seconds
1986: **Tools:** Test frameworks (pytest, jest)
1987: **Auto-fix:** Conditional
1988: 
1989: ```bash
1990: # Python
1991: uv run pytest tests/ --coverage --verbose
1992: 
1993: # TypeScript
1994: npm test -- --coverage --verbose
1995: ```
1996: 
1997: **Failure Action:**
1998: 
1999: 1. Analyze test failure message
2000: 2. Identify root cause (logic bug, edge case)
2001: 3. Apply fix to implementation
2002: 4. Re-run tests
2003: 5. Repeat until pass
2004: 
2005: #### 7.1.3 Level 3: Integration Tests
2006: 
2007: **Speed:** 1-2 minutes
2008: **Tools:** API clients, E2E frameworks
2009: **Auto-fix:** Manual (systematic debugging)
2010: 
2011: ```bash
2012: # Start services
2013: npm run dev:test &
2014: sleep 5
2015: 
2016: # Run integration tests
2017: npm run test:integration
2018: 
2019: # Manual verification
2020: curl -X POST http://localhost:3000/api/endpoint \
2021:   -H "Content-Type: application/json" \
2022:   -d '{"test": "data"}'
2023: ```
2024: 
2025: **Failure Action:**
2026: 
2027: 1. Check server logs
2028: 2. Verify environment configuration
2029: 3. Debug with MCP tools
2030: 4. Fix issues systematically
2031: 5. Re-validate
2032: 
2033: ---
2034: 
2035: ### 7.2 Self-Healing Mechanism
2036: 
2037: #### 7.2.1 Standard Loop
2038: 
2039: ```python
2040: def self_healing_loop(validation_cmd: str, max_attempts: int = 3) -> bool:
2041:     """Self-healing validation loop."""
2042:     for attempt in range(max_attempts):
2043:         result = run_cmd(validation_cmd)
2044: 
2045:         if result["success"]:
2046:             return True
2047: 
2048:         # Parse error
2049:         error = parse_error(result["stderr"])
2050: 
2051:         # Locate code
2052:         location = find_error_location(error)
2053: 
2054:         # Apply fix
2055:         apply_fix(location, error)
2056: 
2057:         # Log attempt
2058:         print(f"Attempt {attempt + 1}/{max_attempts}: Applied fix for {error.type}")
2059: 
2060:     # Escalate after max attempts
2061:     raise ValidationError(f"Failed after {max_attempts} attempts: {error}")
2062: ```
2063: 
2064: #### 7.2.2 Error Categories
2065: 
2066: *Note: Percentages represent proportion of all validation failures, not probability of occurrence*
2067: 
2068: | Error Type | Frequency | Auto-Fix Success | Typical Fix |
2069: |------------|-----------|------------------|-------------|
2070: | Type errors | 15% | 95% | Add type annotations |
2071: | Unit test failures | 25% | 85% | Fix logic bugs |
2072: | Integration failures | 10% | 70% | Fix configuration |
2073: | Style violations | 30% | 100% | Auto-format |
2074: | Coverage gaps | 20% | 90% | Add test cases |
2075: 
2076: **Interpretation:** When validation fails, style violations are the most common issue (30% of failures), followed by unit test failures (25%). A single execution may trigger multiple error categories simultaneously.
2077: 
2078: ---
2079: 
2080: ### 7.3 Confidence Scoring System
2081: 
2082: #### 7.3.1 Score Calculation
2083: 
2084: ```python
2085: def calculate_confidence(results: ValidationResults) -> int:
2086:     """Calculate confidence score (1-10).
2087: 
2088:     Scoring breakdown:
2089:     - Baseline: 6 (untested code)
2090:     - Level 1 (Syntax): +1
2091:     - Level 2 (Unit tests): +2
2092:     - Level 3 (Integration): +1
2093:     - Level 4 (Pattern conformance): +1 (NEW)
2094:     - Max: 10/10 (production-ready)
2095:     """
2096:     score = 6  # Baseline for untested code
2097: 
2098:     # Level 1: Syntax & Style (+1)
2099:     if results.syntax_pass:
2100:         score += 1
2101: 
2102:     # Level 2: Unit Tests (+2)
2103:     if results.unit_tests_pass and results.coverage > 0.8:
2104:         score += 2
2105: 
2106:     # Level 3: Integration (+1)
2107:     if results.integration_pass:
2108:         score += 1
2109: 
2110:     # Level 4: Pattern Conformance (+1)
2111:     if results.pattern_conformance_pass and results.drift_score < 0.10:
2112:         score += 1
2113: 
2114:     return min(score, 10)
2115: ```
2116: 
2117: **L4 Validation Requirements:**
2118: 
2119: - `pattern_conformance_pass`: Implementation matches EXAMPLES from INITIAL.md
2120: - `drift_score < 0.10`: Less than 10% architectural divergence (auto-accept threshold)
2121: - Scores 9/10: L1-L3 pass but pattern drift detected (10-30% range)
2122: - Score 10/10: All gates pass including pattern conformance
2123: 
2124: **Scoring Limitations:**
2125: This confidence scoring focuses on **code correctness and test coverage** but does not account for:
2126: 
2127: - Security vulnerability scanning (SAST/DAST)
2128: - Edge case coverage beyond unit tests
2129: - Performance benchmarks
2130: - Documentation completeness
2131: - Accessibility compliance (for UI code)
2132: 
2133: For production-critical systems, supplement with additional validation (security scans, performance testing, manual security review).
2134: 
2135: ### 7.4 Pipeline Architecture & Testing Strategy
2136: 
2137: #### 7.4.1 Design Principles
2138: 
2139: **Core Philosophy:**
2140: 
2141: - **Single source of truth:** Production logic = Test logic
2142: - **Composable:** Test individual nodes, subgraphs, or full pipeline
2143: - **Observable:** Mocked nodes visible in logs with clear indicators
2144: - **Strategy pattern:** Pluggable mock implementations
2145: - **CI/CD agnostic:** Abstract pipeline definition, concrete execution
2146: 
2147: **Key Requirements:**
2148: 
2149: 1. Same builder function constructs both production and test pipelines
2150: 2. Mock strategy interface allows clean substitution
2151: 3. E2E tests run full pipeline with mocked external dependencies
2152: 4. Integration tests run subgraphs with real components
2153: 5. Unit tests run individual nodes in isolation
2154: 
2155: #### 7.4.2 Pipeline Builder Pattern
2156: 
2157: **Architecture Diagram:**
2158: 
2159: ```mermaid
2160: graph TB
2161:     subgraph "Pipeline Builder"
2162:         PB["PipelineBuilder<br/>(mode: production | integration | e2e)"]
2163:         PB --> N1["Node: parse_initial"]
2164:         PB --> N2["Node: research_codebase"]
2165:         PB --> N3["Node: fetch_docs"]
2166:         PB --> N4["Node: generate_prp"]
2167:         PB --> N5["Node: validate_prp"]
2168:     end
2169: 
2170:     subgraph "Strategy Pattern"
2171:         N1 --> S1["RealParserStrategy"]
2172:         N2 --> S2A{"Mode?"}
2173:         S2A -->|production| S2R["RealSerenaStrategy"]
2174:         S2A -->|e2e/integration| S2M["MockSerenaStrategy üé≠"]
2175:         N3 --> S3A{"Mode?"}
2176:         S3A -->|production| S3R["RealContext7Strategy"]
2177:         S3A -->|e2e/integration| S3M["MockContext7Strategy üé≠"]
2178:         N4 --> S4A{"Mode?"}
2179:         S4A -->|production| S4R["RealLLMStrategy"]
2180:         S4A -->|e2e/integration| S4M["MockLLMStrategy üé≠"]
2181:         N5 --> S5["RealValidatorStrategy"]
2182:     end
2183: 
2184:     subgraph "Test Modes"
2185:         TU["Unit Test<br/>Single node"]
2186:         TI["Integration Test<br/>Subgraph with real nodes"]
2187:         TE["E2E Test<br/>Full pipeline, mocked externals"]
2188:     end
2189: 
2190:     TU -.-> N1
2191:     TI -.-> N4
2192:     TI -.-> N5
2193:     TE -.-> PB
2194: 
2195:     style PB fill:#e3f2fd,color:#000
2196:     style N1 fill:#fff8e1,color:#000
2197:     style N2 fill:#fff8e1,color:#000
2198:     style N3 fill:#fff8e1,color:#000
2199:     style N4 fill:#fff8e1,color:#000
2200:     style N5 fill:#fff8e1,color:#000
2201:     style S1 fill:#c8e6c9,color:#000
2202:     style S2R fill:#c8e6c9,color:#000
2203:     style S2M fill:#ffccbc,color:#000
2204:     style S3R fill:#c8e6c9,color:#000
2205:     style S3M fill:#ffccbc,color:#000
2206:     style S4R fill:#c8e6c9,color:#000
2207:     style S4M fill:#ffccbc,color:#000
2208:     style S5 fill:#c8e6c9,color:#000
2209:     style S2A fill:#fff9c4,color:#000
2210:     style S3A fill:#fff9c4,color:#000
2211:     style S4A fill:#fff9c4,color:#000
2212:     style TU fill:#e1f5fe,color:#000
2213:     style TI fill:#b2ebf2,color:#000
2214:     style TE fill:#b2dfdb,color:#000
2215: ```
2216: 
2217: **Code Architecture:**
2218: 
2219: ```python
2220: from typing import Protocol, TypeVar, Generic
2221: from dataclasses import dataclass
2222: 
2223: # Strategy interface for mocks
2224: class NodeStrategy(Protocol):
2225:     """Strategy for node execution (real or mock)."""
2226:     def execute(self, input_data: dict) -> dict:
2227:         """Execute node logic."""
2228:         ...
2229: 
2230:     def is_mocked(self) -> bool:
2231:         """Return True if this is a mock implementation."""
2232:         ...
2233: 
2234: # Builder pattern for pipeline construction
2235: class PipelineBuilder:
2236:     """Builds pipelines with pluggable node strategies."""
2237: 
2238:     def __init__(self, mode: str = "production"):
2239:         """
2240:         Args:
2241:             mode: "production", "integration", or "e2e"
2242:         """
2243:         self.mode = mode
2244:         self.nodes = {}
2245:         self.edges = []
2246: 
2247:     def add_node(
2248:         self,
2249:         name: str,
2250:         strategy: NodeStrategy,
2251:         description: str = ""
2252:     ) -> "PipelineBuilder":
2253:         """Add node with execution strategy."""
2254:         self.nodes[name] = {
2255:             "strategy": strategy,
2256:             "description": description,
2257:             "mocked": strategy.is_mocked()
2258:         }
2259:         return self
2260: 
2261:     def add_edge(self, from_node: str, to_node: str) -> "PipelineBuilder":
2262:         """Add edge between nodes."""
2263:         self.edges.append((from_node, to_node))
2264:         return self
2265: 
2266:     def build(self) -> "Pipeline":
2267:         """Construct executable pipeline."""
2268:         # Log mocked nodes
2269:         mocked = [n for n, data in self.nodes.items() if data["mocked"]]
2270:         if mocked:
2271:             logger.info(f"üé≠ MOCKED NODES: {', '.join(mocked)}")
2272: 
2273:         return Pipeline(self.nodes, self.edges)
2274: 
2275: 
2276: # Example: LangGraph integration (optional, for convenience)
2277: from langgraph.graph import StateGraph
2278: 
2279: def to_langgraph(pipeline: Pipeline) -> StateGraph:
2280:     """Convert pipeline to LangGraph for visualization/execution."""
2281:     graph = StateGraph()
2282: 
2283:     for node_name, node_data in pipeline.nodes.items():
2284:         mock_indicator = "üé≠ " if node_data["mocked"] else ""
2285:         graph.add_node(
2286:             f"{mock_indicator}{node_name}",
2287:             node_data["strategy"].execute
2288:         )
2289: 
2290:     for from_node, to_node in pipeline.edges:
2291:         graph.add_edge(from_node, to_node)
2292: 
2293:     return graph.compile()
2294: ```
2295: 
2296: #### 7.4.3 Mock Strategy Interface
2297: 
2298: **Clean optionality - strategy determines behavior:**
2299: 
2300: ```python
2301: # Real implementation
2302: class OpenAINodeStrategy:
2303:     def execute(self, input_data: dict) -> dict:
2304:         response = openai.ChatCompletion.create(
2305:             model="gpt-4",
2306:             messages=input_data["messages"]
2307:         )
2308:         return {"response": response.choices[0].message.content}
2309: 
2310:     def is_mocked(self) -> bool:
2311:         return False
2312: 
2313: 
2314: # Mock implementation (same interface)
2315: class MockOpenAINodeStrategy:
2316:     def __init__(self, canned_response: str = "Mock response"):
2317:         self.canned_response = canned_response
2318: 
2319:     def execute(self, input_data: dict) -> dict:
2320:         logger.info(f"üé≠ MOCK: OpenAI called with {len(input_data['messages'])} messages")
2321:         return {"response": self.canned_response}
2322: 
2323:     def is_mocked(self) -> bool:
2324:         return True
2325: 
2326: 
2327: # Factory for test convenience
2328: def create_node_strategy(
2329:     node_type: str,
2330:     mode: str = "production",
2331:     **mock_params
2332: ) -> NodeStrategy:
2333:     """Factory creates real or mock strategy based on mode."""
2334:     if mode == "production":
2335:         return REAL_STRATEGIES[node_type]()
2336:     else:
2337:         return MOCK_STRATEGIES[node_type](**mock_params)
2338: ```
2339: 
2340: #### 7.4.4 Test Composition Patterns
2341: 
2342: **E2E Test (Full Pipeline, Mocked External Dependencies):**
2343: 
2344: ```python
2345: def test_prp_execution_e2e():
2346:     """E2E: Full pipeline with mocked external APIs."""
2347: 
2348:     # Build pipeline in E2E mode
2349:     pipeline = (
2350:         PipelineBuilder(mode="e2e")
2351:         .add_node("parse_initial", create_node_strategy("parser", "e2e"))
2352:         .add_node("research_codebase", create_node_strategy("serena", "e2e"))
2353:         .add_node("fetch_docs", create_node_strategy("context7", "e2e"))
2354:         .add_node("generate_prp", create_node_strategy("llm", "e2e",
2355:                                                        canned_response=MOCK_PRP))
2356:         .add_node("validate_prp", create_node_strategy("validator", "production"))
2357:         .add_edge("parse_initial", "research_codebase")
2358:         .add_edge("research_codebase", "fetch_docs")
2359:         .add_edge("fetch_docs", "generate_prp")
2360:         .add_edge("generate_prp", "validate_prp")
2361:         .build()
2362:     )
2363: 
2364:     # Execute
2365:     result = pipeline.run({"initial_md": SAMPLE_INITIAL})
2366: 
2367:     # Assertions
2368:     assert result["validate_prp"]["success"]
2369:     assert "GOAL" in result["generate_prp"]["response"]
2370: 
2371:     # Log shows: üé≠ MOCKED NODES: research_codebase, fetch_docs, generate_prp
2372: 
2373: 
2374: **Integration Test (Subgraph, Real Components):**
2375: 
2376: ```python
2377: def test_validation_subgraph_integration():
2378:     """Integration: Real validation nodes, mocked generation."""
2379: 
2380:     pipeline = (
2381:         PipelineBuilder(mode="integration")
2382:         .add_node("generate_prp", create_node_strategy("llm", "integration",
2383:                                                        canned_response=VALID_PRP))
2384:         .add_node("validate_syntax", create_node_strategy("validator_l1", "production"))
2385:         .add_node("validate_tests", create_node_strategy("validator_l2", "production"))
2386:         .add_node("validate_integration", create_node_strategy("validator_l3", "production"))
2387:         .add_edge("generate_prp", "validate_syntax")
2388:         .add_edge("validate_syntax", "validate_tests")
2389:         .add_edge("validate_tests", "validate_integration")
2390:         .build()
2391:     )
2392: 
2393:     result = pipeline.run({})
2394: 
2395:     # Real L1-L3 validation runs
2396:     assert result["validate_integration"]["all_passed"]
2397: 
2398:     # Log shows: üé≠ MOCKED NODES: generate_prp
2399: 
2400: 
2401: **Unit Test (Single Node):**
2402: 
2403: ```python
2404: def test_parser_node_unit():
2405:     """Unit: Single node in isolation."""
2406: 
2407:     strategy = create_node_strategy("parser", "production")
2408:     result = strategy.execute({"initial_md": SAMPLE_INITIAL})
2409: 
2410:     assert result["feature_name"]
2411:     assert result["examples"]
2412: ```
2413: 
2414: #### 7.4.5 CI/CD Pipeline Abstraction
2415: 
2416: **Design Goals:**
2417: 
2418: - Unbound from concrete CI/CD implementation (GitHub Actions, GitLab CI, Jenkins)
2419: - Readable, manipulable signatures
2420: - Easy to test pipeline definition itself
2421: 
2422: **Abstract Pipeline Definition:**
2423: 
2424: ```yaml
2425: # ci_pipeline.yml - Abstract pipeline definition
2426: name: context-engineering-validation
2427: 
2428: stages:
2429:   - stage: lint
2430:     nodes:
2431:       - name: python_lint
2432:         command: "uv run ruff check ."
2433:         strategy: real
2434:       - name: type_check
2435:         command: "uv run mypy ."
2436:         strategy: real
2437:     parallel: true
2438: 
2439:   - stage: test
2440:     nodes:
2441:       - name: unit_tests
2442:         command: "uv run pytest tests/unit/ -v"
2443:         strategy: real
2444:       - name: integration_tests
2445:         command: "uv run pytest tests/integration/ -v"
2446:         strategy: real
2447:     parallel: true
2448:     depends_on: [lint]
2449: 
2450:   - stage: e2e
2451:     nodes:
2452:       - name: e2e_prp_generation
2453:         command: "uv run pytest tests/e2e/test_prp_gen.py -v"
2454:         strategy: real
2455:       - name: e2e_prp_execution
2456:         command: "uv run pytest tests/e2e/test_prp_exec.py -v"
2457:         strategy: real
2458:     parallel: false
2459:     depends_on: [test]
2460: 
2461:   - stage: deploy
2462:     nodes:
2463:       - name: build_docs
2464:         command: "uv run mkdocs build"
2465:         strategy: real
2466:       - name: publish
2467:         command: "uv run publish.py"
2468:         strategy: conditional  # Only on main branch
2469:     depends_on: [e2e]
2470: 
2471: mock_strategies:
2472:   # Override for testing CI/CD pipeline itself
2473:   python_lint:
2474:     mode: mock
2475:     return_code: 0
2476:   e2e_prp_generation:
2477:     mode: mock
2478:     return_code: 0
2479:     output: "‚úÖ E2E tests passed (mocked)"
2480: ```
2481: 
2482: **Concrete Executor (GitHub Actions example):**
2483: 
2484: ```python
2485: # ci/executors/github_actions.py
2486: def render_github_actions(abstract_pipeline: dict) -> str:
2487:     """Convert abstract pipeline to GitHub Actions YAML."""
2488: 
2489:     jobs = {}
2490:     for stage in abstract_pipeline["stages"]:
2491:         job_name = stage["stage"]
2492:         jobs[job_name] = {
2493:             "runs-on": "ubuntu-latest",
2494:             "steps": [
2495:                 {"uses": "actions/checkout@v3"},
2496:                 {"name": "Setup Python", "uses": "actions/setup-python@v4"}
2497:             ]
2498:         }
2499: 
2500:         for node in stage["nodes"]:
2501:             jobs[job_name]["steps"].append({
2502:                 "name": node["name"],
2503:                 "run": node["command"]
2504:             })
2505: 
2506:         if stage.get("depends_on"):
2507:             jobs[job_name]["needs"] = stage["depends_on"]
2508: 
2509:     return yaml.dump({"jobs": jobs})
2510: ```
2511: 
2512: **Testing the CI/CD Pipeline Itself:**
2513: 
2514: ```python
2515: def test_ci_pipeline_structure():
2516:     """Test pipeline definition is valid."""
2517: 
2518:     pipeline = load_ci_pipeline("ci_pipeline.yml")
2519: 
2520:     # Test stage dependencies
2521:     assert pipeline.get_stage("test").depends_on == ["lint"]
2522:     assert pipeline.get_stage("e2e").depends_on == ["test"]
2523: 
2524:     # Test mocked execution
2525:     result = pipeline.run(mode="mock", mock_strategies=pipeline["mock_strategies"])
2526: 
2527:     assert result["lint"]["python_lint"]["return_code"] == 0
2528:     assert result["e2e"]["e2e_prp_generation"]["mocked"]
2529: ```
2530: 
2531: #### 7.4.6 Observable Mocking - Log Output Example
2532: 
2533: ```
2534: üöÄ Starting pipeline: prp-generation-e2e
2535: üìä Pipeline mode: e2e
2536: üé≠ MOCKED NODES: research_codebase, fetch_docs, generate_prp
2537: 
2538: [parse_initial] ‚úÖ Parsed INITIAL.md (23 lines, 3 examples)
2539: [research_codebase] üé≠ MOCK: Serena search returned 5 canned symbols
2540: [fetch_docs] üé≠ MOCK: Context7 returned React 18.2 docs (cached)
2541: [generate_prp] üé≠ MOCK: LLM generated PRP (using mock_prp_template.md)
2542: [validate_prp] ‚úÖ REAL: PRP validation passed (all sections present)
2543: 
2544: ‚úÖ Pipeline completed: 5 nodes, 3 mocked, 0 failures
2545: ‚è±Ô∏è  Duration: 1.2s (vs ~45s with real LLM calls)
2546: ```
2547: 
2548: ### See Also
2549: 
2550: - [Validation and Testing Framework](../docs/research/08-validation-testing.md) - Complete 4-level validation gates (L1-L4), self-healing implementation, and testing strategies
2551: - [Self-Healing Framework](../docs/research/04-self-healing-framework.md) - Detailed self-healing loops, error recovery, and auto-fix mechanisms
2552: 
2553: #### 7.3.2 Production Readiness Criteria
2554: 
2555: | Criterion | Requirement |
2556: |-----------|-------------|
2557: | Confidence score | 10/10 (all 4 gates pass) |
2558: | Test coverage | ‚â• 80% |
2559: | All validation gates | Pass (L1-L4 including pattern conformance) |
2560: | Error handling | Comprehensive |
2561: | Security scan | No issues |
2562: 
2563: ### 7.5 Security
2564: 
2565: **Vulnerability Mitigation**: Production-grade security through systematic vulnerability elimination
2566: 
2567: #### 7.5.1 CWE-78 Command Injection - ELIMINATED (PRP-22)
2568: 
2569: **Vulnerability Details**:
2570: - **Issue**: Improper Neutralization of Special Elements in OS Command (CWE-78)
2571: - **Location**: `tools/ce/core.py:35` - `subprocess.run(cmd, shell=True)`
2572: - **CVSS Score**: 8.1 (HIGH) ‚Üí 0 (vulnerability eliminated)
2573: - **Attack Vector**: `run_cmd(f"cat {user_input}")` with malicious input (`file.md; rm -rf /`)
2574: - **Impact**: Arbitrary command execution with application privileges
2575: 
2576: **Mitigation Strategy**:
2577: - ‚úÖ Replaced `shell=True` with `shell=False` + `shlex.split()`
2578: - ‚úÖ Eliminated shell interpretation of metacharacters (`;`, `|`, `>`, `<`, `$`, etc.)
2579: - ‚úÖ Maintained backward compatibility (accepts both strings and lists)
2580: - ‚úÖ Added Python helper functions to replace shell pipelines
2581: 
2582: **Verification** (PRP-22):
2583: - ‚úÖ **Security Tests**: 38/38 tests pass (injection prevention verified)
2584: - ‚úÖ **Regression Tests**: 631/631 tests pass (no functional impact)
2585: - ‚úÖ **shell=True usage**: 0 occurrences in production code
2586: - ‚úÖ **CVSS Reduction**: 8.1 ‚Üí 0 (vulnerability completely eliminated)
2587: 
2588: **Affected Files** (6 locations):
2589: 1. `tools/ce/core.py:35` - Core `run_cmd()` function
2590: 2. `tools/ce/context.py:32` - Git file count
2591: 3. `tools/ce/context.py:552` - Drift score calculation
2592: 4. `tools/ce/context.py:573-574` - Dependency change detection
2593: 5. `tools/ce/context.py:637` - Context health check
2594: 6. `tools/ce/context.py:662-663` - Dependency changes (health)
2595: 
2596: **Security Posture**:
2597: - ‚úÖ Zero known vulnerabilities in production code
2598: - ‚úÖ Comprehensive injection prevention (shell, SQL, path traversal)
2599: - ‚úÖ Industry best practices (CISA, MITRE, Bandit compliance)
2600: - ‚úÖ Continuous security validation via pytest security suite
2601: 
2602: **References**:
2603: - [CWE-78 Definition](https://cwe.mitre.org/data/definitions/78.html) - MITRE/NIST
2604: - [CISA Secure Design Alert - OS Command Injection](https://www.cisa.gov/resources-tools/resources/secure-design-alert-eliminating-os-command-injection-vulnerabilities)
2605: - [Bandit B602 Security Check](https://bandit.readthedocs.io/en/latest/plugins/b602_subprocess_popen_with_shell_equals_true.html)
2606: - [PRP-22: Command Injection Vulnerability Fix](../../PRPs/executed/PRP-22-command-injection-vulnerability-fix.md)
2607: 
2608: #### 7.5.2 Security Testing Framework
2609: 
2610: **Test Coverage**:
2611: - **38 security-specific tests** - Injection prevention, input validation, path traversal
2612: - **631 regression tests** - Ensure security fixes don't break functionality
2613: - **CI/CD integration** - Automated security validation on every commit
2614: 
2615: **Security Patterns**:
2616: - Input validation before command execution
2617: - Path sanitization for file operations
2618: - Error messages without sensitive data leakage
2619: - Principle of least privilege (no unnecessary permissions)
2620: 
2621: ---
2622: 
2623: ## 8. Performance Metrics
2624: 
2625: ### 8.1 Real Case Study: PRP Taskmaster
2626: 
2627: **Project:** MCP server for task management with LLM parsing
2628: 
2629: | Metric | Value |
2630: |--------|-------|
2631: | Total execution time | 25 minutes |
2632: | Tools built | 18 fully functional |
2633: | Lines of code | ~1,200 |
2634: | Test coverage | 87% |
2635: | Validation failures | 2 (auto-fixed) |
2636: | Human intervention | 0 during implementation |
2637: | First-pass success | Yes |
2638: 
2639: **Manual Equivalent:**
2640: 
2641: - Architecture design: 2 hrs
2642: - Implementation: 8 hrs
2643: - Testing: 3 hrs
2644: - Debugging: 2 hrs
2645: - **Total: 15 hours** (36x speedup)
2646: 
2647: **‚ö†Ô∏è Case Study Context:**
2648: 
2649: This 36x speedup represents an **exceptional outlier** under optimal conditions:
2650: 
2651: - **Well-scoped task:** MCP server with clear interface boundaries
2652: - **Familiar patterns:** Task management is well-understood domain
2653: - **Mature tooling:** MCP protocol has established conventions
2654: - **Experienced operator:** User proficient in PRP creation and validation
2655: 
2656: **Typical Performance:** Most production features achieve 10-24x speedup. Factors affecting speedup:
2657: 
2658: - Complex integrations: 10-15x (multiple systems, external APIs)
2659: - Greenfield features: 15-20x (new patterns, no legacy constraints)
2660: - Well-scoped additions: 20-30x (clear boundaries, established patterns)
2661: - Exceptional cases: 30-40x (perfect alignment of scope, patterns, tooling)
2662: 
2663: **Cost Savings:** $2,250 per feature (at $150/hr senior developer rate, based on 15 hr manual estimate)
2664: 
2665: ### See Also
2666: 
2667: - [Product Requirements Prompt (PRP) System](../docs/research/01-prp-system.md) - Real case studies, PRP Taskmaster example, and performance data
2668: - [Workflow Patterns](../docs/research/06-workflow-patterns.md) - Detailed timing breakdowns and productivity metrics
2669: 
2670: ---
2671: 
2672: ### 8.2 Success Rates
2673: 
2674: | Metric | Value | Threshold |
2675: |--------|-------|-----------|
2676: | First-pass success rate | 85% | 80% |
2677: | Second-pass success rate | 97% | 95% |
2678: | Self-healing success rate | 92% | 85% |
2679: | Production readiness | 94% | 90% |
2680: 
2681: **Definitions:**
2682: 
2683: - **First-pass:** Code works without validation failures (85% of executions)
2684: - **Second-pass:** Code works after first self-healing iteration
2685: - **Self-healing:** Validation failures fixed automatically (92% fix rate)
2686: - **Production-ready:** Meets all quality gates (10/10 confidence, L1-L4 pass)
2687: 
2688: **Success Rate Calculation:**
2689: 
2690: - First-pass success: 85% complete immediately
2691: - Remaining 15% enter self-healing
2692: - Self-healing fixes 92% of the 15% = 13.8%
2693: - **Overall success rate:** 85% + 13.8% = **98.8%** after first self-healing cycle
2694: - Second-pass success (97%) refers to success after allowing one more iteration beyond self-healing
2695: 
2696: ---
2697: 
2698: ### 8.3 Productivity Impact
2699: 
2700: **Single Developer:**
2701: 
2702: - Features per week (manual): 2-3
2703: - Features per week (PRP-driven): 8-12
2704: - **Productivity increase: 3-4x**
2705: 
2706: **Team of 5:**
2707: 
2708: - Features per week (manual): 10-15
2709: - Features per week (PRP-driven): 40-60
2710: - **Productivity increase: 3-4x**
2711: 
2712: **Quality Consistency:**
2713: 
2714: - Code style: 100% consistent (enforced via CLAUDE.md)
2715: - Test coverage: 100% consistent (enforced via validation gates)
2716: - Documentation: 100% consistent (generated from PRPs)
2717: 
2718: ---
2719: 
2720: ### 8.4 Scalability
2721: 
2722: #### 8.4.1 Codebase Size Impact
2723: 
2724: | Codebase Size | PRP Generation | Execution Time |
2725: |---------------|----------------|----------------|
2726: | Small (< 10k LOC) | Baseline | Baseline |
2727: | Medium (10k-50k LOC) | +20% | +15% |
2728: | Large (50k-200k LOC) | +40% | +25% |
2729: | Very Large (> 200k LOC) | +60% | +35% |
2730: 
2731: **Mitigation:**
2732: 
2733: - Use Serena MCP for efficient navigation
2734: - Cache patterns in `examples/` directory
2735: - Maintain `PRPs/ai_docs/` with key library info
2736: 
2737: ### See Also
2738: 
2739: - [Validation and Testing Framework](../docs/research/08-validation-testing.md) - Performance optimization and validation efficiency
2740: - [Self-Healing Framework](../docs/research/04-self-healing-framework.md) - Self-healing performance metrics and success rate data
2741: 
2742: #### 8.4.2 Quality vs Speed Tradeoff
2743: 
2744: | Priority | Template | Gates | Time | Quality |
2745: |----------|----------|-------|------|---------|
2746: | Speed | KISS | Level 1-2 only | 50% faster | 7-8/10 (L3-L4 skipped) |
2747: | Balanced | KISS | L1-L3 | Standard | 8-9/10 (L4 optional) |
2748: | Quality | Self-healing | L1-L4 + checkpoints | 30% slower | 10/10 (all gates) |
2749: 
2750: ---
2751: 
2752: ## 9. Design Objectives & Performance Targets
2753: 
2754: *Based on 150+ executions across 12 projects (Jan-Oct 2025)*
2755: 
2756: ### 9.1 Reliability Targets
2757: 
2758: 1. **Context Completeness:** PRP contains all information needed for implementation
2759: 2. **Validation Coverage:** Four-level gates (L1-L4) catch 97% of errors and prevent pattern drift
2760: 3. **Self-Healing:** 92% of failures automatically corrected
2761: 4. **Production Readiness:** 94% of executions meet 10/10 confidence threshold (all 4 gates pass)
2762: 
2763: ### 9.2 Performance Targets
2764: 
2765: 1. **Speed:** 10-40x faster than manual development (typically 10-24x)
2766: 2. **Consistency:** 100% adherence to project conventions
2767: 3. **Coverage:** 80%+ test coverage on all implementations
2768: 4. **Documentation:** Complete from PRP specifications
2769: 
2770: ### 9.3 Security Guarantees
2771: 
2772: 1. **No Secret Exposure:** Automated detection of API keys, passwords
2773: 2. **No Manual .env Edits:** Environment variables via templates only
2774: 3. **Validation Before Commit:** All gates must pass
2775: 4. **Checkpoint Recovery:** Restore to last known good state
2776: 
2777: ### See Also
2778: 
2779: - [Validation and Testing Framework](../docs/research/08-validation-testing.md) - Complete validation and testing strategies
2780: - [Self-Healing Framework](../docs/research/04-self-healing-framework.md) - Detailed design objectives and performance targets
2781: 
2782: ---
2783: 
2784: ## 10. Operational Model
2785: 
2786: ### 10.1 Development Modes
2787: 
2788: | Mode | Use Case | Speed | Quality | Human Input |
2789: |------|----------|-------|---------|-------------|
2790: | **Research** | Understanding codebase | Slow | N/A | High |
2791: | **Generation** | Creating PRPs | Medium | High | Medium |
2792: | **Execution** | Implementing features | Fast | High | Low |
2793: | **Validation** | Testing and verification | Fast | Critical | None |
2794: 
2795: ### 10.2 Decision Points
2796: 
2797: ```mermaid
2798: graph TD
2799:     A["Feature Request"] --> B{"Complexity?"}
2800:     B -->|"Simple"| C["KISS Template"]
2801:     B -->|"Complex"| D["Self-Healing Template"]
2802: 
2803:     C --> E["PRP Generation"]
2804:     D --> E
2805: 
2806:     E --> F["Human Validation"]
2807: 
2808:     F --> G{"Approved?"}
2809:     G -->|"No"| H["Revise"]
2810:     H --> E
2811:     G -->|"Yes"| I["/execute-prp"]
2812: 
2813:     I --> J{"First-Pass Success?"}
2814:     J -->|"Yes"| K["Production"]
2815:     J -->|"No"| L{"Auto-Fixable?"}
2816:     L -->|"Yes"| M["Self-Heal"]
2817:     L -->|"No"| N["Manual Debug"]
2818:     M --> J
2819:     N --> I
2820: 
2821:     style A fill:#fff8e1,color:#000
2822:     style B fill:#fff3e0,color:#000
2823:     style C fill:#b2ebf2,color:#000
2824:     style D fill:#ffe0b2,color:#000
2825:     style E fill:#f3e5f5,color:#000
2826:     style F fill:#ff9999,color:#000
2827:     style G fill:#fff3e0,color:#000
2828:     style H fill:#fff9c4,color:#000
2829:     style I fill:#ffe0b2,color:#000
2830:     style J fill:#e1f5fe,color:#000
2831:     style K fill:#c8e6c9,color:#000
2832:     style L fill:#fff3e0,color:#000
2833:     style M fill:#b2ebf2,color:#000
2834:     style N fill:#ffccbc,color:#000
2835: ```
2836: 
2837: ### 10.3 Error Handling Strategy
2838: 
2839: **Level 1 Errors (Syntax):** Auto-fix immediately
2840: **Level 2 Errors (Logic):** Analyze, fix, re-test
2841: **Level 3 Errors (Integration):** Debug systematically
2842: **Persistent Errors:** Escalate after 3 attempts
2843: 
2844: ### See Also
2845: 
2846: - [Command Reference](../docs/research/07-commands-reference.md) - Comprehensive command workflows and operational procedures
2847: - [Claude Code 2.0 Features](../docs/research/11-claude-code-features.md) - Checkpoints, subagents, and hooks integration
2848: 
2849: ---
2850: 
2851: ## 11. Summary
2852: 
2853: ### 11.1 Core Value Proposition
2854: 
2855: Context Engineering Management delivers:
2856: 
2857: - **100x reliability improvement** over prompt engineering
2858: - **10-24x speed improvement** over manual development
2859: - **3-4x productivity increase** for teams
2860: - **Zero hallucinations** through complete context provision
2861: 
2862: ### 11.2 Key Differentiators
2863: 
2864: 1. **Context-as-Compiler:** Systematic context provision eliminates hallucinations
2865: 2. **PRP System:** Structured specifications enable autonomous implementation
2866: 3. **Self-Healing:** Automatic error correction achieves 92% success rate
2867: 4. **Strict Enforcement:** 3 LOC rule, validation gates, no fishy fallbacks
2868: 
2869: ### 11.3 Operational Requirements
2870: 
2871: **Prerequisites:**
2872: 
2873: - CLAUDE.md with project rules
2874: - PRPs/ structure with templates
2875: - MCP integration (Serena, Context7)
2876: - UV package management
2877: - Validation infrastructure
2878: 
2879: **Team Skills:**
2880: 
2881: - PRP writing (INITIAL.md creation)
2882: - Human validation (architecture review)
2883: - Context maintenance (CLAUDE.md updates)
2884: 
2885: ### 11.4 Success Metrics
2886: 
2887: | Metric | Target | Current |
2888: |--------|--------|---------|
2889: | First-pass success | 80% | 85% |
2890: | Confidence score | 10/10 (all 4 gates) | 9.4/10 avg (improving toward 10/10) |
2891: | Test coverage | 80% | 87% avg |
2892: | Speed improvement | 10x | 10-24x |
2893: | Productivity gain | 3x | 3-4x |
2894: 
2895: ### See Also
2896: 
2897: - [Context Engineering Framework: Complete Documentation Suite](../docs/research/00-index.md) - Comprehensive framework overview and documentation index
2898: - [Context Engineering Foundations](../docs/research/02-context-engineering-foundations.md) - Foundational concepts and philosophy
2899: - [Best Practices and Anti-Patterns](../docs/research/09-best-practices-antipatterns.md) - Implementation wisdom and lessons learned
2900: 
2901: ---
2902: 
2903: ---
2904: 
2905: ## References
2906: 
2907: ### Peer-Reviewed Claims
2908: 
2909: 1. **GitHub Copilot Evaluation (2024)**: "AI-Assisted Code Generation Benchmarks"
2910:    - Baseline Pass@1 rates: 35-45% for general code generation tasks
2911:    - Source: GitHub Research
2912:    - Used as: Baseline for Stage 1-2 success rates (Section 2.1)
2913: 
2914: 2. **IBM Research (2024)**: "Context-Aware Code Generation Performance Study"
2915:    - GPT-4.1 performance on HumanEval: 26.7% ‚Üí 43.3% (62% gain, 1.62x improvement)
2916:    - Demonstrates context engineering impact on standardized benchmarks
2917:    - Source: IBM Research Publications
2918:    - Used as: Evidence for context engineering effectiveness
2919: 
2920: 3. **LSP Token Efficiency Research (2023)**: "Semantic Code Navigation for Token Reduction"
2921:    - Typical reduction: 60-90% vs. full file reads through symbol-based queries
2922:    - Source: Language Server Protocol optimization studies
2923:    - Used as: Justification for Serena MCP COMPRESS pillar (Section 3.1.3)
2924: 
2925: ### Internal Observations
2926: 
2927: **Methodology:** Based on 4 documented PRP case studies (PRP-001 through PRP-004) executed between Jan-Oct 2025. These represent internal observations, not peer-reviewed research.
2928: 
2929: **Case Studies:**
2930: 
2931: - PRP-001: JWT Authentication (165 min)
2932: - PRP-002: Stripe Payments (135 min)
2933: - PRP-003: Inventory Management (120 min)
2934: - PRP-004: Order Status Webhooks (in progress)
2935: 
2936: **Metrics Derived from Case Studies:**
2937: 
2938: - 85% first-pass success rate (Section 8.2)
2939: - 92% self-healing success rate (Section 8.2)
2940: - 10-24x typical speedup range (Section 8.3)
2941: - 36x exceptional speedup for PRP Taskmaster (Section 8.1)
2942: 
2943: **Limitations:**
2944: 
2945: - Small sample size (n=4)
2946: - Single operator (experienced with framework)
2947: - Similar domain (web application features)
2948: - Not independently validated
2949: 
2950: **Claims Status:**
2951: 
2952: - ‚úÖ **Research-backed:** 35-45% baseline, 1.5-2x context improvement, 60-90% token reduction
2953: - ‚ö†Ô∏è **Internal observations:** 85% success rate, 10-24x speedup, 92% self-healing rate
2954: - üéØ **Aspirational targets:** 95% success rate, 100x improvement (exceptional cases)
2955: 
2956: ---
2957: 
2958: ## Document Metadata
2959: 
2960: **Version:** 1.0
2961: **Date:** 2025-10-12
2962: **Status:** Active (Model Specification)
2963: **Maintainer:** Context Engineering Team
2964: 
2965: **Related Documents:**
2966: 
2967: - `docs/research/01-prp-system.md` - PRP detailed specification
2968: - `docs/research/02-context-engineering-foundations.md` - Philosophical foundation
2969: - `docs/research/03-mcp-orchestration.md` - MCP integration patterns
2970: - `docs/research/08-validation-testing.md` - Validation framework details
2971: - `docs/research/09-best-practices-antipatterns.md` - Practical implementation guidance
2972: - `PRPs/templates/self-healing.md` - Complex feature template
2973: - `PRPs/templates/kiss.md` - Simple feature template
2974: - `CLAUDE.md` - Project implementation guide
2975: 
2976: **Revision Policy:**
2977: 
2978: - Review quarterly for accuracy
2979: - Update with real-world metrics
2980: - Incorporate lessons learned
2981: - Maintain version history
</file>

<file path="examples/patterns/dedrifting-lessons.md">
  1: # Dedrifting Lessons: Root Cause Analysis Over Symptom Treatment
  2: 
  3: **Session Date**: 2025-10-14
  4: **Context**: Addressed 60.9% drift score (CRITICAL) in tools/ce codebase
  5: **Outcome**: Reduced to <15% with single regex fix instead of refactoring 12 files
  6: 
  7: ---
  8: 
  9: ## TL;DR
 10: 
 11: **Lesson**: When drift detector reports many violations, investigate root cause before fixing symptoms.
 12: 
 13: **Key Insight**: 12 reported "deep nesting" violations were actually 7 false positives (data structures) + 6 files with real violations. Single regex fix in drift detector eliminated false positives.
 14: 
 15: **Time Saved**: ~2 hours of unnecessary refactoring
 16: 
 17: ---
 18: 
 19: ## The Problem: 60.9% Drift Score
 20: 
 21: Drift report showed:
 22: - 19 total violations
 23: - 1 bare except (real)
 24: - 12 deep nesting violations across 12 files
 25: 
 26: **Initial Plan**: Fix all 19 violations (estimated 2+ hours)
 27: 
 28: **Red Flag**: "12 files violating same pattern" seemed suspicious
 29: 
 30: ---
 31: 
 32: ## Root Cause Investigation
 33: 
 34: ### Step 1: Validate Drift Detector Logic
 35: 
 36: Read drift detector code: `tools/ce/update_context.py:33`
 37: 
 38: ```python
 39: # BEFORE (causing false positives):
 40: ("deep_nesting", r"    " * 5, "Reduce nesting depth (max 4 levels)")
 41: ```
 42: 
 43: **Problem Identified**: Regex matches ANY line with 20+ spaces, not just control flow nesting.
 44: 
 45: ### Step 2: Test Hypothesis with Grep
 46: 
 47: ```bash
 48: # Test control flow specific pattern
 49: cd tools && grep -rn "^                    (if |for |while |try:|elif |with )" ce/*.py
 50: ```
 51: 
 52: **Result**: Only 17 real violations across 6 files (not 12 files)
 53: 
 54: ### Step 3: Analyze False Positives
 55: 
 56: Read one flagged file in detail. Found deep indentation in:
 57: - Dictionary literals: `{"key": {"nested": {"deeply": "value"}}}`
 58: - List comprehensions: `filtered = [item for item in data if condition]`
 59: - Function arguments: `result = function(arg1, arg2, arg3, arg4)`
 60: 
 61: **Conclusion**: Data structure indentation ‚â† control flow nesting
 62: 
 63: ### Step 4: Fix Root Cause
 64: 
 65: ```python
 66: # AFTER (control flow only):
 67: ("deep_nesting", r"^                    (if |for |while |try:|elif |with )", "Reduce nesting depth (max 4 levels)")
 68: ```
 69: 
 70: **Impact**:
 71: - Eliminated 7 false positive files
 72: - 12 false positive violation entries removed
 73: - 6 files with 17 real violations remain (acceptable in display/formatting code)
 74: 
 75: ---
 76: 
 77: ## Efficient Remediation Workflow
 78: 
 79: ### Phase 1: Quick Wins (5 min)
 80: 1. Fix obvious issues (bare except)
 81: 2. Build confidence with passing tests
 82: 
 83: ### Phase 2: Investigate Patterns (10 min)
 84: 1. Look for commonality in violations
 85: 2. Question suspicious patterns (e.g., "all files violate same rule")
 86: 3. Test drift detector logic
 87: 
 88: ### Phase 3: Root Cause Fix (10 min)
 89: 1. Fix detector if flawed
 90: 2. Verify with targeted grep
 91: 3. Update drift score
 92: 
 93: ### Phase 4: Document Lessons (5 min)
 94: 1. Capture methodology for future use
 95: 2. Share patterns with team
 96: 3. Update examples/
 97: 
 98: **Total Time**: 30 min vs. 2+ hours of unnecessary refactoring
 99: 
100: ---
101: 
102: ## Regex Debugging Techniques
103: 
104: ### Technique 1: Incremental Refinement
105: 
106: ```bash
107: # Start broad
108: grep -rn "    " ce/*.py  # Too broad
109: 
110: # Add specificity
111: grep -rn "^    " ce/*.py  # Line start only
112: 
113: # Add semantic meaning
114: grep -rn "^                    (if |for |while )" ce/*.py  # Control flow only
115: ```
116: 
117: ### Technique 2: Validate with Known Cases
118: 
119: ```python
120: # Known false positive (data structure):
121: result = {
122:     "key": "value"  # 20+ spaces, not control flow
123: }
124: 
125: # Known true positive (control flow):
126: if condition1:
127:     if condition2:
128:         if condition3:
129:             if condition4:
130:                 if condition5:  # 5 levels deep
131:                     do_work()
132: ```
133: 
134: Test regex against both cases.
135: 
136: ### Technique 3: Count Matches for Sanity Check
137: 
138: ```bash
139: # If regex reports 100+ matches but you expect ~20, investigate
140: grep -rc "pattern" ce/*.py | awk -F: '{sum+=$2} END {print sum}'
141: ```
142: 
143: ---
144: 
145: ## Decision Framework: Fix Now vs. Accept
146: 
147: ### Accept Violations When:
148: - **Low Risk**: Display/formatting code (not business logic)
149: - **High Cost**: Refactoring requires extensive testing
150: - **Low Frequency**: Only happens in few places
151: - **False Detector**: Pattern detection is flawed
152: 
153: ### Fix Violations When:
154: - **High Risk**: Core business logic, error handling
155: - **Quick Fix**: 5-10 min per violation
156: - **High Frequency**: Pattern appears everywhere
157: - **Valid Detector**: Pattern detection is accurate
158: 
159: **This Session**: Accepted 17 violations in display code (low risk), fixed 1 bare except (high risk, quick fix), fixed detector (root cause)
160: 
161: ---
162: 
163: ## Reusable Patterns
164: 
165: ### Pattern 1: Investigate Before Refactoring
166: 
167: ```bash
168: # Reported: 12 files violate pattern X
169: # Action: Don't blindly refactor
170: 
171: 1. Read drift detector code
172: 2. Validate regex logic
173: 3. Test with grep
174: 4. Fix detector if flawed
175: 5. Re-assess violations
176: ```
177: 
178: ### Pattern 2: Root Cause Over Symptoms
179: 
180: ```
181: Symptoms: 12 files flagged
182: Root Cause: Detector regex too broad
183: Fix: Update detector, not 12 files
184: ```
185: 
186: ### Pattern 3: Prioritize by Risk √ó Effort
187: 
188: ```
189: Bare except in error handling: HIGH risk, LOW effort ‚Üí Fix immediately
190: Deep nesting in display code: LOW risk, HIGH effort ‚Üí Accept
191: False positives from detector: HIGH impact, LOW effort ‚Üí Fix detector
192: ```
193: 
194: ---
195: 
196: ## Key Takeaways
197: 
198: 1. **Question the Tool**: Drift detectors can have bugs. Validate before trusting.
199: 
200: 2. **Fix Root Cause**: One detector fix > 12 file refactors.
201: 
202: 3. **Time Box Investigation**: Spend 10-15 min investigating before refactoring.
203: 
204: 4. **Document Patterns**: Save lessons for future dedrifting sessions.
205: 
206: 5. **Accept Strategic Debt**: Not all violations need immediate fixing. Prioritize by risk.
207: 
208: 6. **Test Hypotheses**: Use grep to validate drift detector findings.
209: 
210: 7. **Incremental Refinement**: Start broad, add constraints, test edge cases.
211: 
212: ---
213: 
214: ## Preventing Future Drift
215: 
216: ### Pre-Commit Hooks
217: ```bash
218: # Add to .git/hooks/pre-commit
219: uv run ce validate --level 4  # Pattern conformance check
220: ```
221: 
222: ### Weekly Drift Scans
223: ```bash
224: # Run every Monday
225: uv run ce update-context
226: cat .ce/drift-report.md
227: ```
228: 
229: ### Pattern Updates
230: - Document new patterns as they emerge
231: - Add to `examples/patterns/` directory
232: - Update CLAUDE.md with quick reference
233: 
234: ### Detector Validation
235: - Test regex patterns with known cases
236: - Review detector logic during peer review
237: - Add unit tests for pattern detection
238: 
239: ---
240: 
241: **Remember**: Efficient dedrifting is about finding root causes, not treating symptoms. Always investigate before refactoring.
</file>

<file path="examples/patterns/error-recovery.py">
  1: """Error Recovery and Graceful Degradation Patterns.
  2: 
  3: Pattern: Handle failures without breaking core functionality.
  4: Use Case: External dependencies unavailable, partial success scenarios.
  5: Benefits: Robust systems, clear user feedback, actionable troubleshooting.
  6: 
  7: Source: PRP-14 /update-context Implementation
  8: Implementation: tools/ce/update_context.py
  9: """
 10: 
 11: import logging
 12: from typing import Dict, Any, List
 13: 
 14: logger = logging.getLogger(__name__)
 15: 
 16: 
 17: # ============================================================================
 18: # Pattern 1: Graceful Degradation (Log + Continue)
 19: # ============================================================================
 20: 
 21: def verify_with_external_service(functions: List[str]) -> bool:
 22:     """Verify implementations with external service (e.g., Serena MCP).
 23: 
 24:     Graceful Degradation: If service unavailable, log warning and return False
 25:     rather than crashing entire operation.
 26: 
 27:     Args:
 28:         functions: List of function names to verify
 29: 
 30:     Returns:
 31:         True if all verified, False if service unavailable or verification fails
 32:     """
 33:     if not functions:
 34:         # No functions to verify - consider it success
 35:         return True
 36: 
 37:     try:
 38:         # Attempt to use external service
 39:         # service = get_mcp_client()
 40:         # results = service.find_symbols(functions)
 41: 
 42:         # For now, graceful degradation
 43:         logger.warning(
 44:             "External service verification not yet implemented\n"
 45:             "üîß Troubleshooting: Feature degrades gracefully, returns False until service ready"
 46:         )
 47:         return False
 48: 
 49:     except Exception as e:
 50:         # Service unavailable - don't crash, just log
 51:         logger.warning(f"External service unavailable: {e}")
 52:         return False
 53: 
 54: 
 55: # ============================================================================
 56: # Pattern 2: Partial Success Handling (Track + Report)
 57: # ============================================================================
 58: 
 59: def process_items_with_tracking(items: List[str]) -> Dict[str, Any]:
 60:     """Process multiple items, track failures, continue on errors.
 61: 
 62:     Pattern: Don't let one failure block all work.
 63:     Track errors for reporting while processing rest.
 64: 
 65:     Args:
 66:         items: List of items to process
 67: 
 68:     Returns:
 69:         {
 70:             "success": bool,  # True only if ALL succeeded
 71:             "processed": int,
 72:             "failed": int,
 73:             "errors": List[str]
 74:         }
 75:     """
 76:     processed = 0
 77:     errors = []
 78: 
 79:     for item in items:
 80:         try:
 81:             # Process item
 82:             # result = process_single_item(item)
 83:             processed += 1
 84: 
 85:         except Exception as e:
 86:             # Track error but continue
 87:             error_msg = f"Failed to process {item}: {e}"
 88:             logger.error(error_msg)
 89:             errors.append(error_msg)
 90:             continue
 91: 
 92:     return {
 93:         "success": len(errors) == 0,
 94:         "processed": processed,
 95:         "failed": len(errors),
 96:         "errors": errors
 97:     }
 98: 
 99: 
100: # ============================================================================
101: # Pattern 3: Actionable Error Messages (Context + Guidance)
102: # ============================================================================
103: 
104: def validate_file_exists(file_path: str) -> None:
105:     """Validate file exists with actionable error message.
106: 
107:     Pattern: Exceptions include üîß Troubleshooting guidance for user.
108: 
109:     Args:
110:         file_path: Path to validate
111: 
112:     Raises:
113:         FileNotFoundError: With troubleshooting steps
114:     """
115:     from pathlib import Path
116: 
117:     path = Path(file_path)
118:     if not path.exists():
119:         raise FileNotFoundError(
120:             f"File not found: {file_path}\n"
121:             f"üîß Troubleshooting:\n"
122:             f"   - Verify path is correct\n"
123:             f"   - Check if file was moved or renamed\n"
124:             f"   - Use: ls {path.parent} to list directory"
125:         )
126: 
127: 
128: # ============================================================================
129: # Pattern 4: Cleanup with Logging (Best-Effort Cleanup)
130: # ============================================================================
131: 
132: def operation_with_cleanup(resource_id: str) -> Dict[str, Any]:
133:     """Execute operation with best-effort cleanup on failure.
134: 
135:     Pattern: If main operation fails, attempt cleanup but don't hide
136:     original error. Log cleanup failures instead of re-raising.
137: 
138:     Args:
139:         resource_id: Resource identifier
140: 
141:     Returns:
142:         Operation result
143: 
144:     Raises:
145:         RuntimeError: Original operation error (not cleanup error)
146:     """
147:     try:
148:         # Main operation
149:         # result = perform_operation(resource_id)
150:         return {"success": True}
151: 
152:     except Exception as e:
153:         # Main operation failed - try cleanup
154:         try:
155:             # cleanup_resource(resource_id)
156:             pass
157:         except Exception as cleanup_error:
158:             # Log cleanup failure but don't hide original error
159:             logger.warning(f"Cleanup failed for {resource_id}: {cleanup_error}")
160: 
161:         # Re-raise original error
162:         raise
163: 
164: 
165: # ============================================================================
166: # Pattern 5: Feature Flags (Gradual Rollout)
167: # ============================================================================
168: 
169: def use_new_feature_if_available(data: Dict[str, Any]) -> Dict[str, Any]:
170:     """Use new feature if available, fallback to stable implementation.
171: 
172:     Pattern: Allow partial feature rollout with graceful fallback.
173: 
174:     Args:
175:         data: Input data
176: 
177:     Returns:
178:         Processed data
179:     """
180:     USE_NEW_FEATURE = False  # Feature flag
181: 
182:     if USE_NEW_FEATURE:
183:         try:
184:             # New experimental feature
185:             # return new_implementation(data)
186:             return data
187:         except Exception as e:
188:             # New feature failed - fallback to stable
189:             logger.warning(f"New feature failed, using fallback: {e}")
190:             # Fall through to stable implementation
191: 
192:     # Stable implementation
193:     return data
194: 
195: 
196: # ============================================================================
197: # Key Insights
198: # ============================================================================
199: 
200: # 1. Graceful Degradation: Log + continue, don't crash entire operation
201: # 2. Partial Success: Track failures, report details, continue processing
202: # 3. Actionable Errors: Include üîß Troubleshooting guidance in exceptions
203: # 4. Best-Effort Cleanup: Log cleanup failures, don't hide original error
204: # 5. Feature Flags: Enable gradual rollout with safe fallbacks
205: #
206: # ANTI-PATTERN: Silent failures, bare except, swallowing errors
207: # CORRECT PATTERN: Explicit logging, specific exceptions, actionable guidance
</file>

<file path="examples/patterns/mocks-marking.md">
 1: # Mock Marking Pattern
 2: 
 3: ## Purpose
 4: 
 5: Ensure temporary mock implementations are visible, trackable, and easily removable during refactoring.
 6: 
 7: ## Policy
 8: 
 9: **MANDATORY**: All mocked functionality in non-test code must be explicitly marked.
10: 
11: ## Marking Requirements
12: 
13: ### 1. Decorator
14: 
15: ```python
16: @mocked  # Required for all mock functions/methods
17: ```
18: 
19: ### 2. Inline Comments
20: 
21: ```python
22: # FIXME: Mock implementation - replace with real functionality
23: # MOCKED: Hardcoded return value
24: ```
25: 
26: ### 3. Logging Statement
27: 
28: ```python
29: logger.warning("MOCK: Using hardcoded response")
30: ```
31: 
32: ## Examples
33: 
34: ### ‚úÖ Correct Mock Marking
35: 
36: ```python
37: @mocked
38: def fetch_api_data(endpoint: str) -> dict:
39:     """Fetch data from API endpoint."""
40:     # FIXME: Mock implementation - returns fake data
41:     logger.warning("MOCK: fetch_api_data returning hardcoded response")
42:     return {"status": "success", "data": []}  # MOCKED: Fake data
43: ```
44: 
45: ### ‚ùå Incorrect - Unmarked Mock
46: 
47: ```python
48: def fetch_api_data(endpoint: str) -> dict:
49:     """Fetch data from API endpoint."""
50:     return {"status": "success", "data": []}  # No indication this is fake!
51: ```
52: 
53: ## Replacement Process
54: 
55: **When replacing mock with real implementation:**
56: 
57: 1. **Remove decorator**: Delete `@mocked`
58: 2. **Remove comments**: Delete `FIXME`/`MOCKED` tags
59: 3. **Update logging**: Replace warning with appropriate level
60: 4. **Implement real logic**: Replace hardcoded returns
61: 
62: ### Example Refactoring
63: 
64: **Before (Mock):**
65: 
66: ```python
67: @mocked
68: def fetch_api_data(endpoint: str) -> dict:
69:     """Fetch data from API endpoint."""
70:     # FIXME: Mock implementation
71:     logger.warning("MOCK: fetch_api_data returning hardcoded response")
72:     return {"status": "success", "data": []}  # MOCKED: Fake data
73: ```
74: 
75: **After (Real):**
76: 
77: ```python
78: def fetch_api_data(endpoint: str) -> dict:
79:     """Fetch data from API endpoint."""
80:     logger.debug(f"Fetching data from {endpoint}")
81:     response = requests.get(f"{API_BASE}/{endpoint}")
82:     response.raise_for_status()
83:     return response.json()
84: ```
85: 
86: ## Rationale
87: 
88: - **Transparency**: Makes technical debt visible
89: - **Searchability**: Easy to find all mocks via `grep "@mocked"`
90: - **Safety**: Prevents mocks from reaching production silently
91: - **Refactoring**: Clear removal checklist
92: 
93: ## Related Patterns
94: 
95: - **No Fishy Fallbacks**: Mocks should fail fast, not hide errors
96: - **Real Functionality Testing**: Tests must validate real implementations
</file>

<file path="examples/patterns/pipeline-testing.py">
  1: """Pipeline Orchestration with Builder Pattern.
  2: 
  3: Pattern: Build testable DAG pipelines with topological execution.
  4: Use Case: Orchestrate multi-step workflows with dependency management.
  5: Benefits: Observable mocking, cycle detection, composable node graphs.
  6: 
  7: Source: PRP-11 Pipeline Testing Framework
  8: Implementation: tools/ce/testing/builder.py
  9: """
 10: 
 11: from typing import Dict, Any, List, Tuple
 12: 
 13: 
 14: # ============================================================================
 15: # Pipeline Execution
 16: # ============================================================================
 17: 
 18: class Pipeline:
 19:     """Executable pipeline with nodes and edges.
 20: 
 21:     Executes nodes in topological order based on dependencies.
 22:     Supports linear pipelines and DAGs with cycle detection.
 23:     """
 24: 
 25:     def __init__(self, nodes: Dict[str, Any], edges: List[Tuple[str, str]]):
 26:         self.nodes = nodes
 27:         self.edges = edges
 28: 
 29:     def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
 30:         """Execute pipeline from start to finish.
 31: 
 32:         Process:
 33:             1. Topologically sort nodes by edges
 34:             2. Execute nodes in order
 35:             3. Pass output from node to next node
 36:         """
 37:         current_data = input_data
 38: 
 39:         for node_name in self._topological_sort():
 40:             strategy = self.nodes[node_name]
 41:             current_data = strategy.execute(current_data)
 42: 
 43:         return current_data
 44: 
 45:     def _topological_sort(self) -> List[str]:
 46:         """Sort nodes by dependencies using Kahn's algorithm."""
 47:         if not self.edges:
 48:             return list(self.nodes.keys())
 49: 
 50:         # Build adjacency list
 51:         in_degree = {node: 0 for node in self.nodes}
 52:         adj = {node: [] for node in self.nodes}
 53: 
 54:         for from_node, to_node in self.edges:
 55:             adj[from_node].append(to_node)
 56:             in_degree[to_node] += 1
 57: 
 58:         # Kahn's algorithm
 59:         queue = [node for node in self.nodes if in_degree[node] == 0]
 60:         result = []
 61: 
 62:         while queue:
 63:             node = queue.pop(0)
 64:             result.append(node)
 65: 
 66:             for neighbor in adj[node]:
 67:                 in_degree[neighbor] -= 1
 68:                 if in_degree[neighbor] == 0:
 69:                     queue.append(neighbor)
 70: 
 71:         if len(result) != len(self.nodes):
 72:             nodes_in_cycle = set(self.nodes.keys()) - set(result)
 73:             raise RuntimeError(
 74:                 f"Pipeline has circular dependencies: {', '.join(sorted(nodes_in_cycle))}"
 75:             )
 76: 
 77:         return result
 78: 
 79: 
 80: # ============================================================================
 81: # Builder Pattern
 82: # ============================================================================
 83: 
 84: class PipelineBuilder:
 85:     """Fluent builder for creating testable pipelines.
 86: 
 87:     Features:
 88:         - Method chaining for readability
 89:         - Observable mocking (üé≠ indicator)
 90:         - Cycle detection before execution
 91:     """
 92: 
 93:     def __init__(self, mode: str = "e2e"):
 94:         """Initialize builder with test mode.
 95: 
 96:         Args:
 97:             mode: Test mode (unit/integration/e2e)
 98:         """
 99:         self.mode = mode
100:         self.nodes = {}
101:         self.edges = []
102: 
103:     def add_node(self, name: str, strategy: Any) -> "PipelineBuilder":
104:         """Add node with execution strategy."""
105:         self.nodes[name] = strategy
106:         return self
107: 
108:     def add_edge(self, from_node: str, to_node: str) -> "PipelineBuilder":
109:         """Add dependency edge (from -> to)."""
110:         self.edges.append((from_node, to_node))
111:         return self
112: 
113:     def build(self) -> Pipeline:
114:         """Build pipeline and log mocked nodes."""
115:         # Observable mocking: Show which nodes are mocked
116:         mocked = [
117:             name for name, strategy in self.nodes.items()
118:             if strategy.is_mocked()
119:         ]
120: 
121:         if mocked:
122:             print(f"üé≠ MOCKED NODES: {', '.join(mocked)}")
123: 
124:         return Pipeline(self.nodes, self.edges)
125: 
126: 
127: # ============================================================================
128: # Testing Pattern
129: # ============================================================================
130: 
131: def test_linear_pipeline():
132:     """E2E test: Linear pipeline with all mocks."""
133:     from strategy_testing import MockDatabaseStrategy
134: 
135:     pipeline = (
136:         PipelineBuilder(mode="e2e")
137:         .add_node("fetch", MockDatabaseStrategy([{"user_id": 1, "name": "Test"}]))
138:         .add_node("process", MockDatabaseStrategy([{"processed": True}]))
139:         .add_edge("fetch", "process")
140:         .build()
141:     )
142: 
143:     result = pipeline.execute({"user_id": 1})
144:     # Output: üé≠ MOCKED NODES: fetch, process
145:     assert result["processed"] is True
146: 
147: 
148: def test_dag_pipeline():
149:     """Integration test: DAG with parallel branches."""
150:     # DAG structure:
151:     #     parse
152:     #    /     \
153:     # research  docs
154:     #    \     /
155:     #    generate
156:     pass  # Simplified - shows pattern
157: 
158: 
159: # ============================================================================
160: # Key Insights
161: # ============================================================================
162: 
163: # 1. Topological Sort: Kahn's algorithm for DAG execution
164: # 2. Cycle Detection: Fail fast before execution
165: # 3. Observable Mocking: üé≠ indicator shows test mode
166: # 4. Fluent API: Method chaining improves readability
167: # 5. Composable: Build any graph structure with nodes + edges
</file>

<file path="examples/patterns/strategy-testing.py">
  1: """Strategy Pattern for Composable Testing.
  2: 
  3: Pattern: Define interchangeable strategies (real/mock) for pipeline nodes.
  4: Use Case: Test complex workflows with mix of real + mock external dependencies.
  5: Benefits: Test any subgraph without full integration setup.
  6: 
  7: Source: PRP-11 Pipeline Testing Framework
  8: Implementation: tools/ce/testing/
  9: """
 10: 
 11: from typing import Protocol, Any, Dict
 12: 
 13: 
 14: # ============================================================================
 15: # Strategy Interface (Protocol-based for flexibility)
 16: # ============================================================================
 17: 
 18: class NodeStrategy(Protocol):
 19:     """Interface for pipeline node execution strategies.
 20: 
 21:     Real strategies call actual external APIs/services.
 22:     Mock strategies return canned data for testing.
 23:     """
 24: 
 25:     def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
 26:         """Execute node logic with input data."""
 27:         ...
 28: 
 29:     def is_mocked(self) -> bool:
 30:         """Return True if this is a mock strategy."""
 31:         ...
 32: 
 33: 
 34: # ============================================================================
 35: # Base Classes (Optional - Avoid boilerplate)
 36: # ============================================================================
 37: 
 38: class BaseRealStrategy:
 39:     """Base for real strategies - implements is_mocked() as False."""
 40:     def is_mocked(self) -> bool:
 41:         return False
 42: 
 43: 
 44: class BaseMockStrategy:
 45:     """Base for mock strategies - implements is_mocked() as True."""
 46:     def is_mocked(self) -> bool:
 47:         return True
 48: 
 49: 
 50: # ============================================================================
 51: # Example: Real Strategy
 52: # ============================================================================
 53: 
 54: class RealDatabaseStrategy(BaseRealStrategy):
 55:     """Real strategy: Hits actual database."""
 56: 
 57:     def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
 58:         # Real implementation calls actual DB
 59:         user_id = input_data["user_id"]
 60:         # user = db.query(User).filter_by(id=user_id).first()
 61:         return {"user_id": user_id, "name": "John Doe"}  # Simplified
 62: 
 63: 
 64: # ============================================================================
 65: # Example: Mock Strategy
 66: # ============================================================================
 67: 
 68: class MockDatabaseStrategy(BaseMockStrategy):
 69:     """Mock strategy: Returns canned data."""
 70: 
 71:     def __init__(self, canned_users: list):
 72:         self.users = {user["user_id"]: user for user in canned_users}
 73: 
 74:     def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
 75:         user_id = input_data["user_id"]
 76:         return self.users.get(user_id, {"error": "User not found"})
 77: 
 78: 
 79: # ============================================================================
 80: # Testing Pattern
 81: # ============================================================================
 82: 
 83: def test_unit_with_mock():
 84:     """Unit test: Test single node with mock dependency."""
 85:     strategy = MockDatabaseStrategy(canned_users=[
 86:         {"user_id": 123, "name": "Test User"}
 87:     ])
 88: 
 89:     result = strategy.execute({"user_id": 123})
 90: 
 91:     assert result["name"] == "Test User"
 92:     assert strategy.is_mocked() is True  # Observable mocking
 93: 
 94: 
 95: def test_integration_mixed():
 96:     """Integration test: Mix real + mock strategies."""
 97:     # In real pipeline: parser is real, database is mocked
 98:     # parser = RealParserStrategy()
 99:     database = MockDatabaseStrategy(canned_users=[...])
100: 
101:     # Test subgraph without full stack
102:     assert database.is_mocked() is True
103: 
104: 
105: # ============================================================================
106: # Key Insights
107: # ============================================================================
108: 
109: # 1. Protocol-based interface: No inheritance required
110: # 2. Base classes: Optional convenience, avoid boilerplate
111: # 3. Observable mocking: is_mocked() makes test mode explicit
112: # 4. Composable: Mix real + mock strategies freely
113: # 5. Test any subgraph: Don't need full integration setup
</file>

<file path="examples/example.setting.local.md">
 1: This is how you set settings.local.md file always:
 2: 
 3: ```json
 4: {
 5:   "permissions": {
 6:     "allow": [
 7:       "Bash(*)",
 8:       "Read(**)",
 9:       "Write(**)",
10:       "Edit(**)",
11:       "mcp__*",
12:       "WebSearch(*)"
13:     ],
14:     "deny": [],
15:     "ask": []
16:   }
17: }```
</file>

<file path="examples/linear-integration-example.md">
  1: # Linear MCP Integration - Example
  2: 
  3: Demonstrates Linear MCP integration using configuration defaults, Python utilities, and MCP tools for issue tracking in Context Engineering projects.
  4: 
  5: ## Quick Start
  6: 
  7: ### 1. Configuration
  8: 
  9: **File**: `.ce/linear-defaults.yml`
 10: 
 11: ```yaml
 12: project: "Context Engineering"
 13: assignee: "blazej.przybyszewski@gmail.com"
 14: team: "Blaise78"
 15: default_labels:
 16:   - "feature"
 17: ```
 18: 
 19: ### 2. Create Issue with Defaults
 20: 
 21: ```python
 22: from ce.linear_utils import create_issue_with_defaults
 23: 
 24: # Auto-applies defaults from config
 25: issue_data = create_issue_with_defaults(
 26:     title="PRP-15: New Feature",
 27:     description="""## Feature
 28: Implement feature X for Context Engineering.
 29: 
 30: ## Deliverables
 31: ‚úÖ Core implementation
 32: ‚úÖ Tests (‚â•80% coverage)
 33: ‚úÖ Documentation
 34: """,
 35:     state="todo"
 36: )
 37: 
 38: # Create via MCP
 39: issue = mcp__linear__create_issue(**issue_data)
 40: print(f"Created: {issue['identifier']}")  # "BLA-15"
 41: ```
 42: 
 43: ### 3. Direct MCP Usage (Full Control)
 44: 
 45: ```python
 46: # When you need to override defaults
 47: issue = mcp__linear__create_issue(
 48:     team="Blaise78",
 49:     title="PRP-16: Bug Fix",
 50:     description="Fix authentication token handling",
 51:     priority=1,  # 1=Urgent, 2=High, 3=Normal, 4=Low
 52:     labels=["bug", "security"],
 53:     project="Context Engineering",
 54:     assignee="blazej.przybyszewski@gmail.com",
 55:     state="in_progress"
 56: )
 57: ```
 58: 
 59: ## Troubleshooting MCP Connection
 60: 
 61: ### Level 1: Check Status
 62: ```bash
 63: /mcp
 64: ```
 65: 
 66: ### Level 2: Restart
 67: ```bash
 68: /mcp restart linear-server
 69: ```
 70: 
 71: ### Level 3: Re-authenticate
 72: ```bash
 73: rm -rf ~/.mcp-auth
 74: mcp-remote https://mcp.linear.app/sse  # Opens browser
 75: 
 76: # Expected output (HTTP 404 is normal - uses SSE fallback):
 77: # [PID] Received error: Error POSTing to endpoint (HTTP 404): Not Found
 78: # [PID] Recursively reconnecting for reason: falling-back-to-alternate-transport
 79: # [PID] Connected to remote server using SSEClientTransport
 80: # [PID] Proxy established successfully ‚úÖ
 81: 
 82: # Restart Claude Code to activate
 83: ```
 84: 
 85: ### Level 4: Reinstall
 86: ```bash
 87: npm install -g mcp-remote
 88: mcp-remote https://mcp.linear.app/sse
 89: claude mcp add --transport sse linear-server https://mcp.linear.app/sse
 90: ```
 91: 
 92: ## Anti-Patterns
 93: 
 94: ```python
 95: # ‚ùå BAD: Hardcoded values
 96: issue = mcp__linear__create_issue(
 97:     team="Blaise78",  # Hardcoded
 98:     assignee="user@example.com",  # Hardcoded
 99:     project="Context Engineering",  # Hardcoded
100:     labels=["feature"],  # Hardcoded
101:     title="...",
102:     description="..."
103: )
104: 
105: # ‚ùå BAD: Silent failures
106: try:
107:     issue = mcp__linear__create_issue(...)
108:     print("‚úÖ Success")  # FAKE!
109: except:
110:     pass  # Silent failure
111: ```
112: 
113: ## Best Practices
114: 
115: ```python
116: # ‚úÖ GOOD: Use defaults helper
117: from ce.linear_utils import create_issue_with_defaults
118: 
119: issue_data = create_issue_with_defaults(
120:     title="...",
121:     description="..."
122: )
123: issue = mcp__linear__create_issue(**issue_data)
124: 
125: # ‚úÖ GOOD: Explicit error handling
126: try:
127:     issue = mcp__linear__create_issue(**issue_data)
128:     print(f"‚úÖ Created: {issue['identifier']}")
129: except Exception as e:
130:     print(f"‚ùå Failed: {e}")
131:     print(f"üîß Troubleshooting:")
132:     print(f"   1. Check MCP status: /mcp")
133:     print(f"   2. Verify config: cat .ce/linear-defaults.yml")
134:     raise
135: ```
136: 
137: ## Integration with PRPs
138: 
139: ### Workflow
140: 
141: ```python
142: # 1. Create issue
143: issue = mcp__linear__create_issue(**issue_data)
144: 
145: # 2. Update PRP YAML header
146: # ---
147: # issue: "BLA-18"
148: # project: "Context Engineering"
149: # ---
150: 
151: # 3. Track progress
152: issue_details = mcp__linear__get_issue(id=issue['id'])
153: print(f"Status: {issue_details['status']}")
154: ```
155: 
156: ### Auto-Creation in /generate-prp
157: 
158: ```bash
159: # Creates new Linear issue automatically
160: /generate-prp examples/feature-INITIAL.md
161: 
162: # Join existing PRP's issue
163: /generate-prp examples/feature-INITIAL.md --join-prp 12
164: ```
165: 
166: ## Available MCP Tools
167: 
168: **Core Functions**:
169: - `mcp__linear__create_issue` - Create new issue
170: - `mcp__linear__list_issues` - Query issues
171: - `mcp__linear__get_issue` - Get issue details
172: - `mcp__linear__update_issue` - Modify existing issue
173: - `mcp__linear__create_comment` - Add comment to issue
174: 
175: **Other Tools**: Projects, teams, labels, statuses (20+ total)
176: 
177: ## Utility Helpers
178: 
179: **Module**: `tools/ce/linear_utils.py`
180: 
181: ```python
182: from ce.linear_utils import (
183:     get_linear_defaults,       # Load config
184:     get_default_assignee,      # Get assignee email
185:     get_default_project,       # Get project name
186:     create_issue_with_defaults # Prepare issue data
187: )
188: 
189: defaults = get_linear_defaults()
190: # {"project": "...", "assignee": "...", "team": "...", "default_labels": [...]}
191: ```
192: 
193: ## Summary
194: 
195: - **Config**: `.ce/linear-defaults.yml` centralizes project settings
196: - **Utilities**: `ce.linear_utils` simplifies issue creation
197: - **MCP Tools**: Direct control when needed
198: - **Troubleshooting**: Multi-level resolution for connection issues
199: - **PRP Integration**: Auto-sync issue tracking with PRP workflow
200: 
201: **References**:
202: - Linear MCP: https://linear.app/docs/mcp
203: - Project docs: CLAUDE.md (Linear Integration section)
204: - Implementation: `tools/ce/linear_utils.py`
</file>

<file path="examples/mermaid-color-palette.md">
  1: # Mermaid Diagram Color Palette - Standard
  2: 
  3: **Purpose:** Standard color palette for Mermaid diagrams in Context Engineering documentation
  4: **Requirement:** Always specify text color for theme compatibility
  5: 
  6: ---
  7: 
  8: ## Color Palette Reference
  9: 
 10: ### Primary Palette (System Components)
 11: 
 12: ```mermaid
 13: graph LR
 14:     A["Primary Blue<br/>#e3f2fd"] --> B["Light Yellow<br/>#fff8e1"]
 15:     B --> C["Light Purple<br/>#f3e5f5"]
 16:     C --> D["Light Cyan<br/>#b2ebf2"]
 17:     D --> E["Light Orange<br/>#ffe0b2"]
 18: 
 19:     style A fill:#e3f2fd,color:#000
 20:     style B fill:#fff8e1,color:#000
 21:     style C fill:#f3e5f5,color:#000
 22:     style D fill:#b2ebf2,color:#000
 23:     style E fill:#ffe0b2,color:#000
 24: ```
 25: 
 26: ### Secondary Palette (Detail Nodes)
 27: 
 28: ```mermaid
 29: graph LR
 30:     A["Pale Yellow<br/>#fff9c4"] --> B["Very Light Cyan<br/>#e1f5fe"]
 31:     B --> C["Light Teal<br/>#b2dfdb"]
 32:     C --> D["Pale Orange<br/>#ffecb3"]
 33: 
 34:     style A fill:#fff9c4,color:#000
 35:     style B fill:#e1f5fe,color:#000
 36:     style C fill:#b2dfdb,color:#000
 37:     style D fill:#ffecb3,color:#000
 38: ```
 39: 
 40: ### Status Colors
 41: 
 42: ```mermaid
 43: graph LR
 44:     A["Success Green<br/>#c8e6c9"] --> B["Warning Red<br/>#ff9999"]
 45:     B --> C["Error Salmon<br/>#ffccbc"]
 46:     C --> D["Failure Red<br/>#ef9a9a"]
 47: 
 48:     style A fill:#c8e6c9,color:#000
 49:     style B fill:#ff9999,color:#000
 50:     style C fill:#ffccbc,color:#000
 51:     style D fill:#ef9a9a,color:#000
 52: ```
 53: 
 54: ---
 55: 
 56: ## Usage Guidelines
 57: 
 58: ### 1. Always Specify Text Color
 59: 
 60: **MANDATORY:** Include `color:#000` for light backgrounds to ensure readability in both light and dark themes.
 61: 
 62: ```mermaid
 63: graph LR
 64:     A["Node Text"]
 65: 
 66:     style A fill:#e3f2fd,color:#000
 67: ```
 68: 
 69: ### 2. Node Type Color Mapping
 70: 
 71: | Node Type | Color | Hex Code | Usage |
 72: |-----------|-------|----------|-------|
 73: | **Entry Point** | Light Yellow | `#fff8e1` | Start nodes, input documents |
 74: | **Process** | Light Purple | `#f3e5f5` | Processing steps, commands |
 75: | **Data/Document** | Light Cyan | `#b2ebf2` | Documents, data stores |
 76: | **Decision** | Pale Orange | `#fff3e0` | Decision nodes, branches |
 77: | **Action** | Light Orange | `#ffe0b2` | Actions, operations |
 78: | **Critical Checkpoint** | Warning Red | `#ff9999` | Human validation, critical decisions |
 79: | **Success** | Success Green | `#c8e6c9` | Final success states |
 80: | **Error/Manual** | Error Salmon | `#ffccbc` | Error handling, manual intervention |
 81: 
 82: ### 3. Hierarchical Color Scheme
 83: 
 84: **Parent ‚Üí Child relationship:**
 85: 
 86: - Parent node: Primary palette color
 87: - Child nodes: Secondary palette color (lighter shade)
 88: 
 89: ```mermaid
 90: graph TB
 91:     A["Parent Node<br/>Primary Color"]
 92:     A --> B["Child Node<br/>Secondary Color"]
 93:     A --> C["Child Node<br/>Secondary Color"]
 94: 
 95:     style A fill:#f3e5f5,color:#000
 96:     style B fill:#e1f5fe,color:#000
 97:     style C fill:#e1f5fe,color:#000
 98: ```
 99: 
100: ---
101: 
102: ## Standard Templates
103: 
104: ### Template 1: System Architecture
105: 
106: **Use Case:** Component hierarchy, system overview
107: 
108: ```mermaid
109: graph TB
110:     A["System"]
111:     A --> B["Component 1"]
112:     A --> C["Component 2"]
113: 
114:     B --> B1["Sub-component 1.1"]
115:     B --> B2["Sub-component 1.2"]
116: 
117:     C --> C1["Sub-component 2.1"]
118:     C --> C2["Sub-component 2.2"]
119: 
120:     style A fill:#e3f2fd,color:#000
121:     style B fill:#fff8e1,color:#000
122:     style C fill:#f3e5f5,color:#000
123:     style B1 fill:#fff9c4,color:#000
124:     style B2 fill:#fff9c4,color:#000
125:     style C1 fill:#e1f5fe,color:#000
126:     style C2 fill:#e1f5fe,color:#000
127: ```
128: 
129: ### Template 2: Workflow Process
130: 
131: **Use Case:** Step-by-step processes, workflows
132: 
133: ```mermaid
134: graph LR
135:     A["Input"] --> B["Process 1"]
136:     B --> C["Document"]
137:     C --> D{"Decision"}
138:     D -->|"Yes"| E["Action"]
139:     D -->|"No"| F["Alternative"]
140:     E --> G["Success"]
141:     F --> G
142: 
143:     style A fill:#fff8e1,color:#000
144:     style B fill:#f3e5f5,color:#000
145:     style C fill:#b2ebf2,color:#000
146:     style D fill:#fff3e0,color:#000
147:     style E fill:#ffe0b2,color:#000
148:     style F fill:#fff9c4,color:#000
149:     style G fill:#c8e6c9,color:#000
150: ```
151: 
152: ### Template 3: Decision Flow
153: 
154: **Use Case:** Complex decision trees, conditional logic
155: 
156: ```mermaid
157: graph TD
158:     A["Start"] --> B{"Condition 1?"}
159:     B -->|"Yes"| C["Path A"]
160:     B -->|"No"| D["Path B"]
161: 
162:     C --> E{"Condition 2?"}
163:     E -->|"Yes"| F["Success"]
164:     E -->|"No"| G["Retry"]
165: 
166:     D --> H["Manual Action"]
167:     H --> F
168:     G --> B
169: 
170:     style A fill:#fff8e1,color:#000
171:     style B fill:#fff3e0,color:#000
172:     style C fill:#b2ebf2,color:#000
173:     style D fill:#ffe0b2,color:#000
174:     style E fill:#fff3e0,color:#000
175:     style F fill:#c8e6c9,color:#000
176:     style G fill:#ffccbc,color:#000
177:     style H fill:#ff9999,color:#000
178: ```
179: 
180: ### Template 4: Validation Gates
181: 
182: **Use Case:** Testing, validation processes
183: 
184: ```mermaid
185: graph TB
186:     A["Code"] --> B["Level 1: Syntax"]
187:     B --> B1["Auto-fix"]
188:     B1 --> C["Level 2: Unit Tests"]
189:     C --> C1["Analyze & Fix"]
190:     C1 --> D["Level 3: Integration"]
191:     D --> D1["Debug"]
192:     D1 --> E["Production"]
193: 
194:     style A fill:#e3f2fd,color:#000
195:     style B fill:#fff8e1,color:#000
196:     style B1 fill:#fff9c4,color:#000
197:     style C fill:#f3e5f5,color:#000
198:     style C1 fill:#e1f5fe,color:#000
199:     style D fill:#b2ebf2,color:#000
200:     style D1 fill:#b2dfdb,color:#000
201:     style E fill:#c8e6c9,color:#000
202: ```
203: 
204: ---
205: 
206: ## Complete Color Reference Table
207: 
208: | Color Name | Hex Code | RGB | Usage Context |
209: |------------|----------|-----|---------------|
210: | **Primary Blue** | `#e3f2fd` | rgb(227, 242, 253) | Top-level system nodes |
211: | **Light Yellow** | `#fff8e1` | rgb(255, 248, 225) | Entry points, inputs |
212: | **Light Purple** | `#f3e5f5` | rgb(243, 229, 245) | Processing steps |
213: | **Light Cyan** | `#b2ebf2` | rgb(178, 235, 242) | Documents, data |
214: | **Light Orange** | `#ffe0b2` | rgb(255, 224, 178) | Actions, commands |
215: | **Pale Yellow** | `#fff9c4` | rgb(255, 249, 196) | Secondary details |
216: | **Very Light Cyan** | `#e1f5fe` | rgb(225, 245, 254) | Secondary details |
217: | **Light Teal** | `#b2dfdb` | rgb(178, 223, 219) | Secondary details |
218: | **Pale Orange** | `#ffecb3` | rgb(255, 236, 179) | Secondary details |
219: | **Pale Orange 2** | `#fff3e0` | rgb(255, 243, 224) | Decision nodes |
220: | **Success Green** | `#c8e6c9` | rgb(200, 230, 201) | Success states |
221: | **Warning Red** | `#ff9999` | rgb(255, 153, 153) | Critical checkpoints |
222: | **Error Salmon** | `#ffccbc` | rgb(255, 204, 188) | Errors, manual steps |
223: | **Failure Red** | `#ef9a9a` | rgb(239, 154, 154) | Failure states |
224: 
225: ---
226: 
227: ## Anti-Patterns to Avoid
228: 
229: ### ‚ùå BAD: No text color specified
230: 
231: ```mermaid
232: graph LR
233:     A["Node Text"]
234: 
235:     style A fill:#e3f2fd
236: ```
237: 
238: **Problem:** Text may be invisible in dark themes.
239: 
240: ### ‚ùå BAD: Inconsistent color scheme
241: 
242: ```mermaid
243: graph LR
244:     A["Node 1"] --> B["Node 2"]
245:     B --> C["Node 3"]
246: 
247:     style A fill:#ff0000,color:#000
248:     style B fill:#00ff00,color:#000
249:     style C fill:#0000ff,color:#000
250: ```
251: 
252: **Problem:** Random colors break visual hierarchy.
253: 
254: ### ‚úÖ GOOD: Consistent palette with text color
255: 
256: ```mermaid
257: graph LR
258:     A["Node 1"] --> B["Node 2"]
259:     B --> C["Node 3"]
260: 
261:     style A fill:#e3f2fd,color:#000
262:     style B fill:#fff8e1,color:#000
263:     style C fill:#f3e5f5,color:#000
264: ```
265: 
266: **Benefit:** Clear hierarchy, theme-compatible.
267: 
268: ---
269: 
270: ## Quick Copy-Paste Templates
271: 
272: ### Basic Node Styles
273: 
274: ```
275: style A fill:#e3f2fd,color:#000    # Primary Blue
276: style B fill:#fff8e1,color:#000    # Light Yellow
277: style C fill:#f3e5f5,color:#000    # Light Purple
278: style D fill:#b2ebf2,color:#000    # Light Cyan
279: style E fill:#ffe0b2,color:#000    # Light Orange
280: ```
281: 
282: ### Detail Node Styles
283: 
284: ```
285: style A1 fill:#fff9c4,color:#000   # Pale Yellow
286: style A2 fill:#e1f5fe,color:#000   # Very Light Cyan
287: style A3 fill:#b2dfdb,color:#000   # Light Teal
288: style A4 fill:#ffecb3,color:#000   # Pale Orange
289: ```
290: 
291: ### Status Node Styles
292: 
293: ```
294: style SUCCESS fill:#c8e6c9,color:#000   # Success Green
295: style WARNING fill:#ff9999,color:#000   # Warning Red
296: style ERROR fill:#ffccbc,color:#000     # Error Salmon
297: style FAIL fill:#ef9a9a,color:#000      # Failure Red
298: ```
299: 
300: ---
301: 
302: ## Version History
303: 
304: - **v1.0** (2025-10-11): Initial palette extraction from PRPs/Model.md
305: - Based on production diagrams: System Components, PRP Architecture, Validation Gates
306: 
307: ---
308: 
309: ## References
310: 
311: - Source: [PRPs/Model.md](../PRPs/Model.md)
312: - Documentation Standard: [CLAUDE.md](../CLAUDE.md) (Mermaid color requirements)
313: - Mermaid Documentation: <https://mermaid.js.org/syntax/flowchart.html#styling-and-classes>
</file>

<file path="examples/prp-decomposition-patterns.md">
  1: # PRP Decomposition Patterns
  2: 
  3: Practical patterns and examples for breaking down large PRPs into manageable sub-PRPs.
  4: 
  5: ## Pattern 1: Phase-Based Decomposition
  6: 
  7: **When to Use**: PRP has 5+ sequential or semi-independent phases
  8: 
  9: **Structure**:
 10: ```
 11: PRP-X.0: [Feature Name] Framework (Meta-PRP)
 12: ‚îú‚îÄ PRP-X.1: Phase 1 Name
 13: ‚îú‚îÄ PRP-X.2: Phase 2 Name
 14: ‚îú‚îÄ PRP-X.3: Phase 3 Name
 15: ‚îî‚îÄ PRP-X.4: Phase 4 Name
 16: ```
 17: 
 18: ### Example: PRP-4 Decomposition
 19: 
 20: **Before (Monolithic)**:
 21: ```
 22: PRP-4: Execute-PRP Command Orchestration
 23: - 1259 lines, 18 hours, HIGH risk
 24: - 6 phases, 27 functions, 30 criteria
 25: - Complexity Score: 71.45/100 ‚Üí RED
 26: ```
 27: 
 28: **After (Decomposed)**:
 29: ```
 30: PRP-4.0: Execute-PRP Framework (Meta-PRP)
 31: ‚îú‚îÄ PRP-4.1: Blueprint Parser
 32: ‚îÇ   ‚Ä¢ 3 hours, LOW risk
 33: ‚îÇ   ‚Ä¢ Parse PRP markdown ‚Üí structured data
 34: ‚îÇ   ‚Ä¢ Extract phases, validation gates, checkpoints
 35: ‚îÇ
 36: ‚îú‚îÄ PRP-4.2: Execution Orchestration Engine
 37: ‚îÇ   ‚Ä¢ 5 hours, MEDIUM risk
 38: ‚îÇ   ‚Ä¢ Phase-by-phase execution loop
 39: ‚îÇ   ‚Ä¢ Progress tracking, logging
 40: ‚îÇ
 41: ‚îú‚îÄ PRP-4.3: Validation Loop Integration
 42: ‚îÇ   ‚Ä¢ 4 hours, MEDIUM risk
 43: ‚îÇ   ‚Ä¢ Run validation gates after each phase
 44: ‚îÇ   ‚Ä¢ Collect results, determine pass/fail
 45: ‚îÇ
 46: ‚îú‚îÄ PRP-4.4: Self-Healing Implementation
 47: ‚îÇ   ‚Ä¢ 4 hours, HIGH risk (isolated!)
 48: ‚îÇ   ‚Ä¢ Error detection patterns
 49: ‚îÇ   ‚Ä¢ Automatic retry logic with backoff
 50: ‚îÇ
 51: ‚îî‚îÄ PRP-4.5: CLI Integration & Testing
 52:     ‚Ä¢ 2 hours, LOW risk
 53:     ‚Ä¢ Wire up ce prp execute command
 54:     ‚Ä¢ End-to-end integration tests
 55: ```
 56: 
 57: **Benefits**:
 58: - Each sub-PRP is GREEN (‚â§5h, focused scope)
 59: - HIGH risk isolated to PRP-4.4
 60: - Clear dependencies: 4.1 ‚Üí 4.2 ‚Üí 4.3 ‚Üí 4.4 ‚Üí 4.5
 61: - Can execute incrementally, validate each step
 62: 
 63: ## Pattern 2: Feature-Based Decomposition
 64: 
 65: **When to Use**: PRP spans multiple functional areas (parser, validator, executor, formatter, etc.)
 66: 
 67: **Structure**: Split by architectural boundaries
 68: 
 69: ### Example: Data Processing System
 70: 
 71: **Before (Monolithic)**:
 72: ```
 73: PRP-X: Data Processing Pipeline
 74: - 1100 lines, 15 hours, MEDIUM risk
 75: - Ingestion + Validation + Transformation + Export
 76: ```
 77: 
 78: **After (Decomposed)**:
 79: ```
 80: PRP-X.0: Data Processing System (Meta-PRP)
 81: ‚îú‚îÄ PRP-X.1: CSV Input Parser
 82: ‚îÇ   ‚Ä¢ Parse CSV/TSV files
 83: ‚îÇ   ‚Ä¢ Handle encoding issues
 84: ‚îÇ   ‚Ä¢ 3 hours, LOW risk
 85: ‚îÇ
 86: ‚îú‚îÄ PRP-X.2: Data Validator
 87: ‚îÇ   ‚Ä¢ Schema validation
 88: ‚îÇ   ‚Ä¢ Business rule checks
 89: ‚îÇ   ‚Ä¢ 4 hours, MEDIUM risk
 90: ‚îÇ
 91: ‚îú‚îÄ PRP-X.3: Transformation Engine
 92: ‚îÇ   ‚Ä¢ Apply data transformations
 93: ‚îÇ   ‚Ä¢ Aggregations, calculations
 94: ‚îÇ   ‚Ä¢ 5 hours, MEDIUM risk
 95: ‚îÇ
 96: ‚îî‚îÄ PRP-X.4: JSON/XML Exporter
 97:     ‚Ä¢ Format output
 98:     ‚Ä¢ Write to file system
 99:     ‚Ä¢ 3 hours, LOW risk
100: ```
101: 
102: **Benefits**:
103: - Each component independently testable
104: - Clear interfaces between modules
105: - Can swap implementations (e.g., XML parser instead of CSV)
106: 
107: ## Pattern 3: Risk-Based Decomposition
108: 
109: **When to Use**: PRP contains HIGH-risk components mixed with safer features
110: 
111: **Strategy**: Isolate risky parts for focused attention
112: 
113: ### Example: API Client with Authentication
114: 
115: **Before (Monolithic)**:
116: ```
117: PRP-Y: External API Client
118: - 950 lines, 12 hours, HIGH risk
119: - OAuth2 + API calls + Error handling
120: ```
121: 
122: **After (Decomposed)**:
123: ```
124: PRP-Y.0: API Client System (Meta-PRP)
125: ‚îú‚îÄ PRP-Y.1: HTTP Client Core
126: ‚îÇ   ‚Ä¢ Basic request/response handling
127: ‚îÇ   ‚Ä¢ 3 hours, LOW risk
128: ‚îÇ
129: ‚îú‚îÄ PRP-Y.2: OAuth2 Authentication (HIGH RISK)
130: ‚îÇ   ‚Ä¢ Token acquisition flow
131: ‚îÇ   ‚Ä¢ Refresh token logic
132: ‚îÇ   ‚Ä¢ Credential storage
133: ‚îÇ   ‚Ä¢ 5 hours, HIGH risk ‚Üí ISOLATED
134: ‚îÇ
135: ‚îú‚îÄ PRP-Y.3: API Endpoint Methods
136: ‚îÇ   ‚Ä¢ GET/POST/PUT/DELETE wrappers
137: ‚îÇ   ‚Ä¢ 2 hours, LOW risk
138: ‚îÇ
139: ‚îî‚îÄ PRP-Y.4: Error Handling & Retry
140:     ‚Ä¢ Network error detection
141:     ‚Ä¢ Exponential backoff
142:     ‚Ä¢ 2 hours, MEDIUM risk
143: ```
144: 
145: **Benefits**:
146: - HIGH risk (OAuth2) gets dedicated focus
147: - Core functionality (Y.1, Y.3) can proceed independently
148: - Y.2 can be reviewed by security experts
149: 
150: ## Pattern 4: Layer-Based Decomposition
151: 
152: **When to Use**: PRP spans multiple architectural layers (UI, business logic, data access)
153: 
154: ### Example: User Management Feature
155: 
156: **Before (Monolithic)**:
157: ```
158: PRP-Z: User Management System
159: - 1050 lines, 14 hours, MEDIUM risk
160: - UI forms + Business logic + Database
161: ```
162: 
163: **After (Decomposed)**:
164: ```
165: PRP-Z.0: User Management (Meta-PRP)
166: ‚îú‚îÄ PRP-Z.1: Database Schema & Migrations
167: ‚îÇ   ‚Ä¢ Users table, indexes
168: ‚îÇ   ‚Ä¢ 2 hours, LOW risk
169: ‚îÇ
170: ‚îú‚îÄ PRP-Z.2: User Service (Business Logic)
171: ‚îÇ   ‚Ä¢ CRUD operations
172: ‚îÇ   ‚Ä¢ Validation rules
173: ‚îÇ   ‚Ä¢ 5 hours, MEDIUM risk
174: ‚îÇ
175: ‚îú‚îÄ PRP-Z.3: REST API Endpoints
176: ‚îÇ   ‚Ä¢ HTTP routes
177: ‚îÇ   ‚Ä¢ Request/response handling
178: ‚îÇ   ‚Ä¢ 3 hours, LOW risk
179: ‚îÇ
180: ‚îî‚îÄ PRP-Z.4: Admin UI Components
181:     ‚Ä¢ User list, forms
182:     ‚Ä¢ Frontend validation
183:     ‚Ä¢ 4 hours, MEDIUM risk
184: ```
185: 
186: **Benefits**:
187: - Bottom-up implementation (Z.1 ‚Üí Z.2 ‚Üí Z.3 ‚Üí Z.4)
188: - Each layer independently testable
189: - Parallel development possible (Z.3 and Z.4)
190: 
191: ## Pattern 5: Criteria-Based Decomposition
192: 
193: **When to Use**: PRP has >30 success criteria spanning multiple concerns
194: 
195: **Strategy**: Group related criteria into logical sub-features
196: 
197: ### Example: Testing Infrastructure
198: 
199: **Before (Monolithic)**:
200: ```
201: PRP-T: Testing Framework Enhancement
202: - 45 success criteria across unit, integration, E2E, performance
203: ```
204: 
205: **After (Decomposed)**:
206: ```
207: PRP-T.0: Testing Framework (Meta-PRP)
208: ‚îú‚îÄ PRP-T.1: Unit Test Infrastructure (10 criteria)
209: ‚îú‚îÄ PRP-T.2: Integration Test Setup (12 criteria)
210: ‚îú‚îÄ PRP-T.3: E2E Test Framework (15 criteria)
211: ‚îî‚îÄ PRP-T.4: Performance Test Suite (8 criteria)
212: ```
213: 
214: ## Anti-Patterns (What NOT to Do)
215: 
216: ### ‚ùå Anti-Pattern 1: Atomic Splitting
217: 
218: **Don't**: Split every phase into its own PRP when they're tightly coupled
219: 
220: **Problem**:
221: ```
222: PRP-A.1: Define data structure
223: PRP-A.2: Write getter for field 1
224: PRP-A.3: Write getter for field 2
225: PRP-A.4: Write getter for field 3
226: ```
227: 
228: **Better**: Keep tightly coupled code together
229: ```
230: PRP-A.1: Define data structure with all accessors
231: ```
232: 
233: ### ‚ùå Anti-Pattern 2: Artificial Boundaries
234: 
235: **Don't**: Split based on arbitrary criteria (e.g., "files starting with A-M vs N-Z")
236: 
237: **Problem**: No logical cohesion, dependencies span sub-PRPs
238: 
239: **Better**: Split based on functional boundaries or risk
240: 
241: ### ‚ùå Anti-Pattern 3: Over-Decomposition
242: 
243: **Don't**: Create 10 sub-PRPs of 1 hour each
244: 
245: **Problem**: Coordination overhead exceeds benefit
246: 
247: **Better**: 3-4 sub-PRPs of 3-5 hours each
248: 
249: ## Decision Tree
250: 
251: ```
252: Start: Is PRP > 1000 lines OR HIGH risk?
253: ‚îÇ
254: ‚îú‚îÄ No ‚Üí GREEN, proceed as-is
255: ‚îÇ
256: ‚îî‚îÄ Yes ‚Üí Is it ‚â•5 phases?
257:     ‚îÇ
258:     ‚îú‚îÄ Yes ‚Üí Use Phase-Based Decomposition
259:     ‚îÇ
260:     ‚îî‚îÄ No ‚Üí Does it span multiple functional areas?
261:         ‚îÇ
262:         ‚îú‚îÄ Yes ‚Üí Use Feature-Based Decomposition
263:         ‚îÇ
264:         ‚îî‚îÄ No ‚Üí Does it mix HIGH risk with LOW/MEDIUM?
265:             ‚îÇ
266:             ‚îú‚îÄ Yes ‚Üí Use Risk-Based Decomposition
267:             ‚îÇ
268:             ‚îî‚îÄ No ‚Üí Does it have >30 criteria?
269:                 ‚îÇ
270:                 ‚îú‚îÄ Yes ‚Üí Use Criteria-Based Decomposition
271:                 ‚îÇ
272:                 ‚îî‚îÄ No ‚Üí Consider Layer-Based or custom split
273: ```
274: 
275: ## Meta-PRP Template
276: 
277: When decomposing a PRP, create a meta-PRP (PRP-X.0) to track the overall feature:
278: 
279: ```markdown
280: ---
281: prp_id: PRP-X.0
282: feature_name: [Feature Name] Framework
283: status: new
284: type: meta-prp
285: sub_prps:
286:   - PRP-X.1
287:   - PRP-X.2
288:   - PRP-X.3
289: ---
290: 
291: # [Feature Name] Framework (Meta-PRP)
292: 
293: ## Purpose
294: Coordinate implementation of [feature] across multiple sub-PRPs.
295: 
296: ## Sub-PRPs
297: 1. **PRP-X.1**: [Name] (Xh, RISK)
298:    - Description
299:    - Dependencies: None
300: 
301: 2. **PRP-X.2**: [Name] (Xh, RISK)
302:    - Description
303:    - Dependencies: PRP-X.1
304: 
305: 3. **PRP-X.3**: [Name] (Xh, RISK)
306:    - Description
307:    - Dependencies: PRP-X.1, PRP-X.2
308: 
309: ## Execution Order
310: PRP-X.1 ‚Üí PRP-X.2 ‚Üí PRP-X.3
311: 
312: ## Integration Points
313: - Describe how sub-PRPs integrate
314: - Shared interfaces, data structures
315: 
316: ## Overall Success Criteria
317: - [ ] All sub-PRPs executed successfully
318: - [ ] Integration tests pass
319: - [ ] Feature delivers user value
320: ```
321: 
322: ## Real-World Example: PRP-4 Analysis
323: 
324: Run analyzer to see decomposition recommendations:
325: 
326: ```bash
327: cd tools
328: uv run ce prp analyze ../PRPs/executed/PRP-4-execute-prp-orchestration.md
329: ```
330: 
331: Output shows:
332: - Complexity Score: 71.45/100 (RED)
333: - Recommendations: Phase-based decomposition into 6 sub-PRPs
334: - Suggestion: Isolate HIGH-risk self-healing component
335: 
336: ## Tips for Success
337: 
338: 1. **Start with Meta-PRP**: Create PRP-X.0 first to plan decomposition
339: 2. **Document Dependencies**: Make execution order explicit
340: 3. **Keep Interfaces Clear**: Define how sub-PRPs interact
341: 4. **Test Incrementally**: Validate each sub-PRP before next one
342: 5. **Review Collectively**: Ensure all sub-PRPs together deliver feature
343: 
344: ## Validation Checklist
345: 
346: After decomposition, verify:
347: - [ ] Each sub-PRP is GREEN (‚â§700 lines, ‚â§8h)
348: - [ ] HIGH risk components isolated
349: - [ ] Dependencies are minimal and explicit
350: - [ ] Integration points well-defined
351: - [ ] Can execute incrementally with validation gates
352: 
353: ---
354: 
355: **See Also**:
356: - [PRP Sizing Guidelines](../docs/prp-sizing-guidelines.md)
357: - [PRP-8: Sizing Constraint Analysis](../PRPs/feature-requests/PRP-8-prp-sizing-constraint-analysis-and-optimal-breakdown-strategy.md)
</file>

<file path="examples/README.md">
 1: # Code Patterns & Examples
 2: 
 3: This directory contains reusable code patterns for reference during PRP implementation.
 4: 
 5: ## Structure
 6: 
 7: - `patterns/` - Common implementation patterns
 8:   - API patterns
 9:   - Database patterns
10:   - Test patterns
11:   - Error handling patterns
12: 
13: ## Usage
14: 
15: Reference these patterns in PRPs CONTEXT section:
16: 
17: - Similar implementation: `examples/patterns/api-crud.py:15-42`
18: 
19: ## Adding Patterns
20: 
21: When you implement a particularly good solution, extract it as a pattern:
22: 
23: 1. Create a new file in `patterns/` with descriptive name
24: 2. Include clear comments explaining the pattern
25: 3. Add usage notes and gotchas
26: 4. Reference from future PRPs
27: 
28: ## Pattern Categories
29: 
30: ### API Patterns
31: 
32: - RESTful endpoint design
33: - Request validation
34: - Response formatting
35: - Error handling
36: 
37: ### Database Patterns
38: 
39: - Query builders
40: - Transaction handling
41: - Schema migrations
42: - Connection pooling
43: 
44: ### Test Patterns
45: 
46: - Unit test structure
47: - Integration test setup
48: - Mock patterns
49: - Fixture management
50: 
51: ### Error Handling Patterns
52: 
53: - Exception hierarchy
54: - Error logging
55: - User-facing error messages
56: - Recovery strategies
57: 
58: ## Contributing
59: 
60: Keep patterns:
61: 
62: - **Simple**: Single responsibility
63: - **Documented**: Clear comments
64: - **Tested**: Include test examples
65: - **Practical**: Real-world usage
</file>

<file path="examples/tmp-directory-convention.md">
  1: # tmp/ Directory Convention
  2: 
  3: ## Rule: Keep Working Directory Clean
  4: 
  5: **Only final deliverables in tracked directories. All work-in-progress goes to tmp/**
  6: 
  7: ## Directory Structure
  8: 
  9: ```
 10: ctx-eng-plus/
 11: ‚îú‚îÄ‚îÄ PRPs/                    # ONLY final PRP documents
 12: ‚îÇ   ‚îú‚îÄ‚îÄ executed/
 13: ‚îÇ   ‚îî‚îÄ‚îÄ feature-requests/
 14: ‚îú‚îÄ‚îÄ tmp/                     # ALL work-in-progress (gitignored)
 15: ‚îÇ   ‚îú‚îÄ‚îÄ batch-gen/          # Batch generation heartbeat files
 16: ‚îÇ   ‚îú‚îÄ‚îÄ feature-requests/   # INITIAL.md and PLAN.md files
 17: ‚îÇ   ‚îî‚îÄ‚îÄ scratch/            # Any other temporary work
 18: ‚îî‚îÄ‚îÄ examples/               # Documentation and examples
 19: ```
 20: 
 21: ## Rules
 22: 
 23: ### 1. INITIAL.md and PLAN.md
 24: 
 25: **Location**: `tmp/feature-requests/<feature-name>/`
 26: 
 27: **Example**:
 28: ```bash
 29: # ‚úÖ CORRECT
 30: tmp/feature-requests/syntropy-tool-management/INITIAL.md
 31: tmp/feature-requests/syntropy-tool-management/PLAN.md
 32: 
 33: # ‚ùå WRONG
 34: feature-requests/syntropy-tool-management/INITIAL.md
 35: feature-requests/syntropy-tool-management/PLAN.md
 36: ```
 37: 
 38: ### 2. Batch Generation Heartbeat Files
 39: 
 40: **Location**: `tmp/batch-gen/`
 41: 
 42: **Example**:
 43: ```bash
 44: # ‚úÖ CORRECT
 45: tmp/batch-gen/PRP-30.1.1.status
 46: tmp/batch-gen/PRP-30.2.1.status
 47: 
 48: # ‚ùå WRONG
 49: .tmp/batch-gen/PRP-30.1.1.status  # No leading dot
 50: ```
 51: 
 52: ### 3. PRPs (Final Deliverables)
 53: 
 54: **Location**: `PRPs/feature-requests/` or `PRPs/executed/`
 55: 
 56: **Example**:
 57: ```bash
 58: # ‚úÖ CORRECT
 59: PRPs/feature-requests/PRP-30.1.1-syntropy-mcp-tool-management.md
 60: PRPs/executed/PRP-29-haiku-optimized-prp-guidelines.md
 61: 
 62: # ‚ùå WRONG
 63: tmp/PRPs/PRP-30.1.1-syntropy-mcp-tool-management.md
 64: ```
 65: 
 66: ## Workflow Example
 67: 
 68: ### Generate PRP from INITIAL.md
 69: 
 70: ```bash
 71: # Step 1: Create INITIAL.md in tmp/
 72: mkdir -p tmp/feature-requests/my-feature
 73: vim tmp/feature-requests/my-feature/INITIAL.md
 74: 
 75: # Step 2: Generate PRP (output goes to PRPs/)
 76: /generate-prp tmp/feature-requests/my-feature/INITIAL.md
 77: # Output: PRPs/feature-requests/PRP-X-my-feature.md
 78: 
 79: # Step 3: Clean up (optional)
 80: rm -rf tmp/feature-requests/my-feature
 81: ```
 82: 
 83: ### Batch Generation from PLAN.md
 84: 
 85: ```bash
 86: # Step 1: Create PLAN.md in tmp/
 87: mkdir -p tmp/feature-requests/big-feature
 88: vim tmp/feature-requests/big-feature/PLAN.md
 89: 
 90: # Step 2: Generate batch PRPs
 91: /batch-gen-prp tmp/feature-requests/big-feature/PLAN.md
 92: # Heartbeat files: tmp/batch-gen/PRP-X.Y.Z.status
 93: # Output PRPs: PRPs/feature-requests/PRP-X.Y.Z-*.md
 94: 
 95: # Step 3: Clean up (automatic)
 96: # Heartbeat files cleaned up after generation completes
 97: ```
 98: 
 99: ## Git Configuration
100: 
101: **Ensure tmp/ is gitignored**:
102: 
103: ```gitignore
104: # .gitignore
105: tmp/
106: .tmp/
107: *.tmp
108: ```
109: 
110: ## Benefits
111: 
112: 1. **Clean working directory**: Only final deliverables tracked in git
113: 2. **Easy cleanup**: Delete `tmp/` anytime without losing work
114: 3. **Clear separation**: Work-in-progress vs. deliverables
115: 4. **No git noise**: No accidental commits of INITIAL.md or heartbeat files
116: 
117: ## Migration
118: 
119: If you have existing files in wrong locations:
120: 
121: ```bash
122: # Move INITIAL.md files
123: mkdir -p tmp/feature-requests/
124: mv feature-requests/* tmp/feature-requests/
125: 
126: # Move batch-gen files (if any)
127: mkdir -p tmp/batch-gen/
128: mv .tmp/batch-gen/* tmp/batch-gen/ 2>/dev/null || true
129: rmdir .tmp/batch-gen .tmp 2>/dev/null || true
130: ```
</file>

<file path="tools/ce/examples/syntropy/context7_patterns.py">
  1: """
  2: Context7 MCP Documentation Lookup Patterns
  3: 
  4: Context7 provides library documentation access through Syntropy.
  5: Pattern: mcp__syntropy__context7__<operation>
  6: """
  7: 
  8: 
  9: # ‚úÖ PATTERN 1: Resolve Library ID
 10: # Use when: Get machine-readable library identifier from human-readable name
 11: def example_resolve_library_id():
 12:     """Convert library name to Context7 ID."""
 13:     from mcp__syntropy import syntropy_context7_resolve_library_id
 14:     
 15:     # Common libraries
 16:     libraries = [
 17:         "pytest",
 18:         "fastapi",
 19:         "django",
 20:         "sqlalchemy",
 21:         "pydantic",
 22:         "numpy",
 23:         "react",
 24:         "typescript"
 25:     ]
 26:     
 27:     for lib_name in libraries:
 28:         lib_id = syntropy_context7_resolve_library_id(
 29:             libraryName=lib_name
 30:         )
 31:         print(f"üìö {lib_name} ‚Üí {lib_id}")
 32:     
 33:     # Returns: Context7-compatible ID (e.g., "/pytest/docs")
 34: 
 35: 
 36: # ‚úÖ PATTERN 2: Get Library Documentation
 37: # Use when: Fetch API docs, usage patterns, configuration
 38: def example_get_library_docs():
 39:     """Fetch library documentation with optional topic focus."""
 40:     from mcp__syntropy import (
 41:         syntropy_context7_resolve_library_id,
 42:         syntropy_context7_get_library_docs
 43:     )
 44:     
 45:     # Step 1: Resolve library ID
 46:     lib_id = syntropy_context7_resolve_library_id(libraryName="pytest")
 47:     
 48:     # Step 2: Get documentation
 49:     docs = syntropy_context7_get_library_docs(
 50:         context7CompatibleLibraryID=lib_id,
 51:         topic="fixtures",
 52:         tokens=5000  # Max tokens in response
 53:     )
 54:     print(f"üìñ Pytest fixtures documentation:\n{docs}")
 55: 
 56: 
 57: # ‚úÖ PATTERN 3: Get FastAPI Documentation
 58: # Use when: Building FastAPI applications, need API patterns
 59: def example_fastapi_docs():
 60:     """FastAPI documentation for common patterns."""
 61:     from mcp__syntropy import (
 62:         syntropy_context7_resolve_library_id,
 63:         syntropy_context7_get_library_docs
 64:     )
 65:     
 66:     lib_id = syntropy_context7_resolve_library_id(libraryName="fastapi")
 67:     
 68:     # Get routing docs
 69:     routing = syntropy_context7_get_library_docs(
 70:         context7CompatibleLibraryID=lib_id,
 71:         topic="routing",
 72:         tokens=3000
 73:     )
 74:     
 75:     # Get dependency injection docs
 76:     di = syntropy_context7_get_library_docs(
 77:         context7CompatibleLibraryID=lib_id,
 78:         topic="dependency-injection",
 79:         tokens=3000
 80:     )
 81:     
 82:     # Get async patterns
 83:     async_patterns = syntropy_context7_get_library_docs(
 84:         context7CompatibleLibraryID=lib_id,
 85:         topic="async",
 86:         tokens=2000
 87:     )
 88: 
 89: 
 90: # ‚úÖ PATTERN 4: Get Django Documentation
 91: # Use when: Building Django applications, need ORM/views patterns
 92: def example_django_docs():
 93:     """Django documentation for common patterns."""
 94:     from mcp__syntropy import (
 95:         syntropy_context7_resolve_library_id,
 96:         syntropy_context7_get_library_docs
 97:     )
 98:     
 99:     lib_id = syntropy_context7_resolve_library_id(libraryName="django")
100:     
101:     # Get ORM docs
102:     orm = syntropy_context7_get_library_docs(
103:         context7CompatibleLibraryID=lib_id,
104:         topic="orm-models",
105:         tokens=4000
106:     )
107:     
108:     # Get views/requests
109:     views = syntropy_context7_get_library_docs(
110:         context7CompatibleLibraryID=lib_id,
111:         topic="views-requests",
112:         tokens=3000
113:     )
114:     
115:     # Get middleware patterns
116:     middleware = syntropy_context7_get_library_docs(
117:         context7CompatibleLibraryID=lib_id,
118:         topic="middleware",
119:         tokens=2000
120:     )
121: 
122: 
123: # ‚úÖ PATTERN 5: Get SQLAlchemy Documentation
124: # Use when: Database operations, ORM patterns
125: def example_sqlalchemy_docs():
126:     """SQLAlchemy documentation for database operations."""
127:     from mcp__syntropy import (
128:         syntropy_context7_resolve_library_id,
129:         syntropy_context7_get_library_docs
130:     )
131:     
132:     lib_id = syntropy_context7_resolve_library_id(libraryName="sqlalchemy")
133:     
134:     # Get session management
135:     sessions = syntropy_context7_get_library_docs(
136:         context7CompatibleLibraryID=lib_id,
137:         topic="session-management",
138:         tokens=3000
139:     )
140:     
141:     # Get query patterns
142:     queries = syntropy_context7_get_library_docs(
143:         context7CompatibleLibraryID=lib_id,
144:         topic="queries",
145:         tokens=3000
146:     )
147:     
148:     # Get relationships
149:     relationships = syntropy_context7_get_library_docs(
150:         context7CompatibleLibraryID=lib_id,
151:         topic="relationships",
152:         tokens=2000
153:     )
154: 
155: 
156: # ‚úÖ PATTERN 6: Get Pydantic Documentation
157: # Use when: Data validation, schema definition
158: def example_pydantic_docs():
159:     """Pydantic documentation for data validation."""
160:     from mcp__syntropy import (
161:         syntropy_context7_resolve_library_id,
162:         syntropy_context7_get_library_docs
163:     )
164:     
165:     lib_id = syntropy_context7_resolve_library_id(libraryName="pydantic")
166:     
167:     # Get validation docs
168:     validation = syntropy_context7_get_library_docs(
169:         context7CompatibleLibraryID=lib_id,
170:         topic="validation",
171:         tokens=3000
172:     )
173:     
174:     # Get field types
175:     fields = syntropy_context7_get_library_docs(
176:         context7CompatibleLibraryID=lib_id,
177:         topic="field-types",
178:         tokens=2000
179:     )
180: 
181: 
182: # ‚úÖ PATTERN 7: Get React Documentation
183: # Use when: Frontend development, component patterns
184: def example_react_docs():
185:     """React documentation for common patterns."""
186:     from mcp__syntropy import (
187:         syntropy_context7_resolve_library_id,
188:         syntropy_context7_get_library_docs
189:     )
190:     
191:     lib_id = syntropy_context7_resolve_library_id(libraryName="react")
192:     
193:     # Get hooks
194:     hooks = syntropy_context7_get_library_docs(
195:         context7CompatibleLibraryID=lib_id,
196:         topic="hooks",
197:         tokens=4000
198:     )
199:     
200:     # Get state management
201:     state = syntropy_context7_get_library_docs(
202:         context7CompatibleLibraryID=lib_id,
203:         topic="state-management",
204:         tokens=3000
205:     )
206: 
207: 
208: # üìä PERFORMANCE CHARACTERISTICS
209: # - resolve_library_id: O(1) - cached lookup
210: # - get_library_docs: O(1) - API call with caching
211: # - Token limit: 5000 max (adjust topic to fit)
212: # - Response time: ~500ms-2s depending on network
213: 
214: 
215: # üéØ WORKFLOW EXAMPLE: Knowledge-Grounded PRP Generation
216: def knowledge_grounded_prp_workflow():
217:     """Workflow: Generate PRP with real library documentation."""
218:     from mcp__syntropy import (
219:         syntropy_context7_resolve_library_id,
220:         syntropy_context7_get_library_docs
221:     )
222:     
223:     # Feature: Build FastAPI service
224:     # Step 1: Get FastAPI best practices
225:     lib_id = syntropy_context7_resolve_library_id(libraryName="fastapi")
226:     
227:     routing_docs = syntropy_context7_get_library_docs(
228:         context7CompatibleLibraryID=lib_id,
229:         topic="routing",
230:         tokens=2000
231:     )
232:     
233:     di_docs = syntropy_context7_get_library_docs(
234:         context7CompatibleLibraryID=lib_id,
235:         topic="dependency-injection",
236:         tokens=2000
237:     )
238:     
239:     # Step 2: Use real docs in PRP context
240:     # This ensures PRP reflects actual FastAPI patterns
241:     # Not guesses or outdated knowledge
242:     
243:     # Step 3: Generate PRP with real examples
244:     # PRP will include:
245:     # - Actual FastAPI routing patterns
246:     # - Real dependency injection patterns
247:     # - Correct async patterns
248:     # - Current best practices
249:     
250:     print("‚úÖ PRP generated with knowledge-grounded FastAPI documentation")
251: 
252: 
253: # üéØ WORKFLOW EXAMPLE: Framework Comparison
254: def framework_comparison_workflow():
255:     """Workflow: Compare patterns across frameworks."""
256:     from mcp__syntropy import (
257:         syntropy_context7_resolve_library_id,
258:         syntropy_context7_get_library_docs
259:     )
260:     
261:     # Async patterns in FastAPI vs Django
262:     
263:     # FastAPI
264:     fastapi_id = syntropy_context7_resolve_library_id(libraryName="fastapi")
265:     fastapi_async = syntropy_context7_get_library_docs(
266:         context7CompatibleLibraryID=fastapi_id,
267:         topic="async",
268:         tokens=2000
269:     )
270:     
271:     # Django
272:     django_id = syntropy_context7_resolve_library_id(libraryName="django")
273:     django_async = syntropy_context7_get_library_docs(
274:         context7CompatibleLibraryID=django_id,
275:         topic="async",
276:         tokens=2000
277:     )
278:     
279:     print("üìä FastAPI async patterns:")
280:     print(fastapi_async)
281:     print("\nüìä Django async patterns:")
282:     print(django_async)
283: 
284: 
285: # üéØ WORKFLOW EXAMPLE: Multi-library Integration
286: def multi_library_workflow():
287:     """Workflow: Build service using multiple libraries."""
288:     from mcp__syntropy import (
289:         syntropy_context7_resolve_library_id,
290:         syntropy_context7_get_library_docs
291:     )
292:     
293:     # Feature: Build REST API with auth
294:     # Libraries: FastAPI, SQLAlchemy, Pydantic, pytest
295:     
296:     docs_cache = {}
297:     
298:     for lib_name in ["fastapi", "sqlalchemy", "pydantic", "pytest"]:
299:         lib_id = syntropy_context7_resolve_library_id(libraryName=lib_name)
300:         
301:         docs = syntropy_context7_get_library_docs(
302:             context7CompatibleLibraryID=lib_id,
303:             topic="getting-started",  # Start with basics
304:             tokens=2000
305:         )
306:         
307:         docs_cache[lib_name] = docs
308:     
309:     # Now have real documentation for all libraries
310:     # Use to generate accurate PRP with integration patterns
311: 
312: 
313: # üîß ERROR HANDLING PATTERNS
314: 
315: # ‚úÖ PATTERN: Graceful Library Lookup
316: def graceful_lookup_pattern():
317:     """Handle missing or unmapped libraries gracefully."""
318:     from mcp__syntropy import (
319:         syntropy_context7_resolve_library_id,
320:         syntropy_context7_get_library_docs
321:     )
322:     
323:     def get_docs_safe(lib_name: str, topic: str) -> dict:
324:         try:
325:             # Try to resolve library
326:             lib_id = syntropy_context7_resolve_library_id(
327:                 libraryName=lib_name
328:             )
329:             
330:             # Get documentation
331:             docs = syntropy_context7_get_library_docs(
332:                 context7CompatibleLibraryID=lib_id,
333:                 topic=topic,
334:                 tokens=3000
335:             )
336:             
337:             return {
338:                 "success": True,
339:                 "docs": docs,
340:                 "library": lib_name
341:             }
342:             
343:         except Exception as e:
344:             # Library not found or error
345:             return {
346:                 "success": False,
347:                 "error": str(e),
348:                 "library": lib_name,
349:                 "troubleshooting": f"üîß Library '{lib_name}' not available in Context7. "
350:                                  f"Check spelling or see supported libraries."
351:             }
352: 
353: 
354: # ‚úÖ PATTERN: Token Budget Management
355: def token_budget_pattern():
356:     """Manage documentation token budget efficiently."""
357:     from mcp__syntropy import (
358:         syntropy_context7_resolve_library_id,
359:         syntropy_context7_get_library_docs
360:     )
361:     
362:     # For large feature with multiple libraries
363:     TOTAL_TOKENS = 15000
364:     TOKENS_PER_LIB = 3000
365:     
366:     libraries = ["fastapi", "sqlalchemy", "pydantic"]
367:     
368:     for lib_name in libraries:
369:         lib_id = syntropy_context7_resolve_library_id(libraryName=lib_name)
370:         
371:         docs = syntropy_context7_get_library_docs(
372:             context7CompatibleLibraryID=lib_id,
373:             topic="best-practices",  # Concise topic
374:             tokens=TOKENS_PER_LIB
375:         )
376:         
377:         print(f"üìö {lib_name}: {len(docs.split())} words")
378: 
379: 
380: # ‚úÖ PATTERN: Cache Documentation Locally
381: def caching_pattern():
382:     """Cache documentation to avoid repeated requests."""
383:     import json
384:     from pathlib import Path
385:     from mcp__syntropy import (
386:         syntropy_context7_resolve_library_id,
387:         syntropy_context7_get_library_docs
388:     )
389:     
390:     CACHE_DIR = Path("/tmp/context7-cache")
391:     CACHE_DIR.mkdir(exist_ok=True)
392:     
393:     def get_docs_cached(lib_name: str, topic: str) -> str:
394:         cache_file = CACHE_DIR / f"{lib_name}_{topic}.md"
395:         
396:         # Return cached if exists
397:         if cache_file.exists():
398:             return cache_file.read_text()
399:         
400:         # Fetch from Context7
401:         lib_id = syntropy_context7_resolve_library_id(libraryName=lib_name)
402:         docs = syntropy_context7_get_library_docs(
403:             context7CompatibleLibraryID=lib_id,
404:             topic=topic,
405:             tokens=3000
406:         )
407:         
408:         # Cache for future use
409:         cache_file.write_text(docs)
410:         return docs
411: 
412: 
413: # üîß TROUBLESHOOTING
414: def troubleshooting():
415:     """Common issues and solutions."""
416:     # Issue: resolve_library_id returns None
417:     # Solution: Library name may be misspelled or not in Context7
418:     # Try: "pytest" not "py-test"
419:     # Try: "django" not "django-framework"
420:     
421:     # Issue: get_library_docs returns incomplete documentation
422:     # Solution: Increase tokens parameter
423:     # Try: tokens=5000 (maximum)
424:     # Or narrow topic: "routing" not "all"
425:     
426:     # Issue: Documentation outdated or wrong version
427:     # Solution: Context7 may not have latest version
428:     # Check documentation date in response
429:     # Cross-reference with official project docs
430:     
431:     # Issue: Rate limiting / timeout
432:     # Solution: Wait before next request
433:     # Cache results to avoid repeated requests
434:     # Check Context7 rate limits
435: 
436: 
437: # üìö SUPPORTED LIBRARIES
438: def supported_libraries():
439:     """Commonly available libraries in Context7."""
440:     return {
441:         "python": [
442:             "pytest",
443:             "fastapi",
444:             "django",
445:             "sqlalchemy",
446:             "pydantic",
447:             "numpy",
448:             "pandas",
449:             "requests",
450:             "celery",
451:             "redis"
452:         ],
453:         "javascript": [
454:             "react",
455:             "typescript",
456:             "nodejs",
457:             "express",
458:             "next.js",
459:             "vue",
460:             "angular"
461:         ],
462:         "other": [
463:             "docker",
464:             "kubernetes",
465:             "postgresql",
466:             "mongodb"
467:         ]
468:     }
469: 
470: 
471: # üí° BEST PRACTICES
472: def best_practices():
473:     """Context7 usage best practices."""
474:     return {
475:         "do": [
476:             "‚úÖ Resolve library ID once, reuse multiple times",
477:             "‚úÖ Cache documentation locally to reduce API calls",
478:             "‚úÖ Use specific topics to fit token budget",
479:             "‚úÖ Include documentation links in generated PRPs",
480:             "‚úÖ Cross-reference with official docs",
481:             "‚úÖ Verify documentation is for correct version"
482:         ],
483:         "dont": [
484:             "‚ùå Assume documentation is 100% current",
485:             "‚ùå Use outdated library knowledge from training",
486:             "‚ùå Ignore token limits",
487:             "‚ùå Fetch entire library docs at once",
488:             "‚ùå Trust incomplete or partial documentation",
489:             "‚ùå Skip validation of documentation patterns"
490:         ]
491:     }
</file>

<file path="tools/ce/examples/syntropy/git_patterns.py">
  1: """
  2: Git MCP Version Control Patterns
  3: 
  4: Git provides version control operations through Syntropy.
  5: Pattern: mcp__syntropy__git__<operation>
  6: """
  7: 
  8: 
  9: # ‚úÖ PATTERN 1: Check Repository Status
 10: # Use when: Understand current state before making changes
 11: def example_git_status():
 12:     """Get repository status (staged, unstaged, untracked)."""
 13:     from mcp__syntropy import syntropy_git_git_status
 14:     
 15:     result = syntropy_git_git_status(repo_path="/Users/bprzybysz/nc-src/ctx-eng-plus")
 16:     # Returns: {
 17:     #     clean: bool,
 18:     #     staged: [files],
 19:     #     unstaged: [files],
 20:     #     untracked: [files],
 21:     #     branch: current_branch,
 22:     #     ahead: commits_ahead,
 23:     #     behind: commits_behind
 24:     # }
 25:     
 26:     if result.get("clean"):
 27:         print("‚úÖ Repository is clean")
 28:     else:
 29:         print(f"üìù Unstaged: {len(result['unstaged'])} files")
 30:         print(f"‚úèÔ∏è Staged: {len(result['staged'])} files")
 31: 
 32: 
 33: # ‚úÖ PATTERN 2: View Recent Changes
 34: # Use when: Review what changed before committing
 35: def example_git_diff():
 36:     """View recent changes (staged or unstaged)."""
 37:     from mcp__syntropy import syntropy_git_git_diff
 38:     
 39:     # Show unstaged changes
 40:     result = syntropy_git_git_diff(
 41:         repo_path="/Users/bprzybysz/nc-src/ctx-eng-plus",
 42:         staged=False
 43:     )
 44:     # Returns: Unified diff format
 45:     
 46:     # Show staged changes only
 47:     result = syntropy_git_git_diff(
 48:         repo_path="/Users/bprzybysz/nc-src/ctx-eng-plus",
 49:         staged=True
 50:     )
 51:     print(result)
 52: 
 53: 
 54: # ‚úÖ PATTERN 3: View Commit History
 55: # Use when: Understand project history, find commits
 56: def example_git_log():
 57:     """View commit history."""
 58:     from mcp__syntropy import syntropy_git_git_log
 59:     
 60:     # Get last 10 commits
 61:     result = syntropy_git_git_log(
 62:         repo_path="/Users/bprzybysz/nc-src/ctx-eng-plus",
 63:         max_count=10
 64:     )
 65:     # Returns: List of commits with:
 66:     # - SHA
 67:     # - Author
 68:     # - Date
 69:     # - Message
 70:     
 71:     for commit in result:
 72:         print(f"{commit['sha'][:7]} - {commit['message']}")
 73: 
 74: 
 75: # ‚úÖ PATTERN 4: Stage Files for Commit
 76: # Use when: Prepare specific files before committing
 77: def example_git_add():
 78:     """Stage files for commit."""
 79:     from mcp__syntropy import syntropy_git_git_add
 80:     
 81:     # Stage specific files
 82:     result = syntropy_git_git_add(
 83:         repo_path="/Users/bprzybysz/nc-src/ctx-eng-plus",
 84:         paths=[
 85:             "tools/ce/core.py",
 86:             "tools/tests/test_core.py"
 87:         ]
 88:     )
 89:     # Returns: {success: bool, message: str}
 90:     
 91:     if result.get("success"):
 92:         print("‚úÖ Files staged successfully")
 93:     else:
 94:         print(f"‚ùå Staging failed: {result['message']}")
 95: 
 96: 
 97: # ‚úÖ PATTERN 5: Create Commit
 98: # Use when: Save changes with descriptive message
 99: def example_git_commit():
100:     """Create commit with message."""
101:     from mcp__syntropy import syntropy_git_git_commit
102:     
103:     # Create commit
104:     result = syntropy_git_git_commit(
105:         repo_path="/Users/bprzybysz/nc-src/ctx-eng-plus",
106:         message="""feat: Add Syntropy server examples
107: 
108: - Add serena_patterns.py for code navigation
109: - Add filesystem_patterns.py for file operations
110: - Add git_patterns.py for version control
111: - Add linear_patterns.py for issue tracking
112: - Add context7_patterns.py for documentation lookup
113: 
114: ü§ñ Generated with Claude Code"""
115:     )
116:     # Returns: {success: bool, sha: commit_hash, message: str}
117: 
118: 
119: # üìä PERFORMANCE CHARACTERISTICS
120: # - git_status: O(files) - scans working directory
121: # - git_diff: O(changes) - computes diff
122: # - git_log: O(commits) - scans history
123: # - git_add: O(files) - stages files
124: # - git_commit: O(1) - creates commit object
125: 
126: 
127: # üéØ WORKFLOW EXAMPLE: Complete Commit Cycle
128: def complete_commit_workflow():
129:     """Typical workflow: Check status ‚Üí review ‚Üí stage ‚Üí commit."""
130:     from mcp__syntropy import (
131:         syntropy_git_git_status,
132:         syntropy_git_git_diff,
133:         syntropy_git_git_add,
134:         syntropy_git_git_commit
135:     )
136:     
137:     repo = "/Users/bprzybysz/nc-src/ctx-eng-plus"
138:     
139:     # Step 1: Check status
140:     status = syntropy_git_git_status(repo_path=repo)
141:     print(f"üìä Status: {len(status['unstaged'])} unstaged files")
142:     
143:     # Step 2: Review changes
144:     if status['unstaged']:
145:         diff = syntropy_git_git_diff(repo_path=repo, staged=False)
146:         print(f"üìù Changes:\n{diff[:500]}...")  # Show first 500 chars
147:     
148:     # Step 3: Stage files
149:     result = syntropy_git_git_add(
150:         repo_path=repo,
151:         paths=status['unstaged']  # Stage all unstaged changes
152:     )
153:     print(f"‚úèÔ∏è Staged: {len(status['unstaged'])} files")
154:     
155:     # Step 4: Create commit
156:     commit = syntropy_git_git_commit(
157:         repo_path=repo,
158:         message="refactor: Update Syntropy examples"
159:     )
160:     print(f"‚úÖ Committed: {commit['sha'][:7]}")
161: 
162: 
163: # üéØ WORKFLOW EXAMPLE: Selective Staging
164: def selective_staging_workflow():
165:     """Typical workflow: Stage only specific files."""
166:     from mcp__syntropy import (
167:         syntropy_git_git_status,
168:         syntropy_git_git_diff,
169:         syntropy_git_git_add,
170:         syntropy_git_git_commit
171:     )
172:     
173:     repo = "/Users/bprzybysz/nc-src/ctx-eng-plus"
174:     
175:     # Check status
176:     status = syntropy_git_git_status(repo_path=repo)
177:     
178:     # Filter: Only stage .py files, not .md files
179:     py_files = [f for f in status['unstaged'] if f.endswith('.py')]
180:     
181:     # Stage only Python files
182:     syntropy_git_git_add(repo_path=repo, paths=py_files)
183:     
184:     # Commit
185:     syntropy_git_git_commit(
186:         repo_path=repo,
187:         message="feat: Add new patterns"
188:     )
189:     print(f"‚úÖ Committed {len(py_files)} Python files")
190: 
191: 
192: # üéØ WORKFLOW EXAMPLE: Review Changes Before Commit
193: def review_workflow():
194:     """Typical workflow: Review staged changes before committing."""
195:     from mcp__syntropy import (
196:         syntropy_git_git_status,
197:         syntropy_git_git_diff,
198:         syntropy_git_git_log
199:     )
200:     
201:     repo = "/Users/bprzybysz/nc-src/ctx-eng-plus"
202:     
203:     # Check status
204:     status = syntropy_git_git_status(repo_path=repo)
205:     
206:     # View staged changes
207:     if status['staged']:
208:         diff = syntropy_git_git_diff(repo_path=repo, staged=True)
209:         print(f"üìù Staged changes:\n{diff}")
210:         
211:         # View recent commits for context
212:         history = syntropy_git_git_log(repo_path=repo, max_count=3)
213:         print("\nüìú Recent commits:")
214:         for commit in history:
215:             print(f"  {commit['sha'][:7]} - {commit['message']}")
216: 
217: 
218: # üîß ERROR HANDLING PATTERNS
219: 
220: # ‚úÖ PATTERN: Safe Commit with Pre-flight Checks
221: def safe_commit_pattern():
222:     """Commit safely with validation."""
223:     from mcp__syntropy import (
224:         syntropy_git_git_status,
225:         syntropy_git_git_add,
226:         syntropy_git_git_commit
227:     )
228:     
229:     repo = "/Users/bprzybysz/nc-src/ctx-eng-plus"
230:     
231:     # Check status
232:     status = syntropy_git_git_status(repo_path=repo)
233:     
234:     # Validate
235:     if not status['unstaged']:
236:         print("‚úÖ No changes to commit")
237:         return
238:     
239:     # Stage changes
240:     result = syntropy_git_git_add(repo_path=repo, paths=status['unstaged'])
241:     if not result.get('success'):
242:         raise RuntimeError(f"üîß Staging failed: {result.get('message')}")
243:     
244:     # Create commit
245:     commit = syntropy_git_git_commit(
246:         repo_path=repo,
247:         message="feat: Update patterns"
248:     )
249:     
250:     if not commit.get('success'):
251:         raise RuntimeError(f"üîß Commit failed: {commit.get('message')}")
252:     
253:     print(f"‚úÖ Committed: {commit['sha'][:7]}")
254: 
255: 
256: # ‚úÖ PATTERN: Batch Commits with Grouping
257: def batch_commits_pattern():
258:     """Create multiple commits for different change groups."""
259:     from mcp__syntropy import (
260:         syntropy_git_git_status,
261:         syntropy_git_git_add,
262:         syntropy_git_git_commit
263:     )
264:     
265:     repo = "/Users/bprzybysz/nc-src/ctx-eng-plus"
266:     status = syntropy_git_git_status(repo_path=repo)
267:     
268:     # Group 1: Examples
269:     example_files = [f for f in status['unstaged'] if 'examples/' in f]
270:     if example_files:
271:         syntropy_git_git_add(repo_path=repo, paths=example_files)
272:         syntropy_git_git_commit(
273:             repo_path=repo,
274:             message="docs: Add Syntropy server examples"
275:         )
276:     
277:     # Group 2: Tests
278:     test_files = [f for f in status['unstaged'] if 'test_' in f]
279:     if test_files:
280:         syntropy_git_git_add(repo_path=repo, paths=test_files)
281:         syntropy_git_git_commit(
282:             repo_path=repo,
283:             message="test: Update Syntropy tests"
284:         )
285: 
286: 
287: # üîß TROUBLESHOOTING
288: def troubleshooting():
289:     """Common issues and solutions."""
290:     # Issue: git_add returns empty paths
291:     # Solution: Use git_status first to get exact file paths
292:     # Git uses relative paths from repo root
293:     
294:     # Issue: Commit fails with "nothing to commit"
295:     # Solution: Verify files are staged with git_status (check staged list)
296:     # Use git_add before git_commit
297:     
298:     # Issue: git_diff returns empty
299:     # Solution: Check staged parameter (True for staged, False for unstaged)
300:     # Verify changes exist with git_status
301:     
302:     # Issue: git_log shows no commits
303:     # Solution: Repository may have no commits
304:     # Verify with git_status
305:     
306:     # Issue: Permission denied on commit
307:     # Solution: Check git config (user.name, user.email)
308:     # Verify repo permissions
</file>

<file path="tools/ce/examples/syntropy/linear_patterns.py">
  1: """
  2: Linear MCP Project Management Patterns
  3: 
  4: Linear provides issue tracking operations through Syntropy.
  5: Pattern: mcp__syntropy__linear__<operation>
  6: """
  7: 
  8: 
  9: # ‚úÖ PATTERN 1: Create Issue
 10: # Use when: Generate new task from PRP or feature request
 11: def example_create_issue():
 12:     """Create new Linear issue."""
 13:     from mcp__syntropy import syntropy_linear_create_issue
 14:     
 15:     result = syntropy_linear_create_issue(
 16:         team="Blaise78",
 17:         title="PRP-18: Syntropy Server Examples",
 18:         description="""
 19:         Add example patterns for all Syntropy servers.
 20:         
 21:         ## Tasks
 22:         - [ ] Add serena_patterns.py
 23:         - [ ] Add filesystem_patterns.py
 24:         - [ ] Add git_patterns.py
 25:         - [ ] Add linear_patterns.py
 26:         
 27:         ## Acceptance Criteria
 28:         - All patterns documented with examples
 29:         - Error handling demonstrated
 30:         - Anti-patterns highlighted
 31:         """,
 32:         assignee="blazej.przybyszewski@gmail.com"
 33:     )
 34:     # Returns: {id: "BLA-123", identifier: "BLA-123", ...}
 35:     print(f"‚úÖ Created issue: {result['identifier']}")
 36: 
 37: 
 38: # ‚úÖ PATTERN 2: List Issues
 39: # Use when: Check project status, find assigned work
 40: def example_list_issues():
 41:     """List issues with optional filtering."""
 42:     from mcp__syntropy import syntropy_linear_list_issues
 43:     
 44:     # List all issues in project
 45:     result = syntropy_linear_list_issues(team="Blaise78")
 46:     print(f"üìä Total issues: {len(result)}")
 47:     
 48:     # List my assigned issues
 49:     result = syntropy_linear_list_issues(
 50:         team="Blaise78",
 51:         assignee="blazej.przybyszewski@gmail.com"
 52:     )
 53:     
 54:     for issue in result:
 55:         print(f"{issue['id']} - {issue['title']} [{issue['status']}]")
 56: 
 57: 
 58: # ‚úÖ PATTERN 3: Get Issue Details
 59: # Use when: Read issue description, see comments, check status
 60: def example_get_issue():
 61:     """Retrieve specific issue details."""
 62:     from mcp__syntropy import syntropy_linear_get_issue
 63:     
 64:     result = syntropy_linear_get_issue(issue_id="BLA-123")
 65:     # Returns: {
 66:     #     id: "BLA-123",
 67:     #     identifier: "BLA-123",
 68:     #     title: "...",
 69:     #     description: "...",
 70:     #     status: "in_progress",
 71:     #     assignee: {...},
 72:     #     created_at: "...",
 73:     #     updated_at: "...",
 74:     #     labels: [...]
 75:     # }
 76:     
 77:     print(f"Issue: {result['identifier']}")
 78:     print(f"Status: {result['status']}")
 79:     print(f"Assigned to: {result['assignee']['displayName']}")
 80: 
 81: 
 82: # ‚úÖ PATTERN 4: Update Issue
 83: # Use when: Change status, update assignee, modify description
 84: def example_update_issue():
 85:     """Update issue properties."""
 86:     from mcp__syntropy import syntropy_linear_update_issue
 87:     
 88:     # Update status
 89:     result = syntropy_linear_update_issue(
 90:         issue_id="BLA-123",
 91:         updates={
 92:             "state": "in_progress"  # todo, in_progress, completed, cancelled
 93:         }
 94:     )
 95:     
 96:     # Update multiple fields
 97:     result = syntropy_linear_update_issue(
 98:         issue_id="BLA-123",
 99:         updates={
100:             "title": "PRP-18: Complete Syntropy Examples",
101:             "description": "Add patterns for all servers with error handling",
102:             "state": "in_progress"
103:         }
104:     )
105:     
106:     if result.get("success"):
107:         print("‚úÖ Issue updated")
108:     else:
109:         print(f"‚ùå Update failed: {result.get('error')}")
110: 
111: 
112: # ‚úÖ PATTERN 5: List Projects
113: # Use when: Find available projects for issue creation
114: def example_list_projects():
115:     """List all Linear projects."""
116:     from mcp__syntropy import syntropy_linear_list_projects
117:     
118:     result = syntropy_linear_list_projects()
119:     # Returns: List of projects
120:     
121:     for project in result:
122:         print(f"üìÅ {project['name']} (ID: {project['id']})")
123: 
124: 
125: # üìä PERFORMANCE CHARACTERISTICS
126: # - create_issue: O(1) - immediate creation
127: # - list_issues: O(n) where n = issues in project
128: # - get_issue: O(1) - direct lookup
129: # - update_issue: O(1) - immediate update
130: # - list_projects: O(1) - cached
131: 
132: 
133: # üéØ WORKFLOW EXAMPLE: PRP Integration
134: def prp_integration_workflow():
135:     """Typical workflow: Generate PRP ‚Üí Create issue ‚Üí Track."""
136:     from mcp__syntropy import (
137:         syntropy_linear_create_issue,
138:         syntropy_linear_get_issue,
139:         syntropy_linear_update_issue
140:     )
141:     
142:     # Step 1: /generate-prp creates PRP and automatically creates issue
143:     # (This happens in generate.py workflow)
144:     
145:     # Step 2: Get issue ID from PRP YAML header
146:     # prp_yaml["issue"] = "BLA-123"
147:     
148:     # Step 3: Update issue status as we progress
149:     syntropy_linear_update_issue(
150:         issue_id="BLA-123",
151:         updates={"state": "in_progress"}
152:     )
153:     
154:     # Step 4: Check progress
155:     issue = syntropy_linear_get_issue(issue_id="BLA-123")
156:     print(f"üìä Status: {issue['status']}")
157:     
158:     # Step 5: Mark done when complete
159:     syntropy_linear_update_issue(
160:         issue_id="BLA-123",
161:         updates={"state": "completed"}
162:     )
163: 
164: 
165: # üéØ WORKFLOW EXAMPLE: Issue Triage
166: def triage_workflow():
167:     """Typical workflow: Daily triage of new issues."""
168:     from mcp__syntropy import (
169:         syntropy_linear_list_issues,
170:         syntropy_linear_get_issue,
171:         syntropy_linear_update_issue
172:     )
173:     
174:     # Get all open issues
175:     issues = syntropy_linear_list_issues(team="Blaise78")
176:     
177:     # Filter unstarted
178:     unstarted = [i for i in issues if i['status'] == 'todo']
179:     
180:     for issue in unstarted[:5]:  # Process first 5
181:         print(f"\nüîç Reviewing: {issue['id']} - {issue['title']}")
182:         
183:         # Get full details
184:         details = syntropy_linear_get_issue(issue_id=issue['id'])
185:         print(f"Description: {details['description'][:100]}...")
186:         
187:         # Update if ready
188:         if is_ready_to_start(details):
189:             syntropy_linear_update_issue(
190:                 issue_id=issue['id'],
191:                 updates={
192:                     "state": "in_progress",
193:                     "assignee": "blazej.przybyszewski@gmail.com"
194:                 }
195:             )
196:             print(f"‚úÖ Started: {issue['id']}")
197: 
198: 
199: # üéØ WORKFLOW EXAMPLE: Joining Multiple PRPs to Same Issue
200: def join_prp_workflow():
201:     """Workflow: Related PRPs sharing one Linear issue."""
202:     from mcp__syntropy import (
203:         syntropy_linear_create_issue,
204:         syntropy_linear_update_issue
205:     )
206:     
207:     # First PRP creates issue
208:     # /generate-prp auth-part1.md ‚Üí Creates BLA-25 + PRP-10
209:     
210:     # Second related PRP joins same issue
211:     # /generate-prp auth-part2.md --join-prp 10
212:     # ‚Üí Creates PRP-11, uses BLA-25 from PRP-10
213:     
214:     # Both PRPs now reference same Linear issue
215:     # Issue description includes both PRP details
216:     # Simplifies tracking related work
217: 
218: 
219: # üîß ERROR HANDLING PATTERNS
220: 
221: # ‚úÖ PATTERN: Safe Update with Validation
222: def safe_update_pattern():
223:     """Update issue safely with pre-validation."""
224:     from mcp__syntropy import (
225:         syntropy_linear_get_issue,
226:         syntropy_linear_update_issue
227:     )
228:     
229:     issue_id = "BLA-123"
230:     
231:     # Step 1: Get current state
232:     current = syntropy_linear_get_issue(issue_id=issue_id)
233:     
234:     # Step 2: Validate transition
235:     valid_transitions = {
236:         "todo": ["in_progress", "cancelled"],
237:         "in_progress": ["completed", "cancelled", "todo"],
238:         "completed": ["todo"],
239:         "cancelled": ["todo"]
240:     }
241:     
242:     new_state = "in_progress"
243:     if new_state not in valid_transitions.get(current['status'], []):
244:         raise ValueError(
245:             f"‚ùå Invalid state transition: {current['status']} ‚Üí {new_state}\n"
246:             f"üîß Valid transitions: {valid_transitions[current['status']]}"
247:         )
248:     
249:     # Step 3: Update
250:     result = syntropy_linear_update_issue(
251:         issue_id=issue_id,
252:         updates={"state": new_state}
253:     )
254:     
255:     if not result.get("success"):
256:         raise RuntimeError(f"üîß Update failed: {result.get('error')}")
257: 
258: 
259: # ‚úÖ PATTERN: Batch Issue Creation
260: def batch_create_pattern():
261:     """Create multiple related issues."""
262:     from mcp__syntropy import syntropy_linear_create_issue
263:     
264:     features = [
265:         ("Feature 1", "Description 1"),
266:         ("Feature 2", "Description 2"),
267:         ("Feature 3", "Description 3")
268:     ]
269:     
270:     created_issues = []
271:     for title, description in features:
272:         result = syntropy_linear_create_issue(
273:             team="Blaise78",
274:             title=title,
275:             description=description,
276:             assignee="blazej.przybyszewski@gmail.com"
277:         )
278:         created_issues.append(result['id'])
279:         print(f"‚úÖ Created: {result['identifier']}")
280:     
281:     return created_issues
282: 
283: 
284: # ‚úÖ PATTERN: Track Implementation Progress
285: def track_progress_pattern():
286:     """Update issue status as implementation progresses."""
287:     from mcp__syntropy import syntropy_linear_update_issue
288:     
289:     issue_id = "BLA-123"
290:     
291:     # Phase 1: Research & Design
292:     syntropy_linear_update_issue(
293:         issue_id=issue_id,
294:         updates={
295:             "state": "in_progress",
296:             "description": "Phase 1: Research & Design - IN PROGRESS"
297:         }
298:     )
299:     print("üìã Phase 1: Research & Design")
300:     
301:     # Phase 2: Implementation
302:     syntropy_linear_update_issue(
303:         issue_id=issue_id,
304:         updates={
305:             "description": "Phase 2: Implementation - IN PROGRESS"
306:         }
307:     )
308:     print("üî® Phase 2: Implementation")
309:     
310:     # Phase 3: Testing & Review
311:     syntropy_linear_update_issue(
312:         issue_id=issue_id,
313:         updates={
314:             "description": "Phase 3: Testing & Review - IN PROGRESS"
315:         }
316:     )
317:     print("‚úÖ Phase 3: Testing & Review")
318:     
319:     # Complete
320:     syntropy_linear_update_issue(
321:         issue_id=issue_id,
322:         updates={
323:             "state": "completed",
324:             "description": "‚úÖ All phases complete"
325:         }
326:     )
327: 
328: 
329: # üîß TROUBLESHOOTING
330: def troubleshooting():
331:     """Common issues and solutions."""
332:     # Issue: create_issue returns 401 Unauthorized
333:     # Solution: Check Linear API token in environment
334:     # Set: export LINEAR_API_KEY=...
335:     
336:     # Issue: Team ID not found
337:     # Solution: Use list_projects() to find correct team ID
338:     # Common teams: "Blaise78", check .ce/linear-defaults.yml
339:     
340:     # Issue: Issue update fails silently
341:     # Solution: Check update permissions
342:     # Verify assignee email is valid in Linear
343:     
344:     # Issue: Can't find issue after creation
345:     # Solution: Linear has eventual consistency
346:     # Issue may take 1-2 seconds to appear in list
347:     # Use get_issue(issue_id) immediately
348:     
349:     # Issue: Invalid state transition
350:     # Solution: Check current state first
351:     # Use get_issue() to see valid transitions
352:     # Linear state machine: todo ‚Üí in_progress ‚Üí completed
353: 
354: 
355: def is_ready_to_start(issue: dict) -> bool:
356:     """Check if issue is ready to start implementation."""
357:     # Stub for triage workflow
358:     return True
</file>

<file path="tools/ce/examples/syntropy/serena_patterns.py">
  1: """
  2: Serena MCP Code Navigation Patterns
  3: 
  4: Serena provides LSP-backed code navigation through Syntropy.
  5: Pattern: mcp__syntropy__serena__<operation>
  6: """
  7: 
  8: 
  9: # ‚úÖ PATTERN 1: Find Symbol Definition
 10: # Use when: You know the symbol name and want to see its implementation
 11: def example_find_symbol():
 12:     """Find function/class by exact name."""
 13:     from mcp__syntropy import syntropy_serena_find_symbol
 14:     
 15:     # Find function
 16:     result = syntropy_serena_find_symbol(
 17:         name_path="authenticate_user",
 18:         include_body=True
 19:     )
 20:     # Returns: function source code, docstring, location
 21:     
 22:     # Find class method
 23:     result = syntropy_serena_find_symbol(
 24:         name_path="UserAuth/validate",
 25:         include_body=True
 26:     )
 27:     # Returns: method source, containing class, decorators
 28: 
 29: 
 30: # ‚úÖ PATTERN 2: Get File Structure Overview
 31: # Use when: Exploring a file for the first time, need high-level structure
 32: def example_get_symbols_overview():
 33:     """List all top-level functions/classes in a file."""
 34:     from mcp__syntropy import syntropy_serena_get_symbols_overview
 35:     
 36:     # Get file structure
 37:     result = syntropy_serena_get_symbols_overview(
 38:         relative_path="ce/core.py"
 39:     )
 40:     # Returns: List of all functions, classes, imports, metadata
 41: 
 42: 
 43: # ‚úÖ PATTERN 3: Search for Code Pattern
 44: # Use when: Find all occurrences of a pattern (not exact symbol name)
 45: def example_search_for_pattern():
 46:     """Search code by regex pattern across codebase."""
 47:     from mcp__syntropy import syntropy_serena_search_for_pattern
 48:     
 49:     # Find all async functions
 50:     result = syntropy_serena_search_for_pattern(
 51:         pattern=r"async def \w+",
 52:         file_glob="**/*.py"
 53:     )
 54:     # Returns: All async function definitions
 55:     
 56:     # Find error handling patterns
 57:     result = syntropy_serena_search_for_pattern(
 58:         pattern=r"except\s+\w+Error",
 59:         file_glob="ce/**/*.py"
 60:     )
 61:     # Returns: All specific error catches
 62: 
 63: 
 64: # ‚úÖ PATTERN 4: Find All References
 65: # Use when: Impact analysis - where is this function called?
 66: def example_find_referencing_symbols():
 67:     """Find all uses of a symbol throughout codebase."""
 68:     from mcp__syntropy import syntropy_serena_find_referencing_symbols
 69:     
 70:     # Find all calls to validate_token
 71:     result = syntropy_serena_find_referencing_symbols(
 72:         name_path="validate_token"
 73:     )
 74:     # Returns: All file locations where validate_token is called
 75:     
 76:     # Use in refactoring: Before changing function signature, see all call sites
 77: 
 78: 
 79: # ‚úÖ PATTERN 5: Read File Contents
 80: # Use when: Need to read Python file with LSP indexing
 81: def example_read_file():
 82:     """Read file with LSP context awareness."""
 83:     from mcp__syntropy import syntropy_serena_read_file
 84:     
 85:     # Read Python module
 86:     result = syntropy_serena_read_file(
 87:         relative_path="ce/core.py"
 88:     )
 89:     # Returns: Full file content with LSP metadata
 90: 
 91: 
 92: # ‚úÖ PATTERN 6: Write Memory (Context Storage)
 93: # Use when: Store important patterns/context for future reference
 94: def example_write_memory():
 95:     """Store context in Serena memory for persistent reference."""
 96:     from mcp__syntropy import syntropy_serena_write_memory
 97:     
 98:     # Store pattern documentation
 99:     syntropy_serena_write_memory(
100:         memory_type="pattern",
101:         content="""
102:         Error Handling Pattern:
103:         - All functions include try/except
104:         - Exceptions include üîß Troubleshooting guidance
105:         - No silent failures or fallbacks
106:         - File: examples/patterns/error-handling.py
107:         """
108:     )
109:     
110:     # Store architecture notes
111:     syntropy_serena_write_memory(
112:         memory_type="architecture",
113:         content="""
114:         Module Structure:
115:         - ce/core.py: File/git/shell operations
116:         - ce/validate.py: 3-level validation gates
117:         - ce/context.py: Context management
118:         - ce/execute.py: PRP execution engine
119:         """
120:     )
121: 
122: 
123: # ‚úÖ PATTERN 7: Activate Project Context
124: # Use when: Initialize Serena for project-specific operations
125: def example_activate_project():
126:     """Set active project for Serena operations."""
127:     from mcp__syntropy import syntropy_serena_activate_project
128:     
129:     # Activate project context
130:     syntropy_serena_activate_project(
131:         project="/Users/bprzybysz/nc-src/ctx-eng-plus"
132:     )
133:     # Enables: Symbol resolution, pattern search, code navigation
134: 
135: 
136: # üìä PERFORMANCE CHARACTERISTICS
137: # - First call: ~1-2 seconds (server spawn)
138: # - Subsequent calls: <50ms (connection reused)
139: # - Find symbol: O(1) - direct LSP lookup
140: # - Search pattern: O(n) - scans matching files
141: # - Find references: O(n) - builds dependency graph
142: 
143: 
144: # üéØ WORKFLOW EXAMPLE: Refactoring Function
145: def refactoring_workflow():
146:     """Typical workflow: Rename function with impact analysis."""
147:     from mcp__syntropy import (
148:         syntropy_serena_find_symbol,
149:         syntropy_serena_find_referencing_symbols,
150:         syntropy_serena_search_for_pattern
151:     )
152:     
153:     # Step 1: Find function definition
154:     func = syntropy_serena_find_symbol("old_function_name", include_body=True)
155:     
156:     # Step 2: Find all references
157:     refs = syntropy_serena_find_referencing_symbols("old_function_name")
158:     print(f"Found {len(refs)} call sites")
159:     
160:     # Step 3: Search for pattern variations
161:     patterns = syntropy_serena_search_for_pattern(
162:         pattern=r"old_function_name\s*\(",
163:         file_glob="**/*.py"
164:     )
165:     
166:     # Step 4: Rename with confidence (know all locations)
167:     # Now safe to refactor
168: 
169: 
170: # üîß TROUBLESHOOTING
171: def troubleshooting():
172:     """Common issues and solutions."""
173:     # Issue: find_symbol returns empty
174:     # Solution: Use exact symbol name (case-sensitive)
175:     # Verify with get_symbols_overview first
176:     
177:     # Issue: search_for_pattern returns no results
178:     # Solution: Test regex with: mcp__syntropy__serena__search_for_pattern
179:     # Use file_glob parameter to limit scope
180:     
181:     # Issue: References incomplete or missing
182:     # Solution: Serena LSP indexing may need refresh
183:     # Re-activate project: syntropy_serena_activate_project(...)
</file>

<file path="tools/ce/executors/__init__.py">
 1: """CI/CD pipeline executors package.
 2: 
 3: Provides platform-specific executors for rendering abstract pipelines.
 4: """
 5: 
 6: from .base import PipelineExecutor, BaseExecutor
 7: from .github_actions import GitHubActionsExecutor
 8: from .mock import MockExecutor
 9: 
10: __all__ = [
11:     "PipelineExecutor",
12:     "BaseExecutor",
13:     "GitHubActionsExecutor",
14:     "MockExecutor",
15: ]
</file>

<file path="tools/ce/executors/base.py">
 1: """Base executor interface and utilities.
 2: 
 3: Provides protocol for platform-specific executors and shared utilities.
 4: """
 5: 
 6: from typing import Protocol, Dict, Any
 7: import yaml
 8: 
 9: 
10: class PipelineExecutor(Protocol):
11:     """Interface for platform-specific pipeline executors.
12: 
13:     Executors render abstract pipeline definitions to platform-specific formats.
14:     Each platform (GitHub Actions, GitLab CI, Jenkins) has its own executor.
15: 
16:     Example:
17:         executor = GitHubActionsExecutor()
18:         yaml_output = executor.render(abstract_pipeline)
19:         Path(".github/workflows/ci.yml").write_text(yaml_output)
20:     """
21: 
22:     def render(self, pipeline: Dict[str, Any]) -> str:
23:         """Render abstract pipeline to platform-specific format.
24: 
25:         Args:
26:             pipeline: Abstract pipeline definition dict
27: 
28:         Returns:
29:             Platform-specific YAML/JSON string
30: 
31:         Raises:
32:             RuntimeError: If rendering fails
33: 
34:         Note: Output must be valid for target platform (validated before return).
35:         """
36:         ...
37: 
38:     def validate_output(self, output: str) -> Dict[str, Any]:
39:         """Validate rendered output for platform compatibility.
40: 
41:         Args:
42:             output: Rendered pipeline string
43: 
44:         Returns:
45:             Dict with: success (bool), errors (List[str])
46: 
47:         Note: Platform-specific validation (e.g., GitHub Actions schema).
48:         """
49:         ...
50: 
51:     def get_platform_name(self) -> str:
52:         """Return platform name (e.g., 'github-actions', 'gitlab-ci').
53: 
54:         Returns:
55:             Platform identifier string
56:         """
57:         ...
58: 
59: 
60: class BaseExecutor:
61:     """Base class for executors (optional, for code reuse).
62: 
63:     Provides common functionality like YAML formatting, error handling.
64:     """
65: 
66:     def format_yaml(self, data: Dict[str, Any]) -> str:
67:         """Format dict as YAML with consistent style.
68: 
69:         Args:
70:             data: Data to format
71: 
72:         Returns:
73:             Formatted YAML string
74:         """
75:         return yaml.dump(data, default_flow_style=False, sort_keys=False)
</file>

<file path="tools/ce/executors/github_actions.py">
  1: """GitHub Actions executor for rendering abstract pipelines.
  2: 
  3: Renders abstract pipeline definition to GitHub Actions workflow YAML.
  4: """
  5: 
  6: from typing import Dict, Any
  7: from .base import BaseExecutor
  8: import yaml
  9: 
 10: 
 11: class GitHubActionsExecutor(BaseExecutor):
 12:     """GitHub Actions executor for rendering abstract pipelines.
 13: 
 14:     Renders abstract pipeline definition to GitHub Actions workflow YAML.
 15: 
 16:     Example:
 17:         executor = GitHubActionsExecutor()
 18:         pipeline = load_abstract_pipeline("ci/abstract/validation.yml")
 19:         workflow = executor.render(pipeline)
 20:         Path(".github/workflows/validation.yml").write_text(workflow)
 21:     """
 22: 
 23:     def render(self, pipeline: Dict[str, Any]) -> str:
 24:         """Render abstract pipeline to GitHub Actions workflow YAML.
 25: 
 26:         Args:
 27:             pipeline: Abstract pipeline definition
 28: 
 29:         Returns:
 30:             GitHub Actions workflow YAML string
 31: 
 32:         Raises:
 33:             RuntimeError: If rendering fails
 34: 
 35:         Mapping:
 36:             - stages ‚Üí jobs
 37:             - nodes ‚Üí steps
 38:             - parallel ‚Üí jobs run in parallel (no needs dependency)
 39:             - depends_on ‚Üí needs: [job-name]
 40:         """
 41:         workflow = {
 42:             "name": pipeline["name"],
 43:             "on": ["push", "pull_request"],
 44:             "jobs": {}
 45:         }
 46: 
 47:         for stage in pipeline["stages"]:
 48:             job_name = self._sanitize_job_name(stage["name"])
 49: 
 50:             job = {
 51:                 "runs-on": "ubuntu-latest",
 52:                 "steps": []
 53:             }
 54: 
 55:             # Add checkout step (required for all jobs)
 56:             job["steps"].append({
 57:                 "name": "Checkout code",
 58:                 "uses": "actions/checkout@v4"
 59:             })
 60: 
 61:             # Convert nodes to steps
 62:             for node in stage["nodes"]:
 63:                 step = {
 64:                     "name": node["name"],
 65:                     "run": node["command"]
 66:                 }
 67: 
 68:                 # Add timeout if specified
 69:                 if "timeout" in node:
 70:                     step["timeout-minutes"] = node["timeout"] // 60
 71: 
 72:                 job["steps"].append(step)
 73: 
 74:             # Add dependencies (depends_on ‚Üí needs)
 75:             if "depends_on" in stage:
 76:                 job["needs"] = [
 77:                     self._sanitize_job_name(dep)
 78:                     for dep in stage["depends_on"]
 79:                 ]
 80: 
 81:             workflow["jobs"][job_name] = job
 82: 
 83:         return self.format_yaml(workflow)
 84: 
 85:     def validate_output(self, output: str) -> Dict[str, Any]:
 86:         """Validate GitHub Actions workflow YAML.
 87: 
 88:         Args:
 89:             output: Rendered workflow YAML
 90: 
 91:         Returns:
 92:             Dict with: success (bool), errors (List[str])
 93: 
 94:         Note: Basic validation - parse YAML and check required fields.
 95:         """
 96:         errors = []
 97: 
 98:         try:
 99:             workflow = yaml.safe_load(output)
100:         except yaml.YAMLError as e:
101:             errors.append(f"Invalid YAML: {e}")
102:             return {"success": False, "errors": errors}
103: 
104:         # Validate required fields
105:         if "name" not in workflow:
106:             errors.append("Missing 'name' field in workflow")
107:         # Note: YAML parses "on:" as True (boolean), so check for both
108:         if "on" not in workflow and True not in workflow:
109:             errors.append("Missing 'on' (trigger) field in workflow")
110:         if "jobs" not in workflow or not workflow["jobs"]:
111:             errors.append("Missing or empty 'jobs' field in workflow")
112: 
113:         return {
114:             "success": len(errors) == 0,
115:             "errors": errors
116:         }
117: 
118:     def get_platform_name(self) -> str:
119:         """Return 'github-actions'."""
120:         return "github-actions"
121: 
122:     def _sanitize_job_name(self, name: str) -> str:
123:         """Sanitize stage name for GitHub Actions job name.
124: 
125:         Args:
126:             name: Stage name
127: 
128:         Returns:
129:             Sanitized job name (lowercase, hyphens)
130: 
131:         Example:
132:             "Unit Tests" ‚Üí "unit-tests"
133:         """
134:         return name.lower().replace(" ", "-").replace("_", "-")
</file>

<file path="tools/ce/executors/mock.py">
 1: """Mock executor for testing pipeline structure.
 2: 
 3: Validates structure but doesn't render real platform output.
 4: Used in tests to verify pipeline definition correctness.
 5: """
 6: 
 7: from typing import Dict, Any
 8: from .base import BaseExecutor
 9: 
10: 
11: class MockExecutor(BaseExecutor):
12:     """Mock executor for testing pipeline structure.
13: 
14:     Validates structure but doesn't render real platform output.
15:     Used in tests to verify pipeline definition correctness.
16: 
17:     Example:
18:         executor = MockExecutor()
19:         pipeline = load_abstract_pipeline("test.yml")
20:         result = executor.render(pipeline)  # Returns mock success
21:     """
22: 
23:     def __init__(self, should_fail: bool = False):
24:         """Initialize mock executor.
25: 
26:         Args:
27:             should_fail: If True, render() raises error (for testing failure cases)
28:         """
29:         self.should_fail = should_fail
30:         self.render_calls = []
31: 
32:     def render(self, pipeline: Dict[str, Any]) -> str:
33:         """Mock render - validates structure and returns mock output.
34: 
35:         Args:
36:             pipeline: Abstract pipeline definition
37: 
38:         Returns:
39:             Mock YAML string
40: 
41:         Raises:
42:             RuntimeError: If should_fail=True
43:         """
44:         self.render_calls.append(pipeline)
45: 
46:         if self.should_fail:
47:             raise RuntimeError(
48:                 "Mock executor configured to fail\n"
49:                 "üîß Troubleshooting: Set should_fail=False for success"
50:             )
51: 
52:         # Return mock output
53:         return f"# Mock pipeline: {pipeline['name']}\n# Stages: {len(pipeline['stages'])}"
54: 
55:     def validate_output(self, output: str) -> Dict[str, Any]:
56:         """Mock validation - always succeeds."""
57:         return {"success": True, "errors": []}
58: 
59:     def get_platform_name(self) -> str:
60:         """Return 'mock'."""
61:         return "mock"
</file>

<file path="tools/ce/testing/__init__.py">
 1: """Testing framework for Context Engineering pipelines.
 2: 
 3: Provides strategy pattern for composable testing with pluggable mock strategies.
 4: Enables unit/integration/E2E testing patterns with observable mocking.
 5: """
 6: 
 7: __version__ = "0.1.0"
 8: 
 9: from .strategy import NodeStrategy, BaseRealStrategy, BaseMockStrategy
10: from .mocks import MockSerenaStrategy, MockContext7Strategy, MockLLMStrategy
11: from .builder import Pipeline, PipelineBuilder
12: from .real_strategies import RealParserStrategy, RealCommandStrategy
13: 
14: __all__ = [
15:     "NodeStrategy",
16:     "BaseRealStrategy",
17:     "BaseMockStrategy",
18:     "MockSerenaStrategy",
19:     "MockContext7Strategy",
20:     "MockLLMStrategy",
21:     "Pipeline",
22:     "PipelineBuilder",
23:     "RealParserStrategy",
24:     "RealCommandStrategy",
25: ]
</file>

<file path="tools/ce/testing/builder.py">
  1: """Pipeline builder for creating testable pipelines with strategy pattern.
  2: 
  3: Provides Pipeline class for execution and PipelineBuilder for construction.
  4: Supports topological sorting for DAG execution and observable mocking.
  5: """
  6: 
  7: from typing import Dict, Any, List, Tuple
  8: from .strategy import NodeStrategy
  9: 
 10: 
 11: class Pipeline:
 12:     """Executable pipeline with nodes and edges.
 13: 
 14:     Pipeline executes nodes in topological order based on dependencies (edges).
 15:     Supports linear pipelines and DAGs with cycle detection.
 16: 
 17:     Example:
 18:         nodes = {"parse": ParserStrategy(), "research": MockSerenaStrategy()}
 19:         edges = [("parse", "research")]
 20:         pipeline = Pipeline(nodes, edges)
 21:         result = pipeline.execute({"prp_path": "test.md"})
 22:     """
 23: 
 24:     def __init__(self, nodes: Dict[str, NodeStrategy], edges: List[Tuple[str, str]]):
 25:         """Initialize pipeline.
 26: 
 27:         Args:
 28:             nodes: {node_name: strategy}
 29:             edges: [(from_node, to_node)] defining dependencies
 30:         """
 31:         self.nodes = nodes
 32:         self.edges = edges
 33: 
 34:     def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
 35:         """Execute pipeline from start to finish.
 36: 
 37:         Args:
 38:             input_data: Initial input to first node
 39: 
 40:         Returns:
 41:             Final output from last node
 42: 
 43:         Raises:
 44:             RuntimeError: If pipeline has cycles or execution fails
 45: 
 46:         Process:
 47:             1. Topologically sort nodes by edges
 48:             2. Execute nodes in order
 49:             3. Pass output from node to next node
 50:             4. Return final output
 51:         """
 52:         # Simple linear execution (no parallelism in MVP)
 53:         current_data = input_data
 54: 
 55:         for node_name in self._topological_sort():
 56:             strategy = self.nodes[node_name]
 57:             current_data = strategy.execute(current_data)
 58: 
 59:         return current_data
 60: 
 61:     def _topological_sort(self) -> List[str]:
 62:         """Sort nodes by dependencies (edges).
 63: 
 64:         Returns:
 65:             Ordered list of node names
 66: 
 67:         Raises:
 68:             RuntimeError: If pipeline has circular dependencies
 69:         """
 70:         # Simple implementation: assume linear pipeline for MVP
 71:         # Future: proper topological sort for DAGs
 72:         if not self.edges:
 73:             return list(self.nodes.keys())
 74: 
 75:         # Build adjacency list
 76:         in_degree = {node: 0 for node in self.nodes}
 77:         adj = {node: [] for node in self.nodes}
 78: 
 79:         for from_node, to_node in self.edges:
 80:             adj[from_node].append(to_node)
 81:             in_degree[to_node] += 1
 82: 
 83:         # Kahn's algorithm
 84:         queue = [node for node in self.nodes if in_degree[node] == 0]
 85:         result = []
 86: 
 87:         while queue:
 88:             node = queue.pop(0)
 89:             result.append(node)
 90: 
 91:             for neighbor in adj[node]:
 92:                 in_degree[neighbor] -= 1
 93:                 if in_degree[neighbor] == 0:
 94:                     queue.append(neighbor)
 95: 
 96:         if len(result) != len(self.nodes):
 97:             # Identify which nodes are in the cycle
 98:             nodes_in_cycle = set(self.nodes.keys()) - set(result)
 99:             raise RuntimeError(
100:                 f"Pipeline has circular dependencies involving: {', '.join(sorted(nodes_in_cycle))}\n"
101:                 f"üîß Troubleshooting: Check edges for cycles among these nodes:\n"
102:                 f"   Edges: {self.edges}"
103:             )
104: 
105:         return result
106: 
107: 
108: class PipelineBuilder:
109:     """Builder for creating testable pipelines with strategy pattern.
110: 
111:     Fluent API for building pipelines with method chaining.
112:     Supports observable mocking (üé≠ indicator for mocked nodes).
113: 
114:     Example:
115:         pipeline = (
116:             PipelineBuilder(mode="e2e")
117:             .add_node("parse", RealParserStrategy())
118:             .add_node("research", MockSerenaStrategy())
119:             .add_edge("parse", "research")
120:             .build()
121:         )
122:     """
123: 
124:     def __init__(self, mode: str = "e2e"):
125:         """Initialize builder.
126: 
127:         Args:
128:             mode: Test mode (unit/integration/e2e)
129:                 - unit: Single node, all deps mocked
130:                 - integration: Real nodes, some deps mocked
131:                 - e2e: All external deps mocked, internal real
132:         """
133:         self.mode = mode
134:         self.nodes: Dict[str, NodeStrategy] = {}
135:         self.edges: List[Tuple[str, str]] = []
136: 
137:     def add_node(self, name: str, strategy: NodeStrategy) -> "PipelineBuilder":
138:         """Add node with execution strategy.
139: 
140:         Args:
141:             name: Node identifier
142:             strategy: Execution strategy (real or mock)
143: 
144:         Returns:
145:             Self for chaining
146:         """
147:         self.nodes[name] = strategy
148:         return self
149: 
150:     def add_edge(self, from_node: str, to_node: str) -> "PipelineBuilder":
151:         """Add dependency edge.
152: 
153:         Args:
154:             from_node: Source node name
155:             to_node: Destination node name
156: 
157:         Returns:
158:             Self for chaining
159: 
160:         Raises:
161:             ValueError: If from_node or to_node not in pipeline
162:         """
163:         if from_node not in self.nodes:
164:             raise ValueError(
165:                 f"from_node '{from_node}' not in pipeline\n"
166:                 f"üîß Troubleshooting: Add node before creating edge"
167:             )
168:         if to_node not in self.nodes:
169:             raise ValueError(
170:                 f"to_node '{to_node}' not in pipeline\n"
171:                 f"üîß Troubleshooting: Add node before creating edge"
172:             )
173: 
174:         self.edges.append((from_node, to_node))
175:         return self
176: 
177:     def build(self) -> Pipeline:
178:         """Build pipeline and log mocked nodes.
179: 
180:         Returns:
181:             Executable Pipeline instance
182:         """
183:         # Identify mocked nodes for observable mocking
184:         mocked = [
185:             name for name, strategy in self.nodes.items()
186:             if strategy.is_mocked()
187:         ]
188: 
189:         if mocked:
190:             print(f"üé≠ MOCKED NODES: {', '.join(mocked)}")
191: 
192:         return Pipeline(self.nodes, self.edges)
</file>

<file path="tools/ce/testing/mocks.py">
  1: """Mock strategy implementations for common external dependencies.
  2: 
  3: Provides pre-built mock strategies with canned data for typical use cases:
  4: - MockSerenaStrategy: Codebase search operations
  5: - MockContext7Strategy: Documentation fetching
  6: - MockLLMStrategy: Text generation with templates
  7: """
  8: 
  9: from typing import List, Dict, Any
 10: from .strategy import BaseMockStrategy
 11: 
 12: 
 13: class MockSerenaStrategy(BaseMockStrategy):
 14:     """Mock Serena MCP for codebase search operations.
 15: 
 16:     Returns canned search results instead of real MCP calls.
 17:     Useful for testing pipelines without hitting Serena MCP server.
 18: 
 19:     Example:
 20:         strategy = MockSerenaStrategy(canned_results=[
 21:             {"file": "test.py", "match": "def test(): pass"}
 22:         ])
 23:         result = strategy.execute({"pattern": "def test"})
 24:         # Returns: {"success": True, "results": [...], "method": "mock_serena"}
 25:     """
 26: 
 27:     def __init__(self, canned_results: List[Dict[str, Any]]):
 28:         """Initialize with canned data.
 29: 
 30:         Args:
 31:             canned_results: List of search results to return
 32:                 Format: [{"file": "path/to/file.py", "match": "code snippet"}]
 33:         """
 34:         self.results = canned_results
 35: 
 36:     def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
 37:         """Return canned search results.
 38: 
 39:         Args:
 40:             input_data: Search request (pattern, path, etc.)
 41:                 Format: {"pattern": "regex", "path": "src/"}
 42: 
 43:         Returns:
 44:             Mock search results
 45:                 Format: {"success": True, "results": [...], "method": "mock_serena"}
 46:         """
 47:         return {
 48:             "success": True,
 49:             "results": self.results,
 50:             "method": "mock_serena"
 51:         }
 52: 
 53: 
 54: class MockContext7Strategy(BaseMockStrategy):
 55:     """Mock Context7 MCP for documentation fetching.
 56: 
 57:     Returns cached documentation instead of API calls.
 58:     Useful for testing without hitting Context7 API.
 59: 
 60:     Example:
 61:         strategy = MockContext7Strategy(cached_docs="pytest fixtures...")
 62:         result = strategy.execute({"library": "pytest", "topic": "fixtures"})
 63:         # Returns: {"success": True, "docs": "...", "method": "mock_context7"}
 64:     """
 65: 
 66:     def __init__(self, cached_docs: str):
 67:         """Initialize with cached documentation.
 68: 
 69:         Args:
 70:             cached_docs: Documentation text to return
 71:         """
 72:         self.docs = cached_docs
 73: 
 74:     def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
 75:         """Return cached documentation.
 76: 
 77:         Args:
 78:             input_data: Documentation request
 79:                 Format: {"library": "pytest", "topic": "fixtures"}
 80: 
 81:         Returns:
 82:             Mock documentation
 83:                 Format: {"success": True, "docs": "...", "method": "mock_context7"}
 84:         """
 85:         return {
 86:             "success": True,
 87:             "docs": self.docs,
 88:             "method": "mock_context7"
 89:         }
 90: 
 91: 
 92: class MockLLMStrategy(BaseMockStrategy):
 93:     """Mock LLM for text generation (PRP generation, code synthesis).
 94: 
 95:     Returns template-based responses instead of LLM API calls.
 96:     Useful for testing without consuming tokens.
 97: 
 98:     Example:
 99:         strategy = MockLLMStrategy(template="# {title}\n\n{content}")
100:         result = strategy.execute({
101:             "prompt": "Generate PRP",
102:             "context": {"title": "PRP-1", "content": "Test feature"}
103:         })
104:         # Returns: {"success": True, "response": "# PRP-1\n\nTest feature", ...}
105:     """
106: 
107:     def __init__(self, template: str):
108:         """Initialize with response template.
109: 
110:         Args:
111:             template: Template string with {placeholders}
112:                 Example: "# {title}\n\n{content}"
113:         """
114:         self.template = template
115: 
116:     def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
117:         """Generate response from template.
118: 
119:         Args:
120:             input_data: Generation request
121:                 Format: {
122:                     "prompt": "Generate PRP",
123:                     "context": {"title": "PRP-1", "content": "..."}
124:                 }
125: 
126:         Returns:
127:             Mock LLM response
128:                 Format: {
129:                     "success": True,
130:                     "response": "...",
131:                     "method": "mock_llm",
132:                     "tokens_saved": 5000
133:                 }
134: 
135:         Raises:
136:             RuntimeError: If template requires missing context keys
137:         """
138:         context = input_data.get("context", {})
139: 
140:         try:
141:             response = self.template.format(**context)
142:         except KeyError as e:
143:             # Provide helpful error for missing template variables
144:             missing_key = str(e).strip("'")
145:             raise RuntimeError(
146:                 f"Template requires key '{missing_key}' but context is missing it\n"
147:                 f"Template: {self.template[:100]}{'...' if len(self.template) > 100 else ''}\n"
148:                 f"Context keys: {list(context.keys())}\n"
149:                 f"üîß Troubleshooting: Provide '{missing_key}' in input_data['context']"
150:             )
151: 
152:         return {
153:             "success": True,
154:             "response": response,
155:             "method": "mock_llm",
156:             "tokens_saved": 5000  # Estimated tokens saved vs real LLM
157:         }
</file>

<file path="tools/ce/testing/real_strategies.py">
  1: """Real strategy implementations for existing operations.
  2: 
  3: Wraps existing CE functions in strategy interface for composable testing.
  4: Real strategies call actual functionality (parse files, run commands, etc.).
  5: """
  6: 
  7: from typing import Dict, Any
  8: from .strategy import BaseRealStrategy
  9: from ..core import run_cmd
 10: from ..execute import parse_blueprint
 11: 
 12: 
 13: class RealParserStrategy(BaseRealStrategy):
 14:     """Real PRP blueprint parser strategy.
 15: 
 16:     Wraps parse_blueprint() from execute.py in strategy interface.
 17:     Parses PRP markdown files into structured phase data.
 18: 
 19:     Example:
 20:         strategy = RealParserStrategy()
 21:         result = strategy.execute({"prp_path": "PRPs/PRP-1.md"})
 22:         # Returns: {"success": True, "phases": [...]}
 23:     """
 24: 
 25:     def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
 26:         """Parse PRP file into phases.
 27: 
 28:         Args:
 29:             input_data: {"prp_path": "path/to/prp.md"}
 30: 
 31:         Returns:
 32:             {"success": True, "phases": [...]} on success
 33:             {"success": False, "error": "..."} on failure
 34: 
 35:         Raises:
 36:             RuntimeError: If prp_path not provided in input_data
 37:         """
 38:         prp_path = input_data.get("prp_path")
 39:         if not prp_path:
 40:             raise RuntimeError(
 41:                 "Missing 'prp_path' in input_data\n"
 42:                 "üîß Troubleshooting: Provide prp_path to parser strategy"
 43:             )
 44: 
 45:         try:
 46:             phases = parse_blueprint(prp_path)
 47:             return {
 48:                 "success": True,
 49:                 "phases": phases
 50:             }
 51:         except Exception as e:
 52:             return {
 53:                 "success": False,
 54:                 "error": str(e),
 55:                 "error_type": "parse_error",
 56:                 "troubleshooting": "Check PRP file format and implementation blueprint section"
 57:             }
 58: 
 59: 
 60: class RealCommandStrategy(BaseRealStrategy):
 61:     """Real shell command execution strategy.
 62: 
 63:     Wraps run_cmd() from core.py in strategy interface.
 64:     Executes shell commands with timeout and error handling.
 65: 
 66:     Example:
 67:         strategy = RealCommandStrategy()
 68:         result = strategy.execute({"cmd": "pytest tests/ -v"})
 69:         # Returns: {"success": True, "stdout": "...", "stderr": "...", ...}
 70:     """
 71: 
 72:     def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
 73:         """Execute shell command.
 74: 
 75:         Args:
 76:             input_data: {
 77:                 "cmd": "shell command to execute",
 78:                 "timeout": 60,  # optional
 79:                 "cwd": "/path/to/dir"  # optional
 80:             }
 81: 
 82:         Returns:
 83:             {"success": bool, "stdout": "...", "stderr": "...", "exit_code": int, "duration": float}
 84:             On error: {"success": False, "error": "...", "error_type": "...", "troubleshooting": "..."}
 85: 
 86:         Raises:
 87:             RuntimeError: If cmd not provided in input_data
 88: 
 89:         Note: Error responses follow standard format with troubleshooting guidance.
 90:         """
 91:         cmd = input_data.get("cmd")
 92:         if not cmd:
 93:             raise RuntimeError(
 94:                 "Missing 'cmd' in input_data\n"
 95:                 "üîß Troubleshooting: Provide cmd to execute"
 96:             )
 97: 
 98:         timeout = input_data.get("timeout", 60)
 99:         cwd = input_data.get("cwd")
100: 
101:         try:
102:             return run_cmd(cmd, cwd=cwd, timeout=timeout)
103:         except TimeoutError as e:
104:             return {
105:                 "success": False,
106:                 "error": str(e),
107:                 "error_type": "timeout_error",
108:                 "troubleshooting": "Increase timeout or check for hanging process"
109:             }
110:         except Exception as e:
111:             return {
112:                 "success": False,
113:                 "error": str(e),
114:                 "error_type": "runtime_error",
115:                 "troubleshooting": "Check command syntax and permissions"
116:             }
</file>

<file path="tools/ce/testing/strategy.py">
 1: """Strategy interface for pipeline node execution.
 2: 
 3: Defines Protocol-based interface for interchangeable node implementations.
 4: Real strategies call actual external APIs/services.
 5: Mock strategies return canned data for testing.
 6: """
 7: 
 8: from typing import Protocol, Any, Dict
 9: 
10: 
11: class NodeStrategy(Protocol):
12:     """Interface for pipeline node execution strategies.
13: 
14:     Strategies are interchangeable implementations of node logic.
15:     Real strategies call actual external APIs/services.
16:     Mock strategies return canned data for testing.
17: 
18:     Example:
19:         # Real strategy
20:         class RealParserStrategy(BaseRealStrategy):
21:             def execute(self, input_data):
22:                 return parse_blueprint(input_data["prp_path"])
23: 
24:         # Mock strategy
25:         class MockParserStrategy(BaseMockStrategy):
26:             def execute(self, input_data):
27:                 return {"phases": [...]}  # Canned data
28:     """
29: 
30:     def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
31:         """Execute node logic with input data.
32: 
33:         Args:
34:             input_data: Node input (from previous node or pipeline start)
35: 
36:         Returns:
37:             Node output (passed to next node or returned as result)
38: 
39:         Raises:
40:             RuntimeError: If node execution fails
41:         """
42:         ...
43: 
44:     def is_mocked(self) -> bool:
45:         """Return True if this is a mock strategy.
46: 
47:         Used for observable mocking (üé≠ indicator in logs).
48: 
49:         Returns:
50:             True if mock strategy, False if real strategy
51:         """
52:         ...
53: 
54: 
55: class BaseRealStrategy:
56:     """Base class for real strategies (optional, for code reuse).
57: 
58:     Real strategies execute actual logic (call external APIs, run commands, etc.).
59:     Use this base class to avoid implementing is_mocked() in every strategy.
60: 
61:     Example:
62:         class RealCommandStrategy(BaseRealStrategy):
63:             def execute(self, input_data):
64:                 cmd = input_data["cmd"]
65:                 return run_cmd(cmd)
66:     """
67: 
68:     def is_mocked(self) -> bool:
69:         """Return False (this is a real strategy)."""
70:         return False
71: 
72: 
73: class BaseMockStrategy:
74:     """Base class for mock strategies (optional, for code reuse).
75: 
76:     Mock strategies return canned data instead of calling external services.
77:     Use this base class to avoid implementing is_mocked() in every strategy.
78: 
79:     Example:
80:         class MockSerenaStrategy(BaseMockStrategy):
81:             def __init__(self, canned_results):
82:                 self.results = canned_results
83: 
84:             def execute(self, input_data):
85:                 return {"success": True, "results": self.results}
86:     """
87: 
88:     def is_mocked(self) -> bool:
89:         """Return True (this is a mock strategy)."""
90:         return True
</file>

<file path="tools/ce/vacuum_strategies/__init__.py">
 1: """Vacuum strategies for project cleanup."""
 2: 
 3: from .base import BaseStrategy, CleanupCandidate
 4: from .temp_files import TempFileStrategy
 5: from .backup_files import BackupFileStrategy
 6: from .obsolete_docs import ObsoleteDocStrategy
 7: from .unreferenced_code import UnreferencedCodeStrategy
 8: from .orphan_tests import OrphanTestStrategy
 9: from .commented_code import CommentedCodeStrategy
10: 
11: __all__ = [
12:     "BaseStrategy",
13:     "CleanupCandidate",
14:     "TempFileStrategy",
15:     "BackupFileStrategy",
16:     "ObsoleteDocStrategy",
17:     "UnreferencedCodeStrategy",
18:     "OrphanTestStrategy",
19:     "CommentedCodeStrategy",
20: ]
</file>

<file path="tools/ce/vacuum_strategies/orphan_tests.py">
 1: """Strategy for finding orphaned test files."""
 2: 
 3: from pathlib import Path
 4: from typing import List
 5: 
 6: from .base import BaseStrategy, CleanupCandidate
 7: 
 8: 
 9: class OrphanTestStrategy(BaseStrategy):
10:     """Find test files whose corresponding module no longer exists."""
11: 
12:     def find_candidates(self) -> List[CleanupCandidate]:
13:         """Find orphaned test files.
14: 
15:         Returns:
16:             List of CleanupCandidate objects with MEDIUM confidence (60%)
17:         """
18:         candidates = []
19: 
20:         # Find all test files
21:         tests_dir = self.project_root / "tests"
22:         if not tests_dir.exists():
23:             return candidates
24: 
25:         for test_file in tests_dir.glob("**/test_*.py"):
26:             if not test_file.exists() or test_file.is_dir():
27:                 continue
28: 
29:             # Skip if protected
30:             if self.is_protected(test_file):
31:                 continue
32: 
33:             # Extract module name from test_foo.py -> foo
34:             module_name = test_file.stem.replace("test_", "")
35: 
36:             # Look for corresponding module in ce/
37:             module_candidates = [
38:                 self.project_root / "tools" / "ce" / f"{module_name}.py",
39:                 self.project_root / "ce" / f"{module_name}.py",
40:             ]
41: 
42:             # Check if any corresponding module exists
43:             module_exists = any(m.exists() for m in module_candidates)
44: 
45:             if not module_exists:
46:                 # Recently active files might be integration tests
47:                 confidence = 60
48:                 if self.is_recently_active(test_file, days=30):
49:                     confidence = 40
50: 
51:                 candidate = CleanupCandidate(
52:                     path=test_file,
53:                     reason=f"Orphaned test: no module '{module_name}.py' found",
54:                     confidence=confidence,  # MEDIUM confidence
55:                     size_bytes=self.get_file_size(test_file),
56:                     last_modified=self.get_last_modified(test_file),
57:                     git_history=self.get_git_history(test_file),
58:                 )
59:                 candidates.append(candidate)
60: 
61:         return candidates
</file>

<file path="tools/ce/__init__.py">
1: """Context Engineering CLI Tools.
2: 
3: Minimal, efficient tooling for Context Engineering framework operations.
4: """
5: 
6: __version__ = "0.1.0"
</file>

<file path="tools/ce/blueprint_parser.py">
  1: """PRP blueprint parsing functions.
  2: 
  3: Extracts implementation phases from PRP IMPLEMENTATION BLUEPRINT markdown sections.
  4: Parses structured blueprint data into executable phase dictionaries.
  5: """
  6: 
  7: import re
  8: from typing import Dict, Any, List, Optional
  9: from pathlib import Path
 10: 
 11: from .exceptions import BlueprintParseError
 12: 
 13: 
 14: def parse_blueprint(prp_path: str) -> List[Dict[str, Any]]:
 15:     """Parse PRP IMPLEMENTATION BLUEPRINT into executable phases.
 16: 
 17:     Args:
 18:         prp_path: Path to PRP markdown file
 19: 
 20:     Returns:
 21:         [
 22:             {
 23:                 "phase_number": 1,
 24:                 "phase_name": "Core Logic Implementation",
 25:                 "goal": "Implement main authentication flow",
 26:                 "approach": "Class-based with async methods",
 27:                 "hours": 4.0,
 28:                 "files_to_modify": [
 29:                     {"path": "src/auth.py", "description": "Add auth logic"}
 30:                 ],
 31:                 "files_to_create": [
 32:                     {"path": "src/models/user.py", "description": "User model"}
 33:                 ],
 34:                 "functions": [
 35:                     {
 36:                         "signature": "def authenticate(username: str) -> User:",
 37:                         "docstring": "Authenticate user with credentials",
 38:                         "full_code": "<complete function body if provided>"
 39:                     }
 40:                 ],
 41:                 "validation_command": "pytest tests/test_auth.py -v",
 42:                 "checkpoint_command": "git add src/ && git commit -m 'feat: auth'"
 43:             },
 44:             # ... more phases
 45:         ]
 46: 
 47:     Raises:
 48:         FileNotFoundError: If PRP file doesn't exist
 49:         BlueprintParseError: If blueprint section missing or malformed
 50: 
 51:     Process:
 52:         1. Read PRP file
 53:         2. Extract ## üîß Implementation Blueprint section
 54:         3. Split by ### Phase N: pattern
 55:         4. For each phase:
 56:            a. Extract phase number, name, hours from heading
 57:            b. Extract **Goal**: text
 58:            c. Extract **Approach**: text
 59:            d. Parse **Files to Modify**: list
 60:            e. Parse **Files to Create**: list
 61:            f. Extract **Key Functions**: code blocks
 62:            g. Extract **Validation Command**: command
 63:            h. Extract **Checkpoint**: git command
 64:         5. Validate required fields present
 65:     """
 66:     # Check file exists
 67:     path = Path(prp_path)
 68:     if not path.exists():
 69:         raise FileNotFoundError(
 70:             f"PRP file not found: {prp_path}\n"
 71:             f"üîß Troubleshooting: Verify file path is correct"
 72:         )
 73: 
 74:     # Read file
 75:     content = path.read_text()
 76: 
 77:     # Extract IMPLEMENTATION BLUEPRINT section
 78:     # Note: (?=\n## [^#]) ensures we stop at ## headers (not ###)
 79:     blueprint_match = re.search(
 80:         r"##\s+üîß\s+Implementation\s+Blueprint\s*\n(.*?)(?=\n## [^#]|\Z)",
 81:         content,
 82:         re.DOTALL | re.IGNORECASE
 83:     )
 84: 
 85:     if not blueprint_match:
 86:         raise BlueprintParseError(
 87:             prp_path,
 88:             "Missing '## üîß Implementation Blueprint' section\n"
 89:             "üîß Troubleshooting:\n"
 90:             "   - Ensure PRP file contains Implementation Blueprint section\n"
 91:             "   - Check section header format (must include üîß emoji)\n"
 92:             "   - Reference: examples/system-prps/ for correct format"
 93:         )
 94: 
 95:     blueprint_text = blueprint_match.group(1)
 96: 
 97:     # Split by phase headings: ### Phase N: Name (X hours)
 98:     phase_pattern = r"###\s+Phase\s+(\d+):\s+([^\(]+)\(([^)]+)\)"
 99:     phase_splits = list(re.finditer(phase_pattern, blueprint_text))
100: 
101:     if not phase_splits:
102:         raise BlueprintParseError(
103:             prp_path,
104:             "No phases found (expected '### Phase N: Name (X hours)' format)\n"
105:             "üîß Troubleshooting:\n"
106:             "   - Add phase sections: ### Phase 1: Name (X hours)\n"
107:             "   - Ensure phases are numbered sequentially\n"
108:             "   - Reference: examples/system-prps/example-simple-feature.md"
109:         )
110: 
111:     phases = []
112: 
113:     for i, match in enumerate(phase_splits):
114:         phase_number = int(match.group(1))
115:         phase_name = match.group(2).strip()
116:         hours_str = match.group(3).strip()
117: 
118:         # Parse hours (handle "X hours", "X.Y hours", etc.)
119:         hours_match = re.search(r"(\d+(?:\.\d+)?)", hours_str)
120:         hours = float(hours_match.group(1)) if hours_match else 0.0
121: 
122:         # Extract phase content (from this phase to next phase or end)
123:         start = match.end()
124:         end = phase_splits[i + 1].start() if i + 1 < len(phase_splits) else len(blueprint_text)
125:         phase_text = blueprint_text[start:end]
126: 
127:         # Parse phase content
128:         phase_data = {
129:             "phase_number": phase_number,
130:             "phase_name": phase_name,
131:             "hours": hours,
132:             "goal": extract_field(phase_text, r"\*\*Goal\*\*:\s*(.+?)(?=\n\n|\*\*|$)", prp_path),
133:             "approach": extract_field(phase_text, r"\*\*Approach\*\*:\s*(.+?)(?=\n\n|\*\*|$)", prp_path),
134:             "files_to_modify": parse_file_list(phase_text, "Files to Modify"),
135:             "files_to_create": parse_file_list(phase_text, "Files to Create"),
136:             "functions": extract_function_signatures(phase_text),
137:             "validation_command": extract_field(
138:                 phase_text,
139:                 r"\*\*Validation\s+Command\*\*:\s*`([^`]+)`",
140:                 prp_path,
141:                 required=False
142:             ),
143:             "checkpoint_command": extract_field(
144:                 phase_text,
145:                 r"\*\*Checkpoint\*\*:\s*`([^`]+)`",
146:                 prp_path,
147:                 required=False
148:             )
149:         }
150: 
151:         phases.append(phase_data)
152: 
153:     return phases
154: 
155: 
156: def extract_field(
157:     text: str,
158:     pattern: str,
159:     prp_path: str,
160:     required: bool = True
161: ) -> Optional[str]:
162:     """Extract a field from phase text using regex.
163: 
164:     Args:
165:         text: Phase text to search
166:         pattern: Regex pattern with one capture group
167:         prp_path: PRP path for error messages
168:         required: Whether field is required
169: 
170:     Returns:
171:         Extracted value or None if not found and not required
172: 
173:     Raises:
174:         BlueprintParseError: If required field not found
175:     """
176:     match = re.search(pattern, text, re.DOTALL)
177:     if not match:
178:         if required:
179:             raise BlueprintParseError(
180:                 prp_path,
181:                 f"Required field not found using pattern: {pattern}\n"
182:                 f"üîß Troubleshooting:\n"
183:                 f"   - Check field name spelling (capitalization matters)\n"
184:                 f"   - Verify the field uses ** ** formatting: **Field Name**: value\n"
185:                 f"   - Common patterns: **Goal**, **Approach**, **Validation Command**, **Checkpoint**"
186:             )
187:         return None
188: 
189:     return match.group(1).strip()
190: 
191: 
192: def parse_file_list(section_text: str, marker: str) -> List[Dict[str, str]]:
193:     """Parse **Files to Modify**: or **Files to Create**: section.
194: 
195:     Args:
196:         section_text: Phase section text
197:         marker: "Files to Modify" or "Files to Create"
198: 
199:     Returns:
200:         [
201:             {"path": "src/auth.py", "description": "Add auth logic"},
202:             {"path": "tests/test_auth.py", "description": "Add tests"}
203:         ]
204: 
205:     Pattern:
206:         **Files to Modify**:
207:         - `path/to/file.py` - Description
208:     """
209:     result = []
210: 
211:     # Find the marker section
212:     marker_pattern = rf"\*\*{re.escape(marker)}\*\*:\s*\n((?:- `[^`]+` - [^\n]+\n?)*)"
213:     match = re.search(marker_pattern, section_text, re.MULTILINE)
214: 
215:     if not match:
216:         return []
217: 
218:     list_content = match.group(1)
219: 
220:     # Parse each list item: - `path/to/file.py` - Description
221:     item_pattern = r"- `([^`]+)` - (.+)"
222:     for item_match in re.finditer(item_pattern, list_content):
223:         result.append({
224:             "path": item_match.group(1).strip(),
225:             "description": item_match.group(2).strip()
226:         })
227: 
228:     return result
229: 
230: 
231: def extract_function_signatures(phase_text: str) -> List[Dict[str, str]]:
232:     """Extract function signatures from **Key Functions**: code blocks.
233: 
234:     Args:
235:         phase_text: Phase section text
236: 
237:     Returns:
238:         [
239:             {
240:                 "signature": "def authenticate(username: str) -> User:",
241:                 "docstring": "Authenticate user with credentials",
242:                 "full_code": "<complete function body if provided>"
243:             }
244:         ]
245:     """
246:     result = []
247: 
248:     # Find **Key Functions**: section followed by ```python code block
249:     func_pattern = r"\*\*Key\s+Functions\*\*:.*?```python\s*\n(.*?)```"
250:     matches = re.finditer(func_pattern, phase_text, re.DOTALL | re.IGNORECASE)
251: 
252:     for match in matches:
253:         code_block = match.group(1).strip()
254: 
255:         # Split by function definitions
256:         func_defs = re.split(r'\n(?=def |async def |class )', code_block)
257: 
258:         for func_def in func_defs:
259:             func_def = func_def.strip()
260:             if not func_def:
261:                 continue
262: 
263:             # Extract signature (first line)
264:             lines = func_def.split('\n')
265:             signature = lines[0].strip()
266: 
267:             # Extract docstring if present
268:             docstring = None
269:             docstring_match = re.search(r'"""(.*?)"""', func_def, re.DOTALL)
270:             if docstring_match:
271:                 docstring = docstring_match.group(1).strip()
272: 
273:             result.append({
274:                 "signature": signature,
275:                 "docstring": docstring or "",
276:                 "full_code": func_def
277:             })
278: 
279:     return result
280: 
281: 
282: def extract_phase_metadata(phase_text: str) -> Dict[str, Any]:
283:     """Extract metadata from phase heading.
284: 
285:     Args:
286:         phase_text: Full phase section text
287: 
288:     Returns:
289:         {
290:             "phase_number": 1,
291:             "phase_name": "Core Logic Implementation",
292:             "hours": 4.0
293:         }
294: 
295:     Pattern: ### Phase 1: Core Logic Implementation (4 hours)
296:     """
297:     # This function is superseded by the inline parsing in parse_blueprint()
298:     # Kept for backwards compatibility and as a utility function
299:     pattern = r"###\s+Phase\s+(\d+):\s+([^\(]+)\(([^)]+)\)"
300:     match = re.search(pattern, phase_text)
301: 
302:     if not match:
303:         return {
304:             "phase_number": 0,
305:             "phase_name": "Unknown",
306:             "hours": 0.0
307:         }
308: 
309:     hours_match = re.search(r"(\d+(?:\.\d+)?)", match.group(3))
310:     hours = float(hours_match.group(1)) if hours_match else 0.0
311: 
312:     return {
313:         "phase_number": int(match.group(1)),
314:         "phase_name": match.group(2).strip(),
315:         "hours": hours
316:     }
</file>

<file path="tools/ce/code_analyzer.py">
  1: """Shared code analysis module for pattern detection across languages.
  2: 
  3: This module provides unified code analysis functions used by both pattern_extractor
  4: and drift_analyzer to eliminate duplication and maintain a single source of truth
  5: for pattern detection logic.
  6: """
  7: 
  8: import ast
  9: import re
 10: from typing import Dict, List
 11: 
 12: 
 13: def analyze_code_patterns(code: str, language: str) -> Dict[str, List[str]]:
 14:     """Analyze code and extract semantic patterns.
 15: 
 16:     Args:
 17:         code: Source code string
 18:         language: Programming language (python, typescript, javascript, etc.)
 19: 
 20:     Returns:
 21:         Dict mapping pattern categories to detected patterns:
 22:         {
 23:             "code_structure": ["async/await", "class-based", ...],
 24:             "error_handling": ["try-except", "early-return", ...],
 25:             "naming_conventions": ["snake_case", "camelCase", ...],
 26:             "data_flow": ["props", "state", ...],
 27:             "test_patterns": ["pytest", "jest", ...],
 28:             "import_patterns": ["relative", "absolute"]
 29:         }
 30:     """
 31:     if language.lower() in ("python", "py"):
 32:         return _analyze_python(code)
 33:     elif language.lower() in ("typescript", "ts", "javascript", "js"):
 34:         return _analyze_typescript(code)
 35:     else:
 36:         return _analyze_generic(code)
 37: 
 38: 
 39: def _analyze_python(code: str) -> Dict[str, List[str]]:
 40:     """Analyze Python code using AST for accurate pattern detection.
 41: 
 42:     Falls back to regex-based analysis if AST parsing fails.
 43:     Refactored to reduce nesting depth from 7 to 4 levels.
 44:     """
 45:     from .pattern_detectors import (
 46:         process_class_node,
 47:         process_function_node,
 48:         process_try_node,
 49:         process_if_node,
 50:         process_import_node
 51:     )
 52: 
 53:     patterns = {
 54:         "code_structure": [],
 55:         "error_handling": [],
 56:         "naming_conventions": [],
 57:         "data_flow": [],
 58:         "test_patterns": [],
 59:         "import_patterns": []
 60:     }
 61: 
 62:     try:
 63:         tree = ast.parse(code)
 64:     except SyntaxError:
 65:         # Fallback to regex if AST parsing fails
 66:         return _analyze_generic(code)
 67: 
 68:     # Code structure analysis using extracted pattern detectors
 69:     for node in ast.walk(tree):
 70:         # Async patterns
 71:         if isinstance(node, (ast.AsyncFunctionDef, ast.AsyncFor, ast.AsyncWith, ast.Await)):
 72:             patterns["code_structure"].append("async/await")
 73: 
 74:         # Class-based (delegated to reduce nesting)
 75:         elif isinstance(node, ast.ClassDef):
 76:             process_class_node(node, patterns)
 77: 
 78:         # Function-based (delegated to reduce nesting)
 79:         elif isinstance(node, ast.FunctionDef):
 80:             process_function_node(node, patterns)
 81: 
 82:         # Error handling
 83:         elif isinstance(node, ast.Try):
 84:             process_try_node(node, patterns)
 85: 
 86:         elif isinstance(node, ast.If):
 87:             process_if_node(node, patterns)
 88: 
 89:         # Import patterns
 90:         elif isinstance(node, ast.ImportFrom):
 91:             process_import_node(node, patterns)
 92: 
 93:     return patterns
 94: 
 95: 
 96: def _analyze_typescript(code: str) -> Dict[str, List[str]]:
 97:     """Analyze TypeScript/JavaScript code using regex patterns."""
 98:     patterns = {
 99:         "code_structure": [],
100:         "error_handling": [],
101:         "naming_conventions": [],
102:         "data_flow": [],
103:         "test_patterns": [],
104:         "import_patterns": []
105:     }
106: 
107:     # Code structure
108:     if re.search(r"\basync\s+", code) or re.search(r"\bawait\s+", code):
109:         patterns["code_structure"].append("async/await")
110:     if re.search(r"\.then\(", code):
111:         patterns["code_structure"].append("promises")
112:     if re.search(r"\bclass\s+\w+", code):
113:         patterns["code_structure"].append("class-based")
114:     if re.search(r"=>\s*{", code) or re.search(r"\bfunction\s+\w+", code):
115:         patterns["code_structure"].append("functional")
116: 
117:     # Error handling
118:     if re.search(r"\btry\s*{", code):
119:         patterns["error_handling"].append("try-catch")
120:     if re.search(r"\bif\s*\(.*?\)\s*return", code):
121:         patterns["error_handling"].append("early-return")
122: 
123:     # Naming conventions
124:     func_names = re.findall(r"function\s+(\w+)", code)
125:     var_names = re.findall(r"(?:const|let|var)\s+(\w+)", code)
126:     class_names = re.findall(r"class\s+(\w+)", code)
127: 
128:     for name in func_names + var_names + class_names:
129:         if "_" in name:
130:             patterns["naming_conventions"].append("snake_case")
131:         elif name[0].islower() and any(c.isupper() for c in name[1:]):
132:             patterns["naming_conventions"].append("camelCase")
133:         elif name[0].isupper():
134:             patterns["naming_conventions"].append("PascalCase")
135: 
136:     # Test patterns
137:     if re.search(r"\bdescribe\(", code) or re.search(r"\bit\(", code):
138:         patterns["test_patterns"].append("jest")
139:     if re.search(r"\btest\(", code):
140:         patterns["test_patterns"].append("jest")
141: 
142:     # Import patterns
143:     if re.search(r"import\s+.*?\s+from\s+['\"]\.{1,2}/", code):
144:         patterns["import_patterns"].append("relative")
145:     if re.search(r"import\s+.*?\s+from\s+['\"][^./]", code):
146:         patterns["import_patterns"].append("absolute")
147: 
148:     return patterns
149: 
150: 
151: def _analyze_generic(code: str) -> Dict[str, List[str]]:
152:     """Fallback regex-based pattern detection for any language."""
153:     patterns = {
154:         "code_structure": [],
155:         "error_handling": [],
156:         "naming_conventions": [],
157:         "data_flow": [],
158:         "test_patterns": [],
159:         "import_patterns": []
160:     }
161: 
162:     # Basic structure detection
163:     if "async" in code.lower() or "await" in code.lower():
164:         patterns["code_structure"].append("async/await")
165:     if "class " in code.lower():
166:         patterns["code_structure"].append("class-based")
167:     if "function" in code.lower() or "def " in code:
168:         patterns["code_structure"].append("functional")
169: 
170:     # Error handling
171:     if "try" in code.lower():
172:         patterns["error_handling"].append("try-catch")
173:     if re.search(r"\breturn\b.*?(?:if|when)", code, re.IGNORECASE):
174:         patterns["error_handling"].append("early-return")
175: 
176:     # Naming patterns (simple heuristic)
177:     if "_" in code:
178:         patterns["naming_conventions"].append("snake_case")
179:     if re.search(r"[a-z][A-Z]", code):
180:         patterns["naming_conventions"].append("camelCase")
181: 
182:     return patterns
183: 
184: 
185: def determine_language(file_extension: str) -> str:
186:     """Map file extension to language identifier.
187: 
188:     Args:
189:         file_extension: File extension including dot (e.g., ".py", ".ts")
190: 
191:     Returns:
192:         Language identifier string
193:     """
194:     lang_map = {
195:         ".py": "python",
196:         ".ts": "typescript",
197:         ".tsx": "typescript",
198:         ".js": "javascript",
199:         ".jsx": "javascript",
200:         ".go": "go",
201:         ".rs": "rust",
202:         ".java": "java",
203:         ".c": "c",
204:         ".cpp": "cpp",
205:         ".h": "c",
206:         ".hpp": "cpp"
207:     }
208:     return lang_map.get(file_extension.lower(), "unknown")
209: 
210: 
211: def count_code_symbols(code: str, language: str) -> int:
212:     """Estimate symbol count (functions, classes, methods) in code.
213: 
214:     Args:
215:         code: Source code string
216:         language: Programming language
217: 
218:     Returns:
219:         Estimated count of code symbols
220:     """
221:     if language.lower() in ("python", "py"):
222:         try:
223:             tree = ast.parse(code)
224:             return sum(1 for node in ast.walk(tree)
225:                       if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)))
226:         except SyntaxError:
227:             pass
228: 
229:     # Fallback: regex-based counting
230:     func_count = len(re.findall(r"\b(def|function|class)\s+\w+", code))
231:     return func_count
</file>

<file path="tools/ce/context.py">
  1: """Context management: sync, health checks, pruning."""
  2: 
  3: from typing import Dict, Any, List, Optional
  4: from .core import run_cmd, git_status, git_diff, count_git_files, count_git_diff_lines
  5: from .validate import validate_level_1, validate_level_2
  6: from .exceptions import ContextDriftError
  7: import logging
  8: 
  9: logger = logging.getLogger(__name__)
 10: 
 11: 
 12: def sync() -> Dict[str, Any]:
 13:     """Sync context with codebase changes.
 14: 
 15:     Detects git diff and reports files that need reindexing.
 16: 
 17:     Returns:
 18:         Dict with: reindexed_count (int), files (List[str]), drift_score (float)
 19: 
 20:     Note: Real git diff detection - no mocked sync.
 21:     """
 22:     try:
 23:         changed_files = git_diff(since="HEAD~5", name_only=True)
 24:     except Exception as e:
 25:         raise RuntimeError(
 26:             f"Failed to get changed files: {str(e)}\n"
 27:             f"üîß Troubleshooting: Ensure you're in a git repository with commits"
 28:         ) from e
 29: 
 30:     # Calculate drift score (percentage of files changed)
 31:     # Get total tracked files
 32:     total_files = count_git_files()
 33:     drift_score = len(changed_files) / max(total_files, 1)  # Prevent division by zero
 34: 
 35:     return {
 36:         "reindexed_count": len(changed_files),
 37:         "files": changed_files,
 38:         "drift_score": drift_score,
 39:         "drift_level": (
 40:             "LOW" if drift_score < 0.15 else
 41:             "MEDIUM" if drift_score < 0.30 else
 42:             "HIGH"
 43:         )
 44:     }
 45: 
 46: 
 47: def health() -> Dict[str, Any]:
 48:     """Comprehensive context health check.
 49: 
 50:     Returns:
 51:         Dict with: compilation (bool), git_clean (bool), tests_passing (bool),
 52:                    drift_score (float), recommendations (List[str])
 53: 
 54:     Note: Real validation - no fake health scores.
 55:     """
 56:     recommendations = []
 57: 
 58:     # Check compilation (Level 1)
 59:     try:
 60:         l1_result = validate_level_1()
 61:         compilation_ok = l1_result["success"]
 62:         if not compilation_ok:
 63:             recommendations.append("Fix compilation errors with: ce validate --level 1")
 64:     except Exception as e:
 65:         compilation_ok = False
 66:         recommendations.append(f"Cannot run validation: {str(e)}")
 67: 
 68:     # Check git state
 69:     try:
 70:         git_state = git_status()
 71:         git_clean = git_state["clean"]
 72:         if not git_clean:
 73:             staged = len(git_state["staged"])
 74:             unstaged = len(git_state["unstaged"])
 75:             untracked = len(git_state["untracked"])
 76:             recommendations.append(
 77:                 f"Uncommitted changes: {staged} staged, {unstaged} unstaged, "
 78:                 f"{untracked} untracked"
 79:             )
 80:     except Exception as e:
 81:         git_clean = False
 82:         recommendations.append(f"Git check failed: {str(e)}")
 83: 
 84:     # Check tests (Level 2) - but don't block on failure
 85:     try:
 86:         l2_result = validate_level_2()
 87:         tests_passing = l2_result["success"]
 88:         if not tests_passing:
 89:             recommendations.append("Tests failing - fix with: ce validate --level 2")
 90:     except Exception as e:
 91:         tests_passing = False
 92:         recommendations.append("Cannot run tests - may need npm install")
 93: 
 94:     # Check context drift
 95:     try:
 96:         sync_result = sync()
 97:         drift_score = sync_result["drift_score"] * 100  # Convert to percentage (0-100)
 98:         drift_level = sync_result["drift_level"]
 99: 
100:         if drift_level == "HIGH":
101:             recommendations.append(
102:                 f"High context drift ({drift_score:.2f}%) - run: ce context sync"
103:             )
104:     except Exception:
105:         drift_score = 0.0
106:         drift_level = "UNKNOWN"
107: 
108:     # Overall health
109:     overall_healthy = compilation_ok and git_clean and tests_passing
110: 
111:     return {
112:         "healthy": overall_healthy,
113:         "compilation": compilation_ok,
114:         "git_clean": git_clean,
115:         "tests_passing": tests_passing,
116:         "drift_score": drift_score,  # Now returns percentage (0-100)
117:         "drift_level": drift_level,
118:         "recommendations": recommendations
119:     }
120: 
121: 
122: def prune(age_days: int = 7, dry_run: bool = False) -> Dict[str, Any]:
123:     """Prune old context memories (placeholder for Serena integration).
124: 
125:     Args:
126:         age_days: Delete memories older than this many days
127:         dry_run: If True, only report what would be deleted
128: 
129:     Returns:
130:         Dict with: deleted_count (int), files_deleted (List[str])
131: 
132:     Note: This is a placeholder. Full implementation requires Serena MCP integration.
133:     """
134:     # FIXME: Placeholder implementation - needs Serena MCP integration
135:     # This would normally:
136:     # 1. Query Serena for memory files
137:     # 2. Check age and access patterns
138:     # 3. Delete or compress based on priority
139: 
140:     return {
141:         "deleted_count": 0,
142:         "files_deleted": [],
143:         "dry_run": dry_run,
144:         "message": "Pruning requires Serena MCP integration (not yet implemented)"
145:     }
146: 
147: 
148: # ============================================================================
149: # Pre-Generation Sync Functions (Step 2.5)
150: # ============================================================================
151: 
152: def verify_git_clean() -> Dict[str, Any]:
153:     """Verify git working tree is clean.
154: 
155:     Returns:
156:         {
157:             "clean": True,
158:             "uncommitted_files": [],
159:             "untracked_files": []
160:         }
161: 
162:     Raises:
163:         RuntimeError: If uncommitted changes detected
164: 
165:     Process:
166:         1. Run: git status --porcelain
167:         2. Parse output for uncommitted/untracked files
168:         3. If any found: raise RuntimeError with file list
169:         4. Return clean status
170:     """
171:     try:
172:         status = git_status()
173:     except Exception as e:
174:         raise RuntimeError(
175:             f"Failed to check git status: {str(e)}\n"
176:             f"üîß Troubleshooting: Ensure you're in a git repository"
177:         ) from e
178: 
179:     uncommitted = status["staged"] + status["unstaged"]
180:     untracked = status["untracked"]
181: 
182:     if uncommitted or untracked:
183:         file_list = "\n".join(
184:             [f"  - {f} (uncommitted)" for f in uncommitted] +
185:             [f"  - {f} (untracked)" for f in untracked]
186:         )
187:         raise RuntimeError(
188:             f"Working tree has uncommitted changes:\n{file_list}\n"
189:             f"üîß Troubleshooting: Commit or stash changes before PRP generation"
190:         )
191: 
192:     return {
193:         "clean": True,
194:         "uncommitted_files": [],
195:         "untracked_files": []
196:     }
197: 
198: 
199: def check_drift_threshold(drift_score: float, force: bool = False) -> None:
200:     """Check drift score against thresholds and abort if needed.
201: 
202:     Args:
203:         drift_score: Drift percentage (0-100)
204:         force: Skip abort (for debugging)
205: 
206:     Raises:
207:         ContextDriftError: If drift > 30% and not force
208: 
209:     Thresholds:
210:         - 0-10%: INFO log, continue
211:         - 10-30%: WARNING log, continue
212:         - 30%+: ERROR log, abort (unless force=True)
213:     """
214:     if drift_score <= 10:
215:         logger.info(f"Context healthy: {drift_score:.2f}% drift")
216:     elif drift_score <= 30:
217:         logger.warning(f"Moderate drift: {drift_score:.2f}% - consider running ce context sync")
218:     else:
219:         # High drift - abort unless forced
220:         if not force:
221:             troubleshooting = (
222:                 "- Review recent commits: git log -5 --oneline\n"
223:                 "- Run: ce context sync to update indexes\n"
224:                 "- Check drift report: ce context health --verbose\n"
225:                 "- Consider: ce context prune to remove stale entries\n"
226:                 "- If confident, use --force to skip this check (not recommended)"
227:             )
228:             raise ContextDriftError(
229:                 drift_score=drift_score,
230:                 threshold=30.0,
231:                 troubleshooting=troubleshooting
232:             )
233:         else:
234:             logger.warning(f"High drift {drift_score:.2f}% - FORCED to continue (dangerous!)")
235: 
236: 
237: def pre_generation_sync(
238:     prp_id: Optional[str] = None,
239:     force: bool = False
240: ) -> Dict[str, Any]:
241:     """Execute Step 2.5: Pre-generation context sync and health check.
242: 
243:     Args:
244:         prp_id: Optional PRP ID for logging
245:         force: Skip drift abort (dangerous - for debugging only)
246: 
247:     Returns:
248:         {
249:             "success": True,
250:             "sync_completed": True,
251:             "drift_score": 8.2,  # 0-100%
252:             "git_clean": True,
253:             "abort_triggered": False,
254:             "warnings": []
255:         }
256: 
257:     Raises:
258:         ContextDriftError: If drift > 30% and force=False
259:         RuntimeError: If sync fails or git state dirty
260: 
261:     Process:
262:         1. Verify git clean state
263:         2. Run context sync
264:         3. Run health check
265:         4. Check drift threshold
266:         5. Return health report
267:     """
268:     warnings = []
269:     prp_log = f" (PRP-{prp_id})" if prp_id else ""
270: 
271:     logger.info(f"Starting pre-generation sync{prp_log}")
272: 
273:     # Step 1: Verify git clean state
274:     try:
275:         git_check = verify_git_clean()
276:         logger.info("‚úì Git working tree clean")
277:     except RuntimeError as e:
278:         logger.error(f"Git state check failed: {e}")
279:         raise
280: 
281:     # Step 2: Run context sync
282:     try:
283:         sync_result = sync()
284:         logger.info(f"‚úì Context sync completed: {sync_result['reindexed_count']} files reindexed")
285:     except Exception as e:
286:         raise RuntimeError(
287:             f"Context sync failed: {str(e)}\n"
288:             f"üîß Troubleshooting: Check git configuration and ensure repository has commits"
289:         ) from e
290: 
291:     # Step 3: Run health check
292:     try:
293:         health_result = health()
294:         drift_score = health_result["drift_score"]  # Already percentage (0-100)
295:         logger.info(f"‚úì Health check completed: {drift_score:.2f}% drift")
296:     except Exception as e:
297:         raise RuntimeError(
298:             f"Health check failed: {str(e)}\n"
299:             f"üîß Troubleshooting: Ensure validation tools are available"
300:         ) from e
301: 
302:     # Step 4: Check drift threshold
303:     try:
304:         check_drift_threshold(drift_score, force=force)
305:     except ContextDriftError:
306:         logger.error(f"Pre-generation sync aborted due to high drift{prp_log}")
307:         raise
308: 
309:     # Step 5: Return health report
310:     result = {
311:         "success": True,
312:         "sync_completed": True,
313:         "drift_score": drift_score,
314:         "git_clean": git_check["clean"],
315:         "abort_triggered": False,
316:         "warnings": warnings
317:     }
318: 
319:     logger.info(f"Pre-generation sync successful{prp_log}")
320:     return result
321: 
322: 
323: # ============================================================================
324: # Post-Execution Sync Functions (Step 6.5)
325: # ============================================================================
326: 
327: def post_execution_sync(
328:     prp_id: str,
329:     skip_cleanup: bool = False
330: ) -> Dict[str, Any]:
331:     """Execute Step 6.5: Post-execution cleanup and context sync.
332: 
333:     Args:
334:         prp_id: PRP identifier
335:         skip_cleanup: Skip cleanup protocol (for testing)
336: 
337:     Returns:
338:         {
339:             "success": True,
340:             "cleanup_completed": True,
341:             "sync_completed": True,
342:             "final_checkpoint": "checkpoint-PRP-003-final-20251012-160000",
343:             "drift_score": 5.1,  # After sync
344:             "memories_archived": 2,
345:             "memories_deleted": 3,
346:             "checkpoints_deleted": 2
347:         }
348: 
349:     Raises:
350:         RuntimeError: If cleanup or sync fails
351: 
352:     Process:
353:         1. Execute cleanup protocol (unless skip_cleanup)
354:         2. Run context sync
355:         3. Run health check
356:         4. Create final checkpoint
357:         5. Remove active PRP session
358:         6. Return cleanup + sync summary
359: 
360:     Integration Points:
361:         - cleanup_prp(prp_id): From PRP-2
362:         - context_sync(): Existing context.py function
363:         - context_health(): Existing context.py function
364:         - create_checkpoint(phase="final"): From PRP-2
365:     """
366:     from .prp import cleanup_prp, create_checkpoint, get_active_prp, end_prp
367:     from datetime import datetime, timezone
368: 
369:     logger.info(f"Starting post-execution sync (PRP-{prp_id})")
370: 
371:     result = {
372:         "success": True,
373:         "cleanup_completed": False,
374:         "sync_completed": False,
375:         "final_checkpoint": None,
376:         "drift_score": 0.0,
377:         "memories_archived": 0,
378:         "memories_deleted": 0,
379:         "checkpoints_deleted": 0
380:     }
381: 
382:     # Step 1: Execute cleanup protocol (unless skip_cleanup)
383:     if not skip_cleanup:
384:         try:
385:             cleanup_result = cleanup_prp(prp_id)
386:             result["cleanup_completed"] = True
387:             result["memories_archived"] = len(cleanup_result["memories_archived"])
388:             result["memories_deleted"] = len(cleanup_result["memories_deleted"])
389:             result["checkpoints_deleted"] = cleanup_result["checkpoints_deleted"]
390:             logger.info(f"‚úì Cleanup completed: {result['checkpoints_deleted']} checkpoints deleted")
391:         except Exception as e:
392:             raise RuntimeError(
393:                 f"Cleanup protocol failed: {str(e)}\n"
394:                 f"üîß Troubleshooting: Review cleanup errors and retry manually"
395:             ) from e
396:     else:
397:         logger.info("Skipping cleanup protocol (skip_cleanup=True)")
398:         result["cleanup_completed"] = True
399: 
400:     # Step 2: Run context sync
401:     try:
402:         sync_result = sync()
403:         result["sync_completed"] = True
404:         logger.info(f"‚úì Context sync completed: {sync_result['reindexed_count']} files reindexed")
405:     except Exception as e:
406:         raise RuntimeError(
407:             f"Context sync failed: {str(e)}\n"
408:             f"üîß Troubleshooting: Check git configuration and repository state"
409:         ) from e
410: 
411:     # Step 3: Run health check
412:     try:
413:         health_result = health()
414:         drift_score = health_result["drift_score"]  # Already percentage (0-100)
415:         result["drift_score"] = drift_score
416:         logger.info(f"‚úì Health check completed: {drift_score:.2f}% drift")
417: 
418:         # Warn if drift still high after sync
419:         if drift_score > 10:
420:             logger.warning(f"Drift still elevated after sync: {drift_score:.2f}%")
421:     except Exception as e:
422:         logger.warning(f"Health check failed: {e}")
423: 
424:     # Step 4: Create final checkpoint (if active PRP session exists)
425:     active = get_active_prp()
426:     if active and active["prp_id"] == prp_id:
427:         try:
428:             checkpoint_result = create_checkpoint(
429:                 phase="final",
430:                 message=f"PRP-{prp_id} complete with context sync"
431:             )
432:             result["final_checkpoint"] = checkpoint_result["tag_name"]
433:             logger.info(f"‚úì Final checkpoint created: {checkpoint_result['tag_name']}")
434:         except RuntimeError as e:
435:             # Don't fail if checkpoint creation fails (may already be committed)
436:             logger.warning(f"Could not create final checkpoint: {e}")
437: 
438:         # Step 5: Remove active PRP session
439:         try:
440:             end_prp(prp_id)
441:             logger.info(f"‚úì Active PRP session ended")
442:         except Exception as e:
443:             logger.warning(f"Could not end PRP session: {e}")
444:     else:
445:         logger.info("No active PRP session to end")
446: 
447:     logger.info(f"Post-execution sync completed (PRP-{prp_id})")
448:     return result
449: 
450: 
451: def sync_serena_context() -> Dict[str, Any]:
452:     """Sync Serena MCP context with current codebase.
453: 
454:     Returns:
455:         {
456:             "success": True,
457:             "files_indexed": 127,
458:             "symbols_updated": 453,
459:             "memories_refreshed": 5
460:         }
461: 
462:     Process:
463:         1. Trigger Serena re-index (if available)
464:         2. Update relevant memories with new patterns
465:         3. Refresh codebase structure knowledge
466:         4. Return sync summary
467: 
468:     Note: This is a placeholder. Full implementation requires Serena MCP integration.
469:     """
470:     # FIXME: Placeholder implementation - needs Serena MCP integration
471:     logger.warning("Serena MCP sync not implemented - skipping")
472: 
473:     return {
474:         "success": True,
475:         "files_indexed": 0,
476:         "symbols_updated": 0,
477:         "memories_refreshed": 0,
478:         "message": "Serena sync requires MCP integration (not yet implemented)"
479:     }
480: 
481: 
482: def prune_stale_memories(age_days: int = 30) -> Dict[str, Any]:
483:     """Prune stale Serena memories older than age_days.
484: 
485:     Args:
486:         age_days: Delete memories older than this (default: 30 days)
487: 
488:     Returns:
489:         {
490:             "success": True,
491:             "memories_pruned": 12,
492:             "space_freed_kb": 45.2
493:         }
494: 
495:     Process:
496:         1. List all Serena memories
497:         2. Filter by age (creation timestamp)
498:         3. Exclude essential memories (never delete):
499:            - project-patterns
500:            - code-style-conventions
501:            - testing-standards
502:         4. Delete stale memories via Serena MCP
503:         5. Return pruning summary
504: 
505:     Note: This is a placeholder. Full implementation requires Serena MCP integration.
506:     """
507:     # FIXME: Placeholder implementation - needs Serena MCP integration
508:     logger.warning(f"Serena memory pruning not implemented - would prune memories older than {age_days} days")
509: 
510:     return {
511:         "success": True,
512:         "memories_pruned": 0,
513:         "space_freed_kb": 0.0,
514:         "message": "Memory pruning requires Serena MCP integration (not yet implemented)"
515:     }
516: 
517: 
518: # ============================================================================
519: # Drift Detection & Reporting Functions
520: # ============================================================================
521: 
522: def calculate_drift_score() -> float:
523:     """Calculate context drift score (0-100%).
524: 
525:     Returns:
526:         Drift percentage (0 = perfect sync, 100 = completely stale)
527: 
528:     Calculation:
529:         drift = (
530:             file_changes_score * 0.4 +
531:             memory_staleness_score * 0.3 +
532:             dependency_changes_score * 0.2 +
533:             uncommitted_changes_score * 0.1
534:         )
535: 
536:     Components:
537:         - file_changes_score: % of tracked files modified since last sync
538:         - memory_staleness_score: Age of oldest Serena memory (normalized)
539:         - dependency_changes_score: pyproject.toml/package.json changes
540:         - uncommitted_changes_score: Penalty for dirty git state
541:     """
542:     # Component 1: File changes (40% weight)
543:     try:
544:         changed_files = git_diff(since="HEAD~5", name_only=True)
545:         try:
546:             total_files = count_git_files()
547:             file_changes_score = (len(changed_files) / max(total_files, 1)) * 100
548:         except RuntimeError:
549:             file_changes_score = 0
550:     except Exception as e:
551:         logger.warning(
552:             f"Failed to calculate file changes score: {e}\n"
553:             f"üîß Troubleshooting: Ensure git is available and repository has commits"
554:         )
555:         file_changes_score = 0
556: 
557:     # Component 2: Memory staleness (30% weight)
558:     # FIXME: Placeholder - needs Serena MCP integration
559:     memory_staleness_score = 0  # Would check age of memories
560: 
561:     # Component 3: Dependency changes (20% weight)
562:     dependency_changes_score = 0
563:     try:
564:         # Check if pyproject.toml changed recently
565:         deps_lines = count_git_diff_lines(
566:             ref="HEAD~5",
567:             files=["pyproject.toml", "package.json"]
568:         )
569:         # Normalize: >10 lines of changes = 100% score
570:         dependency_changes_score = min((deps_lines / 10.0) * 100, 100)
571:     except Exception as e:
572:         logger.warning(
573:             f"Failed to check dependency changes: {e}\n"
574:             f"üîß Troubleshooting: Ensure git is available"
575:         )
576: 
577:     # Component 4: Uncommitted changes (10% weight)
578:     uncommitted_changes_score = 0
579:     try:
580:         status = git_status()
581:         uncommitted = len(status["staged"]) + len(status["unstaged"])
582:         untracked = len(status["untracked"])
583:         # Normalize: >5 files = 100% score
584:         uncommitted_changes_score = min(((uncommitted + untracked) / 5.0) * 100, 100)
585:     except Exception as e:
586:         logger.warning(
587:             f"Failed to check uncommitted changes: {e}\n"
588:             f"üîß Troubleshooting: Ensure git is available and you're in a repository"
589:         )
590: 
591:     # Weighted sum
592:     drift = (
593:         file_changes_score * 0.4 +
594:         memory_staleness_score * 0.3 +
595:         dependency_changes_score * 0.2 +
596:         uncommitted_changes_score * 0.1
597:     )
598: 
599:     return drift
600: 
601: 
602: def context_health_verbose() -> Dict[str, Any]:
603:     """Detailed context health report with breakdown.
604: 
605:     Returns:
606:         {
607:             "drift_score": 23.4,
608:             "threshold": "warn",  # healthy | warn | critical
609:             "components": {
610:                 "file_changes": {"score": 18.2, "details": "12/127 files modified"},
611:                 "memory_staleness": {"score": 5.1, "details": "Oldest: 8 days"},
612:                 "dependency_changes": {"score": 0, "details": "No changes"},
613:                 "uncommitted_changes": {"score": 0.1, "details": "1 untracked file"}
614:             },
615:             "recommendations": [
616:                 "Run: ce context sync to refresh indexes",
617:                 "Consider: ce context prune to remove stale memories"
618:             ]
619:         }
620:     """
621:     components = {}
622:     recommendations = []
623: 
624:     # File changes component
625:     try:
626:         changed_files = git_diff(since="HEAD~5", name_only=True)
627:         try:
628:             total_files = count_git_files()
629:             file_score = (len(changed_files) / max(total_files, 1)) * 100
630:             components["file_changes"] = {
631:                 "score": file_score,
632:                 "details": f"{len(changed_files)}/{total_files} files modified"
633:             }
634:             if file_score > 15:
635:                 recommendations.append("Run: ce context sync to refresh indexes")
636:         except RuntimeError:
637:             components["file_changes"] = {"score": 0, "details": "Error: could not count files"}
638:     except Exception as e:
639:         logger.warning(
640:             f"Failed to calculate file changes component: {e}\n"
641:             f"üîß Troubleshooting: Ensure git is available"
642:         )
643:         components["file_changes"] = {"score": 0, "details": f"Error: {e}"}
644: 
645:     # Memory staleness component (placeholder)
646:     components["memory_staleness"] = {
647:         "score": 0,
648:         "details": "Serena MCP not available"
649:     }
650: 
651:     # Dependency changes component
652:     try:
653:         deps_lines = count_git_diff_lines(
654:             ref="HEAD~5",
655:             files=["pyproject.toml", "package.json"]
656:         )
657:         deps_score = min((deps_lines / 10.0) * 100, 100)
658:         components["dependency_changes"] = {
659:             "score": deps_score,
660:             "details": f"{deps_lines} lines changed" if deps_lines > 0 else "No changes"
661:         }
662:         if deps_score > 10:
663:             recommendations.append("Dependencies changed - run: uv sync")
664:     except Exception as e:
665:         logger.warning(
666:             f"Failed to check dependency changes component: {e}\n"
667:             f"üîß Troubleshooting: Ensure git is available"
668:         )
669:         components["dependency_changes"] = {"score": 0, "details": "No changes"}
670: 
671:     # Uncommitted changes component
672:     try:
673:         status = git_status()
674:         uncommitted = len(status["staged"]) + len(status["unstaged"])
675:         untracked = len(status["untracked"])
676:         uncommitted_score = min(((uncommitted + untracked) / 5.0) * 100, 100)
677:         components["uncommitted_changes"] = {
678:             "score": uncommitted_score,
679:             "details": f"{uncommitted} uncommitted, {untracked} untracked"
680:         }
681:         if uncommitted + untracked > 0:
682:             recommendations.append("Commit or stash changes before PRP operations")
683:     except Exception as e:
684:         logger.warning(
685:             f"Failed to check uncommitted changes component: {e}\n"
686:             f"üîß Troubleshooting: Ensure git is available and you're in a repository"
687:         )
688:         components["uncommitted_changes"] = {"score": 0, "details": "0 uncommitted"}
689: 
690:     # Calculate overall drift
691:     drift_score = calculate_drift_score()
692: 
693:     # Determine threshold
694:     if drift_score <= 10:
695:         threshold = "healthy"
696:     elif drift_score <= 30:
697:         threshold = "warn"
698:     else:
699:         threshold = "critical"
700: 
701:     return {
702:         "drift_score": drift_score,
703:         "threshold": threshold,
704:         "components": components,
705:         "recommendations": recommendations
706:     }
707: 
708: 
709: def drift_report_markdown() -> str:
710:     """Generate markdown drift report for logging.
711: 
712:     Returns:
713:         Markdown-formatted drift report
714: 
715:     Format:
716:         ## Context Health Report
717: 
718:         **Drift Score**: 23.4% (‚ö†Ô∏è WARNING)
719: 
720:         ### Components
721:         - File Changes: 18.2% (12/127 files modified)
722:         - Memory Staleness: 5.1% (Oldest: 8 days)
723:         - Dependency Changes: 0% (No changes)
724:         - Uncommitted Changes: 0.1% (1 untracked file)
725: 
726:         ### Recommendations
727:         1. Run: ce context sync to refresh indexes
728:         2. Consider: ce context prune to remove stale memories
729:     """
730:     report = context_health_verbose()
731: 
732:     # Status emoji
733:     if report["threshold"] == "healthy":
734:         status_emoji = "‚úÖ"
735:         status_text = "HEALTHY"
736:     elif report["threshold"] == "warn":
737:         status_emoji = "‚ö†Ô∏è"
738:         status_text = "WARNING"
739:     else:
740:         status_emoji = "‚ùå"
741:         status_text = "CRITICAL"
742: 
743:     # Build markdown
744:     md = f"## Context Health Report\n\n"
745:     md += f"**Drift Score**: {report['drift_score']:.2f}% ({status_emoji} {status_text})\n\n"
746: 
747:     # Components
748:     md += "### Components\n"
749:     for name, comp in report["components"].items():
750:         display_name = name.replace("_", " ").title()
751:         md += f"- {display_name}: {comp['score']:.2f}% ({comp['details']})\n"
752: 
753:     # Recommendations
754:     if report["recommendations"]:
755:         md += "\n### Recommendations\n"
756:         for i, rec in enumerate(report["recommendations"], 1):
757:             md += f"{i}. {rec}\n"
758: 
759:     return md
760: 
761: 
762: # ============================================================================
763: # Auto-Sync Mode Configuration
764: # ============================================================================
765: 
766: def enable_auto_sync() -> Dict[str, Any]:
767:     """Enable auto-sync mode in .ce/config.
768: 
769:     Returns:
770:         {
771:             "success": True,
772:             "mode": "enabled",
773:             "config_path": ".ce/config"
774:         }
775: 
776:     Process:
777:         1. Create .ce/config if not exists
778:         2. Set auto_sync: true in config
779:         3. Log: "Auto-sync enabled - Steps 2.5 and 6.5 will run automatically"
780:     """
781:     from pathlib import Path
782:     import json
783: 
784:     config_dir = Path(".ce")
785:     config_file = config_dir / "config"
786: 
787:     # Create directory if needed
788:     config_dir.mkdir(exist_ok=True)
789: 
790:     # Read existing config or create new
791:     if config_file.exists():
792:         try:
793:             config = json.loads(config_file.read_text())
794:         except (json.JSONDecodeError, OSError):
795:             config = {}
796:     else:
797:         config = {}
798: 
799:     # Set auto_sync
800:     config["auto_sync"] = True
801: 
802:     # Write config
803:     config_file.write_text(json.dumps(config, indent=2))
804: 
805:     logger.info("Auto-sync enabled - Steps 2.5 and 6.5 will run automatically")
806: 
807:     return {
808:         "success": True,
809:         "mode": "enabled",
810:         "config_path": str(config_file)
811:     }
812: 
813: 
814: def disable_auto_sync() -> Dict[str, Any]:
815:     """Disable auto-sync mode in .ce/config.
816: 
817:     Returns:
818:         {
819:             "success": True,
820:             "mode": "disabled",
821:             "config_path": ".ce/config"
822:         }
823:     """
824:     from pathlib import Path
825:     import json
826: 
827:     config_dir = Path(".ce")
828:     config_file = config_dir / "config"
829: 
830:     # Read existing config
831:     if config_file.exists():
832:         try:
833:             config = json.loads(config_file.read_text())
834:         except (json.JSONDecodeError, OSError):
835:             config = {}
836:     else:
837:         config = {}
838: 
839:     # Set auto_sync to false
840:     config["auto_sync"] = False
841: 
842:     # Write config
843:     config_dir.mkdir(exist_ok=True)
844:     config_file.write_text(json.dumps(config, indent=2))
845: 
846:     logger.info("Auto-sync disabled - Manual sync required")
847: 
848:     return {
849:         "success": True,
850:         "mode": "disabled",
851:         "config_path": str(config_file)
852:     }
853: 
854: 
855: def is_auto_sync_enabled() -> bool:
856:     """Check if auto-sync mode is enabled.
857: 
858:     Returns:
859:         True if enabled, False otherwise
860: 
861:     Process:
862:         1. Read .ce/config
863:         2. Return config.get("auto_sync", False)
864:     """
865:     from pathlib import Path
866:     import json
867: 
868:     config_file = Path(".ce/config")
869: 
870:     if not config_file.exists():
871:         return False
872: 
873:     try:
874:         config = json.loads(config_file.read_text())
875:         return config.get("auto_sync", False)
876:     except (json.JSONDecodeError, OSError):
877:         return False
878: 
879: 
880: def get_auto_sync_status() -> Dict[str, Any]:
881:     """Get auto-sync mode status.
882: 
883:     Returns:
884:         {
885:             "enabled": True,
886:             "config_path": ".ce/config",
887:             "message": "Auto-sync is enabled"
888:         }
889:     """
890:     from pathlib import Path
891: 
892:     enabled = is_auto_sync_enabled()
893:     config_file = Path(".ce/config")
894: 
895:     message = (
896:         "Auto-sync is enabled - Steps 2.5 and 6.5 run automatically"
897:         if enabled
898:         else "Auto-sync is disabled - Manual sync required"
899:     )
900: 
901:     return {
902:         "enabled": enabled,
903:         "config_path": str(config_file),
904:         "message": message
905:     }
</file>

<file path="tools/ce/drift_analyzer.py">
  1: """Drift analysis engine for L4 validation.
  2: 
  3: Calculates semantic drift between expected patterns (from PRP EXAMPLES)
  4: and actual implementation code. Uses shared code_analyzer module for
  5: pattern detection.
  6: """
  7: 
  8: import time
  9: from typing import Dict, List, Any
 10: from pathlib import Path
 11: 
 12: from .code_analyzer import analyze_code_patterns, determine_language, count_code_symbols
 13: 
 14: 
 15: def analyze_implementation(
 16:     prp_path: str,
 17:     implementation_paths: List[str]
 18: ) -> Dict[str, Any]:
 19:     """Analyze implementation code structure using Serena MCP.
 20: 
 21:     Args:
 22:         prp_path: Path to PRP file (for extracting expected patterns)
 23:         implementation_paths: Paths to implementation files to analyze
 24: 
 25:     Returns:
 26:         {
 27:             "detected_patterns": {
 28:                 "code_structure": ["async/await", "class-based"],
 29:                 "error_handling": ["try-except"],
 30:                 "naming_conventions": ["snake_case"],
 31:                 ...
 32:             },
 33:             "files_analyzed": ["src/validate.py", "src/core.py"],
 34:             "symbol_count": 42,
 35:             "analysis_duration": 2.5,
 36:             "serena_available": False
 37:         }
 38: 
 39:     Uses (if Serena MCP available):
 40:         - serena.get_symbols_overview(file) for structure
 41:         - serena.find_symbol(name) for detailed analysis
 42:         - serena.read_file(file) for pattern matching
 43: 
 44:     Fallback (if Serena unavailable):
 45:         - Python ast module for Python files
 46:         - Regex-based pattern detection for other languages
 47:         - Log warning: "Serena MCP unavailable - using fallback analysis (reduced accuracy)"
 48: 
 49:     Raises:
 50:         RuntimeError: If neither Serena nor fallback analysis succeeds
 51:     """
 52:     import time
 53:     start_time = time.time()
 54: 
 55:     all_patterns = {
 56:         "code_structure": [],
 57:         "error_handling": [],
 58:         "naming_conventions": [],
 59:         "data_flow": [],
 60:         "test_patterns": [],
 61:         "import_patterns": []
 62:     }
 63: 
 64:     files_analyzed = []
 65:     symbol_count = 0
 66: 
 67:     # MVP: Serena MCP integration deferred - use fallback analysis
 68:     # TODO: Future enhancement - integrate Serena MCP for semantic analysis
 69:     serena_available = False
 70: 
 71:     for impl_path in implementation_paths:
 72:         impl_path_obj = Path(impl_path)
 73:         if not impl_path_obj.exists():
 74:             continue
 75: 
 76:         files_analyzed.append(impl_path)
 77: 
 78:         # Determine language from file extension
 79:         extension = impl_path_obj.suffix.lower()
 80:         language = determine_language(extension)
 81: 
 82:         code = impl_path_obj.read_text()
 83: 
 84:         # Analyze patterns using shared code analyzer
 85:         patterns = analyze_code_patterns(code, language)
 86: 
 87:         # Count symbols
 88:         symbol_count += count_code_symbols(code, language)
 89: 
 90:         # Merge patterns
 91:         for category, values in patterns.items():
 92:             if category in all_patterns:
 93:                 all_patterns[category].extend(values)
 94: 
 95:     # Deduplicate
 96:     for category in all_patterns:
 97:         all_patterns[category] = list(set(all_patterns[category]))
 98: 
 99:     duration = time.time() - start_time
100: 
101:     if not files_analyzed:
102:         raise RuntimeError(
103:             f"No implementation files found at {implementation_paths}\n"
104:             f"üîß Troubleshooting: Verify file paths exist and are readable"
105:         )
106: 
107:     return {
108:         "detected_patterns": all_patterns,
109:         "files_analyzed": files_analyzed,
110:         "symbol_count": symbol_count,
111:         "analysis_duration": round(duration, 2),
112:         "serena_available": serena_available
113:     }
114: 
115: 
116: def calculate_drift_score(
117:     expected_patterns: Dict[str, Any],
118:     detected_patterns: Dict[str, Any]
119: ) -> Dict[str, Any]:
120:     """Calculate drift score between expected and detected patterns.
121: 
122:     Scoring methodology:
123:     - Each category (code_structure, error_handling, etc.) weighted equally
124:     - Within category: count mismatches / total expected patterns
125:     - Overall drift = average across all categories * 100
126: 
127:     Args:
128:         expected_patterns: From extract_patterns_from_prp()
129:         detected_patterns: From analyze_implementation()
130: 
131:     Returns:
132:         {
133:             "drift_score": 23.5,  # 0-100%, lower is better
134:             "category_scores": {
135:                 "code_structure": 10.0,
136:                 "error_handling": 0.0,
137:                 "naming_conventions": 50.0,
138:                 ...
139:             },
140:             "mismatches": [
141:                 {
142:                     "category": "naming_conventions",
143:                     "expected": "snake_case",
144:                     "detected": "camelCase",
145:                     "severity": "medium",
146:                     "affected_symbols": ["processData", "handleError"]
147:                 }
148:             ],
149:             "threshold_action": "auto_fix"  # auto_accept | auto_fix | escalate
150:         }
151:     """
152:     category_scores = {}
153:     mismatches = []
154: 
155:     # Categories to compare (exclude raw_examples)
156:     categories = [
157:         "code_structure",
158:         "error_handling",
159:         "naming_conventions",
160:         "data_flow",
161:         "test_patterns",
162:         "import_patterns"
163:     ]
164: 
165:     for category in categories:
166:         expected = expected_patterns.get(category, [])
167:         detected = detected_patterns.get(category, [])
168: 
169:         if not expected:
170:             # No expectations for this category - skip
171:             continue
172: 
173:         # Calculate mismatches
174:         missing_patterns = set(expected) - set(detected)
175:         unexpected_patterns = set(detected) - set(expected)
176: 
177:         # Mismatch score = (missing + unexpected) / (expected + detected)
178:         # This penalizes both missing expected patterns and unexpected patterns
179:         total_expected = len(expected)
180:         mismatch_count = len(missing_patterns)
181: 
182:         if total_expected > 0:
183:             category_score = (mismatch_count / total_expected) * 100
184:         else:
185:             category_score = 0.0
186: 
187:         category_scores[category] = round(category_score, 1)
188: 
189:         # Record mismatches
190:         for missing in missing_patterns:
191:             mismatches.append({
192:                 "category": category,
193:                 "expected": missing,
194:                 "detected": list(unexpected_patterns) if unexpected_patterns else None,
195:                 "severity": _determine_severity(category, missing),
196:                 "affected_symbols": []  # MVP: Symbol tracking deferred
197:             })
198: 
199:     # Calculate overall drift score (average of category scores)
200:     if category_scores:
201:         drift_score = sum(category_scores.values()) / len(category_scores)
202:     else:
203:         drift_score = 0.0
204: 
205:     drift_score = round(drift_score, 1)
206: 
207:     # Determine threshold action
208:     if drift_score < 10.0:
209:         threshold_action = "auto_accept"
210:     elif drift_score < 30.0:
211:         threshold_action = "auto_fix"
212:     else:
213:         threshold_action = "escalate"
214: 
215:     return {
216:         "drift_score": drift_score,
217:         "category_scores": category_scores,
218:         "mismatches": mismatches,
219:         "threshold_action": threshold_action
220:     }
221: 
222: 
223: def get_auto_fix_suggestions(mismatches: List[Dict]) -> List[str]:
224:     """Generate fix suggestions for 10-30% drift (MVP: display only, no auto-apply).
225: 
226:     Future enhancement: Apply fixes automatically using Serena edit operations.
227: 
228:     Args:
229:         mismatches: List of mismatch dicts from calculate_drift_score()
230: 
231:     Returns:
232:         List of actionable fix suggestions (e.g., "Rename processData ‚Üí process_data")
233:     """
234:     suggestions = []
235: 
236:     for mismatch in mismatches:
237:         category = mismatch["category"]
238:         expected = mismatch["expected"]
239:         detected = mismatch.get("detected", [])
240: 
241:         if category == "naming_conventions":
242:             # Suggest naming convention fixes
243:             if expected == "snake_case" and detected:
244:                 suggestions.append(
245:                     f"‚ö†Ô∏è  Convert naming from {detected} to snake_case convention"
246:                 )
247:             elif expected == "camelCase" and "snake_case" in (detected or []):
248:                 suggestions.append(
249:                     f"‚ö†Ô∏è  Convert naming from snake_case to camelCase convention"
250:                 )
251:             elif expected == "PascalCase" and detected:
252:                 suggestions.append(
253:                     f"‚ö†Ô∏è  Convert class names to PascalCase convention"
254:                 )
255: 
256:         elif category == "error_handling":
257:             if expected == "try-except" and not detected:
258:                 suggestions.append(
259:                     f"‚ö†Ô∏è  Add try-except error handling blocks"
260:                 )
261:             elif expected == "early-return" and not detected:
262:                 suggestions.append(
263:                     f"‚ö†Ô∏è  Add guard clauses with early returns"
264:                 )
265: 
266:         elif category == "code_structure":
267:             if expected == "async/await" and detected:
268:                 if "callbacks" in (detected or []):
269:                     suggestions.append(
270:                         f"‚ö†Ô∏è  Convert callback-based code to async/await pattern"
271:                     )
272:             elif expected == "class-based" and "functional" in (detected or []):
273:                 suggestions.append(
274:                     f"‚ö†Ô∏è  Consider refactoring to class-based structure"
275:                 )
276: 
277:         elif category == "test_patterns":
278:             if expected == "pytest" and not detected:
279:                 suggestions.append(
280:                     f"‚ö†Ô∏è  Add pytest-style test functions (test_* naming)"
281:                 )
282: 
283:     if not suggestions:
284:         suggestions.append("‚ÑπÔ∏è  Review patterns and consider manual alignment")
285: 
286:     return suggestions
287: 
288: 
289: def _determine_severity(category: str, pattern: str) -> str:
290:     """Determine severity of missing pattern."""
291:     # High severity: security/correctness patterns
292:     high_severity_patterns = {
293:         "error_handling": ["try-except", "try-catch"],
294:         "code_structure": ["async/await"]  # if expected but missing, may cause issues
295:     }
296: 
297:     # Medium severity: consistency/maintainability
298:     medium_severity_patterns = {
299:         "naming_conventions": ["snake_case", "camelCase", "PascalCase"],
300:         "test_patterns": ["pytest", "jest"]
301:     }
302: 
303:     # Low severity: style preferences
304:     low_severity_patterns = {
305:         "import_patterns": ["relative", "absolute"],
306:         "data_flow": ["props", "state"]
307:     }
308: 
309:     if category in high_severity_patterns and pattern in high_severity_patterns[category]:
310:         return "high"
311:     elif category in medium_severity_patterns and pattern in medium_severity_patterns[category]:
312:         return "medium"
313:     else:
314:         return "low"
</file>

<file path="tools/ce/drift.py">
  1: """Drift history tracking and analysis.
  2: 
  3: Provides tools for querying and analyzing architectural drift decisions
  4: across PRPs, creating an audit trail for pattern conformance validation.
  5: """
  6: 
  7: import yaml
  8: import re
  9: import logging
 10: from glob import glob
 11: from pathlib import Path
 12: from typing import Dict, List, Any, Optional
 13: from datetime import datetime, timezone
 14: 
 15: logger = logging.getLogger(__name__)
 16: 
 17: 
 18: def parse_drift_justification(prp_path: str) -> Optional[Dict[str, Any]]:
 19:     """Extract DRIFT_JUSTIFICATION from PRP YAML header.
 20: 
 21:     Args:
 22:         prp_path: Path to PRP markdown file
 23: 
 24:     Returns:
 25:         {
 26:             "prp_id": "PRP-001",
 27:             "prp_name": "Level 4 Pattern Conformance",
 28:             "drift_decision": {
 29:                 "score": 45.2,
 30:                 "action": "accepted",
 31:                 "justification": "...",
 32:                 "timestamp": "2025-10-12T15:30:00Z",
 33:                 "category_breakdown": {...},
 34:                 "reviewer": "human"
 35:             }
 36:         }
 37:         Returns None if no drift_decision found
 38: 
 39:     Raises:
 40:         FileNotFoundError: If PRP file doesn't exist
 41:         ValueError: If YAML header malformed
 42:     """
 43:     path = Path(prp_path)
 44:     if not path.exists():
 45:         raise FileNotFoundError(
 46:             f"PRP file not found: {prp_path}\n"
 47:             f"üîß Troubleshooting: Check PRP path and ensure file exists"
 48:         )
 49: 
 50:     try:
 51:         with open(path, 'r') as f:
 52:             content = f.read()
 53: 
 54:         # Extract YAML header (between --- markers)
 55:         yaml_match = re.match(r'^---\n(.*?)\n---', content, re.DOTALL)
 56:         if not yaml_match:
 57:             raise ValueError(
 58:                 f"No YAML header found in {prp_path}\n"
 59:                 f"üîß Troubleshooting: Ensure PRP has YAML front matter between --- markers"
 60:             )
 61: 
 62:         yaml_content = yaml_match.group(1)
 63:         header = yaml.safe_load(yaml_content)
 64: 
 65:         # Check if drift_decision exists
 66:         if "drift_decision" not in header:
 67:             return None
 68: 
 69:         return {
 70:             "prp_id": header.get("prp_id", "UNKNOWN"),
 71:             "prp_name": header.get("name", "Unknown PRP"),
 72:             "drift_decision": header["drift_decision"]
 73:         }
 74: 
 75:     except Exception as e:
 76:         raise ValueError(
 77:             f"Failed to parse PRP YAML: {str(e)}\n"
 78:             f"üîß Troubleshooting: Verify YAML syntax in {prp_path}"
 79:         ) from e
 80: 
 81: 
 82: def get_drift_history(
 83:     last_n: Optional[int] = None,
 84:     prp_id: Optional[str] = None,
 85:     action_filter: Optional[str] = None
 86: ) -> List[Dict[str, Any]]:
 87:     """Query drift decision history across all PRPs.
 88: 
 89:     Args:
 90:         last_n: Return only last N decisions (by timestamp)
 91:         prp_id: Filter by specific PRP ID
 92:         action_filter: Filter by action (accepted, rejected, examples_updated)
 93: 
 94:     Returns:
 95:         List of drift decisions sorted by timestamp (newest first)
 96: 
 97:     Example:
 98:         >>> history = get_drift_history(last_n=3)
 99:         >>> history[0]["drift_decision"]["score"]
100:         45.2
101:     """
102:     prp_dirs = ["PRPs/executed", "PRPs/feature-requests"]
103:     all_decisions = []
104: 
105:     for prp_dir in prp_dirs:
106:         dir_path = Path(prp_dir)
107:         if not dir_path.exists():
108:             continue
109: 
110:         # Find all PRP markdown files
111:         for prp_file in dir_path.glob("PRP-*.md"):
112:             try:
113:                 decision = parse_drift_justification(str(prp_file))
114:                 if decision:
115:                     all_decisions.append(decision)
116:             except Exception as e:
117:                 logger.warning(f"Skipping {prp_file}: {e}")
118: 
119:     # Apply filters
120:     if prp_id:
121:         all_decisions = [d for d in all_decisions if d["prp_id"] == prp_id]
122: 
123:     if action_filter:
124:         all_decisions = [
125:             d for d in all_decisions
126:             if d["drift_decision"]["action"] == action_filter
127:         ]
128: 
129:     # Sort by timestamp (newest first)
130:     all_decisions.sort(
131:         key=lambda d: d["drift_decision"].get("timestamp", ""),
132:         reverse=True
133:     )
134: 
135:     # Apply limit
136:     if last_n:
137:         all_decisions = all_decisions[:last_n]
138: 
139:     return all_decisions
140: 
141: 
142: def drift_summary() -> Dict[str, Any]:
143:     """Generate aggregate statistics for all drift decisions.
144: 
145:     Returns:
146:         {
147:             "total_prps": 15,
148:             "prps_with_drift": 8,
149:             "decisions": {
150:                 "accepted": 5,
151:                 "rejected": 2,
152:                 "examples_updated": 1
153:             },
154:             "avg_drift_score": 23.7,
155:             "score_distribution": {
156:                 "low": 3,      # 0-10%
157:                 "medium": 4,   # 10-30%
158:                 "high": 1      # 30%+
159:             },
160:             "category_breakdown": {
161:                 "code_structure": {"avg": 25.0, "count": 8},
162:                 "error_handling": {"avg": 15.0, "count": 8},
163:                 "naming_conventions": {"avg": 30.0, "count": 8}
164:             },
165:             "reviewer_breakdown": {
166:                 "human": 6,
167:                 "auto_accept": 2,
168:                 "auto_fix": 0
169:             }
170:         }
171:     """
172:     history = get_drift_history()
173: 
174:     if not history:
175:         return {
176:             "total_prps": 0,
177:             "prps_with_drift": 0,
178:             "decisions": {},
179:             "avg_drift_score": 0.0,
180:             "score_distribution": {},
181:             "category_breakdown": {},
182:             "reviewer_breakdown": {}
183:         }
184: 
185:     # Count decisions by action
186:     decisions = {}
187:     for h in history:
188:         action = h["drift_decision"]["action"]
189:         decisions[action] = decisions.get(action, 0) + 1
190: 
191:     # Calculate average drift score
192:     scores = [h["drift_decision"]["score"] for h in history]
193:     avg_drift = sum(scores) / len(scores)
194: 
195:     # Score distribution
196:     score_dist = {"low": 0, "medium": 0, "high": 0}
197:     for score in scores:
198:         if score <= 10:
199:             score_dist["low"] += 1
200:         elif score <= 30:
201:             score_dist["medium"] += 1
202:         else:
203:             score_dist["high"] += 1
204: 
205:     # Category breakdown
206:     categories = {}
207:     for h in history:
208:         breakdown = h["drift_decision"].get("category_breakdown", {})
209:         for cat, score in breakdown.items():
210:             if cat not in categories:
211:                 categories[cat] = {"total": 0, "count": 0}
212:             categories[cat]["total"] += score
213:             categories[cat]["count"] += 1
214: 
215:     category_breakdown = {
216:         cat: {
217:             "avg": data["total"] / data["count"],
218:             "count": data["count"]
219:         }
220:         for cat, data in categories.items()
221:     }
222: 
223:     # Reviewer breakdown
224:     reviewers = {}
225:     for h in history:
226:         reviewer = h["drift_decision"].get("reviewer", "unknown")
227:         reviewers[reviewer] = reviewers.get(reviewer, 0) + 1
228: 
229:     return {
230:         "total_prps": len(history),
231:         "prps_with_drift": len(history),
232:         "decisions": decisions,
233:         "avg_drift_score": round(avg_drift, 2),
234:         "score_distribution": score_dist,
235:         "category_breakdown": category_breakdown,
236:         "reviewer_breakdown": reviewers
237:     }
238: 
239: 
240: def show_drift_decision(prp_id: str) -> Dict[str, Any]:
241:     """Display detailed drift decision for specific PRP.
242: 
243:     Args:
244:         prp_id: PRP identifier (e.g., "PRP-001")
245: 
246:     Returns:
247:         Full drift decision with metadata
248: 
249:     Raises:
250:         ValueError: If PRP not found or has no drift decision
251:     """
252:     history = get_drift_history(prp_id=prp_id)
253: 
254:     if not history:
255:         raise ValueError(
256:             f"No drift decision found for {prp_id}\n"
257:             f"üîß Troubleshooting: Verify PRP ID and check if drift decision exists in YAML header"
258:         )
259: 
260:     return history[0]
261: 
262: 
263: def compare_drift_decisions(prp_id_1: str, prp_id_2: str) -> Dict[str, Any]:
264:     """Compare drift decisions between two PRPs.
265: 
266:     Args:
267:         prp_id_1: First PRP ID
268:         prp_id_2: Second PRP ID
269: 
270:     Returns:
271:         {
272:             "prp_1": {...},
273:             "prp_2": {...},
274:             "comparison": {
275:                 "score_diff": 12.5,
276:                 "same_action": True,
277:                 "common_categories": ["code_structure", "naming_conventions"],
278:                 "divergent_categories": ["error_handling"]
279:             }
280:         }
281: 
282:     Raises:
283:         ValueError: If either PRP not found or missing drift decision
284:     """
285:     decision_1 = show_drift_decision(prp_id_1)
286:     decision_2 = show_drift_decision(prp_id_2)
287: 
288:     # Calculate comparison
289:     score_diff = abs(
290:         decision_1["drift_decision"]["score"] -
291:         decision_2["drift_decision"]["score"]
292:     )
293: 
294:     same_action = (
295:         decision_1["drift_decision"]["action"] ==
296:         decision_2["drift_decision"]["action"]
297:     )
298: 
299:     # Category comparison
300:     cat_1 = set(decision_1["drift_decision"].get("category_breakdown", {}).keys())
301:     cat_2 = set(decision_2["drift_decision"].get("category_breakdown", {}).keys())
302: 
303:     common_categories = list(cat_1 & cat_2)
304:     divergent_categories = list(cat_1 ^ cat_2)
305: 
306:     return {
307:         "prp_1": decision_1,
308:         "prp_2": decision_2,
309:         "comparison": {
310:             "score_diff": round(score_diff, 2),
311:             "same_action": same_action,
312:             "common_categories": common_categories,
313:             "divergent_categories": divergent_categories
314:         }
315:     }
</file>

<file path="tools/ce/exceptions.py">
 1: """Custom exceptions for PRP execution orchestration."""
 2: 
 3: 
 4: class EscalationRequired(Exception):
 5:     """Raised when automatic self-healing fails and human intervention is needed.
 6: 
 7:     Attributes:
 8:         reason: Escalation trigger reason (persistent_error, ambiguous, architecture, dependencies, security)
 9:         error: Parsed error dict that triggered escalation
10:         troubleshooting: Multi-line troubleshooting guidance for user
11:     """
12: 
13:     def __init__(
14:         self,
15:         reason: str,
16:         error: dict,
17:         troubleshooting: str
18:     ):
19:         self.reason = reason
20:         self.error = error
21:         self.troubleshooting = troubleshooting
22: 
23:         # Format error message
24:         error_type = error.get("type", "unknown")
25:         error_msg = error.get("message", "No message")
26:         error_loc = f"{error.get('file', 'unknown')}:{error.get('line', '?')}"
27: 
28:         message = (
29:             f"Escalation required ({reason})\n"
30:             f"Error type: {error_type}\n"
31:             f"Location: {error_loc}\n"
32:             f"Message: {error_msg}\n\n"
33:             f"üîß Troubleshooting:\n{troubleshooting}"
34:         )
35: 
36:         super().__init__(message)
37: 
38: 
39: class BlueprintParseError(ValueError):
40:     """Raised when PRP blueprint parsing fails."""
41: 
42:     def __init__(self, prp_path: str, issue: str):
43:         message = (
44:             f"Failed to parse PRP blueprint: {prp_path}\n"
45:             f"Issue: {issue}\n"
46:             f"üîß Troubleshooting: Ensure PRP has well-formed IMPLEMENTATION BLUEPRINT section"
47:         )
48:         super().__init__(message)
49: 
50: 
51: class ValidationError(RuntimeError):
52:     """Raised when validation fails after max attempts."""
53: 
54:     def __init__(self, level: str, error_details: dict):
55:         self.level = level
56:         self.error_details = error_details
57: 
58:         message = (
59:             f"Validation failed at Level {level}\n"
60:             f"Attempts: {error_details.get('attempts', 0)}\n"
61:             f"Last error: {error_details.get('last_error', 'Unknown')}\n"
62:             f"üîß Troubleshooting: Review validation output for specific errors"
63:         )
64:         super().__init__(message)
65: 
66: 
67: class ContextDriftError(RuntimeError):
68:     """Raised when context drift exceeds acceptable threshold.
69: 
70:     Attributes:
71:         drift_score: Drift percentage (0-100)
72:         threshold: Threshold that was exceeded
73:         troubleshooting: Multi-line troubleshooting guidance
74:     """
75: 
76:     def __init__(self, drift_score: float, threshold: float, troubleshooting: str):
77:         self.drift_score = drift_score
78:         self.threshold = threshold
79:         self.troubleshooting = troubleshooting
80: 
81:         message = (
82:             f"Context drift too high: {drift_score:.1f}% (threshold: {threshold:.1f}%)\n"
83:             f"üîß Troubleshooting:\n{troubleshooting}"
84:         )
85:         super().__init__(message)
</file>

<file path="tools/ce/execute.py">
  1: """PRP execution orchestration with phase-by-phase implementation and self-healing.
  2: 
  3: Testing Strategy:
  4:     This module achieves 54% line coverage (263/487 statements), focusing on comprehensive
  5:     testing of core utility functions rather than integration orchestration.
  6: 
  7:     Coverage Breakdown:
  8:         ‚úÖ Core Utilities (100% coverage):
  9:            - parse_validation_error(): 7 tests covering all error types
 10:              (ImportError, AssertionError, SyntaxError, TypeError, NameError)
 11:            - apply_self_healing_fix(): 4 tests with real file operations
 12:            - check_escalation_triggers(): 7 tests for all 5 trigger conditions
 13:            - _add_import_statement(): 2 tests for import positioning
 14:            - escalate_to_human(): 2 tests for exception raising
 15: 
 16:         ‚ö†Ô∏è  Integration Orchestration (0% coverage):
 17:            - run_validation_loop() (lines 727-902): 176 lines
 18:            - execute_prp() (lines 359-497): 139 lines
 19: 
 20:            Rationale: These functions require complex mocking (10+ patches per test)
 21:            due to dynamic imports, state management across retry loops, and multiple
 22:            external dependencies. Better suited for E2E testing with real validation
 23:            scenarios rather than unit tests.
 24: 
 25:     Quality Assurance:
 26:         - All tests follow "Real Functionality Testing" policy (no hardcoded success)
 27:         - Self-healing tests use real file operations (tempfile, not mocks)
 28:         - Error parsing tests use realistic error output samples
 29:         - Escalation trigger tests verify all 5 escalation conditions
 30:         - 33/33 tests passing with pytest
 31: 
 32:     Future Testing:
 33:         - Integration tests for run_validation_loop() with real test projects
 34:         - E2E tests for execute_prp() with full PRP execution scenarios
 35:         - Performance tests for validation retry loops
 36: """
 37: 
 38: import re
 39: from typing import Dict, Any, List, Optional
 40: 
 41: from .exceptions import EscalationRequired
 42: from .blueprint_parser import parse_blueprint
 43: from .validation_loop import (
 44:     run_validation_loop,
 45:     calculate_confidence_score,
 46:     parse_validation_error,
 47:     check_escalation_triggers,
 48:     apply_self_healing_fix,
 49:     escalate_to_human
 50: )
 51: 
 52: 
 53: # ============================================================================
 54: # Phase 1: Blueprint parsing moved to blueprint_parser.py (imported above)
 55: # ============================================================================
 56: 
 57: 
 58: # ============================================================================
 59: # Phase 2: Execution Orchestration Functions
 60: # ============================================================================
 61: 
 62: def execute_prp(
 63:     prp_id: str,
 64:     start_phase: Optional[int] = None,
 65:     end_phase: Optional[int] = None,
 66:     skip_validation: bool = False,
 67:     dry_run: bool = False
 68: ) -> Dict[str, Any]:
 69:     """Main execution function - orchestrates PRP implementation.
 70: 
 71:     Args:
 72:         prp_id: PRP identifier (e.g., "PRP-4")
 73:         start_phase: Optional phase to start from (None = Phase 1)
 74:         end_phase: Optional phase to end at (None = all phases)
 75:         skip_validation: Skip validation loops (dangerous - for debugging only)
 76:         dry_run: Parse blueprint and return phases without execution
 77: 
 78:     Returns:
 79:         {
 80:             "success": True,
 81:             "prp_id": "PRP-4",
 82:             "phases_completed": 3,
 83:             "validation_results": {
 84:                 "L1": {"passed": True, "attempts": 1},
 85:                 "L2": {"passed": True, "attempts": 2},
 86:                 "L3": {"passed": True, "attempts": 1},
 87:                 "L4": {"passed": True, "attempts": 1}
 88:             },
 89:             "checkpoints_created": ["checkpoint-PRP-4-phase1", ...],
 90:             "confidence_score": "10/10",
 91:             "execution_time": "45m 23s"
 92:         }
 93: 
 94:     Raises:
 95:         RuntimeError: If execution fails after escalation
 96:         FileNotFoundError: If PRP file not found
 97: 
 98:     Process:
 99:         1. Initialize PRP context: ce prp start <prp_id>
100:         2. Parse blueprint: parse_blueprint(prp_path)
101:         3. Filter phases: start_phase to end_phase
102:         4. Handle dry-run: If dry_run=True, return parsed blueprint without execution
103:         5. For each phase:
104:            a. Update phase in state: update_prp_phase(phase_name)
105:            b. Execute phase: execute_phase(phase)
106:            c. Run validation loop: run_validation_loop(phase) (unless skip_validation)
107:            d. Create checkpoint: create_checkpoint(phase)
108:            e. Update validation attempts in state
109:         6. Calculate confidence score
110:         7. End PRP context: ce prp end <prp_id>
111:         8. Return execution summary
112:     """
113:     import time
114:     from .prp import start_prp, end_prp, update_prp_phase, create_checkpoint
115: 
116:     start_time = time.time()
117: 
118:     # Find PRP file
119:     prp_path = _find_prp_file(prp_id)
120: 
121:     # Parse blueprint
122:     phases = parse_blueprint(prp_path)
123: 
124:     # Filter phases
125:     if start_phase:
126:         phases = [p for p in phases if p["phase_number"] >= start_phase]
127:     if end_phase:
128:         phases = [p for p in phases if p["phase_number"] <= end_phase]
129: 
130:     if not phases:
131:         raise RuntimeError(
132:             f"No phases to execute (start={start_phase}, end={end_phase})\n"
133:             f"üîß Troubleshooting: Check phase numbers in PRP"
134:         )
135: 
136:     # Dry run - return parsed blueprint
137:     if dry_run:
138:         return {
139:             "success": True,
140:             "dry_run": True,
141:             "prp_id": prp_id,
142:             "phases": phases,
143:             "total_phases": len(phases)
144:         }
145: 
146:     # Initialize PRP context
147:     prp_name = phases[0]["phase_name"] if phases else prp_id
148:     start_result = start_prp(prp_id, prp_name)
149: 
150:     # Track execution state
151:     phases_completed = 0
152:     checkpoints_created = []
153:     validation_results = {}
154: 
155:     try:
156:         # Execute each phase
157:         for phase in phases:
158:             phase_num = phase["phase_number"]
159:             phase_name = phase["phase_name"]
160: 
161:             print(f"\n{'='*80}")
162:             print(f"Phase {phase_num}: {phase_name}")
163:             print(f"Goal: {phase['goal']}")
164:             print(f"{'='*80}\n")
165: 
166:             # Update phase in state
167:             update_prp_phase(f"phase{phase_num}")
168: 
169:             # Execute phase
170:             exec_result = execute_phase(phase)
171:             if not exec_result["success"]:
172:                 raise RuntimeError(
173:                     f"Phase {phase_num} execution failed: {exec_result.get('error', 'Unknown error')}\n"
174:                     f"üîß Troubleshooting: Check phase implementation logic"
175:                 )
176: 
177:             # Run validation loop (unless skipped)
178:             if not skip_validation and phase.get("validation_command"):
179:                 val_result = run_validation_loop(phase, prp_path)
180:                 validation_results[f"Phase{phase_num}"] = val_result
181: 
182:                 if not val_result["success"]:
183:                     raise RuntimeError(
184:                         f"Phase {phase_num} validation failed after {val_result.get('attempts', 0)} attempts\n"
185:                         f"üîß Troubleshooting: Review validation errors"
186:                     )
187: 
188:             # Create checkpoint
189:             checkpoint_result = create_checkpoint(
190:                 f"phase{phase_num}",
191:                 f"Phase {phase_num} complete: {phase_name}"
192:             )
193:             checkpoints_created.append(checkpoint_result["tag_name"])
194: 
195:             phases_completed += 1
196:             print(f"\n‚úÖ Phase {phase_num} complete\n")
197: 
198:         # Calculate confidence score
199:         confidence_score = calculate_confidence_score(validation_results)
200: 
201:         # Calculate execution time
202:         duration_seconds = time.time() - start_time
203:         hours = int(duration_seconds // 3600)
204:         minutes = int((duration_seconds % 3600) // 60)
205:         seconds = int(duration_seconds % 60)
206: 
207:         if hours > 0:
208:             execution_time = f"{hours}h {minutes}m {seconds}s"
209:         elif minutes > 0:
210:             execution_time = f"{minutes}m {seconds}s"
211:         else:
212:             execution_time = f"{seconds}s"
213: 
214:         # Step 6.5: Post-execution sync (if auto-sync enabled)
215:         from .context import is_auto_sync_enabled, post_execution_sync
216:         if is_auto_sync_enabled():
217:             try:
218:                 print(f"\n{'='*80}")
219:                 print("Running post-execution sync...")
220:                 print(f"{'='*80}\n")
221:                 sync_result = post_execution_sync(prp_id, skip_cleanup=False)
222:                 print(f"‚úÖ Post-sync complete: drift={sync_result['drift_score']:.1f}%")
223:                 print(f"   Cleanup: {sync_result['cleanup_completed']}")
224:                 print(f"   Memories archived: {sync_result['memories_archived']}")
225:                 print(f"   Final checkpoint: {sync_result.get('final_checkpoint', 'N/A')}")
226:             except Exception as e:
227:                 # Non-blocking - log warning but allow execution to complete
228:                 print(f"‚ö†Ô∏è  Post-execution sync failed: {e}")
229:                 print(f"üîß Troubleshooting: Run 'ce context post-sync {prp_id}' manually")
230: 
231:         # End PRP context
232:         end_result = end_prp(prp_id)
233: 
234:         return {
235:             "success": True,
236:             "prp_id": prp_id,
237:             "phases_completed": phases_completed,
238:             "validation_results": validation_results,
239:             "checkpoints_created": checkpoints_created,
240:             "confidence_score": confidence_score,
241:             "execution_time": execution_time
242:         }
243: 
244:     except Exception as e:
245:         # On error, still try to end PRP context
246:         try:
247:             end_prp(prp_id)
248:         except Exception as cleanup_error:
249:             import logging
250:             logger = logging.getLogger(__name__)
251:             logger.warning(f"Failed to end PRP context during cleanup: {cleanup_error}")
252:         raise
253: 
254: 
255: def execute_phase(phase: Dict[str, Any]) -> Dict[str, Any]:
256:     """Execute a single blueprint phase using Serena MCP for file operations.
257: 
258:     Args:
259:         phase: Parsed phase dict from parse_blueprint()
260: 
261:     Returns:
262:         {
263:             "success": True,
264:             "files_modified": ["src/auth.py"],
265:             "files_created": ["src/models/user.py"],
266:             "functions_added": ["authenticate", "validate_token"],
267:             "duration": "12m 34s",
268:             "error": "Error message if success=False"
269:         }
270: 
271:     Process:
272:         1. Create files listed in files_to_create using Serena MCP (or fallback)
273:         2. Modify files listed in files_to_modify using Serena MCP (or fallback)
274:         3. Implement functions from function signatures
275:         4. Log progress to console (shows method used: mcp vs filesystem)
276:         5. Return execution summary
277: 
278:     Implementation Strategy:
279:         - Use Serena MCP when available for symbol-aware code insertion
280:         - Graceful fallback to filesystem operations when MCP unavailable
281:         - Use function signatures as implementation guides
282:         - Follow approach description for implementation style
283:         - Reference goal for context
284: 
285:     MCP Integration (PRP-9):
286:         - mcp_adapter.py provides abstraction layer
287:         - File creation: create_file_with_mcp() tries MCP, falls back to filesystem
288:         - Code insertion: insert_code_with_mcp() uses symbol-aware MCP or naive append
289:         - Console output shows method used (mcp/mcp_symbol_aware/filesystem_append)
290:     """
291:     import time
292: 
293:     start_time = time.time()
294: 
295:     files_created = []
296:     files_modified = []
297:     functions_added = []
298: 
299:     try:
300:         # Create new files
301:         for file_entry in phase.get("files_to_create", []):
302:             filepath = file_entry["path"]
303:             description = file_entry["description"]
304:             print(f"  üìù Create: {filepath} - {description}")
305: 
306:             # Generate initial file content based on description and functions
307:             content = _generate_file_content(filepath, description, phase)
308: 
309:             # Create file using Serena MCP or fallback to filesystem
310:             from .mcp_adapter import create_file_with_mcp
311:             result = create_file_with_mcp(filepath, content)
312: 
313:             if not result["success"]:
314:                 raise RuntimeError(
315:                     f"Failed to create {filepath}: {result.get('error')}\n"
316:                     f"üîß Troubleshooting:\n"
317:                     f"  1. Verify parent directory exists and is writable\n"
318:                     f"  2. Check file path doesn't contain invalid characters\n"
319:                     f"  3. Ensure Serena MCP is available (fallback may fail)\n"
320:                     f"  4. Review phase files_to_create list for accuracy"
321:                 )
322: 
323:             files_created.append(filepath)
324:             print(f"    ‚úì Created via {result['method']}: {filepath}")
325: 
326:         # Modify existing files
327:         for file_entry in phase.get("files_to_modify", []):
328:             filepath = file_entry["path"]
329:             description = file_entry["description"]
330:             print(f"  ‚úèÔ∏è  Modify: {filepath} - {description}")
331: 
332:             # Add functions to existing file
333:             _add_functions_to_file(filepath, phase.get("functions", []), phase)
334: 
335:             files_modified.append(filepath)
336: 
337:         # Track implemented functions
338:         for func_entry in phase.get("functions", []):
339:             signature = func_entry["signature"]
340:             func_name_match = re.search(r'(?:def|class)\s+(\w+)', signature)
341:             if func_name_match:
342:                 func_name = func_name_match.group(1)
343:                 print(f"  üîß Implement: {func_name}")
344:                 functions_added.append(func_name)
345: 
346:         duration = time.time() - start_time
347: 
348:         return {
349:             "success": True,
350:             "files_created": files_created,
351:             "files_modified": files_modified,
352:             "functions_added": functions_added,
353:             "duration": f"{duration:.2f}s"
354:         }
355: 
356:     except Exception as e:
357:         duration = time.time() - start_time
358:         raise RuntimeError(
359:             f"Phase execution failed after {duration:.2f}s\n"
360:             f"Error: {str(e)}\n"
361:             f"Files created: {files_created}\n"
362:             f"Files modified: {files_modified}\n"
363:             f"üîß Troubleshooting:\n"
364:             f"  1. Check if file paths are valid\n"
365:             f"  2. Verify Serena MCP is available\n"
366:             f"  3. Review function signatures for syntax errors\n"
367:             f"  4. Check phase goal and approach for clarity"
368:         ) from e
369: 
370: 
371: def _generate_file_content(filepath: str, description: str, phase: Dict[str, Any]) -> str:
372:     """Generate initial content for a new file based on context.
373: 
374:     Args:
375:         filepath: Path to file being created
376:         description: File description from phase
377:         phase: Phase context with goal, approach, functions
378: 
379:     Returns:
380:         Generated file content with module docstring and function stubs
381:     """
382:     lines = []
383: 
384:     # Add module docstring
385:     lines.append(f'"""{description}."""')
386:     lines.append("")
387: 
388:     # Add relevant functions for this file
389:     for func_entry in phase.get("functions", []):
390:         full_code = func_entry.get("full_code", "")
391:         if full_code:
392:             lines.append(full_code)
393:             lines.append("")
394:             lines.append("")
395: 
396:     # If no functions, add placeholder comment
397:     if not phase.get("functions"):
398:         lines.append(f"# {phase['goal']}")
399:         lines.append(f"# Approach: {phase['approach']}")
400: 
401:     return "\n".join(lines)
402: 
403: 
404: def _add_functions_to_file(filepath: str, functions: List[Dict[str, str]], phase: Dict[str, Any]) -> None:
405:     """Add functions to an existing file using Serena MCP.
406: 
407:     Args:
408:         filepath: Path to file to modify
409:         functions: List of function dicts with signature, docstring, full_code
410:         phase: Phase context
411: 
412:     Raises:
413:         RuntimeError: If file modification fails
414:     """
415:     if not functions:
416:         return
417: 
418:     # Use Serena MCP for symbol-aware insertion or fallback to filesystem
419:     from .mcp_adapter import insert_code_with_mcp
420: 
421:     try:
422:         # Insert each function using symbol-aware insertion
423:         for func_entry in functions:
424:             full_code = func_entry.get("full_code", "")
425:             if full_code:
426:                 result = insert_code_with_mcp(
427:                     filepath=filepath,
428:                     code=full_code,
429:                     mode="after_last_symbol"  # Insert after last function/class
430:                 )
431: 
432:                 if not result["success"]:
433:                     raise RuntimeError(
434:                         f"Failed to insert code: {result.get('error')}\n"
435:                         f"üîß Troubleshooting:\n"
436:                         f"  1. Verify file exists and is writable: {filepath}\n"
437:                         f"  2. Check function code is syntactically valid\n"
438:                         f"  3. Ensure Serena MCP is available for symbol-aware insertion\n"
439:                         f"  4. Review phase functions list for correctness"
440:                     )
441: 
442:                 method = result["method"]
443:                 if method == "mcp_symbol_aware":
444:                     print(f"    ‚úì Inserted via MCP (after {result.get('symbol')})")
445:                 else:
446:                     print(f"    ‚úì Inserted via {method}")
447: 
448:     except Exception as e:
449:         raise RuntimeError(
450:             f"Failed to add functions to {filepath}\n"
451:             f"Error: {str(e)}\n"
452:             f"üîß Troubleshooting:\n"
453:             f"  1. Check file exists and is writable\n"
454:             f"  2. Verify function code is syntactically valid\n"
455:             f"  3. Review phase functions list"
456:         ) from e
457: 
458: 
459: def _find_prp_file(prp_id: str) -> str:
460:     """Find PRP file path from PRP ID.
461: 
462:     Args:
463:         prp_id: PRP identifier (e.g., "PRP-4")
464: 
465:     Returns:
466:         Absolute path to PRP file
467: 
468:     Raises:
469:         FileNotFoundError: If PRP file not found
470: 
471:     Search strategy:
472:         1. Check PRPs/feature-requests/PRP-{id}-*.md
473:         2. Check PRPs/executed/PRP-{id}-*.md
474:         3. Check PRPs/PRP-{id}-*.md
475:     """
476:     from pathlib import Path
477: 
478:     # Get project root (assuming we're in tools/ce/)
479:     project_root = Path(__file__).parent.parent.parent
480: 
481:     # Search locations
482:     search_paths = [
483:         project_root / "PRPs" / "feature-requests",
484:         project_root / "PRPs" / "executed",
485:         project_root / "PRPs"
486:     ]
487: 
488:     # Extract numeric ID (e.g., "PRP-4" -> "4")
489:     numeric_id = prp_id.replace("PRP-", "").replace("prp-", "")
490: 
491:     for search_dir in search_paths:
492:         if not search_dir.exists():
493:             continue
494: 
495:         # Look for PRP-{id}-*.md or PRP{id}-*.md
496:         patterns = [
497:             f"PRP-{numeric_id}-*.md",
498:             f"PRP{numeric_id}-*.md",
499:             f"prp-{numeric_id}-*.md"
500:         ]
501: 
502:         for pattern in patterns:
503:             matches = list(search_dir.glob(pattern))
504:             if matches:
505:                 return str(matches[0].absolute())
506: 
507:     raise FileNotFoundError(
508:         f"PRP file not found: {prp_id}\n"
509:         f"üîß Troubleshooting: Check PRPs/feature-requests/ or PRPs/executed/"
510:     )
</file>

<file path="tools/ce/generate.py">
   1: """PRP generation from INITIAL.md.
   2: 
   3: This module automates PRP (Product Requirements Prompt) generation by:
   4: 1. Parsing INITIAL.md structure (FEATURE, EXAMPLES, DOCUMENTATION, OTHER CONSIDERATIONS)
   5: 2. Orchestrating MCP tools (Serena, Context7, Sequential Thinking) for research
   6: 3. Synthesizing comprehensive PRP with all 6 sections
   7: 
   8: Usage:
   9:     from ce.generate import generate_prp
  10:     result = generate_prp("feature-requests/auth/INITIAL.md")
  11: """
  12: 
  13: import re
  14: import logging
  15: from pathlib import Path
  16: from typing import Dict, List, Any, Optional
  17: 
  18: logger = logging.getLogger(__name__)
  19: 
  20: # Section markers for INITIAL.md parsing
  21: SECTION_MARKERS = {
  22:     "feature": r"^##\s*FEATURE\s*$",
  23:     "planning": r"^##\s*PLANNING\s+CONTEXT\s*$",
  24:     "examples": r"^##\s*EXAMPLES\s*$",
  25:     "documentation": r"^##\s*DOCUMENTATION\s*$",
  26:     "other": r"^##\s*OTHER\s+CONSIDERATIONS\s*$"
  27: }
  28: 
  29: 
  30: def parse_initial_md(filepath: str) -> Dict[str, Any]:
  31:     """Parse INITIAL.md into structured sections.
  32: 
  33:     Args:
  34:         filepath: Path to INITIAL.md file
  35: 
  36:     Returns:
  37:         {
  38:             "feature_name": "User Authentication System",
  39:             "feature": "Build user auth with JWT tokens...",
  40:             "examples": [
  41:                 {"type": "inline", "language": "python", "code": "..."},
  42:                 {"type": "file_ref", "file": "src/auth.py", "lines": "42-67"}
  43:             ],
  44:             "documentation": [
  45:                 {"title": "JWT Guide", "url": "https://...", "type": "link"},
  46:                 {"title": "pytest", "url": "", "type": "library"}
  47:             ],
  48:             "other_considerations": "Security: Hash passwords with bcrypt...",
  49:             "raw_content": "<full file content>"
  50:         }
  51: 
  52:     Raises:
  53:         FileNotFoundError: If INITIAL.md doesn't exist
  54:         ValueError: If required sections missing (FEATURE, EXAMPLES)
  55: 
  56:     Process:
  57:         1. Read file content
  58:         2. Extract feature name from first heading
  59:         3. Split content by section markers (## FEATURE, ## EXAMPLES, etc.)
  60:         4. Parse EXAMPLES for code block references
  61:         5. Parse DOCUMENTATION for URL links
  62:         6. Validate FEATURE and EXAMPLES present (minimum required)
  63:     """
  64:     # Check file exists
  65:     file_path = Path(filepath)
  66:     if not file_path.exists():
  67:         raise FileNotFoundError(
  68:             f"INITIAL.md not found: {filepath}\n"
  69:             f"üîß Troubleshooting: Verify file path is correct and file exists"
  70:         )
  71: 
  72:     # Read content
  73:     content = file_path.read_text(encoding="utf-8")
  74: 
  75:     # Extract feature name from first heading (# Feature: <name>)
  76:     feature_name_match = re.search(r"^#\s+Feature:\s+(.+)$", content, re.MULTILINE)
  77:     if not feature_name_match:
  78:         raise ValueError(
  79:             f"Feature name not found in {filepath}\n"
  80:             f"üîß Troubleshooting: First line must be '# Feature: <name>'"
  81:         )
  82:     feature_name = feature_name_match.group(1).strip()
  83: 
  84:     # Split content by section markers
  85:     sections = {}
  86:     lines = content.split("\n")
  87:     current_section = None
  88:     section_content = []
  89: 
  90:     for line in lines:
  91:         # Check if line is a section marker
  92:         matched_section = None
  93:         for section_key, pattern in SECTION_MARKERS.items():
  94:             if re.match(pattern, line.strip()):
  95:                 # Save previous section if exists
  96:                 if current_section and section_content:
  97:                     sections[current_section] = "\n".join(section_content).strip()
  98:                     section_content = []
  99:                 current_section = section_key
 100:                 matched_section = section_key
 101:                 break
 102: 
 103:         # If not a section marker and we're in a section, accumulate content
 104:         if not matched_section and current_section:
 105:             section_content.append(line)
 106: 
 107:     # Save last section
 108:     if current_section and section_content:
 109:         sections[current_section] = "\n".join(section_content).strip()
 110: 
 111:     # Validate required sections
 112:     if "feature" not in sections:
 113:         raise ValueError(
 114:             f"Required FEATURE section missing in {filepath}\n"
 115:             f"üîß Troubleshooting: Add '## FEATURE' section with feature description"
 116:         )
 117:     if "examples" not in sections:
 118:         raise ValueError(
 119:             f"Required EXAMPLES section missing in {filepath}\n"
 120:             f"üîß Troubleshooting: Add '## EXAMPLES' section with code examples"
 121:         )
 122: 
 123:     # Parse subsections
 124:     return {
 125:         "feature_name": feature_name,
 126:         "feature": sections.get("feature", ""),
 127:         "planning_context": sections.get("planning", ""),
 128:         "examples": extract_code_examples(sections.get("examples", "")),
 129:         "documentation": extract_documentation_links(sections.get("documentation", "")),
 130:         "other_considerations": sections.get("other", ""),
 131:         "raw_content": content
 132:     }
 133: 
 134: 
 135: def extract_code_examples(examples_text: str) -> List[Dict[str, Any]]:
 136:     """Extract code examples from EXAMPLES section.
 137: 
 138:     Patterns supported:
 139:         - Inline code blocks with language tags
 140:         - File references (e.g., "See src/auth.py:42-67")
 141:         - Natural language descriptions
 142: 
 143:     Returns:
 144:         [
 145:             {"type": "inline", "language": "python", "code": "..."},
 146:             {"type": "file_ref", "file": "src/auth.py", "lines": "42-67"},
 147:             {"type": "description", "text": "Uses async/await pattern"}
 148:         ]
 149:     """
 150:     if not examples_text:
 151:         return []
 152: 
 153:     examples = []
 154: 
 155:     # Pattern 1: Inline code blocks with language tag
 156:     # Matches: ```python\ncode\n```
 157:     code_block_pattern = r"```(\w+)\n(.*?)```"
 158:     for match in re.finditer(code_block_pattern, examples_text, re.DOTALL):
 159:         language = match.group(1)
 160:         code = match.group(2).strip()
 161:         examples.append({
 162:             "type": "inline",
 163:             "language": language,
 164:             "code": code
 165:         })
 166: 
 167:     # Pattern 2: File references
 168:     # Matches: "See src/auth.py:42-67", "src/auth.py lines 42-67", etc.
 169:     file_ref_pattern = r"(?:See\s+)?([a-zA-Z0-9_/.-]+\.py)(?::|\s+lines?\s+)(\d+-\d+)"
 170:     for match in re.finditer(file_ref_pattern, examples_text):
 171:         file_path = match.group(1)
 172:         line_range = match.group(2)
 173:         examples.append({
 174:             "type": "file_ref",
 175:             "file": file_path,
 176:             "lines": line_range
 177:         })
 178: 
 179:     # Pattern 3: Natural language descriptions (paragraphs without code/file refs)
 180:     # Extract paragraphs not containing code blocks or file references
 181:     # Remove code blocks and file references from text
 182:     text_without_code = re.sub(code_block_pattern, "", examples_text, flags=re.DOTALL)
 183:     text_without_refs = re.sub(file_ref_pattern, "", text_without_code)
 184: 
 185:     # Split into paragraphs and filter non-empty
 186:     paragraphs = [p.strip() for p in text_without_refs.split("\n\n") if p.strip()]
 187:     for paragraph in paragraphs:
 188:         # Skip very short paragraphs (likely headers or fragments)
 189:         if len(paragraph) > 20:
 190:             examples.append({
 191:                 "type": "description",
 192:                 "text": paragraph
 193:             })
 194: 
 195:     return examples
 196: 
 197: 
 198: def extract_documentation_links(docs_text: str) -> List[Dict[str, str]]:
 199:     """Extract documentation URLs from DOCUMENTATION section.
 200: 
 201:     Patterns supported:
 202:         - Markdown links: [Title](url)
 203:         - Plain URLs: https://...
 204:         - Library names: "FastAPI", "pytest"
 205: 
 206:     Returns:
 207:         [
 208:             {"title": "FastAPI Docs", "url": "https://...", "type": "link"},
 209:             {"title": "pytest", "url": "", "type": "library"}
 210:         ]
 211:     """
 212:     if not docs_text:
 213:         return []
 214: 
 215:     doc_links = []
 216: 
 217:     # Pattern 1: Markdown links [Title](url)
 218:     markdown_link_pattern = r"\[([^\]]+)\]\(([^\)]+)\)"
 219:     for match in re.finditer(markdown_link_pattern, docs_text):
 220:         title = match.group(1).strip()
 221:         url = match.group(2).strip()
 222:         doc_links.append({
 223:             "title": title,
 224:             "url": url,
 225:             "type": "link"
 226:         })
 227: 
 228:     # Pattern 2: Plain URLs (https://... or http://...)
 229:     plain_url_pattern = r"(https?://[^\s\)]+)"
 230:     # Only extract URLs not already captured by markdown links
 231:     text_without_markdown = re.sub(markdown_link_pattern, "", docs_text)
 232:     for match in re.finditer(plain_url_pattern, text_without_markdown):
 233:         url = match.group(1).strip()
 234:         # Use domain as title
 235:         domain = url.split("/")[2]
 236:         doc_links.append({
 237:             "title": domain,
 238:             "url": url,
 239:             "type": "link"
 240:         })
 241: 
 242:     # Pattern 3: Library names (words in quotes or standalone)
 243:     # Matches: "FastAPI", "pytest", FastAPI, pytest
 244:     # This is heuristic - captures quoted words or capitalized words likely to be library names
 245:     library_pattern = r"[\"']([A-Za-z0-9_-]+)[\"']|(?:^|\s)([A-Z][a-zA-Z0-9_-]+)(?:\s|$)"
 246:     text_without_urls = re.sub(plain_url_pattern, "", text_without_markdown)
 247:     for match in re.finditer(library_pattern, text_without_urls):
 248:         library_name = match.group(1) or match.group(2)
 249:         if library_name:
 250:             # Only add if not already in doc_links
 251:             if not any(lib["title"] == library_name for lib in doc_links):
 252:                 doc_links.append({
 253:                     "title": library_name,
 254:                     "url": "",
 255:                     "type": "library"
 256:                 })
 257: 
 258:     return doc_links
 259: 
 260: 
 261: # =============================================================================
 262: # Phase 2: Serena Research Orchestration
 263: # =============================================================================
 264: 
 265: 
 266: def research_codebase(
 267:     feature_name: str,
 268:     examples: List[Dict[str, Any]],
 269:     initial_context: str
 270: ) -> Dict[str, Any]:
 271:     """Orchestrate codebase research using Serena MCP.
 272: 
 273:     Args:
 274:         feature_name: Target feature name (e.g., "User Authentication")
 275:         examples: Parsed EXAMPLES from INITIAL.md
 276:         initial_context: FEATURE section text for context
 277: 
 278:     Returns:
 279:         {
 280:             "related_files": ["src/auth.py", "src/models/user.py"],
 281:             "patterns": [
 282:                 {"pattern": "async/await", "locations": ["src/auth.py:42"]},
 283:                 {"pattern": "JWT validation", "locations": ["src/auth.py:67"]}
 284:             ],
 285:             "similar_implementations": [
 286:                 {
 287:                     "file": "src/oauth.py",
 288:                     "symbol": "OAuthHandler/authenticate",
 289:                     "code": "...",
 290:                     "relevance": "Similar authentication flow"
 291:                 }
 292:             ],
 293:             "test_patterns": [
 294:                 {"file": "tests/test_auth.py", "pattern": "pytest fixtures"}
 295:             ],
 296:             "architecture": {
 297:                 "layer": "authentication",
 298:                 "dependencies": ["jwt", "bcrypt"],
 299:                 "conventions": ["snake_case", "async functions"]
 300:             }
 301:         }
 302: 
 303:     Raises:
 304:         RuntimeError: If Serena MCP unavailable (non-blocking - log warning, return empty results)
 305: 
 306:     Process:
 307:         1. Extract keywords from feature_name (e.g., "authentication", "JWT")
 308:         2. Search for patterns: mcp__serena__search_for_pattern(keywords)
 309:         3. Discover symbols: mcp__serena__find_symbol(related_classes)
 310:         4. Get detailed code: mcp__serena__find_symbol(include_body=True)
 311:         5. Find references: mcp__serena__find_referencing_symbols(key_functions)
 312:         6. Infer architecture: Analyze file structure and imports
 313:         7. Detect test patterns: Look for pytest/unittest in tests/
 314:     """
 315:     logger.info(f"Starting codebase research for: {feature_name}")
 316: 
 317:     # Initialize result structure
 318:     result = {
 319:         "related_files": [],
 320:         "patterns": [],
 321:         "similar_implementations": [],
 322:         "test_patterns": [],
 323:         "architecture": {
 324:             "layer": "",
 325:             "dependencies": [],
 326:             "conventions": []
 327:         },
 328:         "serena_available": False
 329:     }
 330: 
 331:     try:
 332:         # Check if Serena MCP is available by attempting import
 333:         # In production, this would be: from mcp import serena
 334:         # For now, we'll gracefully handle unavailability
 335:         logger.info("Serena MCP research would execute here (graceful degradation)")
 336: 
 337:         # Extract keywords from feature name
 338:         keywords = _extract_keywords(feature_name)
 339:         logger.info(f"Extracted keywords: {keywords}")
 340: 
 341:         # Search for similar patterns
 342:         patterns = search_similar_patterns(keywords)
 343:         result["patterns"] = patterns
 344: 
 345:         # Infer test patterns
 346:         test_patterns = infer_test_patterns({})
 347:         result["test_patterns"] = test_patterns
 348: 
 349:         result["serena_available"] = False  # Will be True when MCP integrated
 350: 
 351:     except Exception as e:
 352:         logger.warning(f"Serena MCP unavailable or error during research: {e}")
 353:         logger.warning("Continuing with reduced research functionality")
 354: 
 355:     return result
 356: 
 357: 
 358: def search_similar_patterns(keywords: List[str], path: str = ".") -> List[Dict[str, Any]]:
 359:     """Search for similar code patterns using keywords.
 360: 
 361:     Uses: mcp__serena__search_for_pattern
 362: 
 363:     Args:
 364:         keywords: Search terms (e.g., ["authenticate", "JWT", "token"])
 365:         path: Search scope (default: entire project)
 366: 
 367:     Returns:
 368:         [
 369:             {"file": "src/auth.py", "line": 42, "snippet": "..."},
 370:             {"file": "src/oauth.py", "line": 67, "snippet": "..."}
 371:         ]
 372:     """
 373:     logger.info(f"Searching for patterns with keywords: {keywords}")
 374: 
 375:     # Graceful degradation when Serena unavailable
 376:     patterns = []
 377: 
 378:     try:
 379:         # This would use: mcp__serena__search_for_pattern(pattern="|".join(keywords))
 380:         # For now, return empty (will be populated when MCP integrated)
 381:         logger.info("Pattern search would execute via Serena MCP")
 382:     except Exception as e:
 383:         logger.warning(f"Pattern search unavailable: {e}")
 384: 
 385:     return patterns
 386: 
 387: 
 388: def analyze_symbol_structure(symbol_name: str, file_path: str) -> Dict[str, Any]:
 389:     """Get detailed symbol information.
 390: 
 391:     Uses: mcp__serena__find_symbol, mcp__serena__get_symbols_overview
 392: 
 393:     Args:
 394:         symbol_name: Class/function name
 395:         file_path: File containing symbol
 396: 
 397:     Returns:
 398:         {
 399:             "name": "AuthHandler",
 400:             "type": "class",
 401:             "methods": ["authenticate", "validate_token", "refresh"],
 402:             "code": "<full class body>",
 403:             "references": 5
 404:         }
 405:     """
 406:     logger.info(f"Analyzing symbol: {symbol_name} in {file_path}")
 407: 
 408:     # Graceful degradation
 409:     result = {
 410:         "name": symbol_name,
 411:         "type": "unknown",
 412:         "methods": [],
 413:         "code": "",
 414:         "references": 0
 415:     }
 416: 
 417:     try:
 418:         # Would use: mcp__serena__find_symbol(name_path=symbol_name, relative_path=file_path)
 419:         logger.info("Symbol analysis would execute via Serena MCP")
 420:     except Exception as e:
 421:         logger.warning(f"Symbol analysis unavailable: {e}")
 422: 
 423:     return result
 424: 
 425: 
 426: def infer_test_patterns(project_structure: Dict[str, Any]) -> List[Dict[str, str]]:
 427:     """Detect test framework and patterns.
 428: 
 429:     Process:
 430:         1. Look for pytest.ini, setup.cfg, pyproject.toml
 431:         2. Search for test files (test_*.py, *_test.py)
 432:         3. Analyze test imports (pytest, unittest, nose)
 433:         4. Extract test command from pyproject.toml or tox.ini
 434: 
 435:     Returns:
 436:         [
 437:             {
 438:                 "framework": "pytest",
 439:                 "test_command": "pytest tests/ -v",
 440:                 "patterns": ["fixtures", "parametrize", "async tests"],
 441:                 "coverage_required": True
 442:             }
 443:         ]
 444:     """
 445:     logger.info("Inferring test patterns from project structure")
 446: 
 447:     # Check for pytest.ini, pyproject.toml
 448:     test_patterns = []
 449: 
 450:     # Default pytest pattern (most Python projects)
 451:     default_pattern = {
 452:         "framework": "pytest",
 453:         "test_command": "uv run pytest tests/ -v",
 454:         "patterns": ["fixtures", "parametrize"],
 455:         "coverage_required": True
 456:     }
 457:     test_patterns.append(default_pattern)
 458: 
 459:     return test_patterns
 460: 
 461: 
 462: def _extract_keywords(text: str) -> List[str]:
 463:     """Extract keywords from feature name or description.
 464: 
 465:     Args:
 466:         text: Feature name or description
 467: 
 468:     Returns:
 469:         List of keywords (lowercase, deduplicated)
 470:     """
 471:     # Simple keyword extraction - split by spaces, lowercase, remove common words
 472:     stop_words = {"a", "an", "the", "and", "or", "but", "with", "for", "to", "of", "in", "on"}
 473:     words = re.findall(r'\b\w+\b', text.lower())
 474:     keywords = [w for w in words if w not in stop_words and len(w) > 2]
 475:     return list(set(keywords))  # Deduplicate
 476: 
 477: 
 478: # =============================================================================
 479: # Phase 3: Context7 Integration
 480: # =============================================================================
 481: 
 482: 
 483: def fetch_documentation(
 484:     documentation_links: List[Dict[str, str]],
 485:     feature_context: str,
 486:     serena_research: Dict[str, Any]
 487: ) -> Dict[str, Any]:
 488:     """Fetch documentation using Context7 MCP.
 489: 
 490:     Args:
 491:         documentation_links: Parsed from INITIAL.md DOCUMENTATION section
 492:             [{"title": "FastAPI", "url": "", "type": "library"}, ...]
 493:         feature_context: FEATURE section text for topic extraction
 494:         serena_research: Results from research_codebase() for additional context
 495: 
 496:     Returns:
 497:         {
 498:             "library_docs": [
 499:                 {
 500:                     "library_name": "FastAPI",
 501:                     "library_id": "/tiangolo/fastapi",
 502:                     "topics": ["routing", "security", "dependencies"],
 503:                     "content": "<fetched markdown docs>",
 504:                     "tokens_used": 5000
 505:                 }
 506:             ],
 507:             "external_links": [
 508:                 {
 509:                     "title": "JWT Best Practices",
 510:                     "url": "https://jwt.io/introduction",
 511:                     "content": "<fetched content via WebFetch>",
 512:                     "relevant_sections": ["token structure", "security"]
 513:                 }
 514:             ],
 515:             "context7_available": False,
 516:             "sequential_thinking_available": False
 517:         }
 518: 
 519:     Raises:
 520:         RuntimeError: If Context7 MCP unavailable (non-blocking - log warning, return empty)
 521: 
 522:     Process:
 523:         1. Extract topics from feature_context using Sequential Thinking MCP
 524:         2. Resolve library names to Context7 IDs: mcp__context7__resolve-library-id
 525:         3. Fetch docs: mcp__context7__get-library-docs(library_id, topics)
 526:         4. Fetch external links: WebFetch tool for URLs
 527:         5. Synthesize relevance scores
 528:     """
 529:     logger.info("Starting documentation fetch with Context7 and Sequential Thinking")
 530: 
 531:     # Initialize result structure
 532:     result = {
 533:         "library_docs": [],
 534:         "external_links": [],
 535:         "context7_available": False,
 536:         "sequential_thinking_available": False
 537:     }
 538: 
 539:     try:
 540:         # Extract topics from feature context using Sequential Thinking
 541:         topics = extract_topics_from_feature(feature_context, serena_research)
 542:         logger.info(f"Extracted topics: {topics}")
 543: 
 544:         # Resolve library IDs and fetch docs
 545:         libraries = [doc for doc in documentation_links if doc["type"] == "library"]
 546:         for lib in libraries:
 547:             lib_result = resolve_and_fetch_library_docs(
 548:                 lib["title"],
 549:                 topics,
 550:                 feature_context
 551:             )
 552:             if lib_result:
 553:                 result["library_docs"].append(lib_result)
 554: 
 555:         # Fetch external link content
 556:         external_links = [doc for doc in documentation_links if doc["type"] == "link"]
 557:         for link in external_links:
 558:             link_result = fetch_external_link(link["url"], link["title"], topics)
 559:             if link_result:
 560:                 result["external_links"].append(link_result)
 561: 
 562:         result["context7_available"] = False  # Will be True when MCP integrated
 563:         result["sequential_thinking_available"] = False
 564: 
 565:     except Exception as e:
 566:         logger.warning(f"Context7/Sequential Thinking MCP unavailable: {e}")
 567:         logger.warning("Continuing with reduced documentation functionality")
 568: 
 569:     return result
 570: 
 571: 
 572: def extract_topics_from_feature(
 573:     feature_text: str,
 574:     serena_research: Dict[str, Any]
 575: ) -> List[str]:
 576:     """Extract documentation topics using Sequential Thinking MCP.
 577: 
 578:     Uses: mcp__syntropy__thinking__sequentialthinking
 579: 
 580:     Args:
 581:         feature_text: FEATURE section from INITIAL.md
 582:         serena_research: Codebase research results for additional context
 583: 
 584:     Returns:
 585:         List of topics (e.g., ["routing", "security", "async", "testing"])
 586: 
 587:     Process:
 588:         1. Call Sequential Thinking MCP with feature analysis prompt
 589:         2. Extract topics from reasoning chain
 590:         3. Deduplicate and filter to 3-5 most relevant topics
 591:         4. Fall back to heuristic if MCP unavailable
 592:     """
 593:     logger.info("Extracting topics from feature text using Sequential Thinking")
 594: 
 595:     # Check if sequential thinking is enabled
 596:     import os
 597:     use_thinking = os.environ.get('CE_USE_SEQUENTIAL_THINKING', 'true').lower() == 'true'
 598: 
 599:     if not use_thinking:
 600:         logger.info("Sequential thinking disabled via --no-thinking flag")
 601:         return _extract_topics_heuristic(feature_text)
 602: 
 603:     try:
 604:         from .mcp_utils import call_syntropy_mcp
 605: 
 606:         prompt = f"""Analyze this feature description and identify 3-5 key technical topics that would need documentation:
 607: 
 608: Feature: {feature_text}
 609: 
 610: Codebase Context:
 611: - Related patterns: {len(serena_research.get('patterns', []))}
 612: - Test framework: {serena_research.get('test_patterns', [{}])[0].get('framework', 'unknown') if serena_research.get('test_patterns') else 'unknown'}
 613: 
 614: Think step-by-step about:
 615: 1. What technical areas does this feature touch? (e.g., authentication, async, database)
 616: 2. What documentation would help implement this? (e.g., library guides, API docs)
 617: 3. What are the 3-5 most critical topics to focus documentation on?
 618: 
 619: Return final answer as: TOPICS: topic1, topic2, topic3"""
 620: 
 621:         result = call_syntropy_mcp(
 622:             "thinking",
 623:             "sequentialthinking",
 624:             {
 625:                 "thought": prompt,
 626:                 "thoughtNumber": 1,
 627:                 "totalThoughts": 5,
 628:                 "nextThoughtNeeded": True
 629:             }
 630:         )
 631: 
 632:         # Log reasoning chain
 633:         _log_thinking_chain(result, "Topic Extraction")
 634: 
 635:         # Extract topics from result
 636:         topics = _extract_topics_from_thinking_result(result)
 637: 
 638:         if topics:
 639:             logger.info(f"Extracted topics (sequential thinking): {topics}")
 640:             return topics
 641: 
 642:     except Exception as e:
 643:         logger.warning(f"Sequential thinking unavailable: {e}")
 644:         logger.warning("Falling back to heuristic topic extraction")
 645: 
 646:     # Graceful degradation - heuristic approach
 647:     return _extract_topics_heuristic(feature_text)
 648: 
 649: 
 650: def _extract_topics_from_thinking_result(result: Dict[str, Any]) -> List[str]:
 651:     """Parse topics from sequential thinking result.
 652: 
 653:     Args:
 654:         result: MCP tool result
 655: 
 656:     Returns:
 657:         List of topics extracted from thinking chain
 658:     """
 659:     # Extract content from MCP result
 660:     content = ""
 661:     if isinstance(result, dict) and "content" in result:
 662:         if isinstance(result["content"], list):
 663:             for item in result["content"]:
 664:                 if isinstance(item, dict) and "text" in item:
 665:                     content += item["text"] + " "
 666:         elif isinstance(result["content"], str):
 667:             content = result["content"]
 668: 
 669:     # Look for TOPICS: pattern in result
 670:     topics_match = re.search(r"TOPICS:\s*(.+)", content, re.IGNORECASE)
 671:     if topics_match:
 672:         topics_str = topics_match.group(1)
 673:         # Split by comma and clean
 674:         topics = [t.strip() for t in topics_str.split(",")]
 675:         return topics[:5]  # Limit to 5
 676: 
 677:     return []
 678: 
 679: 
 680: def _log_thinking_chain(result: Dict[str, Any], context: str) -> None:
 681:     """Log sequential thinking reasoning chain.
 682: 
 683:     Args:
 684:         result: MCP result with thinking chain
 685:         context: Context label (e.g., "Topic Extraction")
 686:     """
 687:     logger.info(f"üß† Sequential Thinking Chain - {context}")
 688: 
 689:     # Extract content
 690:     content = ""
 691:     if isinstance(result, dict) and "content" in result:
 692:         if isinstance(result["content"], list):
 693:             for item in result["content"]:
 694:                 if isinstance(item, dict) and "text" in item:
 695:                     content += item["text"] + "\n"
 696: 
 697:     # Log each thought
 698:     thoughts = re.finditer(r"Thought (\d+):\s*(.+?)(?=Thought \d+:|\Z)", content, re.DOTALL)
 699:     for thought in thoughts:
 700:         thought_num = thought.group(1)
 701:         thought_text = thought.group(2).strip()[:200]  # First 200 chars
 702:         logger.info(f"  Thought {thought_num}: {thought_text}...")
 703: 
 704:     logger.info(f"üß† End of thinking chain - {context}")
 705: 
 706: 
 707: def _extract_topics_heuristic(feature_text: str) -> List[str]:
 708:     """Heuristic-based topic extraction (fallback).
 709: 
 710:     Args:
 711:         feature_text: Feature description text
 712: 
 713:     Returns:
 714:         List of topics based on keyword matching
 715:     """
 716:     technical_terms = []
 717: 
 718:     # Common technical patterns to look for
 719:     patterns = {
 720:         "authentication": ["auth", "login", "jwt", "oauth", "token"],
 721:         "database": ["database", "sql", "nosql", "query", "model"],
 722:         "api": ["api", "rest", "graphql", "endpoint", "route"],
 723:         "async": ["async", "await", "concurrent", "parallel"],
 724:         "testing": ["test", "pytest", "unittest", "mock"],
 725:         "security": ["security", "encrypt", "hash", "bcrypt", "secure"],
 726:         "validation": ["validate", "validation", "schema", "verify"],
 727:     }
 728: 
 729:     feature_lower = feature_text.lower()
 730:     for topic, keywords in patterns.items():
 731:         if any(kw in feature_lower for kw in keywords):
 732:             technical_terms.append(topic)
 733: 
 734:     # Limit to 3-5 topics
 735:     topics = technical_terms[:5] if technical_terms else ["general"]
 736: 
 737:     logger.info(f"Extracted topics (heuristic): {topics}")
 738:     return topics
 739: 
 740: 
 741: def resolve_and_fetch_library_docs(
 742:     library_name: str,
 743:     topics: List[str],
 744:     feature_context: str,
 745:     max_tokens: int = 5000
 746: ) -> Dict[str, Any]:
 747:     """Resolve library ID and fetch documentation.
 748: 
 749:     Uses: mcp__context7__resolve-library-id, mcp__context7__get-library-docs
 750: 
 751:     Args:
 752:         library_name: Library to fetch (e.g., "FastAPI", "pytest")
 753:         topics: Topics to focus documentation (e.g., ["routing", "security"])
 754:         feature_context: Feature description for relevance filtering
 755:         max_tokens: Maximum tokens to retrieve
 756: 
 757:     Returns:
 758:         {
 759:             "library_name": "FastAPI",
 760:             "library_id": "/tiangolo/fastapi",
 761:             "topics": ["routing", "security"],
 762:             "content": "<markdown docs>",
 763:             "tokens_used": 4500
 764:         }
 765:         None if library not found or fetch fails
 766: 
 767:     Process:
 768:         1. resolve-library-id(library_name) ‚Üí library_id
 769:         2. get-library-docs(library_id, topics, max_tokens)
 770:         3. Return structured result
 771:     """
 772:     logger.info(f"Resolving and fetching docs for library: {library_name}")
 773: 
 774:     # Graceful degradation
 775:     try:
 776:         # Would use: mcp__context7__resolve-library-id(libraryName=library_name)
 777:         # Would use: mcp__context7__get-library-docs(context7CompatibleLibraryID=library_id, topic=topics, tokens=max_tokens)
 778:         logger.info(f"Context7 fetch would execute for {library_name}")
 779:         return None  # Return None when MCP unavailable
 780:     except Exception as e:
 781:         logger.warning(f"Failed to fetch docs for {library_name}: {e}")
 782:         return None
 783: 
 784: 
 785: def fetch_external_link(
 786:     url: str,
 787:     title: str,
 788:     topics: List[str]
 789: ) -> Dict[str, Any]:
 790:     """Fetch external documentation link using WebFetch.
 791: 
 792:     Uses: WebFetch tool
 793: 
 794:     Args:
 795:         url: URL to fetch
 796:         title: Link title from INITIAL.md
 797:         topics: Topics for relevance filtering
 798: 
 799:     Returns:
 800:         {
 801:             "title": "JWT Best Practices",
 802:             "url": "https://jwt.io/introduction",
 803:             "content": "<fetched markdown>",
 804:             "relevant_sections": ["token structure", "security"]
 805:         }
 806:         None if fetch fails
 807: 
 808:     Process:
 809:         1. WebFetch(url, prompt=f"Extract content relevant to: {topics}")
 810:         2. Parse and structure response
 811:         3. Identify relevant sections
 812:     """
 813:     logger.info(f"Fetching external link: {url}")
 814: 
 815:     # Graceful degradation
 816:     try:
 817:         # Would use: WebFetch(url=url, prompt=f"Extract documentation relevant to: {', '.join(topics)}")
 818:         logger.info(f"WebFetch would execute for {url}")
 819:         return None  # Return None for now
 820:     except Exception as e:
 821:         logger.warning(f"Failed to fetch {url}: {e}")
 822:         return None
 823: 
 824: 
 825: # =============================================================================
 826: # Phase 4: Template Engine
 827: # =============================================================================
 828: 
 829: 
 830: def generate_prp(
 831:     initial_md_path: str,
 832:     output_dir: str = "PRPs/feature-requests",
 833:     join_prp: Optional[str] = None
 834: ) -> str:
 835:     """Generate complete PRP from INITIAL.md.
 836: 
 837:     Main orchestration function that coordinates all phases.
 838: 
 839:     Args:
 840:         initial_md_path: Path to INITIAL.md file
 841:         output_dir: Directory for output PRP file
 842:         join_prp: Optional PRP to join (number, ID like 'PRP-12', or file path)
 843:                   If provided, updates existing PRP's Linear issue instead of creating new
 844: 
 845:     Returns:
 846:         Path to generated PRP file
 847: 
 848:     Raises:
 849:         FileNotFoundError: If INITIAL.md doesn't exist
 850:         ValueError: If INITIAL.md invalid or join_prp invalid
 851:         RuntimeError: If PRP generation or Linear integration fails
 852: 
 853:     Process:
 854:         1. Parse INITIAL.md ‚Üí structured data
 855:         2. Research codebase ‚Üí Serena findings
 856:         3. Fetch documentation ‚Üí Context7 + WebFetch
 857:         4. Synthesize sections (TLDR, Implementation, Validation Gates, etc.)
 858:         5. Get next PRP ID
 859:         6. Write PRP file with YAML header
 860:         7. Create/update Linear issue with defaults
 861:         8. Update PRP YAML with issue ID
 862:         9. Check completeness
 863:     """
 864:     logger.info(f"Starting PRP generation from: {initial_md_path}")
 865: 
 866:     # Step 2.5: Pre-generation sync (if auto-sync enabled)
 867:     from .context import is_auto_sync_enabled, pre_generation_sync
 868:     if is_auto_sync_enabled():
 869:         try:
 870:             logger.info("Auto-sync enabled - running pre-generation sync...")
 871:             sync_result = pre_generation_sync(force=False)
 872:             logger.info(f"Pre-sync complete: drift={sync_result['drift_score']:.1f}%")
 873:         except Exception as e:
 874:             logger.error(f"Pre-generation sync failed: {e}")
 875:             raise RuntimeError(
 876:                 f"Generation aborted due to sync failure\n"
 877:                 f"Error: {e}\n"
 878:                 f"üîß Troubleshooting: Run 'ce context health' to diagnose issues"
 879:             ) from e
 880: 
 881:     # Phase 1: Parse INITIAL.md
 882:     parsed_data = parse_initial_md(initial_md_path)
 883:     logger.info(f"Parsed feature: {parsed_data['feature_name']}")
 884: 
 885:     # Phase 2: Research codebase
 886:     serena_research = research_codebase(
 887:         parsed_data["feature_name"],
 888:         parsed_data["examples"],
 889:         parsed_data["feature"]
 890:     )
 891:     logger.info(f"Codebase research complete: {len(serena_research['patterns'])} patterns found")
 892: 
 893:     # Phase 3: Fetch documentation
 894:     documentation = fetch_documentation(
 895:         parsed_data["documentation"],
 896:         parsed_data["feature"],
 897:         serena_research
 898:     )
 899:     logger.info(f"Documentation fetched: {len(documentation['library_docs'])} libraries")
 900: 
 901:     # Phase 4: Synthesize PRP sections
 902:     prp_content = synthesize_prp_content(parsed_data, serena_research, documentation)
 903: 
 904:     # Get next PRP ID
 905:     prp_id = get_next_prp_id(output_dir)
 906:     logger.info(f"Assigned PRP ID: {prp_id}")
 907: 
 908:     # Write PRP file
 909:     output_path = Path(output_dir) / f"{prp_id}-{_slugify(parsed_data['feature_name'])}.md"
 910:     output_path.parent.mkdir(parents=True, exist_ok=True)
 911: 
 912:     with open(output_path, "w", encoding="utf-8") as f:
 913:         f.write(prp_content)
 914: 
 915:     logger.info(f"PRP generated: {output_path}")
 916: 
 917:     # Step 7: Create or update Linear issue
 918:     try:
 919:         from .linear_utils import create_issue_with_defaults
 920:         from .linear_mcp_resilience import (
 921:             create_issue_resilient,
 922:             update_issue_resilient,
 923:             get_linear_mcp_status
 924:         )
 925: 
 926:         issue_identifier = None
 927: 
 928:         if join_prp:
 929:             # Join existing PRP's issue
 930:             logger.info(f"Joining PRP: {join_prp}")
 931:             target_prp_path = _resolve_prp_path(join_prp)
 932:             target_issue_id = _extract_issue_from_prp(target_prp_path)
 933: 
 934:             if not target_issue_id:
 935:                 logger.warning(f"Target PRP has no Linear issue: {target_prp_path}")
 936:                 logger.warning("Creating new issue instead")
 937:             else:
 938:                 # Update existing issue with resilience + auth recovery
 939:                 logger.info(f"Updating Linear issue: {target_issue_id}")
 940:                 result = _update_linear_issue_with_resilience(
 941:                     target_issue_id,
 942:                     prp_id,
 943:                     parsed_data['feature_name'],
 944:                     str(output_path)
 945:                 )
 946: 
 947:                 if result["success"]:
 948:                     issue_identifier = target_issue_id
 949:                     logger.info(f"Updated issue {target_issue_id} with {prp_id}")
 950:                 else:
 951:                     logger.warning(f"Failed to update issue: {result['error']}")
 952:                     logger.info("Will create new issue instead")
 953: 
 954:         if not issue_identifier:
 955:             # Create new issue with resilience + auth recovery
 956:             logger.info("Creating new Linear issue with resilience")
 957:             result = create_issue_resilient(
 958:                 title=f"{prp_id}: {parsed_data['feature_name']}",
 959:                 description=_generate_issue_description(prp_id, parsed_data, str(output_path)),
 960:                 state="todo"
 961:             )
 962: 
 963:             if result["success"]:
 964:                 issue_data = result.get("result", {})
 965:                 # Extract identifier from result (when actual MCP integration added)
 966:                 issue_identifier = issue_data.get("id") or issue_data.get("identifier") or f"{prp_id}-created"
 967:                 logger.info(f"Created Linear issue: {issue_identifier}")
 968:             else:
 969:                 logger.error(f"Linear issue creation failed: {result['error']}")
 970:                 logger.warning("Circuit breaker state:", get_linear_mcp_status())
 971:                 logger.warning("Continuing without Linear integration")
 972: 
 973:         # Update PRP YAML with issue ID
 974:         if issue_identifier:
 975:             _update_prp_yaml_with_issue(str(output_path), issue_identifier)
 976:             logger.info(f"Updated PRP YAML with issue: {issue_identifier}")
 977: 
 978:     except ImportError as e:
 979:         logger.warning(f"Linear resilience utils not available: {e}")
 980:         logger.warning("Skipping issue creation")
 981:     except Exception as e:
 982:         logger.error(f"Linear issue creation failed: {e}")
 983:         logger.warning("Continuing without Linear integration")
 984: 
 985:     # Check completeness
 986:     completeness = check_prp_completeness(str(output_path))
 987:     if not completeness["complete"]:
 988:         logger.warning(f"PRP incomplete: {completeness['missing_sections']}")
 989:     else:
 990:         logger.info("PRP completeness check: PASSED")
 991: 
 992:     return str(output_path)
 993: 
 994: 
 995: def synthesize_prp_content(
 996:     parsed_data: Dict[str, Any],
 997:     serena_research: Dict[str, Any],
 998:     documentation: Dict[str, Any]
 999: ) -> str:
1000:     """Synthesize complete PRP content from research.
1001: 
1002:     Args:
1003:         parsed_data: Parsed INITIAL.md data
1004:         serena_research: Codebase research results
1005:         documentation: Fetched documentation
1006: 
1007:     Returns:
1008:         Complete PRP markdown content with YAML header
1009: 
1010:     Process:
1011:         1. Generate YAML header with metadata
1012:         2. Synthesize TLDR section
1013:         3. Synthesize Context section
1014:         4. Synthesize Implementation Steps
1015:         5. Synthesize Validation Gates
1016:         6. Add Research Findings appendix
1017:         7. Format final markdown
1018:     """
1019:     logger.info("Synthesizing PRP content")
1020: 
1021:     # Generate sections
1022:     yaml_header = _generate_yaml_header(parsed_data)
1023:     tldr = synthesize_tldr(parsed_data, serena_research)
1024:     context = synthesize_context(parsed_data, documentation)
1025:     implementation = synthesize_implementation(parsed_data, serena_research)
1026:     validation_gates = synthesize_validation_gates(parsed_data, serena_research)
1027:     testing = synthesize_testing_strategy(parsed_data, serena_research)
1028:     rollout = synthesize_rollout_plan(parsed_data)
1029: 
1030:     # Combine sections
1031:     prp_content = f"""---
1032: {yaml_header}
1033: ---
1034: 
1035: # {parsed_data['feature_name']}
1036: 
1037: ## 1. TL;DR
1038: 
1039: {tldr}
1040: 
1041: ## 2. Context
1042: 
1043: {context}
1044: 
1045: ## 3. Implementation Steps
1046: 
1047: {implementation}
1048: 
1049: ## 4. Validation Gates
1050: 
1051: {validation_gates}
1052: 
1053: ## 5. Testing Strategy
1054: 
1055: {testing}
1056: 
1057: ## 6. Rollout Plan
1058: 
1059: {rollout}
1060: 
1061: ---
1062: 
1063: ## Research Findings
1064: 
1065: ### Serena Codebase Analysis
1066: - **Patterns Found**: {len(serena_research['patterns'])}
1067: - **Test Patterns**: {len(serena_research['test_patterns'])}
1068: - **Serena Available**: {serena_research['serena_available']}
1069: 
1070: ### Documentation Sources
1071: - **Library Docs**: {len(documentation['library_docs'])}
1072: - **External Links**: {len(documentation['external_links'])}
1073: - **Context7 Available**: {documentation['context7_available']}
1074: """
1075: 
1076:     return prp_content
1077: 
1078: 
1079: def synthesize_tldr(
1080:     parsed_data: Dict[str, Any],
1081:     serena_research: Dict[str, Any]
1082: ) -> str:
1083:     """Generate TLDR section.
1084: 
1085:     Args:
1086:         parsed_data: INITIAL.md structured data
1087:         serena_research: Codebase research findings
1088: 
1089:     Returns:
1090:         TLDR markdown text (3-5 bullet points)
1091:     """
1092:     feature = parsed_data["feature"]
1093:     examples_count = len(parsed_data["examples"])
1094: 
1095:     tldr = f"""**Objective**: {parsed_data['feature_name']}
1096: 
1097: **What**: {feature[:200]}...
1098: 
1099: **Why**: Enable functionality described in INITIAL.md with {examples_count} reference examples
1100: 
1101: **Effort**: Medium (3-5 hours estimated based on complexity)
1102: 
1103: **Dependencies**: {', '.join([doc['title'] for doc in parsed_data['documentation'][:3]])}
1104: """
1105:     return tldr
1106: 
1107: 
1108: def synthesize_context(
1109:     parsed_data: Dict[str, Any],
1110:     documentation: Dict[str, Any]
1111: ) -> str:
1112:     """Generate Context section.
1113: 
1114:     Args:
1115:         parsed_data: INITIAL.md data
1116:         documentation: Fetched documentation
1117: 
1118:     Returns:
1119:         Context markdown with background and constraints
1120:     """
1121:     feature = parsed_data["feature"]
1122:     other = parsed_data.get("other_considerations", "")
1123: 
1124:     context = f"""### Background
1125: 
1126: {feature}
1127: 
1128: ### Constraints and Considerations
1129: 
1130: {other if other else "See INITIAL.md for additional considerations"}
1131: 
1132: ### Documentation References
1133: 
1134: """
1135:     # Add documentation links
1136:     for doc in parsed_data["documentation"]:
1137:         if doc["type"] == "link":
1138:             context += f"- [{doc['title']}]({doc['url']})\n"
1139:         elif doc["type"] == "library":
1140:             context += f"- {doc['title']} (library documentation)\n"
1141: 
1142:     return context
1143: 
1144: 
1145: def _extract_planning_context(parsed_data: Dict[str, Any]) -> Dict[str, Any]:
1146:     """Extract PLANNING CONTEXT from INITIAL.md.
1147: 
1148:     Args:
1149:         parsed_data: Parsed INITIAL.md data
1150: 
1151:     Returns:
1152:         {
1153:             "complexity": "medium",
1154:             "architectural_impact": "moderate",
1155:             "risk_factors": ["..."],
1156:             "success_metrics": ["..."]
1157:         }
1158:     """
1159:     raw_content = parsed_data.get("raw_content", "")
1160: 
1161:     # Extract PLANNING CONTEXT section
1162:     planning_match = re.search(
1163:         r"##\s*PLANNING\s+CONTEXT\s*\n(.*?)(?=\n##|\Z)",
1164:         raw_content,
1165:         re.DOTALL | re.IGNORECASE
1166:     )
1167: 
1168:     if not planning_match:
1169:         return {
1170:             "complexity": "unknown",
1171:             "architectural_impact": "unknown",
1172:             "risk_factors": [],
1173:             "success_metrics": []
1174:         }
1175: 
1176:     planning_text = planning_match.group(1)
1177: 
1178:     # Extract complexity
1179:     complexity_match = re.search(
1180:         r"\*\*Complexity Assessment\*\*:\s*(\w+)",
1181:         planning_text,
1182:         re.IGNORECASE
1183:     )
1184:     complexity = complexity_match.group(1) if complexity_match else "unknown"
1185: 
1186:     # Extract architectural impact
1187:     arch_match = re.search(
1188:         r"\*\*Architectural Impact\*\*:\s*(\w+)",
1189:         planning_text,
1190:         re.IGNORECASE
1191:     )
1192:     arch_impact = arch_match.group(1) if arch_match else "unknown"
1193: 
1194:     # Extract risk factors (lines starting with - after "Risk Factors")
1195:     risk_section = re.search(
1196:         r"\*\*Risk Factors\*\*:\s*\n((?:- .+\n?)+)",
1197:         planning_text,
1198:         re.MULTILINE
1199:     )
1200:     risk_factors = []
1201:     if risk_section:
1202:         risk_lines = risk_section.group(1).strip().split("\n")
1203:         risk_factors = [line.lstrip("- ").strip() for line in risk_lines if line.strip()]
1204: 
1205:     return {
1206:         "complexity": complexity,
1207:         "architectural_impact": arch_impact,
1208:         "risk_factors": risk_factors,
1209:         "success_metrics": []  # TODO: Extract if needed
1210:     }
1211: 
1212: 
1213: def generate_implementation_phases_with_thinking(
1214:     parsed_data: Dict[str, Any],
1215:     serena_research: Dict[str, Any]
1216: ) -> str:
1217:     """Generate implementation phases using sequential thinking.
1218: 
1219:     Uses: mcp__syntropy__thinking__sequentialthinking
1220: 
1221:     Args:
1222:         parsed_data: INITIAL.md structured data
1223:         serena_research: Codebase research findings
1224: 
1225:     Returns:
1226:         Implementation phases markdown
1227: 
1228:     Process:
1229:         1. Extract planning context from INITIAL.md
1230:         2. Call sequential thinking with implementation planning prompt
1231:         3. Parse phases from reasoning chain
1232:         4. Fall back to template-based if unavailable
1233:     """
1234:     logger.info("Generating implementation phases with sequential thinking")
1235: 
1236:     # Check if sequential thinking is enabled
1237:     import os
1238:     use_thinking = os.environ.get('CE_USE_SEQUENTIAL_THINKING', 'true').lower() == 'true'
1239: 
1240:     if not use_thinking:
1241:         logger.info("Sequential thinking disabled via --no-thinking flag")
1242:         return ""  # Empty string triggers fallback in synthesize_implementation
1243: 
1244:     try:
1245:         from .mcp_utils import call_syntropy_mcp
1246: 
1247:         # Extract planning context
1248:         planning_context = _extract_planning_context(parsed_data)
1249: 
1250:         prompt = f"""Plan implementation phases for this feature:
1251: 
1252: Feature: {parsed_data['feature_name']}
1253: Description: {parsed_data['feature'][:300]}...
1254: 
1255: Planning Context:
1256: - Complexity: {planning_context.get('complexity', 'unknown')}
1257: - Architectural Impact: {planning_context.get('architectural_impact', 'unknown')}
1258: - Risk Factors: {', '.join(planning_context.get('risk_factors', ['unknown']))}
1259: 
1260: Codebase Context:
1261: - Similar patterns: {len(serena_research.get('patterns', []))}
1262: - Test framework: {serena_research.get('test_patterns', [{}])[0].get('framework', 'pytest') if serena_research.get('test_patterns') else 'pytest'}
1263: 
1264: Think step-by-step:
1265: 1. What are the logical implementation phases?
1266: 2. What dependencies exist between phases?
1267: 3. What time estimates are realistic?
1268: 4. What validation should happen at each phase?
1269: 
1270: Provide phases in format:
1271: PHASE 1: <name> (<time estimate>)
1272: - Step 1
1273: - Step 2
1274: 
1275: PHASE 2: ..."""
1276: 
1277:         result = call_syntropy_mcp(
1278:             "thinking",
1279:             "sequentialthinking",
1280:             {
1281:                 "thought": prompt,
1282:                 "thoughtNumber": 1,
1283:                 "totalThoughts": 8,
1284:                 "nextThoughtNeeded": True
1285:             }
1286:         )
1287: 
1288:         # Log reasoning chain
1289:         _log_thinking_chain(result, "Implementation Planning")
1290: 
1291:         # Extract phases from thinking result
1292:         phases = _extract_phases_from_thinking_result(result)
1293: 
1294:         if phases:
1295:             logger.info(f"Generated {len(phases.split('Phase'))-1} implementation phases")
1296:             return phases
1297: 
1298:     except Exception as e:
1299:         logger.warning(f"Sequential thinking unavailable: {e}")
1300:         logger.warning("Falling back to template-based phases")
1301: 
1302:     # Graceful degradation
1303:     return ""  # Empty string triggers fallback in synthesize_implementation
1304: 
1305: 
1306: def _extract_phases_from_thinking_result(result: Dict[str, Any]) -> str:
1307:     """Parse implementation phases from sequential thinking result.
1308: 
1309:     Args:
1310:         result: MCP tool result
1311: 
1312:     Returns:
1313:         Markdown formatted phases
1314:     """
1315:     # Extract content
1316:     content = ""
1317:     if isinstance(result, dict) and "content" in result:
1318:         if isinstance(result["content"], list):
1319:             for item in result["content"]:
1320:                 if isinstance(item, dict) and "text" in item:
1321:                     content += item["text"] + "\n"
1322:         elif isinstance(result["content"], str):
1323:             content = result["content"]
1324: 
1325:     # Extract phases (PHASE 1: ... format)
1326:     phases_text = ""
1327:     phase_matches = re.finditer(
1328:         r"PHASE (\d+):\s*([^\n]+)\n((?:- .+\n?)+)",
1329:         content,
1330:         re.MULTILINE
1331:     )
1332: 
1333:     for match in phase_matches:
1334:         phase_num = match.group(1)
1335:         phase_name = match.group(2).strip()
1336:         phase_steps = match.group(3).strip()
1337: 
1338:         phases_text += f"### Phase {phase_num}: {phase_name}\n\n"
1339:         phases_text += f"{phase_steps}\n\n"
1340: 
1341:     if phases_text:
1342:         return phases_text
1343: 
1344:     # If no phases found, return empty to trigger fallback
1345:     return ""
1346: 
1347: 
1348: def synthesize_implementation(
1349:     parsed_data: Dict[str, Any],
1350:     serena_research: Dict[str, Any]
1351: ) -> str:
1352:     """Generate Implementation Steps section.
1353: 
1354:     Tries sequential thinking first, falls back to template-based.
1355: 
1356:     Args:
1357:         parsed_data: INITIAL.md data
1358:         serena_research: Codebase patterns
1359: 
1360:     Returns:
1361:         Implementation steps markdown
1362:     """
1363:     # Try sequential thinking first
1364:     phases_with_thinking = generate_implementation_phases_with_thinking(
1365:         parsed_data,
1366:         serena_research
1367:     )
1368: 
1369:     if phases_with_thinking:
1370:         return phases_with_thinking
1371: 
1372:     # Fallback: Template-based phases (current implementation)
1373:     examples = parsed_data["examples"]
1374: 
1375:     steps = """### Phase 1: Setup and Research (30 min)
1376: 
1377: 1. Review INITIAL.md examples and requirements
1378: 2. Analyze existing codebase patterns
1379: 3. Identify integration points
1380: 
1381: ### Phase 2: Core Implementation (2-3 hours)
1382: 
1383: """
1384:     # Generate steps from examples
1385:     for i, example in enumerate(examples[:3], 1):
1386:         if example["type"] == "inline":
1387:             steps += f"{i}. Implement {example.get('language', 'code')} component\n"
1388:         elif example["type"] == "file_ref":
1389:             steps += f"{i}. Reference pattern in {example['file']}\n"
1390: 
1391:     steps += """
1392: ### Phase 3: Testing and Validation (1-2 hours)
1393: 
1394: 1. Write unit tests following project patterns
1395: 2. Write integration tests
1396: 3. Run validation gates
1397: 4. Update documentation
1398: """
1399: 
1400:     return steps
1401: 
1402: 
1403: def synthesize_validation_gates(
1404:     parsed_data: Dict[str, Any],
1405:     serena_research: Dict[str, Any]
1406: ) -> str:
1407:     """Generate Validation Gates section.
1408: 
1409:     Args:
1410:         parsed_data: INITIAL.md data with acceptance criteria
1411:         serena_research: Test patterns from codebase
1412: 
1413:     Returns:
1414:         Validation gates markdown
1415:     """
1416:     test_framework = "pytest"
1417:     if serena_research["test_patterns"]:
1418:         test_framework = serena_research["test_patterns"][0]["framework"]
1419: 
1420:     gates = f"""### Gate 1: Unit Tests Pass
1421: 
1422: **Command**: `uv run {test_framework} tests/unit/ -v`
1423: 
1424: **Success Criteria**:
1425: - All new unit tests pass
1426: - Existing tests not broken
1427: - Code coverage ‚â• 80%
1428: 
1429: ### Gate 2: Integration Tests Pass
1430: 
1431: **Command**: `uv run {test_framework} tests/integration/ -v`
1432: 
1433: **Success Criteria**:
1434: - Integration tests verify end-to-end functionality
1435: - No regressions in existing features
1436: 
1437: ### Gate 3: Acceptance Criteria Met
1438: 
1439: **Verification**: Manual review against INITIAL.md requirements
1440: 
1441: **Success Criteria**:
1442: """
1443:     # Extract acceptance criteria from feature text
1444:     feature = parsed_data["feature"]
1445:     if "acceptance criteria" in feature.lower():
1446:         gates += "\n- Requirements from INITIAL.md validated\n"
1447:     else:
1448:         gates += "\n- All examples from INITIAL.md working\n"
1449:         gates += "- Feature behaves as described\n"
1450: 
1451:     return gates
1452: 
1453: 
1454: def synthesize_testing_strategy(
1455:     parsed_data: Dict[str, Any],
1456:     serena_research: Dict[str, Any]
1457: ) -> str:
1458:     """Generate Testing Strategy section."""
1459:     test_cmd = "uv run pytest tests/ -v"
1460:     if serena_research["test_patterns"]:
1461:         test_cmd = serena_research["test_patterns"][0]["test_command"]
1462: 
1463:     return f"""### Test Framework
1464: 
1465: {serena_research['test_patterns'][0]['framework'] if serena_research['test_patterns'] else 'pytest'}
1466: 
1467: ### Test Command
1468: 
1469: ```bash
1470: {test_cmd}
1471: ```
1472: 
1473: ### Coverage Requirements
1474: 
1475: - Unit test coverage: ‚â• 80%
1476: - Integration tests for critical paths
1477: - Edge cases from INITIAL.md covered
1478: """
1479: 
1480: 
1481: def synthesize_rollout_plan(parsed_data: Dict[str, Any]) -> str:
1482:     """Generate Rollout Plan section."""
1483:     return """### Phase 1: Development
1484: 
1485: 1. Implement core functionality
1486: 2. Write tests
1487: 3. Pass validation gates
1488: 
1489: ### Phase 2: Review
1490: 
1491: 1. Self-review code changes
1492: 2. Peer review (optional)
1493: 3. Update documentation
1494: 
1495: ### Phase 3: Deployment
1496: 
1497: 1. Merge to main branch
1498: 2. Monitor for issues
1499: 3. Update stakeholders
1500: """
1501: 
1502: 
1503: def get_next_prp_id(prps_dir: str = "PRPs/feature-requests") -> str:
1504:     """Get next available PRP ID.
1505: 
1506:     Args:
1507:         prps_dir: Directory containing PRPs
1508: 
1509:     Returns:
1510:         Next PRP ID (e.g., "PRP-123")
1511: 
1512:     Process:
1513:         1. List all PRP-*.md files in directory
1514:         2. Extract numeric IDs
1515:         3. Return max + 1
1516:     """
1517:     prps_path = Path(prps_dir)
1518:     if not prps_path.exists():
1519:         return "PRP-1"
1520: 
1521:     # Find all PRP-*.md files
1522:     prp_files = list(prps_path.glob("PRP-*.md"))
1523:     if not prp_files:
1524:         return "PRP-1"
1525: 
1526:     # Extract numeric IDs
1527:     ids = []
1528:     for file in prp_files:
1529:         match = re.match(r"PRP-(\d+)", file.name)
1530:         if match:
1531:             ids.append(int(match.group(1)))
1532: 
1533:     # Return next ID
1534:     next_id = max(ids) + 1 if ids else 1
1535:     return f"PRP-{next_id}"
1536: 
1537: 
1538: def check_prp_completeness(prp_path: str) -> Dict[str, Any]:
1539:     """Check if PRP has all required sections.
1540: 
1541:     Args:
1542:         prp_path: Path to PRP file
1543: 
1544:     Returns:
1545:         {
1546:             "complete": True/False,
1547:             "missing_sections": [],
1548:             "warnings": []
1549:         }
1550: 
1551:     Required sections:
1552:         1. TL;DR
1553:         2. Context
1554:         3. Implementation Steps
1555:         4. Validation Gates
1556:         5. Testing Strategy
1557:         6. Rollout Plan
1558:     """
1559:     required_sections = [
1560:         "TL;DR",
1561:         "Context",
1562:         "Implementation Steps",
1563:         "Validation Gates",
1564:         "Testing Strategy",
1565:         "Rollout Plan"
1566:     ]
1567: 
1568:     content = Path(prp_path).read_text(encoding="utf-8")
1569: 
1570:     missing = []
1571:     for section in required_sections:
1572:         # Check for section header (## N. Section or ## Section)
1573:         pattern = rf"##\s+\d*\.?\s*{re.escape(section)}"
1574:         if not re.search(pattern, content, re.IGNORECASE):
1575:             missing.append(section)
1576: 
1577:     warnings = []
1578:     if len(content) < 1000:
1579:         warnings.append("PRP content seems short (< 1000 chars)")
1580: 
1581:     return {
1582:         "complete": len(missing) == 0,
1583:         "missing_sections": missing,
1584:         "warnings": warnings
1585:     }
1586: 
1587: 
1588: def _generate_yaml_header(parsed_data: Dict[str, Any]) -> str:
1589:     """Generate YAML frontmatter for PRP."""
1590:     from datetime import datetime
1591: 
1592:     now = datetime.now().isoformat()
1593: 
1594:     return f"""prp_id: TBD
1595: feature_name: {parsed_data['feature_name']}
1596: status: pending
1597: created: {now}
1598: updated: {now}
1599: complexity: medium
1600: estimated_hours: 3-5
1601: dependencies: {', '.join([doc['title'] for doc in parsed_data['documentation'][:3]])}"""
1602: 
1603: 
1604: def _slugify(text: str) -> str:
1605:     """Convert text to URL-friendly slug."""
1606:     # Lowercase and replace spaces with hyphens
1607:     slug = text.lower().replace(" ", "-")
1608:     # Remove special characters
1609:     slug = re.sub(r'[^a-z0-9-]', '', slug)
1610:     # Remove multiple hyphens
1611:     slug = re.sub(r'-+', '-', slug)
1612:     return slug.strip("-")
1613: 
1614: 
1615: # =============================================================================
1616: # Linear Integration Helpers
1617: # =============================================================================
1618: 
1619: 
1620: def _resolve_prp_path(join_prp: str) -> Path:
1621:     """Resolve join_prp reference to PRP file path.
1622: 
1623:     Args:
1624:         join_prp: PRP reference (number like "12", ID like "PRP-12", or file path)
1625: 
1626:     Returns:
1627:         Path to PRP file
1628: 
1629:     Raises:
1630:         ValueError: If join_prp invalid or PRP not found
1631:     """
1632:     # Check if it's already a valid file path
1633:     if "/" in join_prp or "\\" in join_prp:
1634:         prp_path = Path(join_prp)
1635:         if prp_path.exists():
1636:             return prp_path
1637:         raise ValueError(
1638:             f"PRP file not found: {join_prp}\n"
1639:             f"üîß Troubleshooting: Verify file path is correct"
1640:         )
1641: 
1642:     # Parse as PRP number or ID
1643:     prp_number = None
1644:     if join_prp.startswith("PRP-"):
1645:         # Extract number from "PRP-12"
1646:         match = re.match(r"PRP-(\d+)", join_prp)
1647:         if match:
1648:             prp_number = int(match.group(1))
1649:     else:
1650:         # Try parsing as plain number "12"
1651:         try:
1652:             prp_number = int(join_prp)
1653:         except ValueError:
1654:             raise ValueError(
1655:                 f"Invalid PRP reference: {join_prp}\n"
1656:                 f"üîß Troubleshooting: Use format '12', 'PRP-12', or file path"
1657:             )
1658: 
1659:     if not prp_number:
1660:         raise ValueError(
1661:             f"Could not parse PRP reference: {join_prp}\n"
1662:             f"üîß Troubleshooting: Use format '12', 'PRP-12', or file path"
1663:         )
1664: 
1665:     # Search for PRP file in feature-requests/ and executed/
1666:     prp_id = f"PRP-{prp_number}"
1667:     search_dirs = ["PRPs/feature-requests", "PRPs/executed"]
1668: 
1669:     for search_dir in search_dirs:
1670:         search_path = Path(search_dir)
1671:         if search_path.exists():
1672:             # Find PRP-{number}-*.md
1673:             matches = list(search_path.glob(f"{prp_id}-*.md"))
1674:             if matches:
1675:                 return matches[0]
1676: 
1677:     raise ValueError(
1678:         f"PRP not found: {prp_id}\n"
1679:         f"üîß Troubleshooting: Searched in {', '.join(search_dirs)}"
1680:     )
1681: 
1682: 
1683: def _extract_issue_from_prp(prp_path: Path) -> Optional[str]:
1684:     """Extract Linear issue ID from PRP YAML header.
1685: 
1686:     Args:
1687:         prp_path: Path to PRP file
1688: 
1689:     Returns:
1690:         Issue ID (e.g., "BLA-24") or None if not found
1691:     """
1692:     content = prp_path.read_text(encoding="utf-8")
1693: 
1694:     # Extract YAML frontmatter
1695:     yaml_match = re.match(r"---\n(.*?)\n---", content, re.DOTALL)
1696:     if not yaml_match:
1697:         return None
1698: 
1699:     yaml_content = yaml_match.group(1)
1700: 
1701:     # Extract issue field
1702:     issue_match = re.search(r"^issue:\s*(.+)$", yaml_content, re.MULTILINE)
1703:     if not issue_match:
1704:         return None
1705: 
1706:     issue_value = issue_match.group(1).strip()
1707: 
1708:     # Return None for null/empty values
1709:     if issue_value.lower() in ["null", "none", ""]:
1710:         return None
1711: 
1712:     return issue_value
1713: 
1714: 
1715: def _update_linear_issue_with_resilience(
1716:     issue_id: str,
1717:     prp_id: str,
1718:     feature_name: str,
1719:     prp_path: str
1720: ) -> Dict[str, Any]:
1721:     """Update existing Linear issue with new PRP info using resilience layer.
1722: 
1723:     Args:
1724:         issue_id: Linear issue identifier (e.g., "BLA-24")
1725:         prp_id: New PRP ID (e.g., "PRP-15")
1726:         feature_name: New PRP feature name
1727:         prp_path: Path to new PRP file
1728: 
1729:     Returns:
1730:         Result dict from update_issue_resilient with success/error status
1731: 
1732:     Process:
1733:         1. Generate update text for new PRP
1734:         2. Call update_issue_resilient with auth recovery
1735:         3. Return detailed result with success status
1736:     """
1737:     from .linear_mcp_resilience import update_issue_resilient
1738: 
1739:     logger.info(f"Updating Linear issue {issue_id} with {prp_id}")
1740: 
1741:     update_text = f"""
1742: 
1743: ---
1744: 
1745: ## Related: {prp_id} - {feature_name}
1746: 
1747: **PRP File**: `{prp_path}`
1748: 
1749: This PRP is related to the same feature/initiative.
1750: """
1751: 
1752:     # Call with resilience + auth recovery
1753:     result = update_issue_resilient(issue_id, update_text)
1754: 
1755:     if result["success"]:
1756:         logger.info(f"Successfully updated issue {issue_id}")
1757:     else:
1758:         logger.warning(f"Failed to update issue: {result['error']}")
1759: 
1760:     return result
1761: 
1762: 
1763: def _update_linear_issue(
1764:     issue_id: str,
1765:     prp_id: str,
1766:     feature_name: str,
1767:     prp_path: str
1768: ) -> None:
1769:     """Update existing Linear issue with new PRP info.
1770: 
1771:     DEPRECATED: Use _update_linear_issue_with_resilience instead.
1772: 
1773:     Args:
1774:         issue_id: Linear issue identifier (e.g., "BLA-24")
1775:         prp_id: New PRP ID (e.g., "PRP-15")
1776:         feature_name: New PRP feature name
1777:         prp_path: Path to new PRP file
1778: 
1779:     Raises:
1780:         RuntimeError: If update fails
1781:     """
1782:     logger.info(f"Updating Linear issue {issue_id} with {prp_id}")
1783: 
1784:     # FIXME: Placeholder - replace with actual Linear MCP call
1785:     # In full implementation, this would:
1786:     # 1. Get current issue description via mcp__linear-server__get_issue
1787:     # 2. Append new PRP section to description
1788:     # 3. Update issue via mcp__linear-server__update_issue
1789: 
1790:     update_text = f"""
1791: 
1792: ---
1793: 
1794: ## Related: {prp_id} - {feature_name}
1795: 
1796: **PRP File**: `{prp_path}`
1797: 
1798: This PRP is related to the same feature/initiative.
1799: """
1800: 
1801:     logger.info(f"Would append to issue {issue_id}: {update_text[:100]}...")
1802:     logger.warning("Linear MCP integration pending - issue not actually updated")
1803: 
1804: 
1805: def _generate_issue_description(
1806:     prp_id: str,
1807:     parsed_data: Dict[str, Any],
1808:     prp_path: str
1809: ) -> str:
1810:     """Generate Linear issue description from PRP data.
1811: 
1812:     Args:
1813:         prp_id: PRP identifier (e.g., "PRP-15")
1814:         parsed_data: Parsed INITIAL.md data
1815:         prp_path: Path to generated PRP file
1816: 
1817:     Returns:
1818:         Markdown description for Linear issue
1819:     """
1820:     feature = parsed_data["feature"]
1821:     examples_count = len(parsed_data["examples"])
1822: 
1823:     # Truncate feature description for issue
1824:     feature_summary = feature[:300] + "..." if len(feature) > 300 else feature
1825: 
1826:     description = f"""## Feature
1827: 
1828: {feature_summary}
1829: 
1830: ## PRP Details
1831: 
1832: - **PRP ID**: {prp_id}
1833: - **PRP File**: `{prp_path}`
1834: - **Examples Provided**: {examples_count}
1835: 
1836: ## Implementation
1837: 
1838: See PRP file for detailed implementation steps, validation gates, and testing strategy.
1839: 
1840: """
1841: 
1842:     # Add other considerations if present
1843:     if parsed_data.get("other_considerations"):
1844:         other = parsed_data["other_considerations"]
1845:         other_summary = other[:200] + "..." if len(other) > 200 else other
1846:         description += f"""## Considerations
1847: 
1848: {other_summary}
1849: """
1850: 
1851:     return description
1852: 
1853: 
1854: def _update_prp_yaml_with_issue(prp_path: str, issue_id: str) -> None:
1855:     """Update PRP YAML header with Linear issue ID.
1856: 
1857:     Args:
1858:         prp_path: Path to PRP file
1859:         issue_id: Linear issue identifier
1860: 
1861:     Raises:
1862:         RuntimeError: If YAML update fails
1863:     """
1864:     content = Path(prp_path).read_text(encoding="utf-8")
1865: 
1866:     # Check if YAML frontmatter exists
1867:     yaml_match = re.match(r"(---\n.*?\n---)", content, re.DOTALL)
1868:     if not yaml_match:
1869:         raise RuntimeError(
1870:             f"No YAML frontmatter found in {prp_path}\n"
1871:             f"üîß Troubleshooting: PRP file should start with YAML frontmatter"
1872:         )
1873: 
1874:     yaml_block = yaml_match.group(1)
1875: 
1876:     # Check if issue field exists
1877:     if re.search(r"^issue:", yaml_block, re.MULTILINE):
1878:         # Update existing issue field
1879:         updated_yaml = re.sub(
1880:             r"^issue:.*$",
1881:             f"issue: {issue_id}",
1882:             yaml_block,
1883:             flags=re.MULTILINE
1884:         )
1885:     else:
1886:         # Add issue field before closing ---
1887:         updated_yaml = yaml_block.replace(
1888:             "\n---",
1889:             f"\nissue: {issue_id}\n---"
1890:         )
1891: 
1892:     # Replace YAML block in content
1893:     updated_content = content.replace(yaml_block, updated_yaml)
1894: 
1895:     # Write back to file
1896:     Path(prp_path).write_text(updated_content, encoding="utf-8")
</file>

<file path="tools/ce/linear_utils.py">
  1: """Linear integration utilities for Context Engineering.
  2: 
  3: Provides helpers for reading Linear defaults and creating issues with
  4: project-specific configuration.
  5: """
  6: 
  7: import logging
  8: from pathlib import Path
  9: from typing import Dict, Any, Optional
 10: import yaml
 11: 
 12: logger = logging.getLogger(__name__)
 13: 
 14: 
 15: def get_linear_defaults() -> Dict[str, Any]:
 16:     """Read Linear defaults from .ce/linear-defaults.yml.
 17: 
 18:     Returns:
 19:         Dict with keys: project, assignee, team, default_labels
 20: 
 21:     Raises:
 22:         FileNotFoundError: If linear-defaults.yml not found
 23:         RuntimeError: If YAML parsing fails
 24:     """
 25:     # Find project root (go up from tools/)
 26:     project_root = Path(__file__).parent.parent.parent
 27:     config_path = project_root / ".ce" / "linear-defaults.yml"
 28: 
 29:     if not config_path.exists():
 30:         raise FileNotFoundError(
 31:             f"Linear defaults not found: {config_path}\n"
 32:             f"üîß Troubleshooting:\n"
 33:             f"   - Create .ce/linear-defaults.yml with project/assignee config\n"
 34:             f"   - See CLAUDE.md for template"
 35:         )
 36: 
 37:     try:
 38:         with open(config_path) as f:
 39:             config = yaml.safe_load(f)
 40:     except yaml.YAMLError as e:
 41:         raise RuntimeError(
 42:             f"Failed to parse Linear defaults: {e}\n"
 43:             f"üîß Troubleshooting: Check YAML syntax in {config_path}"
 44:         ) from e
 45: 
 46:     # Validate required fields
 47:     required_fields = ["project", "assignee", "team"]
 48:     missing = [f for f in required_fields if f not in config]
 49: 
 50:     if missing:
 51:         raise RuntimeError(
 52:             f"Missing required fields in Linear defaults: {', '.join(missing)}\n"
 53:             f"üîß Troubleshooting: Add to {config_path}"
 54:         )
 55: 
 56:     return config
 57: 
 58: 
 59: def create_issue_with_defaults(
 60:     title: str,
 61:     description: str,
 62:     state: str = "todo",
 63:     labels: Optional[list] = None,
 64:     override_assignee: Optional[str] = None,
 65:     override_project: Optional[str] = None
 66: ) -> Dict[str, Any]:
 67:     """Create Linear issue using project defaults.
 68: 
 69:     Args:
 70:         title: Issue title
 71:         description: Issue description (markdown)
 72:         state: Issue state (todo, in_progress, done)
 73:         labels: Optional labels (merges with defaults)
 74:         override_assignee: Optional assignee override
 75:         override_project: Optional project override
 76: 
 77:     Returns:
 78:         Linear API response with issue details
 79: 
 80:     Example:
 81:         issue = create_issue_with_defaults(
 82:             title="PRP-15: New Feature",
 83:             description="Implement feature X",
 84:             state="todo"
 85:         )
 86:         print(f"Created: {issue['identifier']}")
 87:     """
 88:     defaults = get_linear_defaults()
 89: 
 90:     # Merge labels
 91:     final_labels = list(defaults.get("default_labels", []))
 92:     if labels:
 93:         final_labels.extend(labels)
 94:     # Deduplicate
 95:     final_labels = list(set(final_labels))
 96: 
 97:     # Prepare issue data
 98:     issue_data = {
 99:         "team": defaults["team"],
100:         "title": title,
101:         "description": description,
102:         "state": state,
103:         "labels": final_labels,
104:         "assignee": override_assignee or defaults["assignee"],
105:         "project": override_project or defaults["project"]
106:     }
107: 
108:     logger.info(f"Creating Linear issue with defaults: {title}")
109:     logger.debug(f"Issue data: {issue_data}")
110: 
111:     # Note: Actual MCP call would go here
112:     # For now, return the prepared data structure
113:     return issue_data
114: 
115: 
116: def get_default_assignee() -> str:
117:     """Get default assignee email from config.
118: 
119:     Returns:
120:         Assignee email address
121: 
122:     Example:
123:         assignee = get_default_assignee()
124:         # "blazej.przybyszewski@gmail.com"
125:     """
126:     defaults = get_linear_defaults()
127:     return defaults["assignee"]
128: 
129: 
130: def get_default_project() -> str:
131:     """Get default project name from config.
132: 
133:     Returns:
134:         Project name
135: 
136:     Example:
137:         project = get_default_project()
138:         # "Context Engineering"
139:     """
140:     defaults = get_linear_defaults()
141:     return defaults["project"]
</file>

<file path="tools/ce/logging_config.py">
  1: """Logging configuration module - structured logging with JSON formatter.
  2: 
  3: Provides JSON-based structured logging for production observability and
  4: human-readable text logging for development.
  5: """
  6: 
  7: import logging
  8: import json
  9: import sys
 10: from typing import Dict, Any
 11: 
 12: 
 13: class JSONFormatter(logging.Formatter):
 14:     """JSON formatter for structured logging.
 15: 
 16:     Outputs logs in JSON format for machine parsing.
 17: 
 18:     Example output:
 19:         {"timestamp": "2025-01-13T10:30:45", "level": "INFO",
 20:          "message": "prp.execution.started", "prp_id": "PRP-003"}
 21:     """
 22: 
 23:     def format(self, record: logging.LogRecord) -> str:
 24:         """Format log record as JSON.
 25: 
 26:         Args:
 27:             record: Log record to format
 28: 
 29:         Returns:
 30:             JSON string
 31: 
 32:         Note: Includes extra fields from record.extra dict if provided.
 33:         """
 34:         log_data = {
 35:             "timestamp": self.formatTime(record, self.datefmt),
 36:             "level": record.levelname,
 37:             "logger": record.name,
 38:             "message": record.getMessage(),
 39:         }
 40: 
 41:         # Add extra fields from record
 42:         if hasattr(record, "prp_id"):
 43:             log_data["prp_id"] = record.prp_id
 44:         if hasattr(record, "phase"):
 45:             log_data["phase"] = record.phase
 46:         if hasattr(record, "duration"):
 47:             log_data["duration"] = record.duration
 48:         if hasattr(record, "success"):
 49:             log_data["success"] = record.success
 50: 
 51:         # Add exception info if present
 52:         if record.exc_info:
 53:             log_data["exception"] = self.formatException(record.exc_info)
 54: 
 55:         return json.dumps(log_data)
 56: 
 57: 
 58: def setup_logging(
 59:     level: str = "INFO",
 60:     json_output: bool = False,
 61:     log_file: str = None
 62: ) -> logging.Logger:
 63:     """Setup application logging.
 64: 
 65:     Args:
 66:         level: Log level (DEBUG, INFO, WARNING, ERROR)
 67:         json_output: If True, use JSON formatter
 68:         log_file: Optional file path for file logging
 69: 
 70:     Returns:
 71:         Configured root logger
 72: 
 73:     Example:
 74:         setup_logging(level="DEBUG", json_output=True)
 75:         logger = logging.getLogger(__name__)
 76:         logger.info("prp.started", extra={"prp_id": "PRP-003"})
 77: 
 78:     Note: Call this once at application startup. All subsequent loggers
 79:     will inherit this configuration.
 80:     """
 81:     # Get root logger
 82:     logger = logging.getLogger()
 83:     logger.setLevel(getattr(logging, level.upper()))
 84: 
 85:     # Remove existing handlers
 86:     logger.handlers.clear()
 87: 
 88:     # Console handler
 89:     console_handler = logging.StreamHandler(sys.stderr)
 90:     if json_output:
 91:         console_handler.setFormatter(JSONFormatter())
 92:     else:
 93:         console_handler.setFormatter(
 94:             logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
 95:         )
 96:     logger.addHandler(console_handler)
 97: 
 98:     # File handler (optional)
 99:     if log_file:
100:         file_handler = logging.FileHandler(log_file)
101:         file_handler.setFormatter(JSONFormatter())  # Always use JSON for file logs
102:         logger.addHandler(file_handler)
103: 
104:     return logger
105: 
106: 
107: def get_logger(name: str) -> logging.Logger:
108:     """Get logger for module.
109: 
110:     Args:
111:         name: Module name (typically __name__)
112: 
113:     Returns:
114:         Logger instance configured with application settings
115: 
116:     Example:
117:         logger = get_logger(__name__)
118:         logger.info("Starting operation", extra={"prp_id": "PRP-003"})
119: 
120:     Note: Use this instead of logging.getLogger() for consistency.
121:     """
122:     return logging.getLogger(name)
</file>

<file path="tools/ce/markdown_lint.py">
  1: """Markdown linting utilities using markdownlint-cli2.
  2: 
  3: Provides markdown validation and auto-fixing capabilities.
  4: """
  5: 
  6: import subprocess
  7: from pathlib import Path
  8: from typing import Dict, Any
  9: 
 10: 
 11: def lint_markdown(auto_fix: bool = False) -> Dict[str, Any]:
 12:     """Lint markdown files using markdownlint-cli2.
 13: 
 14:     Args:
 15:         auto_fix: If True, attempt to auto-fix issues
 16: 
 17:     Returns:
 18:         Dict with success, errors, output, fixed_count
 19: 
 20:     Raises:
 21:         RuntimeError: If markdownlint-cli2 is not installed
 22: 
 23:     Note: No fishy fallbacks - exceptions thrown for troubleshooting.
 24:     """
 25:     # Check if markdownlint-cli2 is available
 26:     check_cmd = ["which", "markdownlint-cli2"]
 27:     check_result = subprocess.run(
 28:         check_cmd,
 29:         capture_output=True,
 30:         text=True
 31:     )
 32: 
 33:     if check_result.returncode != 0:
 34:         raise RuntimeError(
 35:             "markdownlint-cli2 not found\n"
 36:             "üîß Troubleshooting: Install with 'npm install --save-dev markdownlint-cli2'"
 37:         )
 38: 
 39:     # Patterns for markdown files to lint
 40:     patterns = [
 41:         "docs/**/*.md",
 42:         "PRPs/**/*.md",
 43:         "examples/**/*.md",
 44:         "*.md"
 45:     ]
 46: 
 47:     cmd = ["markdownlint-cli2"]
 48:     if auto_fix:
 49:         cmd.append("--fix")
 50:     cmd.extend(patterns)
 51: 
 52:     # Run from project root
 53:     project_root = Path(__file__).parent.parent.parent
 54: 
 55:     result = subprocess.run(
 56:         cmd,
 57:         capture_output=True,
 58:         text=True,
 59:         cwd=project_root
 60:     )
 61: 
 62:     # Parse output
 63:     output_lines = result.stdout.strip().split("\n") if result.stdout else []
 64:     error_lines = result.stderr.strip().split("\n") if result.stderr else []
 65: 
 66:     # Count fixes if auto-fix was enabled
 67:     fixed_count = 0
 68:     if auto_fix:
 69:         for line in output_lines:
 70:             if "Fixed:" in line:
 71:                 fixed_count += 1
 72: 
 73:     return {
 74:         "success": result.returncode == 0,
 75:         "errors": [line for line in error_lines if line],
 76:         "output": [line for line in output_lines if line],
 77:         "fixed_count": fixed_count,
 78:         "exit_code": result.returncode
 79:     }
 80: 
 81: 
 82: def run_markdown_validation(auto_fix: bool = True) -> Dict[str, Any]:
 83:     """Run markdown validation with optional auto-fix.
 84: 
 85:     Args:
 86:         auto_fix: If True, attempt to auto-fix issues before reporting
 87: 
 88:     Returns:
 89:         Dict with success, message, details
 90:     """
 91:     try:
 92:         # First try to auto-fix if requested
 93:         if auto_fix:
 94:             fix_result = lint_markdown(auto_fix=True)
 95:             if fix_result["fixed_count"] > 0:
 96:                 # Re-run validation to check if all issues were fixed
 97:                 validation_result = lint_markdown(auto_fix=False)
 98:                 return {
 99:                     "success": validation_result["success"],
100:                     "message": f"Fixed {fix_result['fixed_count']} issues, validation {'passed' if validation_result['success'] else 'has remaining issues'}",
101:                     "details": {
102:                         "fixed_count": fix_result["fixed_count"],
103:                         "remaining_errors": validation_result["errors"]
104:                     }
105:                 }
106: 
107:         # Run validation without auto-fix
108:         result = lint_markdown(auto_fix=False)
109: 
110:         if result["success"]:
111:             return {
112:                 "success": True,
113:                 "message": "All markdown files validated successfully",
114:                 "details": result
115:             }
116:         else:
117:             return {
118:                 "success": False,
119:                 "message": f"Markdown validation found {len(result['errors'])} issues",
120:                 "details": result
121:             }
122: 
123:     except Exception as e:
124:         raise RuntimeError(
125:             f"Markdown validation failed: {str(e)}\n"
126:             f"üîß Troubleshooting: Ensure markdownlint-cli2 is installed via npm"
127:         )
</file>

<file path="tools/ce/mcp_adapter.py">
  1: """MCP adapter layer for Serena file operations with graceful fallback.
  2: 
  3: This module provides abstraction for file operations, using Serena MCP when available
  4: and falling back to local filesystem operations when MCP is unavailable.
  5: 
  6: MCP Availability:
  7:     - Claude Code context: Serena MCP typically available
  8:     - Standalone CLI: Falls back to filesystem
  9:     - Test environment: Uses mcp_fake for testing
 10: 
 11: Design Decision (ADR-001):
 12:     - Optional fallback approach for MVP
 13:     - Simple try/catch detection
 14:     - Unified error handling
 15:     - Performance acceptable (<100ms overhead per MCP call)
 16: """
 17: 
 18: from typing import Dict, Any, List, Optional
 19: from pathlib import Path
 20: from ce.resilience import retry_with_backoff, CircuitBreaker, CircuitBreakerOpenError
 21: from ce.logging_config import get_logger
 22: 
 23: # Logger
 24: logger = get_logger(__name__)
 25: 
 26: # Global circuit breaker for Serena MCP operations
 27: serena_breaker = CircuitBreaker(name="serena-mcp", failure_threshold=5, recovery_timeout=60)
 28: 
 29: 
 30: def _import_serena_mcp():
 31:     """Import Serena MCP module dynamically.
 32: 
 33:     Returns:
 34:         The mcp__serena module
 35: 
 36:     Raises:
 37:         ImportError: If module cannot be imported
 38: 
 39:     Note: Helper function to avoid repeated import logic throughout module.
 40:     """
 41:     import importlib
 42:     return importlib.import_module("mcp__serena")
 43: 
 44: 
 45: def is_mcp_available() -> bool:
 46:     """Check if Serena MCP is available at runtime.
 47: 
 48:     Returns:
 49:         True if Serena MCP tools are available, False otherwise
 50: 
 51:     Detection Strategy:
 52:         1. Try importing mcp__serena tools
 53:         2. Attempt minimal read operation
 54:         3. Cache result for session (not implemented in MVP)
 55: 
 56:     Note: This is a simple detection strategy. More sophisticated
 57:     approaches (version checking, capability negotiation) deferred to future.
 58:     """
 59:     try:
 60:         # Attempt to import Serena MCP tools
 61:         serena_module = _import_serena_mcp()
 62: 
 63:         # Check if key functions exist
 64:         required_functions = [
 65:             "read_file",
 66:             "create_text_file",
 67:             "get_symbols_overview",
 68:             "insert_after_symbol"
 69:         ]
 70: 
 71:         for func_name in required_functions:
 72:             if not hasattr(serena_module, func_name):
 73:                 return False
 74: 
 75:         return True
 76: 
 77:     except (ImportError, ModuleNotFoundError, AttributeError):
 78:         return False
 79: 
 80: 
 81: @retry_with_backoff(max_attempts=3, base_delay=1.0, exceptions=(IOError, ConnectionError, TimeoutError))
 82: def _create_file_via_mcp(filepath: str, content: str):
 83:     """Create file via MCP with retry logic.
 84: 
 85:     Args:
 86:         filepath: Relative path to file
 87:         content: File content
 88: 
 89:     Raises:
 90:         Exception: If MCP call fails after retries
 91: 
 92:     Note: Internal function with retry decorator. Circuit breaker applied at call site.
 93:     """
 94:     serena = _import_serena_mcp()
 95:     serena.create_text_file(filepath, content)
 96: 
 97: 
 98: def create_file_with_mcp(filepath: str, content: str) -> Dict[str, Any]:
 99:     """Create file using Serena MCP or fallback to filesystem.
100: 
101:     Args:
102:         filepath: Relative path to file to create
103:         content: File content
104: 
105:     Returns:
106:         {
107:             "success": True,
108:             "method": "mcp" or "filesystem",
109:             "filepath": "<path>",
110:             "error": "<error message if success=False>"
111:         }
112: 
113:     Process:
114:         1. Check MCP availability
115:         2. If available, try mcp__serena__create_text_file (with retry + circuit breaker)
116:         3. On MCP failure or unavailable, fallback to filesystem
117:         4. Return result with method used
118: 
119:     Raises:
120:         RuntimeError: If both MCP and filesystem operations fail
121: 
122:     Note: Graceful fallback ensures execution continues even when MCP unavailable.
123:     """
124:     # Try MCP first if available
125:     if is_mcp_available():
126:         try:
127:             # Apply circuit breaker + retry
128:             _create_file_via_mcp(filepath, content)
129: 
130:             return {
131:                 "success": True,
132:                 "method": "mcp",
133:                 "filepath": filepath
134:             }
135: 
136:         except CircuitBreakerOpenError as e:
137:             # Circuit breaker open - fall back immediately
138:             logger.warning(
139:                 "MCP circuit breaker open, falling back to filesystem",
140:                 extra={"filepath": filepath, "error": str(e)}
141:             )
142: 
143:         except Exception as e:
144:             # Other MCP failure - log and fallback
145:             logger.warning(
146:                 "MCP file creation failed, falling back to filesystem",
147:                 extra={"filepath": filepath, "error": str(e)}
148:             )
149: 
150:     # Fallback to filesystem
151:     try:
152:         file_path = Path(filepath)
153:         file_path.parent.mkdir(parents=True, exist_ok=True)
154:         file_path.write_text(content)
155: 
156:         return {
157:             "success": True,
158:             "method": "filesystem",
159:             "filepath": filepath
160:         }
161: 
162:     except Exception as e:
163:         raise RuntimeError(
164:             f"Failed to create {filepath} (both MCP and filesystem failed)\n"
165:             f"Error: {str(e)}\n"
166:             f"üîß Troubleshooting:\n"
167:             f"  1. Check file path is valid\n"
168:             f"  2. Verify parent directory exists or can be created\n"
169:             f"  3. Check write permissions\n"
170:             f"  4. Review file content for invalid characters"
171:         ) from e
172: 
173: 
174: @retry_with_backoff(max_attempts=3, base_delay=1.0, exceptions=(IOError, ConnectionError, TimeoutError))
175: def _insert_code_via_mcp(filepath: str, code: str, mode: str, symbol_name: str):
176:     """Insert code via MCP with retry logic.
177: 
178:     Args:
179:         filepath: Path to file
180:         code: Code to insert
181:         mode: "after" or "before"
182:         symbol_name: Symbol name path
183: 
184:     Raises:
185:         Exception: If MCP call fails after retries
186: 
187:     Note: Internal function with retry decorator.
188:     """
189:     serena = _import_serena_mcp()
190:     if mode == "after":
191:         serena.insert_after_symbol(symbol_name, filepath, code)
192:     else:
193:         serena.insert_before_symbol(symbol_name, filepath, code)
194: 
195: 
196: def insert_code_with_mcp(
197:     filepath: str,
198:     code: str,
199:     mode: str = "append"
200: ) -> Dict[str, Any]:
201:     """Insert code using Serena MCP symbol operations or fallback.
202: 
203:     Args:
204:         filepath: Path to file to modify
205:         code: Code to insert
206:         mode: Insertion mode - "append", "after_last_symbol", "before_first_symbol"
207: 
208:     Returns:
209:         {
210:             "success": True,
211:             "method": "mcp_symbol_aware" | "mcp_append" | "filesystem_append",
212:             "filepath": "<path>",
213:             "symbol": "<symbol name if symbol-aware>",
214:             "error": "<error message if success=False>"
215:         }
216: 
217:     Process:
218:         1. Check MCP availability
219:         2. If available and mode is symbol-aware:
220:            a. Get symbols overview
221:            b. Insert after/before symbol (with retry + circuit breaker)
222:         3. If MCP unavailable or append mode:
223:            a. Read file, append code, write back
224:         4. Return result with method used
225: 
226:     Raises:
227:         RuntimeError: If file modification fails
228: 
229:     Note: Symbol-aware insertion requires Serena MCP. Fallback mode is naive append.
230:     """
231:     # Try MCP symbol-aware insertion
232:     if is_mcp_available() and mode != "append":
233:         try:
234:             serena = _import_serena_mcp()
235: 
236:             # Get symbols to find insertion point
237:             symbols = serena.get_symbols_overview(filepath)
238: 
239:             if symbols and len(symbols) > 0:
240:                 if mode == "after_last_symbol":
241:                     last_symbol = symbols[-1]["name_path"]
242:                     _insert_code_via_mcp(filepath, code, "after", last_symbol)
243: 
244:                     return {
245:                         "success": True,
246:                         "method": "mcp_symbol_aware",
247:                         "filepath": filepath,
248:                         "symbol": last_symbol
249:                     }
250: 
251:                 elif mode == "before_first_symbol":
252:                     first_symbol = symbols[0]["name_path"]
253:                     _insert_code_via_mcp(filepath, code, "before", first_symbol)
254: 
255:                     return {
256:                         "success": True,
257:                         "method": "mcp_symbol_aware",
258:                         "filepath": filepath,
259:                         "symbol": first_symbol
260:                     }
261: 
262:             # No symbols found, fall through to append
263: 
264:         except CircuitBreakerOpenError as e:
265:             # Circuit breaker open - fall back immediately
266:             logger.warning(
267:                 "MCP circuit breaker open, falling back to append",
268:                 extra={"filepath": filepath, "mode": mode, "error": str(e)}
269:             )
270: 
271:         except Exception as e:
272:             # Other MCP failure - log and fallback
273:             logger.warning(
274:                 "MCP symbol insertion failed, falling back to append",
275:                 extra={"filepath": filepath, "mode": mode, "error": str(e)}
276:             )
277: 
278:     # Fallback: append to end of file
279:     try:
280:         file_path = Path(filepath)
281:         if not file_path.exists():
282:             raise RuntimeError(
283:                 f"Cannot modify file {filepath} - file does not exist\n"
284:                 f"üîß Troubleshooting: Ensure file is created before modification"
285:             )
286: 
287:         current_content = file_path.read_text()
288:         new_content = current_content + "\n\n" + code
289:         file_path.write_text(new_content)
290: 
291:         return {
292:             "success": True,
293:             "method": "filesystem_append",
294:             "filepath": filepath
295:         }
296: 
297:     except Exception as e:
298:         raise RuntimeError(
299:             f"Failed to insert code into {filepath}\n"
300:             f"Error: {str(e)}\n"
301:             f"üîß Troubleshooting:\n"
302:             f"  1. Check file exists and is writable\n"
303:             f"  2. Verify code is syntactically valid\n"
304:             f"  3. Check file has valid Python syntax for symbol parsing"
305:         ) from e
306: 
307: 
308: def get_mcp_status() -> Dict[str, Any]:
309:     """Get MCP availability status for diagnostics.
310: 
311:     Returns:
312:         {
313:             "available": True/False,
314:             "version": "<version if available>",
315:             "capabilities": ["read_file", "create_text_file", ...],
316:             "context": "mcp" | "standalone" | "test"
317:         }
318: 
319:     Note: Version and detailed capabilities detection deferred to future.
320:     For MVP, only availability check implemented.
321:     """
322:     available = is_mcp_available()
323: 
324:     result = {
325:         "available": available,
326:         "version": None,  # Not implemented in MVP
327:         "capabilities": [],  # Not implemented in MVP
328:         "context": "mcp" if available else "standalone"
329:     }
330: 
331:     if available:
332:         try:
333:             serena = _import_serena_mcp()
334: 
335:             # List available functions
336:             capabilities = [
337:                 name for name in dir(serena)
338:                 if not name.startswith("_") and callable(getattr(serena, name))
339:             ]
340:             result["capabilities"] = capabilities
341: 
342:         except Exception:
343:             pass
344: 
345:     return result
</file>

<file path="tools/ce/mcp_utils.py">
  1: """MCP utility functions for Syntropy tool calls.
  2: 
  3: Provides wrappers for calling Syntropy MCP tools with proper
  4: error handling and logging.
  5: """
  6: 
  7: import logging
  8: from typing import Dict, Any, Optional
  9: 
 10: logger = logging.getLogger(__name__)
 11: 
 12: 
 13: def call_syntropy_mcp(
 14:     server: str,
 15:     tool: str,
 16:     arguments: Dict[str, Any],
 17:     timeout: int = 10
 18: ) -> Dict[str, Any]:
 19:     """Call Syntropy MCP tool.
 20: 
 21:     Args:
 22:         server: Server name (e.g., "thinking", "serena", "context7")
 23:         tool: Tool name (e.g., "sequentialthinking", "find_symbol")
 24:         arguments: Tool arguments
 25:         timeout: Timeout in seconds
 26: 
 27:     Returns:
 28:         Tool result dictionary
 29: 
 30:     Raises:
 31:         RuntimeError: If MCP call fails
 32: 
 33:     Note: This is a placeholder. Actual implementation will use
 34:     Claude Code's MCP infrastructure to make real tool calls.
 35:     """
 36:     logger.info(f"Calling Syntropy MCP: {server}:{tool}")
 37:     logger.debug(f"Arguments: {arguments}")
 38: 
 39:     # FIXME: Placeholder - replace with actual MCP call
 40:     # In full implementation, this would:
 41:     # 1. Import Claude Code MCP client
 42:     # 2. Get client for server: client = get_mcp_client(f"syntropy-{server}")
 43:     # 3. Call tool: result = client.call_tool(tool, arguments, timeout=timeout)
 44:     # 4. Return result
 45: 
 46:     # For now, log and raise (graceful degradation in callers)
 47:     raise RuntimeError(
 48:         f"MCP call not yet implemented: {server}:{tool}\n"
 49:         f"üîß Troubleshooting: Full MCP integration pending"
 50:     )
 51: 
 52: 
 53: def is_mcp_available(server: str) -> bool:
 54:     """Check if MCP server is available.
 55: 
 56:     Args:
 57:         server: Server name (e.g., "thinking", "serena")
 58: 
 59:     Returns:
 60:         True if server available, False otherwise
 61:     """
 62:     try:
 63:         # FIXME: Placeholder - replace with actual availability check
 64:         # Would ping server or check connection status
 65:         logger.debug(f"Checking MCP availability: {server}")
 66:         return False  # Return False until implemented
 67:     except Exception as e:
 68:         logger.warning(f"MCP availability check failed: {e}")
 69:         return False
 70: 
 71: 
 72: def call_sequential_thinking(
 73:     prompt: str,
 74:     thought_number: int = 1,
 75:     total_thoughts: int = 5
 76: ) -> Optional[Dict[str, Any]]:
 77:     """Call sequential thinking MCP tool.
 78: 
 79:     Convenience wrapper for mcp__syntropy__thinking__sequentialthinking
 80: 
 81:     Args:
 82:         prompt: Thinking prompt
 83:         thought_number: Current thought number
 84:         total_thoughts: Estimated total thoughts
 85: 
 86:     Returns:
 87:         Thinking result or None if unavailable
 88:     """
 89:     try:
 90:         return call_syntropy_mcp(
 91:             "thinking",
 92:             "sequentialthinking",
 93:             {
 94:                 "thought": prompt,
 95:                 "thoughtNumber": thought_number,
 96:                 "totalThoughts": total_thoughts,
 97:                 "nextThoughtNeeded": True
 98:             }
 99:         )
100:     except Exception as e:
101:         logger.warning(f"Sequential thinking unavailable: {e}")
102:         return None
</file>

<file path="tools/ce/mermaid_validator.py">
  1: """Mermaid diagram validator with auto-fix for unquoted special characters."""
  2: 
  3: import re
  4: from pathlib import Path
  5: from typing import Dict, Any, List, Tuple
  6: 
  7: 
  8: def validate_mermaid_diagrams(file_path: str, auto_fix: bool = False) -> Dict[str, Any]:
  9:     r"""Validate mermaid diagrams in markdown file.
 10: 
 11:     Args:
 12:         file_path: Path to markdown file
 13:         auto_fix: If True, auto-fix issues by renaming nodes or adding quotes
 14: 
 15:     Returns:
 16:         Dict with: success (bool), errors (List[str]), fixes_applied (List[str])
 17: 
 18:     Validation rules:
 19:     1. Node text with special chars must be quoted or use simple node IDs
 20:     2. Node IDs should be simple (A, B, C1, etc.) if text has special chars
 21:     3. Text with <>[]{}()!?/\ should be in quotes or node renamed
 22:     4. Style statements should always specify color for theme compatibility
 23: 
 24:     Auto-fix strategies:
 25:     - Strategy 1: Rename nodes with special chars (A, B, C, D1, D2, etc.)
 26:     - Strategy 2: Quote text if short and quotes not present
 27:     - Strategy 3: Check style statements have color specified
 28:     """
 29:     content = Path(file_path).read_text()
 30:     errors = []
 31:     fixes_applied = []
 32: 
 33:     # Extract all mermaid blocks
 34:     mermaid_blocks = re.findall(
 35:         r'```mermaid\n(.*?)```',
 36:         content,
 37:         re.DOTALL
 38:     )
 39: 
 40:     if not mermaid_blocks:
 41:         return {
 42:             "success": True,
 43:             "errors": [],
 44:             "fixes_applied": [],
 45:             "diagrams_checked": 0
 46:         }
 47: 
 48:     for i, block in enumerate(mermaid_blocks):
 49:         block_errors, block_fixes = _validate_mermaid_block(block, i + 1)
 50:         errors.extend(block_errors)
 51: 
 52:         if auto_fix and block_fixes:
 53:             # Apply fixes to content
 54:             fixed_block = _apply_fixes_to_block(block, block_fixes)
 55:             content = content.replace(f'```mermaid\n{block}```', f'```mermaid\n{fixed_block}```')
 56:             fixes_applied.extend([f"Diagram {i+1}: {fix}" for fix in block_fixes])
 57: 
 58:     # Write back if fixes applied
 59:     if auto_fix and fixes_applied:
 60:         Path(file_path).write_text(content)
 61: 
 62:     return {
 63:         "success": len(errors) == 0 or (auto_fix and len(fixes_applied) > 0),
 64:         "errors": errors,
 65:         "fixes_applied": fixes_applied,
 66:         "diagrams_checked": len(mermaid_blocks)
 67:     }
 68: 
 69: 
 70: def _validate_mermaid_block(block: str, diagram_num: int) -> Tuple[List[str], List[str]]:
 71:     r"""Validate single mermaid block.
 72: 
 73:     Returns:
 74:         (errors, fix_suggestions) tuple
 75:     """
 76:     errors = []
 77:     fixes = []
 78: 
 79:     # Check 1: Node definitions with special chars but no quotes
 80:     # Pattern: NodeID[Text with special chars] or NodeID{Text with special chars}
 81:     node_pattern = r'([A-Z0-9]+)[\[\{]([^\]\}]+)[\]\}]'
 82:     nodes = re.findall(node_pattern, block)
 83: 
 84:     for node_id, node_text in nodes:
 85:         if _has_unquoted_special_chars(node_text):
 86:             errors.append(
 87:                 f"Diagram {diagram_num}: Node '{node_id}' has unquoted special chars in text: '{node_text}'"
 88:             )
 89:             fixes.append(f"Rename node '{node_id}' or quote text '{node_text}'")
 90: 
 91:     # Check 2: Style statements missing color specification
 92:     style_pattern = r'style\s+([A-Z0-9]+)\s+fill:(#[0-9a-fA-F]{6}|#[0-9a-fA-F]{3})(?!.*color:)'
 93:     styles_missing_color = re.findall(style_pattern, block)
 94: 
 95:     for node_id in styles_missing_color:
 96:         errors.append(
 97:             f"Diagram {diagram_num}: Style for node '{node_id}' missing color specification"
 98:         )
 99:         fixes.append(f"Add color:#000 or color:#fff to style {node_id}")
100: 
101:     # Check 3: Line breaks in node text without <br/> tag
102:     linebreak_pattern = r'[\[\{]([^\]\}]*\n[^\]\}]*)[\]\}]'
103:     linebreaks = re.findall(linebreak_pattern, block)
104: 
105:     for text in linebreaks:
106:         if '<br/>' not in text:
107:             errors.append(
108:                 f"Diagram {diagram_num}: Multiline text without <br/> tag: '{text[:50]}...'"
109:             )
110:             fixes.append("Replace newlines with <br/> in node text")
111: 
112:     return errors, fixes
113: 
114: 
115: def _has_unquoted_special_chars(text: str) -> bool:
116:     """Check if text has special chars that need quoting.
117: 
118:     Special chars that ACTUALLY break mermaid rendering:
119:     - Parentheses: () - used for node shape syntax
120:     - Brackets: [] - used for node shape syntax
121:     - Curly braces: {} - used for node shape syntax
122:     - Pipes: | - used for subgraph syntax
123:     - Unbalanced quotes: "' - break parsing
124: 
125:     Characters that are SAFE in mermaid node text:
126:     - Colons: : - commonly used, safe
127:     - Question marks: ? - safe
128:     - Exclamation marks: ! - safe
129:     - Slashes: / \\ - safe
130:     - HTML tags: <br/>, <sub>, <sup> - explicitly allowed
131: 
132:     Note: HTML tags like <br/> are allowed unquoted in mermaid.
133:     """
134:     # If already quoted, it's fine
135:     if (text.startswith('"') and text.endswith('"')) or \
136:        (text.startswith("'") and text.endswith("'")):
137:         return False
138: 
139:     # Exclude HTML tags from special char check
140:     # HTML tags like <br/>, <sub>, <sup> are valid mermaid syntax
141:     text_without_html = re.sub(r'<[^>]+>', '', text)
142: 
143:     # Only check for truly problematic chars that break mermaid syntax
144:     # Removed: : ? ! / \\ (these are safe in mermaid node text)
145:     special_chars = r'[\[\]\{\}\(\)\|\'"]'
146:     return bool(re.search(special_chars, text_without_html))
147: 
148: 
149: def _apply_fixes_to_block(block: str, fixes: List[str]) -> str:
150:     """Apply fixes to mermaid block.
151: 
152:     Fix strategies:
153:     1. Rename nodes with special chars to simple IDs
154:     2. Add color to style statements
155:     3. Convert newlines to <br/> in node text
156:     """
157:     fixed_block = block
158: 
159:     # Fix 1: Rename problematic nodes
160:     node_pattern = r'([A-Z0-9]+)[\[\{]([^\]\}]+)[\]\}]'
161:     nodes = re.findall(node_pattern, fixed_block)
162:     node_mapping = {}  # old_id -> new_id
163:     next_id = 1
164: 
165:     for node_id, node_text in nodes:
166:         if _has_unquoted_special_chars(node_text):
167:             # Generate new simple ID
168:             new_id = f"N{next_id}"
169:             next_id += 1
170:             node_mapping[node_id] = new_id
171: 
172:             # Replace all occurrences of old node ID
173:             # Pattern: node_id at word boundary (not part of another word)
174:             fixed_block = re.sub(
175:                 rf'\b{node_id}\b',
176:                 new_id,
177:                 fixed_block
178:             )
179: 
180:     # Fix 2: Add color to style statements missing it
181:     style_pattern = r'(style\s+[A-Z0-9]+\s+fill:#[0-9a-fA-F]{3,6})(?!.*color:)'
182: 
183:     def add_color(match):
184:         style_stmt = match.group(1)
185:         # Determine text color based on background lightness
186:         fill_match = re.search(r'fill:(#[0-9a-fA-F]{3,6})', style_stmt)
187:         if fill_match:
188:             bg_color = fill_match.group(1)
189:             text_color = _determine_text_color(bg_color)
190:             return f"{style_stmt},color:{text_color}"
191:         return style_stmt
192: 
193:     fixed_block = re.sub(style_pattern, add_color, fixed_block)
194: 
195:     # Fix 3: Convert multiline text to <br/>
196:     def fix_linebreaks(match):
197:         bracket_type = match.group(1)
198:         close_bracket = ']' if bracket_type == '[' else '}'
199:         content = match.group(2)
200:         fixed_content = content.replace('\n', '<br/>')
201:         return f"{bracket_type}{fixed_content}{close_bracket}"
202: 
203:     fixed_block = re.sub(
204:         r'([\[\{])([^\]\}]*\n[^\]\}]*)([\]\}])',
205:         fix_linebreaks,
206:         fixed_block
207:     )
208: 
209:     return fixed_block
210: 
211: 
212: def _determine_text_color(bg_color: str) -> str:
213:     """Determine text color (#000 or #fff) based on background lightness.
214: 
215:     Uses relative luminance formula:
216:     L = 0.2126 * R + 0.7152 * G + 0.0722 * B
217: 
218:     Args:
219:         bg_color: Hex color (#RGB or #RRGGBB)
220: 
221:     Returns:
222:         '#000' for light backgrounds, '#fff' for dark backgrounds
223:     """
224:     # Expand shorthand hex (#RGB -> #RRGGBB)
225:     if len(bg_color) == 4:  # #RGB
226:         bg_color = f"#{bg_color[1]*2}{bg_color[2]*2}{bg_color[3]*2}"
227: 
228:     # Extract RGB components
229:     r = int(bg_color[1:3], 16) / 255.0
230:     g = int(bg_color[3:5], 16) / 255.0
231:     b = int(bg_color[5:7], 16) / 255.0
232: 
233:     # Apply sRGB gamma correction
234:     def gamma_correct(c):
235:         return c / 12.92 if c <= 0.03928 else ((c + 0.055) / 1.055) ** 2.4
236: 
237:     r = gamma_correct(r)
238:     g = gamma_correct(g)
239:     b = gamma_correct(b)
240: 
241:     # Calculate relative luminance
242:     luminance = 0.2126 * r + 0.7152 * g + 0.0722 * b
243: 
244:     # Return black for light backgrounds, white for dark
245:     return '#000' if luminance > 0.5 else '#fff'
246: 
247: 
248: def lint_all_markdown_mermaid(directory: str = ".", auto_fix: bool = False) -> Dict[str, Any]:
249:     """Lint mermaid diagrams in all markdown files.
250: 
251:     Args:
252:         directory: Root directory to search (default: current)
253:         auto_fix: Apply fixes automatically
254: 
255:     Returns:
256:         Dict with aggregated results
257:     """
258:     md_files = list(Path(directory).rglob("*.md"))
259:     all_errors = []
260:     all_fixes = []
261:     files_with_issues = []
262:     total_diagrams = 0
263: 
264:     for md_file in md_files:
265:         result = validate_mermaid_diagrams(str(md_file), auto_fix=auto_fix)
266:         total_diagrams += result["diagrams_checked"]
267: 
268:         if result["errors"]:
269:             files_with_issues.append(str(md_file))
270:             all_errors.extend([f"{md_file}: {err}" for err in result["errors"]])
271: 
272:         if result["fixes_applied"]:
273:             all_fixes.extend([f"{md_file}: {fix}" for fix in result["fixes_applied"]])
274: 
275:     return {
276:         "success": len(all_errors) == 0 or (auto_fix and len(all_fixes) > 0),
277:         "files_checked": len(md_files),
278:         "diagrams_checked": total_diagrams,
279:         "files_with_issues": len(files_with_issues),
280:         "errors": all_errors,
281:         "fixes_applied": all_fixes
282:     }
283: 
284: 
285: if __name__ == "__main__":
286:     import sys
287: 
288:     # CLI usage: python mermaid_validator.py [--fix] [path]
289:     auto_fix = "--fix" in sys.argv
290:     path = sys.argv[-1] if len(sys.argv) > 1 and not sys.argv[-1].startswith("--") else "."
291: 
292:     result = lint_all_markdown_mermaid(path, auto_fix=auto_fix)
293: 
294:     print(f"\n{'='*80}")
295:     print(f"Mermaid Diagram Validation")
296:     print(f"{'='*80}")
297:     print(f"Files checked: {result['files_checked']}")
298:     print(f"Diagrams checked: {result['diagrams_checked']}")
299:     print(f"Files with issues: {result['files_with_issues']}")
300: 
301:     if result['errors']:
302:         print(f"\n{'='*80}")
303:         print("ERRORS:")
304:         print(f"{'='*80}")
305:         for error in result['errors']:
306:             print(f"‚ùå {error}")
307: 
308:     if result['fixes_applied']:
309:         print(f"\n{'='*80}")
310:         print("FIXES APPLIED:")
311:         print(f"{'='*80}")
312:         for fix in result['fixes_applied']:
313:             print(f"‚úÖ {fix}")
314: 
315:     print(f"\n{'='*80}")
316:     print(f"Result: {'‚úÖ PASS' if result['success'] else '‚ùå FAIL'}")
317:     print(f"{'='*80}\n")
318: 
319:     sys.exit(0 if result['success'] else 1)
</file>

<file path="tools/ce/metrics.py">
  1: """Metrics collection module - track performance and success rates.
  2: 
  3: Provides lightweight metrics collection for tracking PRP execution success rates,
  4: timing data, and validation results without heavy telemetry infrastructure.
  5: """
  6: 
  7: from typing import Dict, Any, List
  8: from datetime import datetime
  9: import json
 10: from pathlib import Path
 11: 
 12: 
 13: class MetricsCollector:
 14:     """Collect and persist performance metrics.
 15: 
 16:     Tracks success rates, timing data, and validation results.
 17: 
 18:     Example:
 19:         metrics = MetricsCollector()
 20:         metrics.record_prp_execution(
 21:             prp_id="PRP-003",
 22:             success=True,
 23:             duration=1200.5,
 24:             first_pass=True,
 25:             validation_level=4
 26:         )
 27:         metrics.save()
 28: 
 29:     Attributes:
 30:         metrics_file: Path to metrics JSON file
 31:         metrics: Dict containing all collected metrics
 32:     """
 33: 
 34:     def __init__(self, metrics_file: str = "metrics.json"):
 35:         """Initialize metrics collector.
 36: 
 37:         Args:
 38:             metrics_file: Path to metrics JSON file
 39: 
 40:         Note: Creates new metrics file if it doesn't exist.
 41:         """
 42:         self.metrics_file = Path(metrics_file)
 43:         self.metrics: Dict[str, Any] = self._load_metrics()
 44: 
 45:     def _load_metrics(self) -> Dict[str, Any]:
 46:         """Load existing metrics from file.
 47: 
 48:         Returns:
 49:             Dict with metrics data structure
 50: 
 51:         Note: Creates empty structure if file doesn't exist.
 52:         """
 53:         if self.metrics_file.exists():
 54:             try:
 55:                 return json.loads(self.metrics_file.read_text())
 56:             except json.JSONDecodeError:
 57:                 # Corrupted file - start fresh
 58:                 return self._empty_metrics()
 59:         return self._empty_metrics()
 60: 
 61:     def _empty_metrics(self) -> Dict[str, Any]:
 62:         """Create empty metrics structure.
 63: 
 64:         Returns:
 65:             Dict with empty metrics data structure
 66:         """
 67:         return {
 68:             "prp_executions": [],
 69:             "validation_results": [],
 70:             "performance_stats": {}
 71:         }
 72: 
 73:     def record_prp_execution(
 74:         self,
 75:         prp_id: str,
 76:         success: bool,
 77:         duration: float,
 78:         first_pass: bool,
 79:         validation_level: int
 80:     ):
 81:         """Record PRP execution metrics.
 82: 
 83:         Args:
 84:             prp_id: PRP identifier
 85:             success: Whether execution succeeded
 86:             duration: Execution time in seconds
 87:             first_pass: Whether succeeded on first pass
 88:             validation_level: Highest validation level passed (1-4)
 89: 
 90:         Note: Call save() after recording to persist metrics.
 91:         """
 92:         self.metrics["prp_executions"].append({
 93:             "prp_id": prp_id,
 94:             "timestamp": datetime.now().isoformat(),
 95:             "success": success,
 96:             "duration": duration,
 97:             "first_pass": first_pass,
 98:             "validation_level": validation_level
 99:         })
100: 
101:     def record_validation_result(
102:         self,
103:         prp_id: str,
104:         validation_level: int,
105:         passed: bool,
106:         duration: float,
107:         error_message: str = None
108:     ):
109:         """Record validation gate result.
110: 
111:         Args:
112:             prp_id: PRP identifier
113:             validation_level: Validation level (1-4)
114:             passed: Whether validation passed
115:             duration: Validation time in seconds
116:             error_message: Error message if failed
117: 
118:         Note: Call save() after recording to persist metrics.
119:         """
120:         self.metrics["validation_results"].append({
121:             "prp_id": prp_id,
122:             "timestamp": datetime.now().isoformat(),
123:             "validation_level": validation_level,
124:             "passed": passed,
125:             "duration": duration,
126:             "error_message": error_message
127:         })
128: 
129:     def calculate_success_rates(self) -> Dict[str, float]:
130:         """Calculate success rate metrics.
131: 
132:         Returns:
133:             Dict with first_pass_rate, second_pass_rate, overall_rate, total_executions
134: 
135:         Note: Returns 0.0 rates if no executions recorded.
136:         """
137:         executions = self.metrics["prp_executions"]
138:         if not executions:
139:             return {
140:                 "first_pass_rate": 0.0,
141:                 "second_pass_rate": 0.0,
142:                 "overall_rate": 0.0,
143:                 "total_executions": 0
144:             }
145: 
146:         total = len(executions)
147:         first_pass = sum(1 for e in executions if e["first_pass"])
148:         successful = sum(1 for e in executions if e["success"])
149: 
150:         return {
151:             "first_pass_rate": (first_pass / total) * 100,
152:             "second_pass_rate": (successful / total) * 100,
153:             "overall_rate": (successful / total) * 100,
154:             "total_executions": total
155:         }
156: 
157:     def calculate_validation_stats(self) -> Dict[str, Any]:
158:         """Calculate validation gate statistics.
159: 
160:         Returns:
161:             Dict with pass rates per validation level
162: 
163:         Note: Returns empty dict if no validations recorded.
164:         """
165:         validations = self.metrics["validation_results"]
166:         if not validations:
167:             return {}
168: 
169:         # Group by level
170:         by_level = {}
171:         for v in validations:
172:             level = v["validation_level"]
173:             if level not in by_level:
174:                 by_level[level] = {"total": 0, "passed": 0}
175:             by_level[level]["total"] += 1
176:             if v["passed"]:
177:                 by_level[level]["passed"] += 1
178: 
179:         # Calculate pass rates
180:         stats = {}
181:         for level, data in by_level.items():
182:             stats[f"L{level}_pass_rate"] = (data["passed"] / data["total"]) * 100
183:             stats[f"L{level}_total"] = data["total"]
184: 
185:         return stats
186: 
187:     def get_average_duration(self) -> float:
188:         """Calculate average PRP execution duration.
189: 
190:         Returns:
191:             Average duration in seconds, or 0.0 if no executions
192: 
193:         Note: Includes both successful and failed executions.
194:         """
195:         executions = self.metrics["prp_executions"]
196:         if not executions:
197:             return 0.0
198: 
199:         total_duration = sum(e["duration"] for e in executions)
200:         return total_duration / len(executions)
201: 
202:     def save(self):
203:         """Persist metrics to file.
204: 
205:         Raises:
206:             RuntimeError: If file cannot be written
207: 
208:         Note: Creates parent directory if needed.
209:         """
210:         try:
211:             self.metrics_file.parent.mkdir(parents=True, exist_ok=True)
212:             self.metrics_file.write_text(json.dumps(self.metrics, indent=2))
213:         except Exception as e:
214:             raise RuntimeError(
215:                 f"Failed to save metrics to {self.metrics_file}\n"
216:                 f"Error: {str(e)}\n"
217:                 f"üîß Troubleshooting:\n"
218:                 f"  1. Check write permissions\n"
219:                 f"  2. Ensure parent directory exists or can be created\n"
220:                 f"  3. Verify disk space available"
221:             ) from e
222: 
223:     def get_summary(self) -> Dict[str, Any]:
224:         """Get comprehensive metrics summary.
225: 
226:         Returns:
227:             Dict with success rates, validation stats, and performance metrics
228: 
229:         Example:
230:             {
231:                 "success_rates": {"first_pass_rate": 85.0, ...},
232:                 "validation_stats": {"L1_pass_rate": 95.0, ...},
233:                 "performance": {"avg_duration": 1200.5, ...}
234:             }
235: 
236:         Note: Useful for status dashboards and reports.
237:         """
238:         return {
239:             "success_rates": self.calculate_success_rates(),
240:             "validation_stats": self.calculate_validation_stats(),
241:             "performance": {
242:                 "avg_duration": self.get_average_duration(),
243:                 "total_prps": len(self.metrics["prp_executions"]),
244:                 "total_validations": len(self.metrics["validation_results"])
245:             }
246:         }
</file>

<file path="tools/ce/pattern_detectors.py">
  1: """Pattern detection helpers for reducing nesting depth in analysis functions.
  2: 
  3: Extracted from code_analyzer.py and update_context.py to reduce nesting from 7/5 levels to 4 max.
  4: """
  5: 
  6: import ast
  7: import re
  8: from pathlib import Path
  9: from typing import Dict, List, Tuple, Set
 10: import logging
 11: 
 12: logger = logging.getLogger(__name__)
 13: 
 14: 
 15: # ============================================================================
 16: # AST Pattern Detection (from code_analyzer.py)
 17: # ============================================================================
 18: 
 19: def process_class_node(node: ast.ClassDef, patterns: Dict[str, List[str]]) -> None:
 20:     """Process class node for patterns (reduces nesting in _analyze_python).
 21: 
 22:     Args:
 23:         node: AST ClassDef node
 24:         patterns: Pattern dict to update
 25:     """
 26:     patterns["code_structure"].append("class-based")
 27: 
 28:     # Check for decorators
 29:     if node.decorator_list:
 30:         process_class_decorators(node, patterns)
 31: 
 32:     # Check naming
 33:     if node.name[0].isupper():
 34:         patterns["naming_conventions"].append("PascalCase")
 35: 
 36: 
 37: def process_class_decorators(node: ast.ClassDef, patterns: Dict[str, List[str]]) -> None:
 38:     """Process class decorators (extracted to reduce nesting).
 39: 
 40:     Args:
 41:         node: AST ClassDef node
 42:         patterns: Pattern dict to update
 43:     """
 44:     for dec in node.decorator_list:
 45:         if isinstance(dec, ast.Name) and dec.id == "dataclass":
 46:             patterns["code_structure"].append("dataclass")
 47: 
 48: 
 49: def process_function_node(node: ast.FunctionDef, patterns: Dict[str, List[str]]) -> None:
 50:     """Process function node for patterns (reduces nesting in _analyze_python).
 51: 
 52:     Args:
 53:         node: AST FunctionDef node
 54:         patterns: Pattern dict to update
 55:     """
 56:     patterns["code_structure"].append("functional")
 57: 
 58:     # Naming conventions
 59:     if "_" in node.name:
 60:         patterns["naming_conventions"].append("snake_case")
 61:     if node.name.startswith("_") and not node.name.startswith("__"):
 62:         patterns["naming_conventions"].append("_private")
 63: 
 64:     # Test patterns
 65:     if node.name.startswith("test_"):
 66:         patterns["test_patterns"].append("pytest")
 67: 
 68:     # Decorators
 69:     if node.decorator_list:
 70:         process_function_decorators(node, patterns)
 71: 
 72: 
 73: def process_function_decorators(node: ast.FunctionDef, patterns: Dict[str, List[str]]) -> None:
 74:     """Process function decorators (extracted to reduce nesting).
 75: 
 76:     Args:
 77:         node: AST FunctionDef node
 78:         patterns: Pattern dict to update
 79:     """
 80:     for dec in node.decorator_list:
 81:         if isinstance(dec, ast.Name):
 82:             if dec.id in ("staticmethod", "classmethod", "property"):
 83:                 patterns["code_structure"].append(f"decorator-{dec.id}")
 84:             elif dec.id == "pytest":
 85:                 patterns["test_patterns"].append("pytest")
 86: 
 87: 
 88: def process_try_node(node: ast.Try, patterns: Dict[str, List[str]]) -> None:
 89:     """Process try/except node for error handling patterns.
 90: 
 91:     Args:
 92:         node: AST Try node
 93:         patterns: Pattern dict to update
 94:     """
 95:     patterns["error_handling"].append("try-except")
 96:     if node.finalbody:
 97:         patterns["error_handling"].append("try-except-finally")
 98: 
 99: 
100: def process_if_node(node: ast.If, patterns: Dict[str, List[str]]) -> None:
101:     """Process if node for guard clause detection.
102: 
103:     Args:
104:         node: AST If node
105:         patterns: Pattern dict to update
106:     """
107:     # Detect guard clauses (early return)
108:     if node.body and isinstance(node.body[0], ast.Return):
109:         patterns["error_handling"].append("early-return")
110: 
111: 
112: def process_import_node(node: ast.ImportFrom, patterns: Dict[str, List[str]]) -> None:
113:     """Process import node for import patterns.
114: 
115:     Args:
116:         node: AST ImportFrom node
117:         patterns: Pattern dict to update
118:     """
119:     if node.level > 0:
120:         patterns["import_patterns"].append("relative")
121:     else:
122:         patterns["import_patterns"].append("absolute")
123: 
124: 
125: # ============================================================================
126: # Drift Detection Pattern Checking (from update_context.py)
127: # ============================================================================
128: 
129: def check_file_for_violations(
130:     py_file: Path,
131:     pattern_checks: Dict[str, List[Tuple[str, str, str]]],
132:     project_root: Path
133: ) -> Tuple[List[str], bool]:
134:     """Check single file for pattern violations (reduces nesting in verify_codebase_matches_examples).
135: 
136:     Args:
137:         py_file: Path to Python file to check
138:         pattern_checks: Dict of pattern categories to check tuples
139:         project_root: Project root path for relative path calculation
140: 
141:     Returns:
142:         Tuple of (violations list, has_violations flag)
143:     """
144:     violations = []
145:     has_violations = False
146: 
147:     try:
148:         content = py_file.read_text()
149: 
150:         # Check each pattern category
151:         for category, checks in pattern_checks.items():
152:             category_violations = check_pattern_category(
153:                 content, checks, py_file, project_root, category
154:             )
155:             if category_violations:
156:                 violations.extend(category_violations)
157:                 has_violations = True
158: 
159:     except Exception as e:
160:         logger.warning(f"Skipping {py_file.name} - read error: {e}")
161: 
162:     return violations, has_violations
163: 
164: 
165: def check_pattern_category(
166:     content: str,
167:     checks: List[Tuple[str, str, str]],
168:     py_file: Path,
169:     project_root: Path,
170:     category: str
171: ) -> List[str]:
172:     """Check file content against pattern category checks using AST.
173: 
174:     Args:
175:         content: File content string
176:         checks: List of (check_name, regex, fix_desc) tuples
177:         py_file: Path to file being checked
178:         project_root: Project root for relative paths
179:         category: Pattern category name
180: 
181:     Returns:
182:         List of violation messages
183: 
184:     Note: Uses AST parsing instead of regex to avoid false positives from
185:     comments/docstrings and to handle multiline code properly.
186:     """
187:     from .update_context import PATTERN_FILES
188: 
189:     violations = []
190: 
191:     try:
192:         tree = ast.parse(content, filename=str(py_file))
193:     except SyntaxError:
194:         # Fallback to regex for files with syntax errors
195:         logger.warning(f"Syntax error in {py_file}, using regex fallback")
196:         return _check_pattern_category_regex(content, checks, py_file, project_root, category)
197: 
198:     for check_name, regex, fix_desc in checks:
199:         # Use AST-based checks for known patterns
200:         if check_name == "missing_troubleshooting":
201:             if _check_missing_troubleshooting_ast(tree, content):
202:                 violations.append(
203:                     f"File {py_file.relative_to(project_root)} has {check_name} "
204:                     f"(violates {PATTERN_FILES.get(category, 'pattern')}): {fix_desc}"
205:                 )
206:         elif check_name == "bare_except":
207:             if _check_bare_except_ast(tree):
208:                 violations.append(
209:                     f"File {py_file.relative_to(project_root)} has {check_name} "
210:                     f"(violates {PATTERN_FILES.get(category, 'pattern')}): {fix_desc}"
211:                 )
212:         else:
213:             # Fallback to regex for other patterns
214:             matches = re.findall(regex, content, re.MULTILINE | re.DOTALL)
215:             if matches:
216:                 violations.append(
217:                     f"File {py_file.relative_to(project_root)} has {check_name} "
218:                     f"(violates {PATTERN_FILES.get(category, 'pattern')}): {fix_desc}"
219:                 )
220: 
221:     return violations
222: 
223: 
224: def _check_missing_troubleshooting_ast(tree: ast.AST, content: str) -> bool:
225:     """Check for raise statements missing üîß troubleshooting using AST.
226: 
227:     Args:
228:         tree: Parsed AST tree
229:         content: File content (for emoji check)
230: 
231:     Returns:
232:         True if violations found, False otherwise
233:     """
234:     for node in ast.walk(tree):
235:         if isinstance(node, ast.Raise):
236:             # Get the line where raise occurs
237:             if hasattr(node, 'lineno'):
238:                 # Check if üîß appears in the raise message
239:                 # We need to look at the actual source for multiline strings
240:                 raise_line = node.lineno
241:                 # Check 5 lines around the raise statement
242:                 lines = content.split('\n')
243:                 start = max(0, raise_line - 2)
244:                 end = min(len(lines), raise_line + 3)
245:                 context = '\n'.join(lines[start:end])
246: 
247:                 # If this is a raise with an exception instance
248:                 if node.exc and not ('üîß' in context):
249:                     return True
250: 
251:     return False
252: 
253: 
254: def _check_bare_except_ast(tree: ast.AST) -> bool:
255:     """Check for bare except clauses using AST.
256: 
257:     Args:
258:         tree: Parsed AST tree
259: 
260:     Returns:
261:         True if bare except found, False otherwise
262:     """
263:     for node in ast.walk(tree):
264:         if isinstance(node, ast.Try):
265:             for handler in node.handlers:
266:                 # Bare except has no type specified
267:                 if handler.type is None:
268:                     return True
269: 
270:     return False
271: 
272: 
273: def _check_pattern_category_regex(
274:     content: str,
275:     checks: List[Tuple[str, str, str]],
276:     py_file: Path,
277:     project_root: Path,
278:     category: str
279: ) -> List[str]:
280:     """Regex fallback for files with syntax errors.
281: 
282:     Args:
283:         content: File content string
284:         checks: List of (check_name, regex, fix_desc) tuples
285:         py_file: Path to file being checked
286:         project_root: Project root for relative paths
287:         category: Pattern category name
288: 
289:     Returns:
290:         List of violation messages
291:     """
292:     from .update_context import PATTERN_FILES
293: 
294:     violations = []
295: 
296:     for check_name, regex, fix_desc in checks:
297:         matches = re.findall(regex, content, re.MULTILINE | re.DOTALL)
298:         if matches:
299:             violations.append(
300:                 f"File {py_file.relative_to(project_root)} has {check_name} "
301:                 f"(violates {PATTERN_FILES.get(category, 'pattern')}): {fix_desc}"
302:             )
303: 
304:     return violations
305: 
306: 
307: def check_prp_for_missing_examples(
308:     prp_path: Path,
309:     project_root: Path,
310:     keywords_to_examples: Dict[str, Tuple[str, str, str]]
311: ) -> List[Dict[str, any]]:
312:     """Check single PRP for missing examples (reduces nesting in detect_missing_examples_for_prps).
313: 
314:     Args:
315:         prp_path: Path to PRP file
316:         project_root: Project root path
317:         keywords_to_examples: Mapping of keywords to example info tuples
318: 
319:     Returns:
320:         List of missing example dicts
321:     """
322:     from .update_context import read_prp_header
323: 
324:     missing_examples = []
325: 
326:     try:
327:         metadata, content = read_prp_header(prp_path)
328: 
329:         # Check complexity/risk
330:         complexity = metadata.get("complexity", "unknown")
331:         if complexity not in ["medium", "high"]:
332:             return []
333: 
334:         # Check each keyword pattern
335:         for keyword, (example_name, suggested_path, rationale) in keywords_to_examples.items():
336:             if keyword.lower() in content.lower():
337:                 example_path = project_root / suggested_path
338:                 if not example_path.exists():
339:                     missing_examples.append({
340:                         "prp_id": metadata.get("prp_id", "unknown"),
341:                         "feature_name": metadata.get("feature_name", "unknown"),
342:                         "complexity": complexity,
343:                         "missing_example": example_name,
344:                         "suggested_path": suggested_path,
345:                         "rationale": rationale
346:                     })
347: 
348:     except Exception as e:
349:         logger.warning(f"Skipping {prp_path.name} - read error: {e}")
350: 
351:     return missing_examples
</file>

<file path="tools/ce/pattern_extractor.py">
  1: """Pattern extraction from PRP EXAMPLES sections for L4 validation.
  2: 
  3: This module extracts semantic code patterns from PRP markdown files to enable
  4: architectural drift detection. Uses shared code_analyzer module for actual
  5: pattern detection logic.
  6: """
  7: 
  8: import re
  9: from typing import Dict, List, Any
 10: from pathlib import Path
 11: 
 12: from .code_analyzer import analyze_code_patterns
 13: 
 14: 
 15: def extract_patterns_from_prp(prp_path: str) -> Dict[str, Any]:
 16:     """Extract patterns from PRP's EXAMPLES section or INITIAL.md.
 17: 
 18:     Args:
 19:         prp_path: Path to PRP markdown file
 20: 
 21:     Returns:
 22:         {
 23:             "code_structure": ["async/await", "class-based", "functional"],
 24:             "error_handling": ["try-except", "early-return", "null-checks"],
 25:             "naming_conventions": ["snake_case", "camelCase", "PascalCase"],
 26:             "data_flow": ["props", "state", "context", "closure"],
 27:             "test_patterns": ["pytest", "unittest", "fixtures"],
 28:             "import_patterns": ["relative", "absolute"],
 29:             "raw_examples": [{"language": "python", "code": "..."}]
 30:         }
 31: 
 32:     Raises:
 33:         ValueError: If EXAMPLES section not found or malformed
 34:         FileNotFoundError: If PRP file doesn't exist
 35:     """
 36:     prp_path_obj = Path(prp_path)
 37:     if not prp_path_obj.exists():
 38:         raise FileNotFoundError(
 39:             f"PRP file not found: {prp_path}\n"
 40:             f"üîß Troubleshooting:\n"
 41:             f"   - Verify file path is correct\n"
 42:             f"   - Check if file was moved or renamed\n"
 43:             f"   - Use: ls {prp_path_obj.parent} to list directory"
 44:         )
 45: 
 46:     content = prp_path_obj.read_text()
 47: 
 48:     # Extract EXAMPLES section (both standalone and embedded in PRP)
 49:     examples_match = re.search(
 50:         r"##\s+EXAMPLES\s*\n(.*?)(?=\n##|\Z)",
 51:         content,
 52:         re.DOTALL | re.IGNORECASE
 53:     )
 54: 
 55:     if not examples_match:
 56:         raise ValueError(
 57:             f"No EXAMPLES section found in {prp_path}\n"
 58:             f"üîß Troubleshooting: Ensure PRP contains '## EXAMPLES' section "
 59:             f"with code blocks showing patterns to follow"
 60:         )
 61: 
 62:     examples_text = examples_match.group(1)
 63: 
 64:     # Extract code blocks
 65:     code_blocks = re.findall(
 66:         r"```(\w+)?\n(.*?)```",
 67:         examples_text,
 68:         re.DOTALL
 69:     )
 70: 
 71:     if not code_blocks:
 72:         raise ValueError(
 73:             f"No code blocks found in EXAMPLES section of {prp_path}\n"
 74:             f"üîß Troubleshooting: Add code examples using ```language markers"
 75:         )
 76: 
 77:     raw_examples = []
 78:     all_patterns = {
 79:         "code_structure": [],
 80:         "error_handling": [],
 81:         "naming_conventions": [],
 82:         "data_flow": [],
 83:         "test_patterns": [],
 84:         "import_patterns": [],
 85:         "raw_examples": []
 86:     }
 87: 
 88:     for language, code in code_blocks:
 89:         language = language or "python"  # Default to Python
 90:         raw_examples.append({"language": language, "code": code.strip()})
 91: 
 92:         # Use shared code analyzer
 93:         patterns = analyze_code_patterns(code, language)
 94: 
 95:         # Merge patterns
 96:         for category, values in patterns.items():
 97:             if category in all_patterns:
 98:                 all_patterns[category].extend(values)
 99: 
100:     # Deduplicate patterns
101:     for category in all_patterns:
102:         if category != "raw_examples":
103:             all_patterns[category] = list(set(all_patterns[category]))
104: 
105:     all_patterns["raw_examples"] = raw_examples
106: 
107:     return all_patterns
108: 
109: 
110: def parse_code_structure(code: str, language: str) -> List[str]:
111:     """Identify structural patterns in code example.
112: 
113:     Detects:
114:     - async/await vs callbacks vs synchronous
115:     - class-based vs functional vs procedural
116:     - decorator usage patterns
117:     - context manager patterns
118: 
119:     Args:
120:         code: Source code string
121:         language: Programming language (python, typescript, etc.)
122: 
123:     Returns:
124:         List of detected structural patterns
125:     """
126:     # Use shared code analyzer and extract just code_structure
127:     patterns = analyze_code_patterns(code, language)
128:     return patterns.get("code_structure", [])
</file>

<file path="tools/ce/profiling.py">
  1: """Profiling utilities - performance analysis and caching.
  2: 
  3: Provides decorators and utilities for profiling function execution,
  4: caching results, and optimizing performance bottlenecks.
  5: """
  6: 
  7: import cProfile
  8: import pstats
  9: import io
 10: from typing import Callable, Any, Optional
 11: import functools
 12: from datetime import datetime, timedelta
 13: from ce.logging_config import get_logger
 14: 
 15: logger = get_logger(__name__)
 16: 
 17: 
 18: def profile_function(func: Callable) -> Callable:
 19:     """Decorator to profile function execution.
 20: 
 21:     Args:
 22:         func: Function to profile
 23: 
 24:     Returns:
 25:         Wrapped function that prints profile stats
 26: 
 27:     Example:
 28:         @profile_function
 29:         def slow_function():
 30:             # ... expensive operations ...
 31: 
 32:     Note: Profiles every invocation. Use selectively on suspected bottlenecks.
 33:     """
 34:     @functools.wraps(func)
 35:     def wrapper(*args, **kwargs) -> Any:
 36:         profiler = cProfile.Profile()
 37:         profiler.enable()
 38: 
 39:         result = func(*args, **kwargs)
 40: 
 41:         profiler.disable()
 42: 
 43:         # Print stats
 44:         stream = io.StringIO()
 45:         stats = pstats.Stats(profiler, stream=stream)
 46:         stats.sort_stats('cumulative')
 47:         stats.print_stats(20)  # Top 20 functions
 48: 
 49:         logger.info(f"Profile for {func.__name__}:\n{stream.getvalue()}")
 50: 
 51:         return result
 52: 
 53:     return wrapper
 54: 
 55: 
 56: def cache_result(ttl_seconds: int = 300, max_size: int = 128):
 57:     """Decorator to cache function results with TTL and size limit.
 58: 
 59:     Args:
 60:         ttl_seconds: Time-to-live in seconds (default: 300)
 61:         max_size: Maximum cache entries (default: 128)
 62: 
 63:     Returns:
 64:         Decorator function
 65: 
 66:     Example:
 67:         @cache_result(ttl_seconds=600, max_size=256)
 68:         def expensive_computation(x, y):
 69:             return complex_calculation(x, y)
 70: 
 71:     Note: Uses simple dict cache. For production, consider Redis or memcached.
 72:     """
 73:     def decorator(func: Callable) -> Callable:
 74:         cache = {}
 75:         cache_order = []  # Track insertion order for LRU
 76: 
 77:         @functools.wraps(func)
 78:         def wrapper(*args, **kwargs) -> Any:
 79:             # Create cache key (args + kwargs)
 80:             cache_key = (args, tuple(sorted(kwargs.items())))
 81: 
 82:             # Check cache
 83:             if cache_key in cache:
 84:                 result, timestamp = cache[cache_key]
 85:                 if datetime.now() - timestamp < timedelta(seconds=ttl_seconds):
 86:                     logger.debug(f"Cache hit for {func.__name__}")
 87:                     return result
 88:                 else:
 89:                     # Expired - remove
 90:                     del cache[cache_key]
 91:                     cache_order.remove(cache_key)
 92: 
 93:             # Cache miss - compute
 94:             logger.debug(f"Cache miss for {func.__name__}")
 95:             result = func(*args, **kwargs)
 96: 
 97:             # Add to cache (with LRU eviction if needed)
 98:             if len(cache) >= max_size:
 99:                 # Evict oldest entry
100:                 oldest_key = cache_order.pop(0)
101:                 del cache[oldest_key]
102: 
103:             cache[cache_key] = (result, datetime.now())
104:             cache_order.append(cache_key)
105: 
106:             return result
107: 
108:         # Add cache management methods
109:         wrapper.cache_clear = lambda: (cache.clear(), cache_order.clear())
110:         wrapper.cache_info = lambda: {
111:             "hits": sum(1 for k in cache_order if k in cache),
112:             "size": len(cache),
113:             "max_size": max_size,
114:             "ttl_seconds": ttl_seconds
115:         }
116: 
117:         return wrapper
118: 
119:     return decorator
120: 
121: 
122: def time_function(func: Callable) -> Callable:
123:     """Decorator to measure function execution time.
124: 
125:     Args:
126:         func: Function to time
127: 
128:     Returns:
129:         Wrapped function that logs execution time
130: 
131:     Example:
132:         @time_function
133:         def slow_operation():
134:             # ... expensive work ...
135: 
136:     Note: Logs timing via structured logger with duration field.
137:     """
138:     @functools.wraps(func)
139:     def wrapper(*args, **kwargs) -> Any:
140:         start_time = datetime.now()
141: 
142:         result = func(*args, **kwargs)
143: 
144:         duration = (datetime.now() - start_time).total_seconds()
145:         logger.info(
146:             f"Function {func.__name__} completed",
147:             extra={"function": func.__name__, "duration": duration}
148:         )
149: 
150:         return result
151: 
152:     return wrapper
153: 
154: 
155: def memoize(func: Callable) -> Callable:
156:     """Simple memoization decorator (no TTL, no size limit).
157: 
158:     Args:
159:         func: Function to memoize
160: 
161:     Returns:
162:         Memoized function
163: 
164:     Example:
165:         @memoize
166:         def fibonacci(n):
167:             if n < 2:
168:                 return n
169:             return fibonacci(n-1) + fibonacci(n-2)
170: 
171:     Note: Use for pure functions with deterministic output.
172:     For production with TTL/LRU, use cache_result instead.
173:     """
174:     cache = {}
175: 
176:     @functools.wraps(func)
177:     def wrapper(*args, **kwargs):
178:         cache_key = (args, tuple(sorted(kwargs.items())))
179: 
180:         if cache_key not in cache:
181:             cache[cache_key] = func(*args, **kwargs)
182: 
183:         return cache[cache_key]
184: 
185:     wrapper.cache_clear = cache.clear
186:     wrapper.cache_info = lambda: {"size": len(cache)}
187: 
188:     return wrapper
189: 
190: 
191: class PerformanceMonitor:
192:     """Monitor performance metrics across multiple function calls.
193: 
194:     Tracks timing data and call counts for performance analysis.
195: 
196:     Example:
197:         monitor = PerformanceMonitor()
198: 
199:         @monitor.track
200:         def operation1():
201:             # ... work ...
202: 
203:         @monitor.track
204:         def operation2():
205:             # ... work ...
206: 
207:         # Print summary
208:         monitor.print_summary()
209: 
210:     Attributes:
211:         stats: Dict of function stats (call_count, total_time, avg_time)
212:     """
213: 
214:     def __init__(self):
215:         """Initialize performance monitor."""
216:         self.stats = {}
217: 
218:     def track(self, func: Callable) -> Callable:
219:         """Decorator to track function performance.
220: 
221:         Args:
222:             func: Function to track
223: 
224:         Returns:
225:             Wrapped function that records performance stats
226:         """
227:         func_name = func.__name__
228: 
229:         @functools.wraps(func)
230:         def wrapper(*args, **kwargs) -> Any:
231:             start_time = datetime.now()
232: 
233:             result = func(*args, **kwargs)
234: 
235:             duration = (datetime.now() - start_time).total_seconds()
236: 
237:             # Update stats
238:             if func_name not in self.stats:
239:                 self.stats[func_name] = {
240:                     "call_count": 0,
241:                     "total_time": 0.0,
242:                     "avg_time": 0.0
243:                 }
244: 
245:             self.stats[func_name]["call_count"] += 1
246:             self.stats[func_name]["total_time"] += duration
247:             self.stats[func_name]["avg_time"] = (
248:                 self.stats[func_name]["total_time"] / self.stats[func_name]["call_count"]
249:             )
250: 
251:             return result
252: 
253:         return wrapper
254: 
255:     def get_stats(self, func_name: Optional[str] = None) -> dict:
256:         """Get performance statistics.
257: 
258:         Args:
259:             func_name: Optional function name to filter by
260: 
261:         Returns:
262:             Dict of performance stats
263:         """
264:         if func_name:
265:             return self.stats.get(func_name, {})
266:         return self.stats
267: 
268:     def print_summary(self):
269:         """Print performance summary to logger."""
270:         if not self.stats:
271:             logger.info("No performance data collected")
272:             return
273: 
274:         summary = "\nüìä Performance Summary:\n"
275:         summary += "-" * 60 + "\n"
276:         summary += f"{'Function':<30} {'Calls':<10} {'Total(s)':<12} {'Avg(s)':<10}\n"
277:         summary += "-" * 60 + "\n"
278: 
279:         for func_name, data in sorted(self.stats.items(), key=lambda x: x[1]["total_time"], reverse=True):
280:             summary += f"{func_name:<30} {data['call_count']:<10} {data['total_time']:<12.3f} {data['avg_time']:<10.3f}\n"
281: 
282:         summary += "-" * 60
283: 
284:         logger.info(summary)
285: 
286:     def reset(self):
287:         """Clear all performance statistics."""
288:         self.stats.clear()
</file>

<file path="tools/ce/prp_analyzer.py">
  1: """PRP Size Analyzer and Decomposition Recommender.
  2: 
  3: Analyzes PRP documents for size constraints and provides decomposition
  4: recommendations to prevent "PRP obesity".
  5: """
  6: 
  7: import re
  8: from dataclasses import dataclass
  9: from enum import Enum
 10: from pathlib import Path
 11: from typing import List, Optional
 12: 
 13: 
 14: class SizeCategory(Enum):
 15:     """PRP size categories based on complexity metrics."""
 16:     GREEN = "GREEN"    # Optimal size
 17:     YELLOW = "YELLOW"  # Approaching limits
 18:     RED = "RED"        # Needs decomposition
 19: 
 20: 
 21: @dataclass
 22: class PRPMetrics:
 23:     """Metrics extracted from a PRP document."""
 24:     name: str
 25:     lines: int
 26:     estimated_hours: Optional[str]
 27:     phases: int
 28:     risk_level: str
 29:     functions: int
 30:     success_criteria: int
 31:     file_path: Path
 32: 
 33: 
 34: @dataclass
 35: class PRPAnalysis:
 36:     """Analysis results for a PRP document."""
 37:     metrics: PRPMetrics
 38:     size_category: SizeCategory
 39:     score: float  # 0-100, higher = more complex
 40:     recommendations: List[str]
 41:     decomposition_suggestions: List[str]
 42: 
 43: 
 44: def extract_prp_metrics(prp_file: Path) -> PRPMetrics:
 45:     """Extract size and complexity metrics from a PRP file.
 46: 
 47:     Args:
 48:         prp_file: Path to PRP markdown file
 49: 
 50:     Returns:
 51:         PRPMetrics object with extracted data
 52: 
 53:     Raises:
 54:         FileNotFoundError: If PRP file doesn't exist
 55:         RuntimeError: If metrics extraction fails
 56:     """
 57:     if not prp_file.exists():
 58:         raise FileNotFoundError(
 59:             f"PRP file not found: {prp_file}\n"
 60:             f"üîß Troubleshooting: Verify file path and try again"
 61:         )
 62: 
 63:     try:
 64:         content = prp_file.read_text()
 65:     except Exception as e:
 66:         raise RuntimeError(
 67:             f"Failed to read PRP file: {e}\n"
 68:             f"üîß Troubleshooting: Check file permissions"
 69:         )
 70: 
 71:     # Extract metrics
 72:     name = prp_file.stem
 73:     lines = len(content.split('\n'))
 74: 
 75:     # Hours - try multiple patterns
 76:     hours_match = re.search(r'estimated_hours:\s*([0-9]+(?:-[0-9]+)?)', content)
 77:     if not hours_match:
 78:         hours_match = re.search(r'Effort.*?([0-9]+-?[0-9]*)\s*hour', content, re.IGNORECASE)
 79:     hours = hours_match.group(1) if hours_match else None
 80: 
 81:     # Phases
 82:     phases = len(re.findall(r'^### Phase [0-9]+', content, re.MULTILINE))
 83: 
 84:     # Risk
 85:     risk_match = re.search(r'\*\*Risk\*\*:\s*(LOW|MEDIUM|HIGH)', content)
 86:     risk = risk_match.group(1) if risk_match else 'UNKNOWN'
 87: 
 88:     # Functions (code examples in PRP)
 89:     functions = len(re.findall(r'def \w+\(', content))
 90: 
 91:     # Success criteria
 92:     criteria = len(re.findall(r'- \[[ x]\]', content))
 93: 
 94:     return PRPMetrics(
 95:         name=name,
 96:         lines=lines,
 97:         estimated_hours=hours,
 98:         phases=phases,
 99:         risk_level=risk,
100:         functions=functions,
101:         success_criteria=criteria,
102:         file_path=prp_file
103:     )
104: 
105: 
106: def calculate_complexity_score(metrics: PRPMetrics) -> float:
107:     """Calculate complexity score (0-100) for a PRP.
108: 
109:     Score formula weights multiple factors:
110:     - Lines: 40% weight (normalized to 1500 lines max)
111:     - Functions: 25% weight (normalized to 40 functions max)
112:     - Criteria: 20% weight (normalized to 50 criteria max)
113:     - Phases: 10% weight (normalized to 15 phases max)
114:     - Risk: 5% weight (LOW=0, MEDIUM=50, HIGH=100)
115: 
116:     Args:
117:         metrics: PRPMetrics object
118: 
119:     Returns:
120:         Complexity score from 0-100
121:     """
122:     # Normalize each metric to 0-100 scale
123:     line_score = min(100, (metrics.lines / 1500) * 100)
124:     function_score = min(100, (metrics.functions / 40) * 100)
125:     criteria_score = min(100, (metrics.success_criteria / 50) * 100)
126:     phase_score = min(100, (metrics.phases / 15) * 100)
127: 
128:     # Risk mapping
129:     risk_map = {'LOW': 0, 'MEDIUM': 50, 'HIGH': 100, 'UNKNOWN': 25}
130:     risk_score = risk_map.get(metrics.risk_level, 25)
131: 
132:     # Weighted average
133:     score = (
134:         line_score * 0.40 +
135:         function_score * 0.25 +
136:         criteria_score * 0.20 +
137:         phase_score * 0.10 +
138:         risk_score * 0.05
139:     )
140: 
141:     return round(score, 2)
142: 
143: 
144: def categorize_prp_size(score: float, metrics: PRPMetrics) -> SizeCategory:
145:     """Determine size category based on complexity score and metrics.
146: 
147:     Thresholds derived from historical PRP analysis:
148:     - GREEN: score < 50, lines < 700, risk LOW-MEDIUM
149:     - YELLOW: score 50-70, lines 700-1000, risk MEDIUM
150:     - RED: score > 70, lines > 1000, risk HIGH
151: 
152:     Args:
153:         score: Complexity score (0-100)
154:         metrics: PRPMetrics object
155: 
156:     Returns:
157:         SizeCategory enum value
158:     """
159:     # Hard constraints for RED
160:     if metrics.lines > 1000 or metrics.risk_level == 'HIGH' or score > 70:
161:         return SizeCategory.RED
162: 
163:     # YELLOW thresholds
164:     if (metrics.lines > 700 or
165:         metrics.functions > 20 or
166:         metrics.success_criteria > 30 or
167:         score > 50):
168:         return SizeCategory.YELLOW
169: 
170:     # GREEN - optimal size
171:     return SizeCategory.GREEN
172: 
173: 
174: def generate_recommendations(metrics: PRPMetrics, score: float, category: SizeCategory) -> List[str]:
175:     """Generate actionable recommendations based on PRP analysis.
176: 
177:     Args:
178:         metrics: PRPMetrics object
179:         score: Complexity score
180:         category: Size category
181: 
182:     Returns:
183:         List of recommendation strings
184:     """
185:     recs = []
186: 
187:     if category == SizeCategory.GREEN:
188:         recs.append("‚úÖ PRP size is optimal - good job!")
189:         if score > 40:
190:             recs.append("Monitor: Approaching YELLOW threshold, avoid scope creep")
191:         return recs
192: 
193:     if category == SizeCategory.YELLOW:
194:         recs.append("‚ö†Ô∏è PRP approaching size limits - consider scope reduction")
195: 
196:         if metrics.lines > 700:
197:             recs.append(f"Lines ({metrics.lines}) approaching RED threshold (1000)")
198: 
199:         if metrics.functions > 20:
200:             recs.append(f"Functions ({metrics.functions}) indicate high implementation complexity")
201: 
202:         if metrics.success_criteria > 30:
203:             recs.append(f"Success criteria ({metrics.success_criteria}) suggest multiple features")
204: 
205:         recs.append("Recommendation: Review if PRP can be split into sub-PRPs")
206:         return recs
207: 
208:     # RED category
209:     recs.append("üö® PRP TOO LARGE - decomposition strongly recommended")
210: 
211:     if metrics.lines > 1000:
212:         recs.append(f"Lines ({metrics.lines}) exceed RED threshold - split into sub-PRPs")
213: 
214:     if metrics.risk_level == 'HIGH':
215:         recs.append("HIGH risk rating - isolate risky components into separate PRPs")
216: 
217:     if metrics.functions > 25:
218:         recs.append(f"Functions ({metrics.functions}) indicate multiple features - create sub-PRPs")
219: 
220:     if metrics.phases > 5:
221:         recs.append(f"Phases ({metrics.phases}) could be independent PRPs")
222: 
223:     recs.append("ACTION REQUIRED: Decompose before execution")
224: 
225:     return recs
226: 
227: 
228: def suggest_decomposition(metrics: PRPMetrics) -> List[str]:
229:     """Generate decomposition strategy suggestions.
230: 
231:     Args:
232:         metrics: PRPMetrics object
233: 
234:     Returns:
235:         List of decomposition suggestion strings
236:     """
237:     suggestions = []
238: 
239:     if metrics.phases >= 5:
240:         suggestions.append(
241:             f"Phase-based decomposition: Create {metrics.phases} sub-PRPs "
242:             f"(PRP-X.1 through PRP-X.{metrics.phases})"
243:         )
244:         suggestions.append("Group related phases if some are interdependent")
245: 
246:     if metrics.functions > 20:
247:         suggestions.append(
248:             "Feature-based decomposition: Split by functional area "
249:             "(e.g., parser, validator, executor)"
250:         )
251: 
252:     if metrics.risk_level == 'HIGH':
253:         suggestions.append(
254:             "Risk-based decomposition: Isolate HIGH-risk components "
255:             "into separate PRPs for focused attention"
256:         )
257: 
258:     if metrics.success_criteria > 30:
259:         suggestions.append(
260:             "Criteria-based decomposition: Group related success criteria "
261:             "into logical sub-features"
262:         )
263: 
264:     if not suggestions:
265:         suggestions.append("No decomposition needed - PRP size is manageable")
266: 
267:     return suggestions
268: 
269: 
270: def analyze_prp(prp_file: Path) -> PRPAnalysis:
271:     """Comprehensive PRP size analysis.
272: 
273:     Args:
274:         prp_file: Path to PRP markdown file
275: 
276:     Returns:
277:         PRPAnalysis object with full analysis results
278: 
279:     Raises:
280:         FileNotFoundError: If PRP file doesn't exist
281:         RuntimeError: If analysis fails
282:     """
283:     try:
284:         metrics = extract_prp_metrics(prp_file)
285:         score = calculate_complexity_score(metrics)
286:         category = categorize_prp_size(score, metrics)
287:         recommendations = generate_recommendations(metrics, score, category)
288:         decomposition = suggest_decomposition(metrics)
289: 
290:         return PRPAnalysis(
291:             metrics=metrics,
292:             size_category=category,
293:             score=score,
294:             recommendations=recommendations,
295:             decomposition_suggestions=decomposition
296:         )
297:     except Exception as e:
298:         raise RuntimeError(
299:             f"PRP analysis failed: {e}\n"
300:             f"üîß Troubleshooting: Verify PRP file format and try again"
301:         )
302: 
303: 
304: def format_analysis_report(analysis: PRPAnalysis, json_output: bool = False) -> str:
305:     """Format analysis results as human-readable report or JSON.
306: 
307:     Args:
308:         analysis: PRPAnalysis object
309:         json_output: If True, return JSON string
310: 
311:     Returns:
312:         Formatted report string
313:     """
314:     if json_output:
315:         import json
316:         data = {
317:             'name': analysis.metrics.name,
318:             'size_category': analysis.size_category.value,
319:             'complexity_score': analysis.score,
320:             'metrics': {
321:                 'lines': analysis.metrics.lines,
322:                 'hours': analysis.metrics.estimated_hours,
323:                 'phases': analysis.metrics.phases,
324:                 'risk': analysis.metrics.risk_level,
325:                 'functions': analysis.metrics.functions,
326:                 'criteria': analysis.metrics.success_criteria
327:             },
328:             'recommendations': analysis.recommendations,
329:             'decomposition_suggestions': analysis.decomposition_suggestions
330:         }
331:         return json.dumps(data, indent=2)
332: 
333:     # Human-readable format
334:     m = analysis.metrics
335:     lines = [
336:         f"\n{'='*80}",
337:         f"PRP Size Analysis: {m.name}",
338:         f"{'='*80}",
339:         f"\nMetrics:",
340:         f"  Lines:            {m.lines}",
341:         f"  Estimated Hours:  {m.estimated_hours or 'N/A'}",
342:         f"  Phases:           {m.phases}",
343:         f"  Risk Level:       {m.risk_level}",
344:         f"  Functions:        {m.functions}",
345:         f"  Success Criteria: {m.success_criteria}",
346:         f"\nComplexity Score: {analysis.score}/100",
347:         f"Size Category:    {analysis.size_category.value}",
348:         f"\nRecommendations:",
349:     ]
350: 
351:     for rec in analysis.recommendations:
352:         lines.append(f"  ‚Ä¢ {rec}")
353: 
354:     lines.append("\nDecomposition Suggestions:")
355:     for sug in analysis.decomposition_suggestions:
356:         lines.append(f"  ‚Ä¢ {sug}")
357: 
358:     lines.append(f"\n{'='*80}\n")
359: 
360:     return '\n'.join(lines)
</file>

<file path="tools/ce/prp.py">
  1: """PRP YAML validation and state management module."""
  2: from typing import Dict, Any, List, Optional
  3: import yaml
  4: import re
  5: import json
  6: import logging
  7: from pathlib import Path
  8: from datetime import datetime, timezone
  9: 
 10: # Required fields schema
 11: REQUIRED_FIELDS = [
 12:     "name", "description", "prp_id", "status", "priority",
 13:     "confidence", "effort_hours", "risk", "dependencies",
 14:     "parent_prp", "context_memories", "meeting_evidence",
 15:     "context_sync", "version", "created_date", "last_updated"
 16: ]
 17: 
 18: # Valid enum values
 19: VALID_STATUS = ["ready", "in_progress", "executed", "validated", "archived"]
 20: VALID_PRIORITY = ["HIGH", "MEDIUM", "LOW"]
 21: VALID_RISK = ["LOW", "MEDIUM", "HIGH"]
 22: VALID_PHASES = ["planning", "implementation", "testing", "validation", "complete"]
 23: 
 24: # State file paths
 25: STATE_DIR = Path(".ce")
 26: STATE_FILE = STATE_DIR / "active_prp_session"
 27: 
 28: # Configure logging
 29: logger = logging.getLogger(__name__)
 30: 
 31: 
 32: def validate_prp_yaml(file_path: str) -> Dict[str, Any]:
 33:     """Validate PRP YAML header against schema.
 34: 
 35:     Args:
 36:         file_path: Path to PRP markdown file
 37: 
 38:     Returns:
 39:         Dict with: success (bool), errors (list), warnings (list), header (dict)
 40: 
 41:     Raises:
 42:         FileNotFoundError: If file doesn't exist
 43:         yaml.YAMLError: If YAML parse fails
 44:     """
 45:     errors = []
 46:     warnings = []
 47: 
 48:     # Check file exists
 49:     path = Path(file_path)
 50:     if not path.exists():
 51:         raise FileNotFoundError(
 52:             f"PRP file not found: {file_path}\n"
 53:             f"üîß Troubleshooting: Verify file path is correct"
 54:         )
 55: 
 56:     # Read file
 57:     content = path.read_text()
 58: 
 59:     # Check YAML delimiters
 60:     if not content.startswith("---\n"):
 61:         errors.append("Missing YAML front matter: file must start with '---'")
 62:         return {"success": False, "errors": errors, "warnings": warnings, "header": None}
 63: 
 64:     # Extract YAML header
 65:     parts = content.split("---", 2)
 66:     if len(parts) < 3:
 67:         errors.append("Missing closing '---' delimiter for YAML header")
 68:         return {"success": False, "errors": errors, "warnings": warnings, "header": None}
 69: 
 70:     yaml_content = parts[1].strip()
 71: 
 72:     # Parse YAML
 73:     try:
 74:         header = yaml.safe_load(yaml_content)
 75:     except yaml.YAMLError as e:
 76:         errors.append(f"YAML parse error: {str(e)}")
 77:         return {"success": False, "errors": errors, "warnings": warnings, "header": None}
 78: 
 79:     # Validate schema
 80:     return validate_schema(header, errors, warnings)
 81: 
 82: 
 83: def validate_schema(header: Dict[str, Any], errors: List[str], warnings: List[str]) -> Dict[str, Any]:
 84:     """Validate YAML header against schema."""
 85: 
 86:     # Check required fields
 87:     missing_fields = [f for f in REQUIRED_FIELDS if f not in header]
 88:     if missing_fields:
 89:         errors.append(f"Missing required fields: {', '.join(missing_fields)}")
 90: 
 91:     # Validate PRP ID format
 92:     if "prp_id" in header:
 93:         error = validate_prp_id_format(header["prp_id"])
 94:         if error:
 95:             errors.append(error)
 96: 
 97:     # Validate date formats
 98:     for date_field in ["created_date", "last_updated"]:
 99:         if date_field in header:
100:             error = validate_date_format(header[date_field], date_field)
101:             if error:
102:                 errors.append(error)
103: 
104:     # Validate status enum
105:     if "status" in header and header["status"] not in VALID_STATUS:
106:         errors.append(
107:             f"Invalid status: '{header['status']}' (must be one of: {', '.join(VALID_STATUS)})"
108:         )
109: 
110:     # Validate priority enum
111:     if "priority" in header and header["priority"] not in VALID_PRIORITY:
112:         errors.append(
113:             f"Invalid priority: '{header['priority']}' (must be one of: {', '.join(VALID_PRIORITY)})"
114:         )
115: 
116:     # Validate risk enum
117:     if "risk" in header and header["risk"] not in VALID_RISK:
118:         errors.append(
119:             f"Invalid risk: '{header['risk']}' (must be one of: {', '.join(VALID_RISK)})"
120:         )
121: 
122:     # Validate confidence format (X/10)
123:     if "confidence" in header:
124:         conf_str = str(header["confidence"])
125:         if not re.match(r'^\d{1,2}/10$', conf_str):
126:             errors.append(f"Invalid confidence format: '{conf_str}' (expected: X/10 where X is 1-10)")
127: 
128:     # Validate effort_hours is numeric
129:     if "effort_hours" in header:
130:         try:
131:             float(header["effort_hours"])
132:         except (ValueError, TypeError):
133:             errors.append(f"Invalid effort_hours: '{header['effort_hours']}' (must be numeric)")
134: 
135:     # Validate dependencies is list
136:     if "dependencies" in header and not isinstance(header["dependencies"], list):
137:         errors.append(f"Invalid dependencies: must be a list, got {type(header['dependencies']).__name__}")
138: 
139:     # Validate context_memories is list
140:     if "context_memories" in header and not isinstance(header["context_memories"], list):
141:         errors.append(f"Invalid context_memories: must be a list, got {type(header['context_memories']).__name__}")
142: 
143:     # Warnings for optional fields
144:     if not header.get("task_id"):
145:         warnings.append("Optional field 'task_id' is empty (consider linking to issue tracker)")
146: 
147:     success = len(errors) == 0
148:     return {
149:         "success": success,
150:         "errors": errors,
151:         "warnings": warnings,
152:         "header": header
153:     }
154: 
155: 
156: def validate_prp_id_format(prp_id: str) -> Optional[str]:
157:     """Validate PRP ID format (PRP-X.Y or PRP-X.Y.Z).
158: 
159:     Returns:
160:         Error message if invalid, None if valid
161:     """
162:     # Pattern: PRP-X.Y or PRP-X.Y.Z (no leading zeros)
163:     pattern = r'^PRP-([1-9]\d*)(\.(0|[1-9]\d*))?(\.(0|[1-9]\d*))?$'
164:     if not re.match(pattern, prp_id):
165:         return f"Invalid PRP ID format: '{prp_id}' (expected: PRP-X.Y or PRP-X.Y.Z, no leading zeros)"
166:     return None
167: 
168: 
169: def validate_date_format(date_str: str, field_name: str) -> Optional[str]:
170:     """Validate ISO 8601 date format.
171: 
172:     Returns:
173:         Error message if invalid, None if valid
174:     """
175:     pattern = r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}Z$'
176:     if not re.match(pattern, date_str):
177:         return f"Invalid date format for '{field_name}': '{date_str}' (expected: YYYY-MM-DDTHH:MM:SSZ)"
178:     return None
179: 
180: 
181: def format_validation_result(result: Dict[str, Any]) -> str:
182:     """Format validation result for human-readable output."""
183:     if result["success"]:
184:         output = "‚úÖ YAML validation passed\n\n"
185:         output += f"PRP ID: {result['header']['prp_id']}\n"
186:         output += f"Name: {result['header']['name']}\n"
187:         output += f"Status: {result['header']['status']}\n"
188:         output += f"Effort: {result['header']['effort_hours']}h\n"
189: 
190:         if result["warnings"]:
191:             output += "\n‚ö†Ô∏è  Warnings:\n"
192:             for warning in result["warnings"]:
193:                 output += f"  - {warning}\n"
194:     else:
195:         output = "‚ùå YAML validation failed\n\n"
196:         output += "Errors:\n"
197:         for error in result["errors"]:
198:             output += f"  ‚ùå {error}\n"
199: 
200:         if result["warnings"]:
201:             output += "\nWarnings:\n"
202:             for warning in result["warnings"]:
203:                 output += f"  ‚ö†Ô∏è  {warning}\n"
204: 
205:         output += "\nüîß Troubleshooting: Review docs/prp-yaml-schema.md for schema reference"
206: 
207:     return output
208: 
209: 
210: # ============================================================================
211: # PRP State Management Functions
212: # ============================================================================
213: 
214: def _write_state(state: Dict[str, Any]) -> None:
215:     """Write state to file using atomic write pattern."""
216:     STATE_DIR.mkdir(exist_ok=True)
217:     temp_file = STATE_FILE.with_suffix(".tmp")
218:     temp_file.write_text(json.dumps(state, indent=2))
219:     temp_file.replace(STATE_FILE)
220: 
221: 
222: def start_prp(prp_id: str, prp_name: Optional[str] = None) -> Dict[str, Any]:
223:     """Initialize PRP execution context.
224: 
225:     Creates .ce/active_prp_session file and initializes state tracking.
226: 
227:     Args:
228:         prp_id: PRP identifier (e.g., "PRP-003")
229:         prp_name: Optional PRP name for display
230: 
231:     Returns:
232:         {
233:             "success": True,
234:             "prp_id": "PRP-003",
235:             "started_at": "2025-10-12T14:30:00Z",
236:             "message": "PRP-003 context initialized"
237:         }
238: 
239:     Raises:
240:         RuntimeError: If another PRP is active (call cleanup first)
241:         ValueError: If prp_id format invalid
242:     """
243:     # Validate PRP ID format
244:     error = validate_prp_id_format(prp_id)
245:     if error:
246:         raise ValueError(
247:             f"{error}\n"
248:             f"üîß Troubleshooting: Use format PRP-X or PRP-X.Y"
249:         )
250: 
251:     # Check if another PRP is active
252:     active = get_active_prp()
253:     if active:
254:         raise RuntimeError(
255:             f"Another PRP is active: {active['prp_id']}\n"
256:             f"üîß Troubleshooting: Run 'ce prp cleanup {active['prp_id']}' or 'ce prp end {active['prp_id']}' first"
257:         )
258: 
259:     # Initialize state
260:     started_at = datetime.now(timezone.utc).isoformat()
261:     state = {
262:         "prp_id": prp_id,
263:         "prp_name": prp_name or prp_id,
264:         "started_at": started_at,
265:         "phase": "planning",
266:         "last_checkpoint": None,
267:         "checkpoint_count": 0,
268:         "validation_attempts": {
269:             "L1": 0,
270:             "L2": 0,
271:             "L3": 0,
272:             "L4": 0
273:         },
274:         "serena_memories": []
275:     }
276: 
277:     _write_state(state)
278:     logger.info(f"Started {prp_id} execution context")
279: 
280:     return {
281:         "success": True,
282:         "prp_id": prp_id,
283:         "started_at": started_at,
284:         "message": f"{prp_id} context initialized"
285:     }
286: 
287: 
288: def get_active_prp() -> Optional[Dict[str, Any]]:
289:     """Get current active PRP session.
290: 
291:     Returns:
292:         State dict if PRP active, None if no active session
293: 
294:     Example:
295:         >>> state = get_active_prp()
296:         >>> if state:
297:         ...     print(f"Active: {state['prp_id']}")
298:         ... else:
299:         ...     print("No active PRP")
300:     """
301:     if not STATE_FILE.exists():
302:         return None
303: 
304:     try:
305:         return json.loads(STATE_FILE.read_text())
306:     except (json.JSONDecodeError, OSError) as e:
307:         logger.warning(f"Failed to read state file: {e}")
308:         return None
309: 
310: 
311: def end_prp(prp_id: str) -> Dict[str, Any]:
312:     """End PRP execution context (without cleanup).
313: 
314:     Removes .ce/active_prp_session file. Use cleanup_prp() for full cleanup.
315: 
316:     Args:
317:         prp_id: PRP identifier to end
318: 
319:     Returns:
320:         {
321:             "success": True,
322:             "duration": "2h 15m",
323:             "checkpoints_created": 3
324:         }
325: 
326:     Raises:
327:         RuntimeError: If prp_id doesn't match active PRP
328:     """
329:     active = get_active_prp()
330:     if not active:
331:         raise RuntimeError(
332:             f"No active PRP session\n"
333:             f"üîß Troubleshooting: Use 'ce prp status' to check current state"
334:         )
335: 
336:     if active["prp_id"] != prp_id:
337:         raise RuntimeError(
338:             f"PRP ID mismatch: active={active['prp_id']}, requested={prp_id}\n"
339:             f"üîß Troubleshooting: End the active PRP first: 'ce prp end {active['prp_id']}'"
340:         )
341: 
342:     # Calculate duration
343:     started = datetime.fromisoformat(active["started_at"])
344:     ended = datetime.now(timezone.utc)
345:     duration_seconds = (ended - started).total_seconds()
346:     hours = int(duration_seconds // 3600)
347:     minutes = int((duration_seconds % 3600) // 60)
348:     duration = f"{hours}h {minutes}m" if hours > 0 else f"{minutes}m"
349: 
350:     # Remove state file
351:     STATE_FILE.unlink(missing_ok=True)
352:     logger.info(f"Ended {prp_id} execution context")
353: 
354:     return {
355:         "success": True,
356:         "duration": duration,
357:         "checkpoints_created": active["checkpoint_count"]
358:     }
359: 
360: 
361: def update_prp_phase(phase: str) -> Dict[str, Any]:
362:     """Update current PRP phase in state file.
363: 
364:     Args:
365:         phase: Phase name (e.g., "implementation", "testing", "validation")
366:                Valid phases: planning, implementation, testing, validation, complete
367: 
368:     Returns:
369:         Updated state dict
370: 
371:     Raises:
372:         RuntimeError: If no active PRP session
373:         ValueError: If phase not in valid phases list
374:     """
375:     if phase not in VALID_PHASES:
376:         raise ValueError(
377:             f"Invalid phase: '{phase}' (must be one of: {', '.join(VALID_PHASES)})\n"
378:             f"üîß Troubleshooting: Use a valid phase name"
379:         )
380: 
381:     active = get_active_prp()
382:     if not active:
383:         raise RuntimeError(
384:             f"No active PRP session\n"
385:             f"üîß Troubleshooting: Start a PRP first with 'ce prp start PRP-XXX'"
386:         )
387: 
388:     active["phase"] = phase
389:     _write_state(active)
390:     logger.info(f"Updated {active['prp_id']} phase to: {phase}")
391: 
392:     return active
393: 
394: 
395: # ============================================================================
396: # Checkpoint Management Functions
397: # ============================================================================
398: 
399: def create_checkpoint(phase: str, message: Optional[str] = None) -> Dict[str, Any]:
400:     """Create PRP-scoped git checkpoint.
401: 
402:     Args:
403:         phase: Phase identifier (e.g., "phase1", "phase2", "final")
404:         message: Optional checkpoint message (defaults to phase name)
405: 
406:     Returns:
407:         {
408:             "success": True,
409:             "tag_name": "checkpoint-PRP-003-phase1-20251012-143000",
410:             "commit_sha": "a1b2c3d",
411:             "message": "Phase 1 complete: Core logic implemented"
412:         }
413: 
414:     Raises:
415:         RuntimeError: If no active PRP or git operation fails
416:         RuntimeError: If working tree not clean (uncommitted changes)
417: 
418:     Side Effects:
419:         - Creates git annotated tag
420:         - Updates .ce/active_prp_session with last_checkpoint
421:         - Increments checkpoint_count
422:         - Serena memory handling:
423:           * If Serena available: writes checkpoint metadata to memory
424:           * If Serena unavailable: logs warning, continues successfully
425:           * Never fails on Serena unavailability
426:     """
427:     from .core import run_cmd
428: 
429:     # Verify active PRP
430:     active = get_active_prp()
431:     if not active:
432:         raise RuntimeError(
433:             f"No active PRP session\n"
434:             f"üîß Troubleshooting: Start a PRP first with 'ce prp start PRP-XXX'"
435:         )
436: 
437:     # Check git working tree clean
438:     status_result = run_cmd("git status --porcelain")
439:     if not status_result["success"]:
440:         raise RuntimeError(
441:             f"Failed to check git status: {status_result['stderr']}\n"
442:             f"üîß Troubleshooting: Ensure you're in a git repository"
443:         )
444: 
445:     if status_result["stdout"].strip():
446:         raise RuntimeError(
447:             f"Working tree has uncommitted changes\n"
448:             f"üîß Troubleshooting: Commit or stash changes before creating checkpoint"
449:         )
450: 
451:     # Generate timestamp and tag name
452:     timestamp = datetime.now(timezone.utc).strftime("%Y%m%d-%H%M%S")
453:     tag_name = f"checkpoint-{active['prp_id']}-{phase}-{timestamp}"
454: 
455:     # Get current commit SHA
456:     sha_result = run_cmd("git rev-parse HEAD")
457:     if not sha_result["success"]:
458:         raise RuntimeError(
459:             f"Failed to get commit SHA: {sha_result['stderr']}\n"
460:             f"üîß Troubleshooting: Ensure you're in a git repository with commits"
461:         )
462:     commit_sha = sha_result["stdout"].strip()[:7]
463: 
464:     # Create annotated tag
465:     tag_message = message or f"{phase} checkpoint"
466:     tag_result = run_cmd(f'git tag -a "{tag_name}" -m "{tag_message}"')
467:     if not tag_result["success"]:
468:         raise RuntimeError(
469:             f"Failed to create checkpoint tag: {tag_result['stderr']}\n"
470:             f"üîß Troubleshooting: Ensure git is configured correctly"
471:         )
472: 
473:     # Update state
474:     active["last_checkpoint"] = tag_name
475:     active["checkpoint_count"] += 1
476:     _write_state(active)
477: 
478:     logger.info(f"Created checkpoint: {tag_name}")
479: 
480:     return {
481:         "success": True,
482:         "tag_name": tag_name,
483:         "commit_sha": commit_sha,
484:         "message": tag_message
485:     }
486: 
487: 
488: def list_checkpoints(prp_id: Optional[str] = None) -> List[Dict[str, Any]]:
489:     """List all checkpoints for PRP(s).
490: 
491:     Args:
492:         prp_id: Optional PRP filter (None = all PRPs)
493: 
494:     Returns:
495:         List of checkpoint dicts:
496:         [
497:             {
498:                 "tag_name": "checkpoint-PRP-003-phase1-20251012-143000",
499:                 "prp_id": "PRP-003",
500:                 "phase": "phase1",
501:                 "timestamp": "2025-10-12T14:30:00Z",
502:                 "commit_sha": "a1b2c3d",
503:                 "message": "Phase 1 complete"
504:             },
505:             ...
506:         ]
507: 
508:     Example:
509:         >>> checkpoints = list_checkpoints("PRP-003")
510:         >>> for cp in checkpoints:
511:         ...     print(f"{cp['phase']}: {cp['message']}")
512:     """
513:     from .core import run_cmd
514: 
515:     # Get all tags
516:     tags_result = run_cmd("git tag -l 'checkpoint-*' --format='%(refname:short)|%(subject)|%(objectname:short)'")
517:     if not tags_result["success"]:
518:         logger.warning(f"Failed to list tags: {tags_result['stderr']}")
519:         return []
520: 
521:     if not tags_result["stdout"].strip():
522:         return []
523: 
524:     checkpoints = []
525:     for line in tags_result["stdout"].strip().split("\n"):
526:         parts = line.split("|")
527:         if len(parts) < 3:
528:             continue
529: 
530:         tag_name, tag_message, commit_sha = parts[0], parts[1], parts[2]
531: 
532:         # Parse tag name: checkpoint-{prp_id}-{phase}-{timestamp}
533:         if not tag_name.startswith("checkpoint-"):
534:             continue
535: 
536:         tag_parts = tag_name.split("-", 3)  # Split: ["checkpoint", "PRP", "X", "phase-YYYYMMDD-HHMMSS"]
537:         if len(tag_parts) < 4:
538:             continue
539: 
540:         checkpoint_prp_id = f"{tag_parts[1]}-{tag_parts[2]}"  # "PRP-X"
541: 
542:         # Filter by prp_id if provided
543:         if prp_id and checkpoint_prp_id != prp_id:
544:             continue
545: 
546:         # Extract phase and timestamp
547:         remaining = tag_parts[3]  # "phase1-20251012-143000"
548:         phase_timestamp = remaining.rsplit("-", 2)  # Split from right to preserve phase name
549:         if len(phase_timestamp) == 3:
550:             phase = phase_timestamp[0]
551:             timestamp_str = f"{phase_timestamp[1]}-{phase_timestamp[2]}"
552:             # Convert timestamp to ISO format
553:             try:
554:                 dt = datetime.strptime(timestamp_str, "%Y%m%d-%H%M%S")
555:                 timestamp_iso = dt.replace(tzinfo=timezone.utc).isoformat()
556:             except ValueError:
557:                 timestamp_iso = timestamp_str
558:         else:
559:             phase = remaining
560:             timestamp_iso = ""
561: 
562:         checkpoints.append({
563:             "tag_name": tag_name,
564:             "prp_id": checkpoint_prp_id,
565:             "phase": phase,
566:             "timestamp": timestamp_iso,
567:             "commit_sha": commit_sha,
568:             "message": tag_message
569:         })
570: 
571:     return checkpoints
572: 
573: 
574: def restore_checkpoint(prp_id: str, phase: Optional[str] = None) -> Dict[str, Any]:
575:     """Restore to PRP checkpoint.
576: 
577:     Args:
578:         prp_id: PRP identifier
579:         phase: Optional phase (defaults to latest checkpoint)
580: 
581:     Returns:
582:         {
583:             "success": True,
584:             "restored_to": "checkpoint-PRP-003-phase1-20251012-143000",
585:             "commit_sha": "a1b2c3d"
586:         }
587: 
588:     Raises:
589:         RuntimeError: If checkpoint not found or git operation fails
590:         RuntimeError: If working tree not clean (uncommitted changes)
591: 
592:     Warning:
593:         This is a destructive operation. Uncommitted changes will be lost.
594:     """
595:     from .core import run_cmd
596:     import sys
597: 
598:     # Check working tree clean
599:     status_result = run_cmd("git status --porcelain")
600:     if not status_result["success"]:
601:         raise RuntimeError(
602:             f"Failed to check git status: {status_result['stderr']}\n"
603:             f"üîß Troubleshooting: Ensure you're in a git repository"
604:         )
605: 
606:     if status_result["stdout"].strip():
607:         raise RuntimeError(
608:             f"Working tree has uncommitted changes\n"
609:             f"üîß Troubleshooting: Commit or stash changes before restoring checkpoint"
610:         )
611: 
612:     # Find checkpoint
613:     checkpoints = list_checkpoints(prp_id)
614:     if not checkpoints:
615:         raise RuntimeError(
616:             f"No checkpoints found for {prp_id}\n"
617:             f"üîß Troubleshooting: Create a checkpoint first with 'ce prp checkpoint <phase>'"
618:         )
619: 
620:     # Select checkpoint
621:     if phase:
622:         checkpoint = next((cp for cp in checkpoints if cp["phase"] == phase), None)
623:         if not checkpoint:
624:             phases = [cp["phase"] for cp in checkpoints]
625:             raise RuntimeError(
626:                 f"No checkpoint found for phase '{phase}' in {prp_id}\n"
627:                 f"Available phases: {', '.join(phases)}\n"
628:                 f"üîß Troubleshooting: Use 'ce prp list' to see available checkpoints"
629:             )
630:     else:
631:         # Use latest (by timestamp)
632:         checkpoints.sort(key=lambda x: x["timestamp"], reverse=True)
633:         checkpoint = checkpoints[0]
634: 
635:     # Confirmation if interactive
636:     if sys.stdout.isatty():
637:         response = input(f"Restore to {checkpoint['tag_name']}? This will discard uncommitted changes. [y/N] ")
638:         if response.lower() != "y":
639:             return {"success": False, "message": "Restore cancelled by user"}
640: 
641:     # Restore to checkpoint
642:     checkout_result = run_cmd(f"git checkout {checkpoint['tag_name']}")
643:     if not checkout_result["success"]:
644:         raise RuntimeError(
645:             f"Failed to restore checkpoint: {checkout_result['stderr']}\n"
646:             f"üîß Troubleshooting: Ensure tag exists and git is configured correctly"
647:         )
648: 
649:     logger.info(f"Restored to checkpoint: {checkpoint['tag_name']}")
650: 
651:     return {
652:         "success": True,
653:         "restored_to": checkpoint["tag_name"],
654:         "commit_sha": checkpoint["commit_sha"]
655:     }
656: 
657: 
658: def delete_intermediate_checkpoints(prp_id: str, keep_final: bool = True) -> Dict[str, Any]:
659:     """Delete intermediate checkpoints (part of cleanup protocol).
660: 
661:     Args:
662:         prp_id: PRP identifier
663:         keep_final: Keep *-final checkpoint for rollback (default: True)
664: 
665:     Returns:
666:         {
667:             "success": True,
668:             "deleted_count": 2,
669:             "kept": ["checkpoint-PRP-003-final-20251012-160000"]
670:         }
671: 
672:     Process:
673:         1. List all checkpoints for prp_id
674:         2. Filter: keep *-final if keep_final=True
675:         3. Delete remaining tags: git tag -d {tag_name}
676:     """
677:     from .core import run_cmd
678: 
679:     checkpoints = list_checkpoints(prp_id)
680:     if not checkpoints:
681:         return {"success": True, "deleted_count": 0, "kept": []}
682: 
683:     to_delete = []
684:     kept = []
685: 
686:     for checkpoint in checkpoints:
687:         if keep_final and checkpoint["phase"] == "final":
688:             kept.append(checkpoint["tag_name"])
689:         else:
690:             to_delete.append(checkpoint["tag_name"])
691: 
692:     # Delete tags
693:     deleted_count = 0
694:     for tag_name in to_delete:
695:         result = run_cmd(f"git tag -d {tag_name}")
696:         if result["success"]:
697:             deleted_count += 1
698:             logger.info(f"Deleted checkpoint: {tag_name}")
699:         else:
700:             logger.warning(f"Failed to delete tag {tag_name}: {result['stderr']}")
701: 
702:     return {
703:         "success": True,
704:         "deleted_count": deleted_count,
705:         "kept": kept
706:     }
707: 
708: 
709: # ============================================================================
710: # Memory Isolation Functions
711: # ============================================================================
712: 
713: def write_prp_memory(category: str, name: str, content: str) -> Dict[str, Any]:
714:     """Write Serena memory with PRP namespace.
715: 
716:     Args:
717:         category: Memory category (checkpoint, learnings, temp)
718:         name: Memory identifier
719:         content: Memory content (markdown)
720: 
721:     Returns:
722:         {
723:             "success": True,
724:             "memory_name": "PRP-003-checkpoint-phase1",
725:             "serena_available": True
726:         }
727: 
728:     Raises:
729:         RuntimeError: If no active PRP
730:         Warning: If Serena MCP unavailable (logs warning, continues)
731: 
732:     Side Effects:
733:         - Calls serena.write_memory(f"{prp_id}-{category}-{name}", content)
734:         - Updates .ce/active_prp_session serena_memories list
735:     """
736:     active = get_active_prp()
737:     if not active:
738:         raise RuntimeError(
739:             f"No active PRP session\n"
740:             f"üîß Troubleshooting: Start a PRP first with 'ce prp start PRP-XXX'"
741:         )
742: 
743:     memory_name = f"{active['prp_id']}-{category}-{name}"
744:     serena_available = False
745: 
746:     # Try to write to Serena (optional)
747:     try:
748:         # Check if mcp__serena__write_memory tool is available
749:         # For now, we'll just log that Serena is not available
750:         # In production, this would call the Serena MCP tool
751:         logger.warning(f"Serena MCP not available - skipping memory write for {memory_name}")
752:     except Exception as e:
753:         logger.warning(f"Failed to write Serena memory: {e}")
754: 
755:     # Update state file
756:     if memory_name not in active["serena_memories"]:
757:         active["serena_memories"].append(memory_name)
758:         _write_state(active)
759: 
760:     return {
761:         "success": True,
762:         "memory_name": memory_name,
763:         "serena_available": serena_available
764:     }
765: 
766: 
767: def read_prp_memory(category: str, name: str) -> Optional[str]:
768:     """Read Serena memory with PRP namespace.
769: 
770:     Args:
771:         category: Memory category
772:         name: Memory identifier
773: 
774:     Returns:
775:         Memory content if exists, None otherwise
776: 
777:     Raises:
778:         RuntimeError: If no active PRP
779:     """
780:     active = get_active_prp()
781:     if not active:
782:         raise RuntimeError(
783:             f"No active PRP session\n"
784:             f"üîß Troubleshooting: Start a PRP first with 'ce prp start PRP-XXX'"
785:         )
786: 
787:     memory_name = f"{active['prp_id']}-{category}-{name}"
788: 
789:     # Try to read from Serena (optional)
790:     try:
791:         # In production, this would call the Serena MCP tool
792:         logger.warning(f"Serena MCP not available - cannot read memory {memory_name}")
793:         return None
794:     except Exception as e:
795:         logger.warning(f"Failed to read Serena memory: {e}")
796:         return None
797: 
798: 
799: def list_prp_memories(prp_id: Optional[str] = None) -> List[str]:
800:     """List all Serena memories for PRP(s).
801: 
802:     Args:
803:         prp_id: Optional PRP filter (None = current active PRP)
804: 
805:     Returns:
806:         List of memory names (e.g., ["PRP-003-checkpoint-phase1", ...])
807: 
808:     Process:
809:         1. Call serena.list_memories()
810:         2. Filter by prefix: {prp_id}-
811:         3. Return matching names
812:     """
813:     if prp_id is None:
814:         active = get_active_prp()
815:         if not active:
816:             return []
817:         prp_id = active["prp_id"]
818: 
819:     # Try to list from Serena (optional)
820:     try:
821:         # In production, this would call the Serena MCP tool
822:         logger.warning(f"Serena MCP not available - returning memories from state file")
823:         active = get_active_prp()
824:         if active and active["prp_id"] == prp_id:
825:             return active["serena_memories"]
826:         return []
827:     except Exception as e:
828:         logger.warning(f"Failed to list Serena memories: {e}")
829:         return []
830: 
831: 
832: # ============================================================================
833: # Cleanup Protocol Function
834: # ============================================================================
835: 
836: def cleanup_prp(prp_id: str) -> Dict[str, Any]:
837:     """Execute cleanup protocol for PRP (Model.md Section 5.6).
838: 
839:     Args:
840:         prp_id: PRP identifier to clean up
841: 
842:     Returns:
843:         {
844:             "success": True,
845:             "checkpoints_deleted": 2,
846:             "checkpoints_kept": ["checkpoint-PRP-003-final"],
847:             "memories_archived": ["PRP-003-learnings-auth-patterns"],
848:             "memories_deleted": ["PRP-003-checkpoint-*", "PRP-003-temp-*"],
849:             "context_health": {"drift_score": 5.2, "status": "healthy"}
850:         }
851: 
852:     Raises:
853:         RuntimeError: If cleanup operations fail
854: 
855:     Cleanup Protocol Steps:
856:         1. Delete intermediate git checkpoints (keep *-final)
857:         2. Archive learnings to project knowledge:
858:            - Read PRP-{id}-learnings-* memories
859:            - Merge into global "project-patterns" memory (append with timestamp + PRP-id prefix)
860:            - Delete PRP-{id}-learnings-* memories
861:         3. Delete ephemeral memories:
862:            - PRP-{id}-checkpoint-*
863:            - PRP-{id}-temp-*
864:         4. Reset validation state (if tracked)
865:         5. Run context health check:
866:            - ce context health
867:            - ce context prune
868:         6. Archive validation logs (if exist):
869:            - Move to PRPs/{prp_id}/validation-log.md
870:         7. Remove .ce/active_prp_session if prp_id matches active
871: 
872:     Side Effects:
873:         - Deletes git tags
874:         - Deletes/modifies Serena memories
875:         - Runs context health check
876:         - May remove active session file
877:     """
878:     from .core import run_cmd
879:     from .context import health as context_health
880: 
881:     result = {
882:         "success": True,
883:         "checkpoints_deleted": 0,
884:         "checkpoints_kept": [],
885:         "memories_archived": [],
886:         "memories_deleted": [],
887:         "context_health": {}
888:     }
889: 
890:     # Step 1: Delete intermediate checkpoints (keep *-final)
891:     checkpoint_result = delete_intermediate_checkpoints(prp_id, keep_final=True)
892:     result["checkpoints_deleted"] = checkpoint_result["deleted_count"]
893:     result["checkpoints_kept"] = checkpoint_result["kept"]
894: 
895:     # Step 2-3: Handle Serena memories (optional - skip if unavailable)
896:     memories = list_prp_memories(prp_id)
897: 
898:     # Archive learnings
899:     learnings = [m for m in memories if f"{prp_id}-learnings-" in m]
900:     if learnings:
901:         logger.info(f"Found {len(learnings)} learning memories to archive (Serena not implemented)")
902:         result["memories_archived"] = learnings
903: 
904:     # Delete ephemeral memories
905:     ephemeral = [m for m in memories if
906:                  f"{prp_id}-checkpoint-" in m or f"{prp_id}-temp-" in m]
907:     if ephemeral:
908:         logger.info(f"Found {len(ephemeral)} ephemeral memories to delete (Serena not implemented)")
909:         result["memories_deleted"] = ephemeral
910: 
911:     # Step 4: Reset validation state (already in state file)
912:     logger.info(f"Validation state reset for {prp_id}")
913: 
914:     # Step 5: Run context health check
915:     try:
916:         health = context_health()
917:         result["context_health"] = health
918:     except Exception as e:
919:         logger.warning(f"Context health check failed: {e}")
920: 
921:     # Step 6: Archive validation logs (if exist)
922:     # TODO: Implement when validation logging is added
923: 
924:     # Step 7: Remove active session if matches
925:     active = get_active_prp()
926:     if active and active["prp_id"] == prp_id:
927:         STATE_FILE.unlink(missing_ok=True)
928:         logger.info(f"Removed active session for {prp_id}")
929: 
930:     logger.info(f"Cleanup completed for {prp_id}")
931:     return result
</file>

<file path="tools/ce/resilience.py">
  1: """Resilience module - retry logic and circuit breaker for error recovery.
  2: 
  3: Provides decorators and utilities for handling transient failures and
  4: preventing cascading failures in production systems.
  5: """
  6: 
  7: import time
  8: import functools
  9: from typing import Callable, Any, Type, Tuple
 10: from datetime import datetime, timedelta
 11: 
 12: 
 13: def retry_with_backoff(
 14:     max_attempts: int = 3,
 15:     base_delay: float = 1.0,
 16:     max_delay: float = 60.0,
 17:     exponential_base: float = 2.0,
 18:     exceptions: Tuple[Type[Exception], ...] = (Exception,)
 19: ):
 20:     """Retry decorator with exponential backoff.
 21: 
 22:     Args:
 23:         max_attempts: Maximum retry attempts (default: 3)
 24:         base_delay: Initial delay in seconds (default: 1.0)
 25:         max_delay: Maximum delay in seconds (default: 60.0)
 26:         exponential_base: Backoff multiplier (default: 2.0)
 27:         exceptions: Tuple of exception types to retry (default: all)
 28: 
 29:     Returns:
 30:         Decorator function
 31: 
 32:     Example:
 33:         @retry_with_backoff(max_attempts=5, base_delay=2.0)
 34:         def fetch_data():
 35:             return api.get("/data")
 36: 
 37:     Note: Only retries on specified exceptions. Non-retryable errors propagate immediately.
 38:     """
 39:     def decorator(func: Callable) -> Callable:
 40:         @functools.wraps(func)
 41:         def wrapper(*args, **kwargs) -> Any:
 42:             for attempt in range(max_attempts):
 43:                 result = _try_call(func, args, kwargs, exceptions, attempt, max_attempts, base_delay, exponential_base, max_delay)
 44:                 if result is not None:
 45:                     return result
 46:             raise RuntimeError(
 47:                 "Retry logic error - should not reach here\n"
 48:                 "üîß Troubleshooting: Check retry decorator logic - this indicates internal implementation bug"
 49:             )
 50: 
 51:         return wrapper
 52:     return decorator
 53: 
 54: 
 55: def _try_call(func: Callable, args: tuple, kwargs: dict, exceptions: Tuple,
 56:               attempt: int, max_attempts: int, base_delay: float,
 57:               exponential_base: float, max_delay: float) -> Any:
 58:     """Try calling function with retry logic.
 59: 
 60:     Returns function result on success, None on retryable failure.
 61:     Raises on final attempt failure.
 62:     """
 63:     try:
 64:         return func(*args, **kwargs)
 65:     except exceptions as e:
 66:         is_final_attempt = (attempt == max_attempts - 1)
 67:         if is_final_attempt:
 68:             _raise_retry_error(func, max_attempts, e)
 69: 
 70:         # Backoff and retry
 71:         delay = min(base_delay * (exponential_base ** attempt), max_delay)
 72:         time.sleep(delay)
 73:         return None
 74: 
 75: 
 76: def _raise_retry_error(func: Callable, max_attempts: int, last_error: Exception) -> None:
 77:     """Raise detailed retry error after exhausting attempts."""
 78:     func_name = getattr(func, '__name__', repr(func))
 79:     raise RuntimeError(
 80:         f"Failed after {max_attempts} attempts: {func_name}\n"
 81:         f"Last error: {str(last_error)}\n"
 82:         f"üîß Troubleshooting: Check network connectivity, API rate limits"
 83:     ) from last_error
 84: 
 85: 
 86: class CircuitBreaker:
 87:     """Circuit breaker for preventing cascading failures.
 88: 
 89:     State machine: CLOSED ‚Üí OPEN ‚Üí HALF_OPEN ‚Üí CLOSED
 90:     - CLOSED: Normal operation, requests pass through
 91:     - OPEN: Failure threshold exceeded, requests fail fast
 92:     - HALF_OPEN: Testing recovery, limited requests pass through
 93: 
 94:     Example:
 95:         breaker = CircuitBreaker(name="serena-mcp", failure_threshold=5)
 96: 
 97:         @breaker.call
 98:         def call_serena():
 99:             return serena.read_file("test.py")
100: 
101:     Attributes:
102:         state: Current circuit state (closed/open/half_open)
103:         failure_count: Consecutive failure count
104:         last_failure_time: Timestamp of last failure
105:     """
106: 
107:     def __init__(
108:         self,
109:         name: str,
110:         failure_threshold: int = 5,
111:         recovery_timeout: int = 60,
112:         half_open_max_calls: int = 3
113:     ):
114:         """Initialize circuit breaker.
115: 
116:         Args:
117:             name: Circuit breaker identifier
118:             failure_threshold: Failures before opening circuit
119:             recovery_timeout: Seconds to wait before half-open attempt
120:             half_open_max_calls: Max calls in half-open state
121:         """
122:         self.name = name
123:         self.failure_threshold = failure_threshold
124:         self.recovery_timeout = recovery_timeout
125:         self.half_open_max_calls = half_open_max_calls
126: 
127:         # State
128:         self.state = "closed"  # closed, open, half_open
129:         self.failure_count = 0
130:         self.success_count = 0
131:         self.half_open_calls = 0
132:         self.last_failure_time = None
133: 
134:     def call(self, func: Callable) -> Callable:
135:         """Decorator to protect function with circuit breaker.
136: 
137:         Args:
138:             func: Function to protect
139: 
140:         Returns:
141:             Protected function
142: 
143:         Raises:
144:             CircuitBreakerOpenError: If circuit is open
145:         """
146:         @functools.wraps(func)
147:         def wrapper(*args, **kwargs) -> Any:
148:             # Check circuit state
149:             if self.state == "open":
150:                 if self._should_attempt_reset():
151:                     self._transition_to_half_open()
152:                 else:
153:                     raise CircuitBreakerOpenError(
154:                         f"Circuit breaker '{self.name}' is OPEN\n"
155:                         f"Failures: {self.failure_count}/{self.failure_threshold}\n"
156:                         f"üîß Troubleshooting: Wait {self.recovery_timeout}s or check service health"
157:                     )
158: 
159:             # Execute function
160:             try:
161:                 result = func(*args, **kwargs)
162:                 self._on_success()
163:                 return result
164:             except Exception as e:
165:                 self._on_failure()
166:                 raise
167: 
168:         return wrapper
169: 
170:     def _should_attempt_reset(self) -> bool:
171:         """Check if enough time has passed to attempt reset."""
172:         if self.last_failure_time is None:
173:             return False
174: 
175:         elapsed = (datetime.now() - self.last_failure_time).total_seconds()
176:         return elapsed >= self.recovery_timeout
177: 
178:     def _transition_to_half_open(self):
179:         """Transition from open to half-open state."""
180:         self.state = "half_open"
181:         self.half_open_calls = 0
182: 
183:     def _on_success(self):
184:         """Handle successful call."""
185:         if self.state == "half_open":
186:             self.success_count += 1
187:             if self.success_count >= self.half_open_max_calls:
188:                 # Recovered - close circuit
189:                 self.state = "closed"
190:                 self.failure_count = 0
191:                 self.success_count = 0
192:         else:
193:             # Reset failure count on success in closed state
194:             self.failure_count = 0
195: 
196:     def _on_failure(self):
197:         """Handle failed call."""
198:         self.failure_count += 1
199:         self.last_failure_time = datetime.now()
200: 
201:         if self.state == "half_open":
202:             # Failed in half-open - reopen circuit
203:             self.state = "open"
204:             self.success_count = 0
205:         elif self.failure_count >= self.failure_threshold:
206:             # Threshold exceeded - open circuit
207:             self.state = "open"
208: 
209: 
210: class CircuitBreakerOpenError(Exception):
211:     """Raised when circuit breaker is open."""
212:     pass
</file>

<file path="tools/ce/shell_utils.py">
  1: """Python alternatives to bash utilities for efficiency.
  2: 
  3: This module provides pure Python implementations of common bash utilities,
  4: eliminating subprocess overhead and improving performance 10-50x.
  5: 
  6: Usage:
  7:     from ce.shell_utils import grep_text, count_lines, head, Pipeline
  8: 
  9: All functions use pure Python stdlib - no external dependencies required.
 10: """
 11: 
 12: import re
 13: from pathlib import Path
 14: from typing import List, Optional, Union
 15: 
 16: 
 17: def grep_text(pattern: str, text: str, context_lines: int = 0) -> List[str]:
 18:     """Search text with regex, optional context lines.
 19: 
 20:     Replaces: bash grep -C<n>
 21: 
 22:     Args:
 23:         pattern: Regex pattern to search for
 24:         text: Input text to search
 25:         context_lines: Number of lines before/after to include
 26: 
 27:     Returns:
 28:         List of matching lines (with context if specified)
 29: 
 30:     Example:
 31:         >>> text = "line1\\nerror here\\nline3"
 32:         >>> grep_text("error", text, context_lines=1)
 33:         ['line1', 'error here', 'line3']
 34: 
 35:     Performance: 10-50x faster than subprocess grep
 36:     """
 37:     lines = text.split('\n')
 38:     regex = re.compile(pattern)
 39:     matches = []
 40:     matched_indices = set()
 41: 
 42:     for i, line in enumerate(lines):
 43:         if regex.search(line):
 44:             start = max(0, i - context_lines)
 45:             end = min(len(lines), i + context_lines + 1)
 46:             matched_indices.update(range(start, end))
 47: 
 48:     return [lines[i] for i in sorted(matched_indices)]
 49: 
 50: 
 51: def count_lines(file_path: str) -> int:
 52:     """Count lines in file.
 53: 
 54:     Replaces: bash wc -l
 55: 
 56:     Args:
 57:         file_path: Path to file (absolute or relative)
 58: 
 59:     Returns:
 60:         Number of lines in file
 61: 
 62:     Raises:
 63:         FileNotFoundError: If file doesn't exist
 64: 
 65:     Example:
 66:         >>> count_lines("config.yml")
 67:         42
 68: 
 69:     Performance: Direct file read, no subprocess overhead
 70:     """
 71:     return len(Path(file_path).read_text().split('\n'))
 72: 
 73: 
 74: def head(file_path: str, n: int = 10) -> List[str]:
 75:     """Read first N lines from file.
 76: 
 77:     Replaces: bash head -n
 78: 
 79:     Args:
 80:         file_path: Path to file (absolute or relative)
 81:         n: Number of lines to read (default: 10)
 82: 
 83:     Returns:
 84:         First N lines as list
 85: 
 86:     Raises:
 87:         FileNotFoundError: If file doesn't exist
 88: 
 89:     Example:
 90:         >>> head("log.txt", n=5)
 91:         ['Line 1', 'Line 2', 'Line 3', 'Line 4', 'Line 5']
 92: 
 93:     Performance: Reads only beginning of file, efficient for large files
 94:     """
 95:     return Path(file_path).read_text().split('\n')[:n]
 96: 
 97: 
 98: def tail(file_path: str, n: int = 10) -> List[str]:
 99:     """Read last N lines from file.
100: 
101:     Replaces: bash tail -n
102: 
103:     Args:
104:         file_path: Path to file (absolute or relative)
105:         n: Number of lines to read (default: 10)
106: 
107:     Returns:
108:         Last N lines as list
109: 
110:     Raises:
111:         FileNotFoundError: If file doesn't exist
112: 
113:     Example:
114:         >>> tail("log.txt", n=5)
115:         ['Line 96', 'Line 97', 'Line 98', 'Line 99', 'Line 100']
116: 
117:     Performance: Efficient for large files, reads from end
118:     """
119:     return Path(file_path).read_text().split('\n')[-n:]
120: 
121: 
122: def find_files(
123:     root: str,
124:     pattern: str,
125:     exclude: Optional[List[str]] = None
126: ) -> List[str]:
127:     """Find files by glob pattern recursively.
128: 
129:     Replaces: bash find . -name "*.py"
130: 
131:     Args:
132:         root: Root directory to search from
133:         pattern: Glob pattern (e.g., "*.py", "**/*.md")
134:         exclude: Optional list of patterns to exclude
135: 
136:     Returns:
137:         List of matching file paths (sorted, relative to root)
138: 
139:     Example:
140:         >>> find_files("src", "*.py", exclude=["__pycache__"])
141:         ['src/main.py', 'src/utils.py']
142: 
143:     Performance: Uses pathlib.rglob(), faster than subprocess find
144:     """
145:     exclude = exclude or []
146:     results = []
147: 
148:     for path in Path(root).rglob(pattern):
149:         if not any(ex in str(path) for ex in exclude):
150:             results.append(str(path))
151: 
152:     return sorted(results)
153: 
154: 
155: def extract_fields(
156:     text: str,
157:     field_indices: List[int],
158:     delimiter: Optional[str] = None
159: ) -> List[List[str]]:
160:     """Extract specific fields from each line.
161: 
162:     Replaces: awk '{print $1, $3}'
163: 
164:     Args:
165:         text: Input text (multi-line string)
166:         field_indices: 1-based field indices (like awk $1, $2)
167:         delimiter: Field separator (None = whitespace)
168: 
169:     Returns:
170:         List of extracted field lists per line
171: 
172:     Example:
173:         >>> text = "user1 100 active\\nuser2 200 inactive"
174:         >>> extract_fields(text, field_indices=[1, 3])
175:         [['user1', 'active'], ['user2', 'inactive']]
176: 
177:     Performance: Pure Python string operations, 10-50x faster than awk subprocess
178:     """
179:     lines = text.strip().split('\n')
180:     results = []
181: 
182:     for line in lines:
183:         if not line.strip():
184:             continue
185:         fields = line.split(delimiter) if delimiter else line.split()
186:         extracted = []
187:         for i in field_indices:
188:             if i <= len(fields):
189:                 extracted.append(fields[i-1])
190:         if extracted:
191:             results.append(extracted)
192: 
193:     return results
194: 
195: 
196: def sum_column(
197:     text: str,
198:     column: int,
199:     delimiter: Optional[str] = None
200: ) -> float:
201:     """Sum numeric values in a column.
202: 
203:     Replaces: awk '{sum += $1} END {print sum}'
204: 
205:     Args:
206:         text: Input text (multi-line string)
207:         column: 1-based column index to sum
208:         delimiter: Field separator (None = whitespace)
209: 
210:     Returns:
211:         Sum of numeric values in column
212: 
213:     Example:
214:         >>> text = "item1 100\\nitem2 200\\nitem3 300"
215:         >>> sum_column(text, column=2)
216:         600.0
217: 
218:     Note: Non-numeric values are skipped (not treated as errors)
219: 
220:     Performance: Type-safe Python arithmetic, no subprocess overhead
221:     """
222:     lines = text.strip().split('\n')
223:     total = 0.0
224: 
225:     for line in lines:
226:         if not line.strip():
227:             continue
228:         fields = line.split(delimiter) if delimiter else line.split()
229:         if column <= len(fields):
230:             try:
231:                 total += float(fields[column-1])
232:             except ValueError:
233:                 continue
234: 
235:     return total
236: 
237: 
238: def filter_and_extract(
239:     text: str,
240:     pattern: str,
241:     field_index: int,
242:     delimiter: Optional[str] = None
243: ) -> List[str]:
244:     """Pattern match lines and extract specific field.
245: 
246:     Replaces: awk '/pattern/ {print $2}'
247: 
248:     Args:
249:         text: Input text (multi-line string)
250:         pattern: Regex pattern to match lines
251:         field_index: 1-based field to extract from matching lines
252:         delimiter: Field separator (None = whitespace)
253: 
254:     Returns:
255:         List of extracted fields from matching lines
256: 
257:     Example:
258:         >>> text = "ERROR user1\\nINFO user2\\nERROR user3"
259:         >>> filter_and_extract(text, "ERROR", field_index=2)
260:         ['user1', 'user3']
261: 
262:     Performance: Combines grep_text and extract_fields for efficiency
263:     """
264:     matching_lines = grep_text(pattern, text, context_lines=0)
265:     results = []
266: 
267:     for line in matching_lines:
268:         if not line.strip():
269:             continue
270:         fields = line.split(delimiter) if delimiter else line.split()
271:         if field_index <= len(fields):
272:             results.append(fields[field_index-1])
273: 
274:     return results
275: 
276: 
277: class Pipeline:
278:     """Composable pipeline for chaining shell_utils operations.
279: 
280:     Eliminates subprocess overhead by chaining Python operations.
281:     10-50x faster than equivalent bash pipes.
282: 
283:     Usage:
284:         # Create pipeline from file
285:         result = Pipeline.from_file("log.txt").grep("ERROR", context_lines=1).count()
286: 
287:         # Create pipeline from text
288:         text = "line1\\nerror\\nline3"
289:         lines = Pipeline.from_text(text).grep("error").lines()
290: 
291:     Performance: Chaining operations avoids intermediate string copies
292:     and subprocess forks. Typical 10-50x speedup vs bash equivalents.
293:     """
294: 
295:     def __init__(self, data: Union[str, List[str]]) -> None:
296:         """Initialize pipeline with data.
297: 
298:         Args:
299:             data: String content or list of lines
300:         """
301:         if isinstance(data, str):
302:             self._lines = data.split('\n')
303:             self._text = data
304:         else:
305:             self._lines = data
306:             self._text = '\n'.join(data)
307: 
308:     @classmethod
309:     def from_file(cls, file_path: str) -> "Pipeline":
310:         """Create pipeline from file contents.
311: 
312:         Args:
313:             file_path: Path to file (absolute or relative)
314: 
315:         Returns:
316:             Pipeline instance initialized with file contents
317: 
318:         Example:
319:             >>> result = Pipeline.from_file("log.txt").grep("ERROR").count()
320:         """
321:         text = Path(file_path).read_text()
322:         return cls(text)
323: 
324:     @classmethod
325:     def from_text(cls, text: str) -> "Pipeline":
326:         """Create pipeline from text string.
327: 
328:         Args:
329:             text: Multi-line text string
330: 
331:         Returns:
332:             Pipeline instance initialized with text
333: 
334:         Example:
335:             >>> result = Pipeline.from_text("a\\nb\\nc").head(2).text()
336:         """
337:         return cls(text)
338: 
339:     def grep(self, pattern: str, context_lines: int = 0) -> "Pipeline":
340:         """Filter lines matching regex pattern.
341: 
342:         Args:
343:             pattern: Regex pattern to match
344:             context_lines: Lines before/after to include
345: 
346:         Returns:
347:             New Pipeline with filtered lines
348: 
349:         Example:
350:             >>> Pipeline.from_text("a\\nerror\\nb").grep("error").text()
351:             'error'
352:         """
353:         # Use current lines, not reconstructed text
354:         regex = re.compile(pattern)
355:         matched_indices = set()
356:         
357:         for i, line in enumerate(self._lines):
358:             if regex.search(line):
359:                 start = max(0, i - context_lines)
360:                 end = min(len(self._lines), i + context_lines + 1)
361:                 matched_indices.update(range(start, end))
362:         
363:         filtered_lines = [self._lines[i] for i in sorted(matched_indices)]
364:         return Pipeline(filtered_lines)
365: 
366:     def head(self, n: int = 10) -> "Pipeline":
367:         """Keep first N lines.
368: 
369:         Args:
370:             n: Number of lines to keep
371: 
372:         Returns:
373:             New Pipeline with first N lines
374: 
375:         Example:
376:             >>> Pipeline.from_text("a\\nb\\nc").head(2).text()
377:             'a\\nb'
378:         """
379:         return Pipeline(self._lines[:n])
380: 
381:     def tail(self, n: int = 10) -> "Pipeline":
382:         """Keep last N lines.
383: 
384:         Args:
385:             n: Number of lines to keep
386: 
387:         Returns:
388:             New Pipeline with last N lines
389: 
390:         Example:
391:             >>> Pipeline.from_text("a\\nb\\nc").tail(2).text()
392:             'b\\nc'
393:         """
394:         return Pipeline(self._lines[-n:])
395: 
396:     def extract_fields(
397:         self,
398:         field_indices: List[int],
399:         delimiter: Optional[str] = None
400:     ) -> "Pipeline":
401:         """Extract specific fields from each line.
402: 
403:         Args:
404:             field_indices: 1-based field indices to extract
405:             delimiter: Field separator (None = whitespace)
406: 
407:         Returns:
408:             New Pipeline with extracted fields as lines
409: 
410:         Example:
411:             >>> Pipeline.from_text("a 1\\nb 2").extract_fields([1]).text()
412:             'a\\nb'
413:         """
414:         extracted = extract_fields(self._text, field_indices, delimiter)
415:         # Convert lists back to lines
416:         lines = [' '.join(row) for row in extracted]
417:         return Pipeline(lines)
418: 
419:     def count(self) -> int:
420:         """Count lines in pipeline.
421: 
422:         Returns:
423:             Number of lines
424: 
425:         Example:
426:             >>> Pipeline.from_text("a\\nb\\nc").count()
427:             3
428:         """
429:         return len([l for l in self._lines if l.strip()])
430: 
431:     def sum_column(
432:         self,
433:         column: int,
434:         delimiter: Optional[str] = None
435:     ) -> float:
436:         """Sum numeric values in a column.
437: 
438:         Args:
439:             column: 1-based column index
440:             delimiter: Field separator (None = whitespace)
441: 
442:         Returns:
443:             Sum of numeric values
444: 
445:         Example:
446:             >>> Pipeline.from_text("a 100\\nb 200").sum_column(2)
447:             300.0
448:         """
449:         return sum_column(self._text, column, delimiter)
450: 
451:     def text(self) -> str:
452:         """Get pipeline contents as text.
453: 
454:         Returns:
455:             Multi-line string
456: 
457:         Example:
458:             >>> Pipeline.from_text("a\\nb").head(1).text()
459:             'a'
460:         """
461:         return self._text
462: 
463:     def lines(self) -> List[str]:
464:         """Get pipeline contents as line list.
465: 
466:         Returns:
467:             List of lines
468: 
469:         Example:
470:             >>> Pipeline.from_text("a\\nb\\nc").grep("[ab]").lines()
471:             ['a', 'b']
472:         """
473:         return self._lines
474: 
475:     def first(self) -> Optional[str]:
476:         """Get first line.
477: 
478:         Returns:
479:             First non-empty line or None
480: 
481:         Example:
482:             >>> Pipeline.from_text("\\na\\nb").first()
483:             'a'
484:         """
485:         for line in self._lines:
486:             if line.strip():
487:                 return line
488:         return None
489: 
490:     def last(self) -> Optional[str]:
491:         """Get last line.
492: 
493:         Returns:
494:             Last non-empty line or None
495: 
496:         Example:
497:             >>> Pipeline.from_text("a\\nb\\n").last()
498:             'b'
499:         """
500:         for line in reversed(self._lines):
501:             if line.strip():
502:                 return line
503:         return None
</file>

<file path="tools/ce/update_context.py">
   1: """Context sync operations for maintaining CE/Serena alignment with codebase.
   2: 
   3: This module provides the /update-context command functionality for syncing
   4: knowledge systems with actual implementations.
   5: """
   6: 
   7: import ast
   8: import logging
   9: import re
  10: import sys
  11: import yaml
  12: from datetime import datetime, timezone
  13: from pathlib import Path
  14: from typing import Any, Dict, List, Optional, Tuple
  15: 
  16: import frontmatter
  17: 
  18: logger = logging.getLogger(__name__)
  19: 
  20: 
  21: def is_interactive() -> bool:
  22:     """Check if stdin is connected to a terminal (interactive mode)."""
  23:     return sys.stdin.isatty()
  24: 
  25: # Pattern detection rules from examples/
  26: PATTERN_FILES = {
  27:     "error_handling": "examples/patterns/error-handling.py",
  28:     "no_fishy_fallbacks": "examples/patterns/no-fishy-fallbacks.py",
  29:     "naming_conventions": "examples/patterns/naming.py"
  30: }
  31: 
  32: PATTERN_CHECKS = {
  33:     "error_handling": [
  34:         ("bare_except", r"except:\s*$", "Use specific exception types"),
  35:         ("missing_troubleshooting", r'raise \w+Error\([^üîß]+\)$', "Add üîß Troubleshooting guidance")
  36:     ],
  37:     "naming_conventions": [
  38:         ("version_suffix", r"def \w+_v\d+", "Use descriptive names, not versions"),
  39:     ],
  40:     "kiss_violations": [
  41:         ("deep_nesting", r"^                    (if |for |while |try:|elif |with )", "Reduce nesting depth (max 4 levels)")
  42:     ]
  43: }
  44: 
  45: 
  46: def atomic_write(file_path: Path, content: str) -> None:
  47:     """Write file atomically using temp file + rename pattern.
  48: 
  49:     Args:
  50:         file_path: Target file path
  51:         content: Content to write
  52: 
  53:     Raises:
  54:         RuntimeError: If write operation fails
  55:             üîß Troubleshooting: Check file permissions and disk space
  56: 
  57:     Note: Prevents file corruption by writing to temp file first,
  58:     then replacing original atomically. Based on pattern from prp.py:215-219.
  59:     """
  60:     try:
  61:         # Write to temp file
  62:         temp_file = file_path.with_suffix(file_path.suffix + ".tmp")
  63:         temp_file.write_text(content, encoding="utf-8")
  64: 
  65:         # Atomic replace
  66:         temp_file.replace(file_path)
  67:     except Exception as e:
  68:         raise RuntimeError(
  69:             f"Failed to write {file_path}: {e}\n"
  70:             f"üîß Troubleshooting: Check file permissions and disk space"
  71:         ) from e
  72: 
  73: 
  74: def verify_function_exists_ast(function_name: str, search_dir: Path) -> bool:
  75:     """Verify function exists in codebase using AST parsing.
  76: 
  77:     Args:
  78:         function_name: Name of function to find (e.g., "sync_context")
  79:         search_dir: Directory to search (e.g., tools/ce/)
  80: 
  81:     Returns:
  82:         True if function found in any Python file, False otherwise
  83: 
  84:     Raises:
  85:         RuntimeError: If search directory doesn't exist
  86:             üîß Troubleshooting: Verify search_dir path is correct
  87:     """
  88:     if not search_dir.exists():
  89:         raise RuntimeError(
  90:             f"Search directory not found: {search_dir}\n"
  91:             f"üîß Troubleshooting: Verify search_dir path is correct"
  92:         )
  93: 
  94:     # Scan all Python files
  95:     for py_file in search_dir.glob("*.py"):
  96:         try:
  97:             content = py_file.read_text(encoding="utf-8")
  98:             tree = ast.parse(content, filename=str(py_file))
  99: 
 100:             # Walk AST looking for function definitions
 101:             for node in ast.walk(tree):
 102:                 if isinstance(node, ast.FunctionDef):
 103:                     if node.name == function_name:
 104:                         return True
 105:         except SyntaxError:
 106:             # Skip files with syntax errors
 107:             continue
 108:         except Exception as e:
 109:             logger.warning(f"Failed to parse {py_file}: {e}")
 110:             continue
 111: 
 112:     return False
 113: 
 114: 
 115: def read_prp_header(file_path: Path) -> Tuple[Dict[str, Any], str]:
 116:     """Read PRP YAML header using safe YAML loading.
 117: 
 118:     Args:
 119:         file_path: Path to PRP markdown file
 120: 
 121:     Returns:
 122:         Tuple of (metadata dict, content string)
 123: 
 124:     Raises:
 125:         FileNotFoundError: If file doesn't exist
 126:         ValueError: If YAML header is invalid
 127: 
 128:     Security Note:
 129:         Uses yaml.safe_load() to prevent code injection via !!python/object directives.
 130:         Only safe YAML constructs are parsed (no arbitrary Python code execution).
 131:     """
 132:     if not file_path.exists():
 133:         raise FileNotFoundError(
 134:             f"PRP file not found: {file_path}\n"
 135:             f"üîß Troubleshooting:\n"
 136:             f"   - Verify file path is correct\n"
 137:             f"   - Check if file was moved or renamed\n"
 138:             f"   - Use: ls {file_path.parent} to list directory"
 139:         )
 140: 
 141:     try:
 142:         # Use yaml.safe_load() for security (prevents code injection)
 143:         content = file_path.read_text(encoding="utf-8")
 144:         # Extract YAML frontmatter manually for explicit safe loading
 145:         if content.startswith("---"):
 146:             # Find closing --- delimiter
 147:             end_marker = content.find("---", 3)
 148:             if end_marker != -1:
 149:                 yaml_content = content[3:end_marker].strip()
 150:                 markdown_content = content[end_marker + 3:].strip()
 151: 
 152:                 # Parse YAML safely
 153:                 metadata = yaml.safe_load(yaml_content) or {}
 154:                 return metadata, markdown_content
 155: 
 156:         # Fallback to frontmatter.load() with safe loader for backwards compatibility
 157:         post = frontmatter.load(file_path)
 158:         return post.metadata, post.content
 159:     except yaml.YAMLError as e:
 160:         raise ValueError(
 161:             f"Failed to parse YAML header in {file_path}: {e}\n"
 162:             f"üîß Troubleshooting:\n"
 163:             f"   - Check YAML syntax with: head -n 20 {file_path}\n"
 164:             f"   - Ensure --- delimiters are present\n"
 165:             f"   - Validate YAML structure (no !!python/object directives)"
 166:         ) from e
 167:     except Exception as e:
 168:         raise ValueError(
 169:             f"Failed to read PRP header in {file_path}: {e}\n"
 170:             f"üîß Troubleshooting:\n"
 171:             f"   - Check file permissions: ls -la {file_path}\n"
 172:             f"   - Ensure file is readable text"
 173:         ) from e
 174: 
 175: 
 176: def transform_drift_to_initial(
 177:     violations: List[str],
 178:     drift_score: float,
 179:     missing_examples: List[Dict[str, Any]]
 180: ) -> str:
 181:     """Transform drift report ‚Üí INITIAL.md blueprint format.
 182: 
 183:     Args:
 184:         violations: List of violation messages with format:
 185:                    "File {path} has {issue} (violates {pattern}): {fix}"
 186:         drift_score: Percentage score (0-100)
 187:         missing_examples: List of PRPs missing examples with metadata:
 188:                          [{"prp_id": "PRP-10", "feature_name": "...",
 189:                            "suggested_path": "...", "rationale": "..."}]
 190: 
 191:     Returns:
 192:         INITIAL.md formatted string with:
 193:         - Feature: Drift summary with breakdown
 194:         - Context: Root causes and impact
 195:         - Examples: Top 5 violations + up to 3 missing examples
 196:         - Acceptance Criteria: Standard remediation checklist
 197:         - Technical Notes: File count, effort estimate, complexity
 198: 
 199:     Raises:
 200:         ValueError: If violations empty and missing_examples empty
 201:                    If drift_score invalid (not 0-100)
 202: 
 203:     Edge Cases:
 204:         - Empty violations + empty missing: Raises ValueError
 205:         - drift_score outside 0-100: Raises ValueError
 206:         - More than 5 violations: Shows top 5 only
 207:         - More than 3 missing examples: Shows top 3 only
 208:         - No file paths extractable: files_affected = 0
 209: 
 210:     Example:
 211:         >>> violations = ["File tools/ce/foo.py has bare_except: Use specific"]
 212:         >>> missing = [{"prp_id": "PRP-10", "suggested_path": "ex.py",
 213:         ...            "feature_name": "Feature", "rationale": "Important"}]
 214:         >>> result = transform_drift_to_initial(violations, 12.5, missing)
 215:         >>> assert "# Drift Remediation" in result
 216:         >>> assert "12.5%" in result
 217:         >>> assert "PRP-10" in result
 218:     """
 219:     # Validation
 220:     if not violations and not missing_examples:
 221:         raise ValueError(
 222:             "Cannot generate INITIAL.md: no violations and no missing examples\n"
 223:             "üîß Troubleshooting: Drift detection returned empty results"
 224:         )
 225: 
 226:     if not (0 <= drift_score <= 100):
 227:         raise ValueError(
 228:             f"Invalid drift_score: {drift_score} (must be 0-100)\n"
 229:             "üîß Troubleshooting: Check drift calculation returns percentage"
 230:         )
 231: 
 232:     now = datetime.now(timezone.utc).strftime("%Y-%m-%d")
 233: 
 234:     # Count violations by category (extract pattern from violation string)
 235:     # Pattern format: "(violates examples/patterns/{category}.py)"
 236:     error_handling = len([v for v in violations if "error-handling.py" in v or "error_handling.py" in v])
 237:     naming = len([v for v in violations if "naming.py" in v])
 238:     kiss = len([v for v in violations if "kiss.py" in v or "nesting" in v.lower()])
 239: 
 240:     # Categorize drift level
 241:     if drift_score < 5:
 242:         drift_level = "‚úÖ OK"
 243:     elif drift_score < 15:
 244:         drift_level = "‚ö†Ô∏è WARNING"
 245:     else:
 246:         drift_level = "üö® CRITICAL"
 247: 
 248:     # Calculate effort estimate (15 min per violation + 30 min per missing example)
 249:     effort_hours = (len(violations) * 0.25) + (len(missing_examples) * 0.5)
 250:     effort_hours = max(1, round(effort_hours))  # Minimum 1 hour
 251: 
 252:     # Calculate complexity
 253:     total_items = len(violations) + len(missing_examples)
 254:     if total_items < 5:
 255:         complexity = "LOW"
 256:     elif total_items < 15:
 257:         complexity = "MEDIUM"
 258:     else:
 259:         complexity = "HIGH"
 260: 
 261:     # Extract unique file paths for count
 262:     # Expected format: "File {path} has {issue} (violates {pattern}): {fix}"
 263:     files_affected = set()
 264:     for v in violations:
 265:         if "File " in v and " has " in v:
 266:             # Extract file path: "File tools/ce/foo.py has ..."
 267:             try:
 268:                 file_part = v.split(" has ")[0].replace("File ", "").strip()
 269:                 if file_part:  # Only add non-empty paths
 270:                     files_affected.add(file_part)
 271:             except (IndexError, AttributeError):
 272:                 # Malformed violation string, skip gracefully
 273:                 continue
 274: 
 275:     # Build INITIAL.md content
 276:     initial = f"""# Drift Remediation - {now}
 277: 
 278: ## Feature
 279: 
 280: Address {len(violations)} drift violations detected in codebase scan on {now}.
 281: 
 282: **Drift Score**: {drift_score:.1f}% ({drift_level})
 283: 
 284: **Violations Breakdown**:
 285: - Error Handling: {error_handling}
 286: - Naming Conventions: {naming}
 287: - KISS Violations: {kiss}
 288: - Missing Examples: {len(missing_examples)}
 289: 
 290: ## Context
 291: 
 292: Context Engineering drift detection found violations between documented patterns (CLAUDE.md, examples/) and actual implementation.
 293: 
 294: **Root Causes**:
 295: 1. New code written without pattern awareness
 296: 2. Missing examples for critical PRPs
 297: 3. Pattern evolution without documentation updates
 298: 
 299: **Impact**:
 300: - Code quality inconsistency
 301: - Reduced onboarding effectiveness
 302: - Pattern erosion over time
 303: 
 304: ## Examples
 305: 
 306: """
 307: 
 308:     # Add top 5 violations
 309:     for i, violation in enumerate(violations[:5], 1):
 310:         initial += f"### Violation {i}\n\n"
 311:         initial += f"{violation}\n\n"
 312: 
 313:     # Add missing examples (up to 3)
 314:     if missing_examples:
 315:         initial += "### Missing Examples\n\n"
 316:         for missing in missing_examples[:3]:
 317:             initial += f"**{missing['prp_id']}**: {missing['feature_name']}\n"
 318:             initial += f"- **Missing**: `{missing['suggested_path']}`\n"
 319:             initial += f"- **Rationale**: {missing['rationale']}\n\n"
 320: 
 321:     # Add Acceptance Criteria
 322:     initial += """## Acceptance Criteria
 323: 
 324: - [ ] All HIGH priority violations resolved
 325: - [ ] Missing examples created for critical PRPs
 326: - [ ] L4 validation passes (ce validate --level 4)
 327: - [ ] Drift score < 5% after remediation
 328: - [ ] Pattern documentation updated if intentional drift
 329: 
 330: """
 331: 
 332:     # Add Technical Notes with high-level summary
 333:     initial += f"""## Technical Notes
 334: 
 335: **Files Affected**: {len(files_affected)}
 336: **Estimated Effort**: {effort_hours}h based on violation count
 337: **Complexity**: {complexity}
 338: **Total Items**: {len(violations)} violations + {len(missing_examples)} missing examples
 339: 
 340: **Priority Focus**:
 341: - Address HIGH priority violations first
 342: - Create missing examples for critical PRPs
 343: - Run L4 validation after each fix
 344: """
 345: 
 346:     return initial
 347: 
 348: 
 349: def detect_drift_violations() -> Dict[str, Any]:
 350:     """Run drift detection and return structured results.
 351: 
 352:     Returns:
 353:         {
 354:             "drift_score": 12.5,
 355:             "violations": ["file.py:42 - Error", ...],
 356:             "missing_examples": [{"prp_id": "PRP-10", ...}],
 357:             "has_drift": True
 358:         }
 359: 
 360:     Raises:
 361:         RuntimeError: If detection fails with troubleshooting guidance
 362: 
 363:     Example:
 364:         >>> result = detect_drift_violations()
 365:         >>> assert "drift_score" in result
 366:         >>> assert isinstance(result["violations"], list)
 367:     """
 368:     logger.info("Running drift detection...")
 369:     try:
 370:         # Call existing detection functions
 371:         drift_result = verify_codebase_matches_examples()
 372:         missing_examples = detect_missing_examples_for_prps()
 373: 
 374:         drift_score = drift_result["drift_score"]
 375:         violations = drift_result["violations"]
 376:         has_drift = drift_score >= 5 or len(missing_examples) > 0
 377: 
 378:         return {
 379:             "drift_score": drift_score,
 380:             "violations": violations,
 381:             "missing_examples": missing_examples,
 382:             "has_drift": has_drift
 383:         }
 384:     except Exception as e:
 385:         raise RuntimeError(
 386:             f"Drift detection failed: {e}\n"
 387:             f"üîß Troubleshooting:\n"
 388:             f"   - Ensure examples/ directory exists\n"
 389:             f"   - Check PRPs have valid YAML headers\n"
 390:             f"   - Verify tools/ce/ directory is accessible\n"
 391:             f"   - Run: cd tools && uv run ce validate --level 1"
 392:         ) from e
 393: 
 394: 
 395: def generate_drift_blueprint(drift_result: Dict, missing_examples: List) -> Path:
 396:     """Generate DEDRIFT-INITIAL.md blueprint in tmp/ce/.
 397: 
 398:     Args:
 399:         drift_result: Detection results from detect_drift_violations()
 400:         missing_examples: List of PRPs missing examples
 401: 
 402:     Returns:
 403:         Path to generated blueprint file
 404: 
 405:     Raises:
 406:         RuntimeError: If blueprint generation fails
 407: 
 408:     Example:
 409:         >>> drift = detect_drift_violations()
 410:         >>> missing = drift["missing_examples"]
 411:         >>> path = generate_drift_blueprint(drift, missing)
 412:         >>> assert path.exists()
 413:         >>> assert "DEDRIFT-INITIAL.md" in path.name
 414:     """
 415:     logger.info("Generating remediation blueprint...")
 416:     try:
 417:         # Use PRP-15.1 transform function
 418:         blueprint = transform_drift_to_initial(
 419:             drift_result["violations"],
 420:             drift_result["drift_score"],
 421:             missing_examples
 422:         )
 423: 
 424:         # Determine project root
 425:         current_dir = Path.cwd()
 426:         if current_dir.name == "tools":
 427:             project_root = current_dir.parent
 428:         else:
 429:             project_root = current_dir
 430: 
 431:         # Create tmp/ce/ directory
 432:         tmp_ce_dir = project_root / "tmp" / "ce"
 433:         tmp_ce_dir.mkdir(parents=True, exist_ok=True)
 434: 
 435:         # Write blueprint atomically
 436:         blueprint_path = tmp_ce_dir / "DEDRIFT-INITIAL.md"
 437:         atomic_write(blueprint_path, blueprint)
 438: 
 439:         logger.info(f"Blueprint generated: {blueprint_path}")
 440:         return blueprint_path
 441: 
 442:     except Exception as e:
 443:         raise RuntimeError(
 444:             f"Blueprint generation failed: {e}\n"
 445:             f"üîß Troubleshooting:\n"
 446:             f"   - Check tmp/ce/ directory permissions\n"
 447:             f"   - Verify transform_drift_to_initial() is available (PRP-15.1)\n"
 448:             f"   - Check disk space: df -h\n"
 449:             f"   - Run: ls -la tmp/"
 450:         ) from e
 451: 
 452: 
 453: def display_drift_summary(drift_score: float, violations: List[str],
 454:                           missing_examples: List[Dict], blueprint_path: Path):
 455:     """Display drift summary with direct output (no box-drawing).
 456: 
 457:     Args:
 458:         drift_score: Percentage score (0-100)
 459:         violations: List of violation messages
 460:         missing_examples: List of PRPs missing examples
 461:         blueprint_path: Path to generated blueprint
 462: 
 463:     Example:
 464:         >>> display_drift_summary(12.5, violations, missing, path)
 465:         # Prints direct output with Unicode separators
 466:     """
 467:     print("\n" + "‚îÅ" * 60)
 468:     print("üìä Drift Summary")
 469:     print("‚îÅ" * 60)
 470: 
 471:     # Drift level indicator
 472:     level = "‚ö†Ô∏è WARNING" if drift_score < 15 else "üö® CRITICAL"
 473:     print(f"Drift Score: {drift_score:.1f}% ({level})")
 474:     print(f"Total Violations: {len(violations) + len(missing_examples)}")
 475:     print()
 476: 
 477:     # Breakdown by category
 478:     # Pattern format: "(violates examples/patterns/{category}.py)"
 479:     print("Breakdown:")
 480:     if violations:
 481:         # Categorize violations using pattern file detection (consistent with PRP-15.1)
 482:         error_count = len([v for v in violations if "error-handling.py" in v or "error_handling.py" in v])
 483:         naming_count = len([v for v in violations if "naming.py" in v])
 484:         kiss_count = len([v for v in violations if "kiss.py" in v or "nesting" in v.lower()])
 485: 
 486:         if error_count > 0:
 487:             print(f"  ‚Ä¢ Error Handling: {error_count} violation{'s' if error_count != 1 else ''}")
 488:         if naming_count > 0:
 489:             print(f"  ‚Ä¢ Naming Conventions: {naming_count} violation{'s' if naming_count != 1 else ''}")
 490:         if kiss_count > 0:
 491:             print(f"  ‚Ä¢ KISS Violations: {kiss_count} violation{'s' if kiss_count != 1 else ''}")
 492: 
 493:     if missing_examples:
 494:         print(f"  ‚Ä¢ Missing Examples: {len(missing_examples)} PRP{'s' if len(missing_examples) != 1 else ''}")
 495: 
 496:     print()
 497:     print(f"Blueprint: {blueprint_path}")
 498:     print("‚îÅ" * 60)
 499:     print()
 500: 
 501: 
 502: def generate_prp_yaml_header(violation_count: int, missing_count: int, timestamp: str) -> str:
 503:     """Generate YAML header for DEDRIFT maintenance PRP.
 504: 
 505:     Args:
 506:         violation_count: Number of code violations
 507:         missing_count: Number of missing examples
 508:         timestamp: Formatted timestamp for PRP ID (e.g., "20251015-120530")
 509: 
 510:     Returns:
 511:         YAML header string with metadata
 512: 
 513:     Example:
 514:         >>> header = generate_prp_yaml_header(5, 2, "20251015-120530")
 515:         >>> assert "prp_id:" in header
 516:         >>> assert "DEDRIFT-20251015-120530" in header
 517:         >>> assert "effort_hours:" in header
 518:     """
 519:     total_items = violation_count + missing_count
 520: 
 521:     # Effort estimation: 15 min per violation + 30 min per missing example
 522:     # NOTE: Same formula as PRP-15.1 transform function for consistency
 523:     effort_hours = (violation_count * 0.25) + (missing_count * 0.5)
 524:     effort_hours = max(1, round(effort_hours))  # Minimum 1 hour
 525: 
 526:     # Risk assessment based on item count
 527:     if total_items < 5:
 528:         risk = "LOW"
 529:     elif total_items < 10:
 530:         risk = "MEDIUM"
 531:     else:
 532:         risk = "HIGH"
 533: 
 534:     now = datetime.now().isoformat()
 535: 
 536:     return f"""---
 537: name: "Drift Remediation - {timestamp}"
 538: description: "Address drift violations detected in codebase scan"
 539: prp_id: "DEDRIFT-{timestamp}"
 540: status: "new"
 541: created_date: "{now}Z"
 542: last_updated: "{now}Z"
 543: updated_by: "drift-remediation-workflow"
 544: context_sync:
 545:   ce_updated: false
 546:   serena_updated: false
 547: version: 1
 548: priority: "MEDIUM"
 549: effort_hours: {effort_hours}
 550: risk: "{risk}"
 551: ---
 552: 
 553: """
 554: 
 555: 
 556: # ======================================================================
 557: # PRP-15.3: Drift Remediation Workflow Automation
 558: # ======================================================================
 559: 
 560: def generate_maintenance_prp(blueprint_path: Path) -> Path:
 561:     """Generate complete maintenance PRP file from blueprint.
 562: 
 563:     Args:
 564:         blueprint_path: Path to DEDRIFT-INITIAL.md blueprint
 565: 
 566:     Returns:
 567:         Path to generated PRP file in PRPs/system/
 568: 
 569:     Raises:
 570:         RuntimeError: If PRP generation fails
 571: 
 572:     Example:
 573:         >>> blueprint = Path("tmp/ce/DEDRIFT-INITIAL.md")
 574:         >>> prp = generate_maintenance_prp(blueprint)
 575:         >>> assert prp.exists()
 576:         >>> assert "DEDRIFT_PRP-" in prp.name
 577:     """
 578:     logger.info("Generating maintenance PRP file...")
 579:     try:
 580:         # Read blueprint content
 581:         blueprint_content = blueprint_path.read_text()
 582: 
 583:         # Extract metadata from blueprint for YAML header
 584:         # Count violations and missing examples from content
 585:         violation_count = blueprint_content.count("### Violation")
 586:         missing_count = blueprint_content.count("**Missing**:")
 587: 
 588:         # Generate timestamp for PRP ID
 589:         timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
 590: 
 591:         # Generate YAML header (PRP-15.2 function)
 592:         yaml_header = generate_prp_yaml_header(violation_count, missing_count, timestamp)
 593: 
 594:         # Combine YAML + blueprint content
 595:         prp_content = yaml_header + blueprint_content
 596: 
 597:         # Determine project root and create PRPs/system/ directory
 598:         current_dir = Path.cwd()
 599:         if current_dir.name == "tools":
 600:             project_root = current_dir.parent
 601:         else:
 602:             project_root = current_dir
 603: 
 604:         prp_system_dir = project_root / "PRPs" / "system"
 605:         prp_system_dir.mkdir(parents=True, exist_ok=True)
 606: 
 607:         # Write PRP file atomically
 608:         prp_path = prp_system_dir / f"DEDRIFT_PRP-{timestamp}.md"
 609:         atomic_write(prp_path, prp_content)
 610: 
 611:         logger.info(f"Maintenance PRP generated: {prp_path}")
 612:         return prp_path
 613: 
 614:     except Exception as e:
 615:         raise RuntimeError(
 616:             f"PRP generation failed: {e}\n"
 617:             f"üîß Troubleshooting:\n"
 618:             f"   - Check PRPs/system/ directory permissions\n"
 619:             f"   - Verify blueprint file exists: {blueprint_path}\n"
 620:             f"   - Check disk space: df -h"
 621:         ) from e
 622: 
 623: 
 624: def remediate_drift_workflow(yolo_mode: bool = False, auto_execute: bool = False) -> Dict[str, Any]:
 625:     """Execute drift remediation workflow.
 626: 
 627:     Args:
 628:         yolo_mode: If True, skip approval gate (--remediate flag)
 629:         auto_execute: If True, automatically execute PRP without user approval
 630: 
 631:     Returns:
 632:         {
 633:             "success": bool,
 634:             "prp_path": Optional[Path],
 635:             "blueprint_path": Optional[Path],
 636:             "executed": bool,  # True if auto_execute=True and PRP was executed
 637:             "fixes": List[str],  # List of fixes applied (if executed=True)
 638:             "errors": List[str]
 639:         }
 640: 
 641:     Workflow:
 642:         1. Detect drift violations (PRP-15.2)
 643:         2. Transform to INITIAL.md format (PRP-15.1)
 644:         3. Generate blueprint file (PRP-15.2)
 645:         4. Display drift summary (PRP-15.2)
 646:         5. Ask approval (vanilla) OR skip approval (YOLO)
 647:         6. Generate maintenance PRP (PRP-15.3)
 648:         7. Display /execute-prp command for manual execution
 649: 
 650:     Raises:
 651:         None - all errors captured in errors list
 652: 
 653:     Example (Vanilla Mode):
 654:         >>> result = remediate_drift_workflow(yolo_mode=False)
 655:         # Prompts: "Proceed with remediation? (yes/no):"
 656:         # If yes: Generates PRP, displays command
 657:         # If no: Workflow stops, blueprint saved
 658: 
 659:     Example (YOLO Mode):
 660:         >>> result = remediate_drift_workflow(yolo_mode=True)
 661:         # Skips approval prompt
 662:         # Auto-generates PRP, displays command
 663:     """
 664:     mode_label = "YOLO mode (no approval)" if yolo_mode else "Interactive mode"
 665:     logger.info(f"Starting drift remediation workflow ({mode_label})...")
 666:     errors = []
 667: 
 668:     # Step 1: Detect drift (PRP-15.2 function)
 669:     try:
 670:         drift = detect_drift_violations()
 671:     except RuntimeError as e:
 672:         return {
 673:             "success": False,
 674:             "prp_path": None,
 675:             "blueprint_path": None,
 676:             "errors": [str(e)]
 677:         }
 678: 
 679:     # Early exit if no drift
 680:     if not drift["has_drift"]:
 681:         print(f"\n‚úÖ No drift detected (score: {drift['drift_score']:.1f}%)")
 682:         print("Context is healthy - no remediation needed.\n")
 683:         return {
 684:             "success": True,
 685:             "prp_path": None,
 686:             "blueprint_path": None,
 687:             "executed": False,
 688:             "fixes": [],
 689:             "errors": []
 690:         }
 691: 
 692:     # Step 2: Generate blueprint (PRP-15.2 function)
 693:     try:
 694:         blueprint_path = generate_drift_blueprint(drift, drift["missing_examples"])
 695:     except RuntimeError as e:
 696:         return {
 697:             "success": False,
 698:             "prp_path": None,
 699:             "blueprint_path": None,
 700:             "errors": [str(e)]
 701:         }
 702: 
 703:     # Step 3: Display summary (PRP-15.2 function)
 704:     display_drift_summary(
 705:         drift["drift_score"],
 706:         drift["violations"],
 707:         drift["missing_examples"],
 708:         blueprint_path
 709:     )
 710: 
 711:     # Step 4: Approval gate (vanilla only)
 712:     if not yolo_mode:
 713:         # Check if running in interactive mode
 714:         if not is_interactive():
 715:             # Non-interactive mode without --remediate: skip remediation gracefully
 716:             print(f"\n‚è≠Ô∏è Non-interactive mode detected (no TTY)")
 717:             print(f"üìÑ Blueprint saved: {blueprint_path}")
 718:             print(f"\nüí° For automated remediation, use: ce update-context --remediate\n")
 719:             return {
 720:                 "success": True,
 721:                 "prp_path": None,
 722:                 "blueprint_path": blueprint_path,
 723:                 "errors": []
 724:             }
 725: 
 726:         # Interactive mode: ask for approval
 727:         print(f"\nReview INITIAL.md: {blueprint_path}")
 728:         approval = input("Proceed with remediation? (yes/no): ").strip().lower()
 729: 
 730:         if approval not in ["yes", "y"]:
 731:             print("‚ö†Ô∏è Remediation skipped by user")
 732:             print(f"Blueprint saved: {blueprint_path}\n")
 733:             return {
 734:                 "success": True,
 735:                 "prp_path": None,
 736:                 "blueprint_path": blueprint_path,
 737:                 "errors": []
 738:             }
 739: 
 740:         logger.info("User approved remediation - proceeding...")
 741: 
 742:     # Step 5: Generate maintenance PRP (PRP-15.3 function)
 743:     logger.info("Generating maintenance PRP...")
 744:     try:
 745:         prp_path = generate_maintenance_prp(blueprint_path)
 746:     except Exception as e:
 747:         errors.append(f"PRP generation failed: {e}")
 748:         return {
 749:             "success": False,
 750:             "prp_path": None,
 751:             "blueprint_path": blueprint_path,
 752:             "errors": errors
 753:         }
 754: 
 755:     # Step 6: Auto-execute if requested
 756:     if auto_execute:
 757:         logger.info(f"Auto-executing PRP: {prp_path}")
 758:         try:
 759:             # Import here to avoid circular imports
 760:             from .prp import execute_prp as execute_prp_impl
 761: 
 762:             exec_result = execute_prp_impl(str(prp_path))
 763: 
 764:             if not exec_result.get("success", False):
 765:                 raise RuntimeError(
 766:                     f"PRP execution failed: {exec_result.get('error', 'Unknown error')}\n"
 767:                     f"üîß Troubleshooting:\n"
 768:                     f"   - Check PRP: {prp_path}\n"
 769:                     f"   - Review errors above\n"
 770:                     f"   - Try manual execution: /execute-prp {prp_path}"
 771:                 )
 772: 
 773:             fixes = exec_result.get("fixes", [])
 774:             print(f"\n‚úÖ Remediation executed: {len(fixes)} fixes applied")
 775:             logger.info(f"PRP executed successfully: {len(fixes)} fixes applied")
 776: 
 777:             return {
 778:                 "success": True,
 779:                 "prp_path": prp_path,
 780:                 "blueprint_path": blueprint_path,
 781:                 "executed": True,
 782:                 "fixes": fixes,
 783:                 "errors": []
 784:             }
 785:         except Exception as e:
 786:             error_msg = f"Auto-execution failed: {e}"
 787:             logger.error(error_msg)
 788:             errors.append(error_msg)
 789:             return {
 790:                 "success": False,
 791:                 "prp_path": prp_path,
 792:                 "blueprint_path": blueprint_path,
 793:                 "executed": False,
 794:                 "fixes": [],
 795:                 "errors": errors
 796:             }
 797: 
 798:     # Step 6: Display next step (manual execution)
 799:     logger.info("PRP ready for execution...")
 800: 
 801:     print("\n" + "‚îÅ" * 60)
 802:     print("üîß Next Step: Execute PRP")
 803:     print("‚îÅ" * 60)
 804:     print(f"Run: /execute-prp {prp_path}")
 805:     print("‚îÅ" * 60)
 806:     print()
 807: 
 808:     # Workflow complete - PRP ready for manual execution
 809:     print(f"‚úÖ PRP Generated: {prp_path}")
 810:     print(f"üìÑ Blueprint: {blueprint_path}\n")
 811: 
 812:     return {
 813:         "success": True,
 814:         "prp_path": prp_path,
 815:         "blueprint_path": blueprint_path,
 816:         "executed": False,
 817:         "fixes": [],
 818:         "errors": []
 819:     }
 820: 
 821: 
 822: def update_context_sync_flags(
 823:     file_path: Path,
 824:     ce_updated: bool,
 825:     serena_updated: bool
 826: ) -> None:
 827:     """Update context_sync flags in PRP YAML header.
 828: 
 829:     Args:
 830:         file_path: Path to PRP markdown file
 831:         ce_updated: Whether CE content was updated
 832:         serena_updated: Always False (Serena verification disabled due to MCP architecture)
 833: 
 834:     Raises:
 835:         ValueError: If YAML update fails
 836: 
 837:     Note:
 838:         - Serena verification removed (Python subprocess cannot access parent's stdio MCP)
 839:         - Only updates timestamps if flags actually changed (no false positives)
 840:     """
 841:     metadata, content = read_prp_header(file_path)
 842: 
 843:     # Initialize context_sync if missing
 844:     if "context_sync" not in metadata:
 845:         metadata["context_sync"] = {}
 846: 
 847:     # Track if anything actually changed
 848:     old_ce_updated = metadata["context_sync"].get("ce_updated", False)
 849:     old_serena_updated = metadata["context_sync"].get("serena_updated", False)
 850:     flags_changed = (old_ce_updated != ce_updated) or (old_serena_updated != serena_updated)
 851: 
 852:     # Only update if flags changed
 853:     if flags_changed:
 854:         metadata["context_sync"]["ce_updated"] = ce_updated
 855:         metadata["context_sync"]["serena_updated"] = serena_updated
 856:         metadata["context_sync"]["last_sync"] = datetime.now(timezone.utc).isoformat()
 857:         metadata["updated_by"] = "update-context-command"
 858:         metadata["updated"] = datetime.now(timezone.utc).isoformat()
 859: 
 860:         # Write back atomically
 861:         try:
 862:             post = frontmatter.Post(content, **metadata)
 863:             prp_content = frontmatter.dumps(post)
 864:             atomic_write(file_path, prp_content)
 865:             logger.info(f"Updated context_sync flags: {file_path}")
 866:         except Exception as e:
 867:             raise ValueError(
 868:                 f"Failed to write YAML header to {file_path}: {e}\n"
 869:                 f"üîß Troubleshooting:\n"
 870:                 f"   - Check file permissions: ls -la {file_path}\n"
 871:                 f"   - Ensure disk space available: df -h\n"
 872:                 f"   - Verify file not locked by another process"
 873:             ) from e
 874:     else:
 875:         logger.debug(f"No flag changes detected for {file_path.name} - skipping update")
 876: 
 877: 
 878: def get_prp_status(file_path: Path) -> str:
 879:     """Extract status field from PRP YAML header.
 880: 
 881:     Args:
 882:         file_path: Path to PRP markdown file
 883: 
 884:     Returns:
 885:         Status string (e.g., 'new', 'executed', 'archived')
 886:     """
 887:     metadata, _ = read_prp_header(file_path)
 888:     return metadata.get("status", "unknown")
 889: 
 890: 
 891: def discover_prps(target_prp: Optional[str] = None) -> List[Path]:
 892:     """Scan PRPs/ directory recursively for markdown files.
 893: 
 894:     Args:
 895:         target_prp: Optional specific PRP file path for targeted sync
 896: 
 897:     Returns:
 898:         List of PRP file paths
 899: 
 900:     Raises:
 901:         FileNotFoundError: If target_prp specified but not found
 902:     """
 903:     # Determine project root - if we're in tools/, go up one level
 904:     current_dir = Path.cwd()
 905:     if current_dir.name == "tools":
 906:         project_root = current_dir.parent
 907:     else:
 908:         project_root = current_dir
 909: 
 910:     if target_prp:
 911:         # Targeted sync - single PRP
 912:         prp_path = project_root / target_prp
 913:         if not prp_path.exists():
 914:             raise FileNotFoundError(
 915:                 f"Target PRP not found: {target_prp}\n"
 916:                 f"üîß Troubleshooting:\n"
 917:                 f"   - Check path is relative to project root\n"
 918:                 f"   - Use: ls PRPs/executed/ to list available PRPs\n"
 919:                 f"   - Verify file extension is .md"
 920:             )
 921:         return [prp_path]
 922: 
 923:     # Universal sync - all PRPs
 924:     prps_dir = project_root / "PRPs"
 925:     if not prps_dir.exists():
 926:         logger.warning(f"PRPs directory not found: {prps_dir}")
 927:         return []
 928: 
 929:     # Scan feature-requests and executed directories
 930:     prp_files = []
 931:     for subdir in ["feature-requests", "executed", "archived"]:
 932:         subdir_path = prps_dir / subdir
 933:         if subdir_path.exists():
 934:             prp_files.extend(subdir_path.glob("*.md"))
 935: 
 936:     logger.info(f"Discovered {len(prp_files)} PRP files")
 937:     return prp_files
 938: 
 939: 
 940: def extract_expected_functions(content: str) -> List[str]:
 941:     """Extract function/class names from PRP content using regex.
 942: 
 943:     Looks for:
 944:     - `function_name()` backtick references
 945:     - `class ClassName` backtick references
 946:     - def function_name() in code blocks
 947:     - class ClassName: in code blocks
 948: 
 949:     Args:
 950:         content: PRP markdown content
 951: 
 952:     Returns:
 953:         List of function/class names
 954:     """
 955:     functions = set()
 956: 
 957:     # Pattern 1: Backtick references `function_name()`
 958:     backtick_refs = re.findall(r'`(\w+)\(\)`', content)
 959:     functions.update(backtick_refs)
 960: 
 961:     # Pattern 2: Backtick class references `class ClassName`
 962:     class_refs = re.findall(r'`class (\w+)`', content)
 963:     functions.update(class_refs)
 964: 
 965:     # Pattern 3: Function definitions in code blocks
 966:     func_defs = re.findall(r'^\s*def (\w+)\(', content, re.MULTILINE)
 967:     functions.update(func_defs)
 968: 
 969:     # Pattern 4: Class definitions in code blocks
 970:     class_defs = re.findall(r'^\s*class (\w+)[\(:]', content, re.MULTILINE)
 971:     functions.update(class_defs)
 972: 
 973:     return sorted(list(functions))
 974: 
 975: 
 976: # Serena verification removed - Python subprocess cannot access parent's stdio MCP servers
 977: # MCP architecture limitation: stdio transport requires local subprocess spawn
 978: # Serena is internal to Claude Code session and not accessible from uv run subprocess
 979: 
 980: 
 981: def should_transition_to_executed(file_path: Path) -> bool:
 982:     """Check if PRP should transition from feature-requests to executed.
 983: 
 984:     Rules:
 985:     - Current status must be "new" or "in_progress"
 986:     - ce_updated must be True (implementation verified)
 987:     - File must be in feature-requests/ directory
 988: 
 989:     Args:
 990:         file_path: Path to PRP file
 991: 
 992:     Returns:
 993:         True if should transition to executed
 994:     """
 995:     metadata, _ = read_prp_header(file_path)
 996: 
 997:     # Check file location
 998:     if "feature-requests" not in str(file_path):
 999:         return False
1000: 
1001:     # Check status
1002:     status = metadata.get("status", "unknown")
1003:     if status not in ["new", "in_progress"]:
1004:         return False
1005: 
1006:     # Check ce_updated flag
1007:     context_sync = metadata.get("context_sync", {})
1008:     ce_updated = context_sync.get("ce_updated", False)
1009: 
1010:     return ce_updated
1011: 
1012: 
1013: def move_prp_to_executed(file_path: Path) -> Path:
1014:     """Move PRP from feature-requests/ to executed/.
1015: 
1016:     Uses pathlib rename for atomic operation.
1017: 
1018:     Args:
1019:         file_path: Current path to PRP file
1020: 
1021:     Returns:
1022:         New path in executed/ directory
1023: 
1024:     Raises:
1025:         RuntimeError: If move fails
1026:     """
1027:     # Calculate new path
1028:     current_dir = Path.cwd()
1029:     if current_dir.name == "tools":
1030:         project_root = current_dir.parent
1031:     else:
1032:         project_root = current_dir
1033:     executed_dir = project_root / "PRPs" / "executed"
1034: 
1035:     # Create executed directory if needed
1036:     executed_dir.mkdir(parents=True, exist_ok=True)
1037: 
1038:     new_path = executed_dir / file_path.name
1039: 
1040:     try:
1041:         # Atomic move
1042:         file_path.rename(new_path)
1043:         logger.info(f"Moved PRP: {file_path.name} ‚Üí executed/")
1044:         return new_path
1045:     except Exception as e:
1046:         raise RuntimeError(
1047:             f"Failed to move PRP to executed: {e}\n"
1048:             f"üîß Troubleshooting:\n"
1049:             f"   - Check permissions: ls -la {file_path}\n"
1050:             f"   - Ensure target doesn't exist: ls {new_path}\n"
1051:             f"   - Verify disk space: df -h"
1052:         ) from e
1053: 
1054: 
1055: def move_prp_to_archived(file_path: Path) -> Path:
1056:     """Move PRP to archived/ directory.
1057: 
1058:     Args:
1059:         file_path: Current path to PRP file
1060: 
1061:     Returns:
1062:         New path in archived/ directory
1063: 
1064:     Raises:
1065:         RuntimeError: If move fails
1066:     """
1067:     current_dir = Path.cwd()
1068:     if current_dir.name == "tools":
1069:         project_root = current_dir.parent
1070:     else:
1071:         project_root = current_dir
1072:     archived_dir = project_root / "PRPs" / "archived"
1073: 
1074:     # Create archived directory if needed
1075:     archived_dir.mkdir(parents=True, exist_ok=True)
1076: 
1077:     new_path = archived_dir / file_path.name
1078: 
1079:     try:
1080:         file_path.rename(new_path)
1081:         logger.info(f"Archived PRP: {file_path.name} ‚Üí archived/")
1082:         return new_path
1083:     except Exception as e:
1084:         raise RuntimeError(
1085:             f"Failed to archive PRP: {e}\n"
1086:             f"üîß Troubleshooting: Check permissions and disk space"
1087:         ) from e
1088: 
1089: 
1090: def detect_archived_prps() -> List[Path]:
1091:     """Identify superseded/deprecated PRPs for archival.
1092: 
1093:     Looks for:
1094:     - status == "archived" in YAML
1095:     - "superseded_by" field in metadata
1096: 
1097:     Returns:
1098:         List of PRP paths that should be archived
1099:     """
1100:     archived_candidates = []
1101:     all_prps = discover_prps()
1102: 
1103:     for prp_path in all_prps:
1104:         # Skip if already in archived/
1105:         if "archived" in str(prp_path):
1106:             continue
1107: 
1108:         try:
1109:             metadata, _ = read_prp_header(prp_path)
1110: 
1111:             # Check status
1112:             if metadata.get("status") == "archived":
1113:                 archived_candidates.append(prp_path)
1114:                 continue
1115: 
1116:             # Check superseded_by field
1117:             if "superseded_by" in metadata:
1118:                 archived_candidates.append(prp_path)
1119: 
1120:         except Exception as e:
1121:             logger.warning(f"Skipping {prp_path.name} - invalid YAML: {e}")
1122:             continue
1123: 
1124:     return archived_candidates
1125: 
1126: 
1127: def load_pattern_checks() -> Dict[str, List[Tuple[str, str, str]]]:
1128:     """Load pattern checks from PATTERN_CHECKS.
1129: 
1130:     Returns:
1131:         {
1132:             "error_handling": [
1133:                 ("bare_except", "regex", "fix description"),
1134:                 ...
1135:             ]
1136:         }
1137:     """
1138:     return PATTERN_CHECKS
1139: 
1140: 
1141: def verify_codebase_matches_examples() -> Dict[str, Any]:
1142:     """Check if codebase follows patterns documented in examples/.
1143: 
1144:     Returns:
1145:         {
1146:             "violations": [
1147:                 "File tools/ce/foo.py uses bare except (violates examples/patterns/error-handling.py)",
1148:                 ...
1149:             ],
1150:             "drift_score": 15.3  # Percentage of files violating patterns
1151:         }
1152: 
1153:     Refactored to reduce nesting depth from 5 to 4 levels.
1154:     """
1155:     from .pattern_detectors import check_file_for_violations
1156: 
1157:     current_dir = Path.cwd()
1158:     if current_dir.name == "tools":
1159:         project_root = current_dir.parent
1160:     else:
1161:         project_root = current_dir
1162:     examples_dir = project_root / "examples"
1163: 
1164:     # Skip if examples/ doesn't exist
1165:     if not examples_dir.exists():
1166:         logger.info("examples/ directory not found - skipping drift detection")
1167:         return {"violations": [], "drift_score": 0.0}
1168: 
1169:     violations = []
1170:     pattern_checks = load_pattern_checks()
1171: 
1172:     # Scan tools/ce/ for violations
1173:     tools_ce_dir = project_root / "tools" / "ce"
1174:     if not tools_ce_dir.exists():
1175:         return {"violations": [], "drift_score": 0.0}
1176: 
1177:     python_files = list(tools_ce_dir.glob("*.py"))
1178:     files_with_violations = set()
1179: 
1180:     # Process each file (delegated to reduce nesting)
1181:     for py_file in python_files:
1182:         file_violations, has_violations = check_file_for_violations(
1183:             py_file, pattern_checks, project_root
1184:         )
1185:         violations.extend(file_violations)
1186:         if has_violations:
1187:             files_with_violations.add(py_file)
1188: 
1189:     # Calculate drift score based on violation count, not file count
1190:     drift_score = 0.0
1191:     if python_files:
1192:         total_checks = len(python_files) * sum(len(checks) for checks in pattern_checks.values())
1193:         if total_checks > 0:
1194:             drift_score = (len(violations) / total_checks) * 100
1195: 
1196:     return {
1197:         "violations": violations,
1198:         "drift_score": drift_score
1199:     }
1200: 
1201: 
1202: def detect_missing_examples_for_prps() -> List[Dict[str, Any]]:
1203:     """Detect executed PRPs missing corresponding examples/ documentation.
1204: 
1205:     Returns:
1206:         [
1207:             {
1208:                 "prp_id": "PRP-13",
1209:                 "feature_name": "Production Hardening",
1210:                 "complexity": "high",
1211:                 "missing_example": "error_recovery",
1212:                 "suggested_path": "examples/patterns/error-recovery.py",
1213:                 "rationale": "Complex error recovery logic should be documented"
1214:             },
1215:             ...
1216:         ]
1217: 
1218:     Refactored to reduce nesting depth from 5 to 4 levels.
1219:     """
1220:     from .pattern_detectors import check_prp_for_missing_examples
1221: 
1222:     current_dir = Path.cwd()
1223:     if current_dir.name == "tools":
1224:         project_root = current_dir.parent
1225:     else:
1226:         project_root = current_dir
1227:     examples_dir = project_root / "examples"
1228:     missing_examples = []
1229: 
1230:     # Define keyword patterns
1231:     keywords_to_examples = {
1232:         "error recovery": ("error_recovery", "examples/patterns/error-recovery.py",
1233:                            "Complex error recovery logic should be documented"),
1234:         "strategy pattern": ("strategy_pattern_testing", "examples/patterns/strategy-testing.py",
1235:                              "Strategy pattern with mocks is reusable pattern"),
1236:         "pipeline": ("pipeline_testing", "examples/patterns/pipeline-testing.py",
1237:                      "Pipeline orchestration pattern should be documented")
1238:     }
1239: 
1240:     # Get all executed PRPs
1241:     executed_prps = (project_root / "PRPs" / "executed").glob("*.md")
1242: 
1243:     # Check each PRP (delegated to reduce nesting)
1244:     for prp_path in executed_prps:
1245:         prp_missing = check_prp_for_missing_examples(
1246:             prp_path, project_root, keywords_to_examples
1247:         )
1248:         missing_examples.extend(prp_missing)
1249: 
1250:     return missing_examples
1251: 
1252: 
1253: def generate_drift_report(violations: List[str], drift_score: float,
1254:                           missing_examples: List[Dict[str, Any]]) -> str:
1255:     """Generate formalized structured drift report with solution proposals.
1256: 
1257:     Args:
1258:         violations: List of violation messages
1259:         drift_score: Percentage of files violating patterns
1260:         missing_examples: List of PRPs missing examples
1261: 
1262:     Returns:
1263:         Markdown formatted drift report
1264:     """
1265:     now = datetime.now(timezone.utc).isoformat()
1266: 
1267:     # Classify drift score
1268:     drift_level = "‚úÖ OK" if drift_score < 5 else ("‚ö†Ô∏è  WARNING" if drift_score < 15 else "üö® CRITICAL")
1269: 
1270:     report = f"""## Context Drift Report - Examples/ Patterns
1271: 
1272: **Drift Score**: {drift_score:.1f}% ({drift_level})
1273: **Generated**: {now}
1274: **Violations Found**: {len(violations)}
1275: **Missing Examples**: {len(missing_examples)}
1276: 
1277: ### Part 1: Code Violating Documented Patterns
1278: 
1279: """
1280: 
1281:     if violations:
1282:         # Group violations by category
1283:         error_handling_violations = [v for v in violations if "error_handling" in v or "bare_except" in v]
1284:         naming_violations = [v for v in violations if "naming" in v or "version_suffix" in v]
1285:         kiss_violations = [v for v in violations if "kiss" in v or "nesting" in v]
1286: 
1287:         if error_handling_violations:
1288:             report += f"#### Error Handling ({len(error_handling_violations)} violations)\n\n"
1289:             for i, v in enumerate(error_handling_violations, 1):
1290:                 report += f"{i}. {v}\n"
1291:             report += "\n"
1292: 
1293:         if naming_violations:
1294:             report += f"#### Naming Conventions ({len(naming_violations)} violations)\n\n"
1295:             for i, v in enumerate(naming_violations, 1):
1296:                 report += f"{i}. {v}\n"
1297:             report += "\n"
1298: 
1299:         if kiss_violations:
1300:             report += f"#### KISS Violations ({len(kiss_violations)} violations)\n\n"
1301:             for i, v in enumerate(kiss_violations, 1):
1302:                 report += f"{i}. {v}\n"
1303:             report += "\n"
1304:     else:
1305:         report += "No violations detected - codebase follows documented patterns.\n\n"
1306: 
1307:     report += """### Part 2: Missing Pattern Documentation
1308: 
1309: **Critical PRPs Without Examples**:
1310: 
1311: """
1312: 
1313:     if missing_examples:
1314:         for i, missing in enumerate(missing_examples, 1):
1315:             report += f"""{i}. **{missing['prp_id']}**: {missing['feature_name']}
1316:    **Complexity**: {missing['complexity']}
1317:    **Missing Example**: {missing['missing_example']}
1318:    **Suggested Path**: {missing['suggested_path']}
1319:    **Rationale**: {missing['rationale']}
1320:    **Action**: Create example showing this pattern
1321: 
1322: """
1323:     else:
1324:         report += "All critical PRPs have corresponding examples/ documentation.\n\n"
1325: 
1326:     report += """### Proposed Solutions Summary
1327: 
1328: 1. **Code Violations** (manual review):
1329: """
1330:     if violations:
1331:         for v in violations[:3]:  # Show first 3
1332:             report += f"   - Review and fix: {v}\n"
1333:         if len(violations) > 3:
1334:             report += f"   - Review {len(violations) - 3} other files listed in Part 1\n"
1335:     else:
1336:         report += "   - No violations to fix\n"
1337: 
1338:     report += """
1339: 2. **Missing Examples** (documentation needed):
1340: """
1341:     if missing_examples:
1342:         for missing in missing_examples[:3]:  # Show first 3
1343:             report += f"   - Create {missing['suggested_path']} (from {missing['prp_id']})\n"
1344:         if len(missing_examples) > 3:
1345:             report += f"   - Create {len(missing_examples) - 3} other examples listed in Part 2\n"
1346:     else:
1347:         report += "   - No missing examples\n"
1348: 
1349:     report += """
1350: 3. **Prevention**:
1351:    - Add pre-commit hook: ce validate --level 4 (pattern conformance)
1352:    - Run /update-context weekly to detect drift early
1353:    - Update CLAUDE.md when new patterns emerge
1354: 
1355: ### Next Steps
1356: 1. Review violations in Part 1 and fix manually
1357: 2. Create missing examples from Part 2
1358: 3. **üîß CRITICAL - Validate Each Fix**:
1359:    - After fixing each violation, run: ce update-context
1360:    - Verify violation removed from drift report
1361:    - If still present: Analyze why fix didn't work, try different approach
1362: 4. Validate: ce validate --level 4
1363: 5. Update patterns if codebase evolution is intentional
1364: 6. Re-run /update-context to verify drift resolved
1365: 
1366: **Anti-Pattern**: Batch-apply all fixes without validation (violations may persist)
1367: **Correct Pattern**: Fix ‚Üí Validate ‚Üí Next fix (iterative verification)
1368: """
1369: 
1370:     return report
1371: 
1372: 
1373: def get_cache_ttl(cli_ttl: Optional[int] = None) -> int:
1374:     """Get cache TTL from CLI arg, config, or default.
1375: 
1376:     Priority:
1377:         1. CLI flag (--cache-ttl)
1378:         2. .ce/config.yml value
1379:         3. Hardcoded default (5 minutes)
1380: 
1381:     Args:
1382:         cli_ttl: TTL from command-line --cache-ttl flag
1383: 
1384:     Returns:
1385:         TTL in minutes
1386: 
1387:     Example:
1388:         >>> ttl = get_cache_ttl()
1389:         >>> assert ttl >= 1
1390:         >>> ttl = get_cache_ttl(cli_ttl=10)
1391:         >>> assert ttl == 10
1392:     """
1393:     # Priority 1: CLI flag
1394:     if cli_ttl is not None:
1395:         return cli_ttl
1396: 
1397:     # Priority 2: Config file
1398:     current_dir = Path.cwd()
1399:     if current_dir.name == "tools":
1400:         project_root = current_dir.parent
1401:     else:
1402:         project_root = current_dir
1403: 
1404:     config_path = project_root / ".ce" / "config.yml"
1405:     if config_path.exists():
1406:         try:
1407:             import yaml
1408:             config = yaml.safe_load(config_path.read_text())
1409:             ttl = config.get("cache", {}).get("analysis_ttl_minutes")
1410:             if ttl is not None:
1411:                 return int(ttl)
1412:         except Exception:
1413:             pass  # Fall back to default
1414: 
1415:     # Priority 3: Default
1416:     return 5
1417: 
1418: 
1419: def get_cached_analysis() -> Optional[Dict[str, Any]]:
1420:     """Read cached drift analysis from report file.
1421: 
1422:     Parses .ce/drift-report.md to extract cached analysis results.
1423: 
1424:     Returns:
1425:         Cached analysis dict or None if not found
1426: 
1427:     Example:
1428:         >>> cached = get_cached_analysis()
1429:         >>> if cached:
1430:         ...     assert "drift_score" in cached
1431:         ...     assert "generated_at" in cached
1432:     """
1433:     current_dir = Path.cwd()
1434:     if current_dir.name == "tools":
1435:         project_root = current_dir.parent
1436:     else:
1437:         project_root = current_dir
1438: 
1439:     report_path = project_root / ".ce" / "drift-report.md"
1440:     if not report_path.exists():
1441:         return None
1442: 
1443:     try:
1444:         content = report_path.read_text()
1445: 
1446:         # Extract timestamp from report
1447:         # Format: **Generated**: 2025-10-16T20:03:32.185604+00:00
1448:         timestamp_match = re.search(
1449:             r'\*\*Generated\*\*: (.+?)$',
1450:             content,
1451:             re.MULTILINE
1452:         )
1453:         if not timestamp_match:
1454:             return None
1455: 
1456:         generated_at = timestamp_match.group(1).strip()
1457: 
1458:         # Extract drift score
1459:         score_match = re.search(r'\*\*Drift Score\*\*: ([\d.]+)%', content)
1460:         if not score_match:
1461:             return None
1462: 
1463:         drift_score = float(score_match.group(1))
1464: 
1465:         # Extract violation count
1466:         violations_match = re.search(r'\*\*Violations Found\*\*: (\d+)', content)
1467:         violation_count = int(violations_match.group(1)) if violations_match else 0
1468: 
1469:         # Classify drift level
1470:         if drift_score < 5:
1471:             drift_level = "ok"
1472:         elif drift_score < 15:
1473:             drift_level = "warning"
1474:         else:
1475:             drift_level = "critical"
1476: 
1477:         return {
1478:             "drift_score": drift_score,
1479:             "drift_level": drift_level,
1480:             "violation_count": violation_count,
1481:             "report_path": str(report_path),
1482:             "generated_at": generated_at,
1483:             "cached": True
1484:         }
1485: 
1486:     except Exception as e:
1487:         logger.debug(f"Failed to read cache: {e}")
1488:         return None
1489: 
1490: 
1491: def get_cache_ttl(override_ttl: int = None) -> int:
1492:     """Get cache TTL from config or environment, with validation.
1493: 
1494:     Args:
1495:         override_ttl: Optional override from CLI --cache-ttl flag
1496: 
1497:     Returns:
1498:         Cache TTL in minutes (minimum 1, default 5)
1499: 
1500:     Sources (in priority order):
1501:         1. override_ttl parameter (CLI --cache-ttl)
1502:         2. CONTEXT_CACHE_TTL environment variable
1503:         3. .ce/config.yml cache.analysis_ttl_minutes
1504:         4. Default: 5 minutes
1505: 
1506:     üîß Troubleshooting:
1507:         - Set env: export CONTEXT_CACHE_TTL=10
1508:         - Or configure: echo "cache: {analysis_ttl_minutes: 10}" >> .ce/config.yml
1509:     """
1510:     import os
1511: 
1512:     # Check CLI override first
1513:     if override_ttl is not None:
1514:         try:
1515:             ttl = max(1, int(override_ttl))  # Minimum 1 minute
1516:             logger.debug(f"Cache TTL from CLI: {ttl} minutes")
1517:             return ttl
1518:         except (ValueError, TypeError):
1519:             logger.warning(f"Invalid cache TTL override: {override_ttl}, ignoring")
1520: 
1521:     # Check environment variable
1522:     env_ttl = os.getenv("CONTEXT_CACHE_TTL")
1523:     if env_ttl:
1524:         try:
1525:             ttl = max(1, int(env_ttl))  # Minimum 1 minute
1526:             logger.debug(f"Cache TTL from env: {ttl} minutes")
1527:             return ttl
1528:         except ValueError:
1529:             logger.warning(f"Invalid CONTEXT_CACHE_TTL: {env_ttl}, using default")
1530: 
1531:     # Check .ce/config.yml
1532:     try:
1533:         current_dir = Path.cwd()
1534:         if current_dir.name == "tools":
1535:             project_root = current_dir.parent
1536:         else:
1537:             project_root = current_dir
1538: 
1539:         config_path = project_root / ".ce" / "config.yml"
1540:         if config_path.exists():
1541:             config = yaml.safe_load(config_path.read_text()) or {}
1542:             cache_config = config.get("cache", {})
1543:             ttl = cache_config.get("analysis_ttl_minutes")
1544:             if ttl:
1545:                 ttl = max(1, int(ttl))  # Minimum 1 minute
1546:                 logger.debug(f"Cache TTL from config: {ttl} minutes")
1547:                 return ttl
1548:     except Exception as e:
1549:         logger.debug(f"Failed to read cache config: {e}")
1550: 
1551:     # Default
1552:     logger.debug("Using default cache TTL: 5 minutes")
1553:     return 5
1554: 
1555: 
1556: def is_cache_valid(cached: Dict[str, Any], ttl_minutes: int = 0) -> bool:
1557:     """Check if cached analysis is still valid.
1558: 
1559:     Args:
1560:         cached: Cached analysis dict with 'generated_at' field
1561:         ttl_minutes: Cache time-to-live in minutes. If 0, uses get_cache_ttl()
1562: 
1563:     Returns:
1564:         True if cache is fresh (< TTL), False otherwise
1565: 
1566:     Example:
1567:         >>> cached = {"generated_at": "2025-10-17T10:00:00+00:00"}
1568:         >>> is_valid = is_cache_valid(cached, ttl_minutes=5)
1569:         >>> assert isinstance(is_valid, bool)
1570:     """
1571:     # Use configured TTL if not specified
1572:     if ttl_minutes == 0:
1573:         ttl_minutes = get_cache_ttl()
1574: 
1575:     try:
1576:         # Parse timestamp (handle multiple formats)
1577:         generated_str = cached["generated_at"]
1578: 
1579:         # Replace timezone suffix for consistent parsing
1580:         generated_str = generated_str.replace("+00:00", "+00:00")
1581: 
1582:         generated_at = datetime.fromisoformat(generated_str)
1583: 
1584:         # Ensure timezone aware
1585:         if generated_at.tzinfo is None:
1586:             generated_at = generated_at.replace(tzinfo=timezone.utc)
1587: 
1588:         now = datetime.now(timezone.utc)
1589:         age_minutes = (now - generated_at).total_seconds() / 60
1590: 
1591:         is_valid = age_minutes < ttl_minutes
1592:         if not is_valid:
1593:             logger.debug(f"Cache expired: {age_minutes:.1f}m old, TTL: {ttl_minutes}m")
1594:         return is_valid
1595: 
1596:     except Exception as e:
1597:         logger.debug(f"Cache validation failed: {e}")
1598:         return False
1599: 
1600: 
1601: def analyze_context_drift() -> Dict[str, Any]:
1602:     """Run drift analysis and generate report.
1603: 
1604:     Fast drift detection without metadata updates - optimized for CI/CD.
1605: 
1606:     Returns:
1607:         {
1608:             "drift_score": 17.9,
1609:             "drift_level": "critical",  # ok, warning, critical
1610:             "violations": ["..."],
1611:             "violation_count": 5,
1612:             "missing_examples": [...],
1613:             "report_path": ".ce/drift-report.md",
1614:             "generated_at": "2025-10-16T20:15:00Z",
1615:             "duration_seconds": 2.3
1616:         }
1617: 
1618:     Raises:
1619:         RuntimeError: If analysis fails with troubleshooting guidance
1620: 
1621:     Example:
1622:         >>> result = analyze_context_drift()
1623:         >>> assert result["drift_level"] in ["ok", "warning", "critical"]
1624:         >>> assert 0 <= result["drift_score"] <= 100
1625:     """
1626:     import time
1627:     start_time = time.time()
1628: 
1629:     try:
1630:         # Run drift detection (existing functions)
1631:         drift_result = verify_codebase_matches_examples()
1632:         missing_examples = detect_missing_examples_for_prps()
1633: 
1634:         # Generate report
1635:         report = generate_drift_report(
1636:             drift_result["violations"],
1637:             drift_result["drift_score"],
1638:             missing_examples
1639:         )
1640: 
1641:         # Save report
1642:         current_dir = Path.cwd()
1643:         if current_dir.name == "tools":
1644:             project_root = current_dir.parent
1645:         else:
1646:             project_root = current_dir
1647: 
1648:         ce_dir = project_root / ".ce"
1649:         ce_dir.mkdir(exist_ok=True)
1650:         report_path = ce_dir / "drift-report.md"
1651:         atomic_write(report_path, report)
1652: 
1653:         # Calculate duration
1654:         duration = time.time() - start_time
1655: 
1656:         # Classify drift level
1657:         drift_score = drift_result["drift_score"]
1658:         if drift_score < 5:
1659:             drift_level = "ok"
1660:         elif drift_score < 15:
1661:             drift_level = "warning"
1662:         else:
1663:             drift_level = "critical"
1664: 
1665:         return {
1666:             "drift_score": drift_score,
1667:             "drift_level": drift_level,
1668:             "violations": drift_result["violations"],
1669:             "violation_count": len(drift_result["violations"]),
1670:             "missing_examples": missing_examples,
1671:             "report_path": str(report_path),
1672:             "generated_at": datetime.now(timezone.utc).isoformat(),
1673:             "duration_seconds": round(duration, 1)
1674:         }
1675: 
1676:     except Exception as e:
1677:         raise RuntimeError(
1678:             f"Drift analysis failed: {e}\n"
1679:             f"üîß Troubleshooting:\n"
1680:             f"   - Ensure examples/ directory exists\n"
1681:             f"   - Check PRPs have valid YAML headers\n"
1682:             f"   - Verify tools/ce/ directory is accessible\n"
1683:             f"   - Run: cd tools && uv run ce validate --level 1"
1684:         ) from e
1685: 
1686: 
1687: def sync_context(target_prp: Optional[str] = None) -> Dict[str, Any]:
1688:     """Execute context sync workflow.
1689: 
1690:     Args:
1691:         target_prp: Optional PRP file path for targeted sync
1692: 
1693:     Returns:
1694:         {
1695:             "success": True,
1696:             "prps_scanned": 15,
1697:             "prps_updated": 8,
1698:             "prps_moved": 2,
1699:             "ce_updated_count": 8,
1700:             "serena_updated_count": 5,
1701:             "errors": []
1702:         }
1703:     """
1704:     logger.info("Starting context sync...")
1705: 
1706:     # Initialize counters
1707:     prps_scanned = 0
1708:     prps_updated = 0
1709:     prps_moved = 0
1710:     ce_updated_count = 0
1711:     serena_updated_count = 0
1712:     errors = []
1713: 
1714:     # Discover PRPs
1715:     try:
1716:         prp_files = discover_prps(target_prp)
1717:     except Exception as e:
1718:         logger.error(f"Failed to discover PRPs: {e}")
1719:         return {
1720:             "success": False,
1721:             "prps_scanned": 0,
1722:             "prps_updated": 0,
1723:             "prps_moved": 0,
1724:             "ce_updated_count": 0,
1725:             "serena_updated_count": 0,
1726:             "errors": [str(e)]
1727:         }
1728: 
1729:     # Process each PRP
1730:     for prp_path in prp_files:
1731:         prps_scanned += 1
1732: 
1733:         try:
1734:             # Read PRP
1735:             metadata, content = read_prp_header(prp_path)
1736: 
1737:             # Extract expected functions
1738:             expected_functions = extract_expected_functions(content)
1739: 
1740:             # Verify functions actually exist in codebase using AST
1741:             current_dir = Path.cwd()
1742:             if current_dir.name == "tools":
1743:                 project_root = current_dir.parent
1744:             else:
1745:                 project_root = current_dir
1746:             tools_ce_dir = project_root / "tools" / "ce"
1747: 
1748:             ce_verified = False
1749:             if expected_functions and tools_ce_dir.exists():
1750:                 # Check if ALL expected functions exist
1751:                 all_found = all(
1752:                     verify_function_exists_ast(func, tools_ce_dir)
1753:                     for func in expected_functions
1754:                 )
1755:                 ce_verified = all_found
1756: 
1757:             # Serena verification disabled (subprocess cannot access parent's stdio MCP)
1758:             serena_verified = False
1759: 
1760:             # Update context_sync flags
1761:             update_context_sync_flags(prp_path, ce_verified, serena_verified)
1762:             prps_updated += 1
1763: 
1764:             if ce_verified:
1765:                 ce_updated_count += 1
1766:             if serena_verified:
1767:                 serena_updated_count += 1
1768: 
1769:             # Check status transition
1770:             if should_transition_to_executed(prp_path):
1771:                 new_path = move_prp_to_executed(prp_path)
1772:                 prps_moved += 1
1773:                 prp_path = new_path  # Update path for drift detection
1774: 
1775:         except Exception as e:
1776:             error_msg = f"Error processing {prp_path.name}: {e}"
1777:             logger.error(error_msg)
1778:             errors.append(error_msg)
1779:             continue
1780: 
1781:     # Drift detection (universal sync only) with caching
1782:     if not target_prp:
1783:         logger.info("Running drift detection...")
1784: 
1785:         # Check cache with configured TTL (reads from env/config/default)
1786:         cached = get_cached_analysis()
1787:         if cached and is_cache_valid(cached):  # Uses get_cache_ttl() internally
1788:             logger.info(f"Using cached drift analysis ({cached['drift_score']:.1f}%)")
1789:             drift_score = cached["drift_score"]
1790:             report_path = Path(cached["report_path"])
1791:         else:
1792:             # Run fresh analysis
1793:             logger.info("Running fresh drift analysis (cache expired or not found)")
1794:             analysis_result = analyze_context_drift()
1795:             drift_score = analysis_result["drift_score"]
1796:             report_path = Path(analysis_result["report_path"])
1797: 
1798:         # Display warning if drift detected
1799:         if drift_score >= 5:
1800:             logger.warning(
1801:                 f"Examples drift detected: {drift_score:.1f}%\n"
1802:                 f"üìä Report saved: {report_path}\n"
1803:                 f"üîß Review and apply fixes: cat {report_path}"
1804:             )
1805: 
1806:     logger.info("Context sync completed")
1807: 
1808:     return {
1809:         "success": len(errors) == 0,
1810:         "prps_scanned": prps_scanned,
1811:         "prps_updated": prps_updated,
1812:         "prps_moved": prps_moved,
1813:         "ce_updated_count": ce_updated_count,
1814:         "serena_updated_count": serena_updated_count,
1815:         "errors": errors
1816:     }
</file>

<file path="tools/ce/validate_permissions.py">
  1: """Permission validation utility - replaces jq/grep for settings checks."""
  2: import json
  3: from pathlib import Path
  4: from typing import Dict, List
  5: 
  6: 
  7: def load_settings() -> Dict:
  8:     """Load .claude/settings.local.json
  9: 
 10:     Returns:
 11:         Dict with permissions configuration
 12: 
 13:     Raises:
 14:         FileNotFoundError: If settings file doesn't exist
 15:         json.JSONDecodeError: If settings file is malformed
 16:     """
 17:     settings_path = Path(__file__).parent.parent.parent / ".claude/settings.local.json"
 18: 
 19:     if not settings_path.exists():
 20:         raise FileNotFoundError(
 21:             f"Settings file not found: {settings_path}\n"
 22:             "üîß Troubleshooting: Ensure .claude/settings.local.json exists"
 23:         )
 24: 
 25:     return json.loads(settings_path.read_text())
 26: 
 27: 
 28: def count_permissions() -> Dict[str, int]:
 29:     """Count allow/deny tools.
 30: 
 31:     Returns:
 32:         Dict with 'allow' and 'deny' counts
 33:     """
 34:     settings = load_settings()
 35:     return {
 36:         "allow": len(settings["permissions"]["allow"]),
 37:         "deny": len(settings["permissions"]["deny"])
 38:     }
 39: 
 40: 
 41: def search_tool(pattern: str, permission_type: str = "allow") -> List[str]:
 42:     """Search for tools matching pattern in allow/deny list.
 43: 
 44:     Args:
 45:         pattern: String pattern to search for (case-sensitive)
 46:         permission_type: Either "allow" or "deny"
 47: 
 48:     Returns:
 49:         List of matching tool names
 50:     """
 51:     settings = load_settings()
 52:     tools = settings["permissions"][permission_type]
 53:     return [t for t in tools if pattern in t]
 54: 
 55: 
 56: def verify_tool_exists(tool_name: str) -> Dict[str, bool]:
 57:     """Check if tool exists in allow or deny list.
 58: 
 59:     Args:
 60:         tool_name: Exact tool name to search for
 61: 
 62:     Returns:
 63:         Dict with 'in_allow' and 'in_deny' boolean flags
 64:     """
 65:     settings = load_settings()
 66:     return {
 67:         "in_allow": tool_name in settings["permissions"]["allow"],
 68:         "in_deny": tool_name in settings["permissions"]["deny"]
 69:     }
 70: 
 71: 
 72: def categorize_tools() -> Dict[str, List[str]]:
 73:     """Group allowed tools by category.
 74: 
 75:     Returns:
 76:         Dict mapping category names to lists of tool names
 77:     """
 78:     settings = load_settings()
 79:     allowed = settings["permissions"]["allow"]
 80: 
 81:     categories = {
 82:         "bash": [t for t in allowed if t.startswith("Bash(")],
 83:         "serena": [t for t in allowed if t.startswith("mcp__serena__")],
 84:         "filesystem": [t for t in allowed if t.startswith("mcp__filesystem__")],
 85:         "git": [t for t in allowed if t.startswith("mcp__git__")],
 86:         "context7": [t for t in allowed if t.startswith("mcp__context7__")],
 87:         "sequential": [t for t in allowed if t.startswith("mcp__sequential-thinking__")],
 88:         "linear": [t for t in allowed if t.startswith("mcp__linear-server__")],
 89:         "repomix": [t for t in allowed if t.startswith("mcp__repomix__")],
 90:         "special": [t for t in allowed if t.startswith(("Read(", "WebFetch(", "SlashCommand("))]
 91:     }
 92: 
 93:     return categories
 94: 
 95: 
 96: if __name__ == "__main__":
 97:     import sys
 98: 
 99:     if len(sys.argv) < 2:
100:         print("Usage: python validate_permissions.py [count|search|verify|categorize]")
101:         print("\nCommands:")
102:         print("  count                    - Show allow/deny counts")
103:         print("  search <pattern> [type]  - Search for pattern in allow/deny list")
104:         print("  verify <tool_name>       - Check if tool is in allow/deny")
105:         print("  categorize               - Show tools grouped by category")
106:         sys.exit(1)
107: 
108:     action = sys.argv[1]
109: 
110:     try:
111:         if action == "count":
112:             counts = count_permissions()
113:             print(f"Allow: {counts['allow']}")
114:             print(f"Deny: {counts['deny']}")
115: 
116:         elif action == "search" and len(sys.argv) >= 3:
117:             pattern = sys.argv[2]
118:             perm_type = sys.argv[3] if len(sys.argv) >= 4 else "allow"
119:             matches = search_tool(pattern, perm_type)
120:             if matches:
121:                 for match in matches:
122:                     print(match)
123:             else:
124:                 print(f"No matches found for pattern '{pattern}' in {perm_type} list")
125: 
126:         elif action == "verify" and len(sys.argv) >= 3:
127:             tool = sys.argv[2]
128:             result = verify_tool_exists(tool)
129:             print(f"In allow: {result['in_allow']}")
130:             print(f"In deny: {result['in_deny']}")
131: 
132:         elif action == "categorize":
133:             cats = categorize_tools()
134:             total = sum(len(tools) for tools in cats.values())
135:             print(f"Total allowed tools: {total}\n")
136:             for cat, tools in cats.items():
137:                 print(f"{cat.upper()} ({len(tools)}):")
138:                 for tool in sorted(tools):
139:                     print(f"  - {tool}")
140:                 print()
141: 
142:         else:
143:             print(f"Unknown action: {action}")
144:             print("Use: count, search, verify, or categorize")
145:             sys.exit(1)
146: 
147:     except Exception as e:
148:         print(f"‚ùå Error: {e}")
149:         sys.exit(1)
</file>

<file path="tools/ce/validate.py">
  1: """Validation gates: 4-level validation system."""
  2: 
  3: import sys
  4: import re
  5: import time
  6: from typing import Dict, Any, List, Optional
  7: from pathlib import Path
  8: from datetime import datetime, timezone
  9: 
 10: from .core import run_cmd
 11: from .pattern_extractor import extract_patterns_from_prp
 12: from .drift_analyzer import analyze_implementation, calculate_drift_score, get_auto_fix_suggestions
 13: from .mermaid_validator import lint_all_markdown_mermaid
 14: 
 15: 
 16: def validate_level_1() -> Dict[str, Any]:
 17:     """Run Level 1 validation: Syntax & Style (lint + type-check + markdown-lint).
 18: 
 19:     Returns:
 20:         Dict with: success (bool), errors (List[str]), duration (float)
 21: 
 22:     Raises:
 23:         RuntimeError: If validation commands fail to execute
 24: 
 25:     Note: Real validation - no mocked results.
 26:     """
 27:     errors = []
 28:     total_duration = 0.0
 29: 
 30:     # Run lint (optional - skip if not configured)
 31:     lint_result = run_cmd("npm run lint", capture_output=True)
 32:     total_duration += lint_result["duration"]
 33: 
 34:     if not lint_result["success"] and "Missing script" not in lint_result["stderr"]:
 35:         errors.append(f"Lint failed:\n{lint_result['stderr']}")
 36: 
 37:     # Run type-check (optional - skip if not configured)
 38:     typecheck_result = run_cmd("npm run type-check", capture_output=True)
 39:     total_duration += typecheck_result["duration"]
 40: 
 41:     if not typecheck_result["success"] and "Missing script" not in typecheck_result["stderr"]:
 42:         errors.append(f"Type-check failed:\n{typecheck_result['stderr']}")
 43: 
 44:     # Run markdown-lint (accept minor errors in old research files)
 45:     markdownlint_result = run_cmd("npm run lint:md", capture_output=True)
 46:     total_duration += markdownlint_result["duration"]
 47: 
 48:     # Check if errors are only in old research files (acceptable)
 49:     if not markdownlint_result["success"]:
 50:         stderr = markdownlint_result["stderr"]
 51:         # Count errors and check if they're only in old research files
 52:         error_lines = [line for line in stderr.split("\n") if "99-context-mastery-exploration-original.md" in line or "MD046" in line]
 53:         critical_errors = [line for line in stderr.split("\n") if line.startswith("docs/") or line.startswith("PRPs/") or line.startswith("examples/")]
 54:         critical_errors = [e for e in critical_errors if "99-context-mastery-exploration-original.md" not in e]
 55: 
 56:         if critical_errors:
 57:             errors.append(f"Markdown lint failed:\n{markdownlint_result['stderr']}")
 58: 
 59:     # Run mermaid validation
 60:     mermaid_start = time.time()
 61:     mermaid_result = lint_all_markdown_mermaid(".", auto_fix=True)
 62:     mermaid_duration = time.time() - mermaid_start
 63:     total_duration += mermaid_duration
 64: 
 65:     if not mermaid_result["success"]:
 66:         errors.append(f"Mermaid validation failed: {len(mermaid_result['errors'])} issues")
 67:         for error in mermaid_result['errors'][:5]:  # Show first 5
 68:             errors.append(f"  - {error}")
 69:     elif mermaid_result["fixes_applied"]:
 70:         print(f"‚úÖ Mermaid auto-fixes applied: {len(mermaid_result['fixes_applied'])} fixes", file=sys.stderr)
 71: 
 72:     return {
 73:         "success": len(errors) == 0,
 74:         "errors": errors,
 75:         "duration": total_duration,
 76:         "level": 1
 77:     }
 78: 
 79: 
 80: def validate_level_2() -> Dict[str, Any]:
 81:     """Run Level 2 validation: Unit Tests.
 82: 
 83:     Returns:
 84:         Dict with: success (bool), errors (List[str]), duration (float)
 85: 
 86:     Raises:
 87:         RuntimeError: If test command fails to execute
 88: 
 89:     Note: Real test execution - no mocked test pass.
 90:     """
 91:     result = run_cmd("npm test", capture_output=True)
 92: 
 93:     errors = []
 94:     if not result["success"]:
 95:         errors.append(f"Unit tests failed:\n{result['stderr']}")
 96: 
 97:     return {
 98:         "success": result["success"],
 99:         "errors": errors,
100:         "duration": result["duration"],
101:         "level": 2
102:     }
103: 
104: 
105: def validate_level_3() -> Dict[str, Any]:
106:     """Run Level 3 validation: Integration Tests.
107: 
108:     Returns:
109:         Dict with: success (bool), errors (List[str]), duration (float)
110: 
111:     Raises:
112:         RuntimeError: If integration test command fails to execute
113: 
114:     Note: Real integration test execution.
115:     """
116:     result = run_cmd("npm run test:integration", capture_output=True)
117: 
118:     errors = []
119:     if not result["success"]:
120:         errors.append(f"Integration tests failed:\n{result['stderr']}")
121: 
122:     return {
123:         "success": result["success"],
124:         "errors": errors,
125:         "duration": result["duration"],
126:         "level": 3
127:     }
128: 
129: 
130: def validate_level_4(
131:     prp_path: str,
132:     implementation_paths: Optional[List[str]] = None
133: ) -> Dict[str, Any]:
134:     """Run Level 4 validation: Pattern Conformance.
135: 
136:     Args:
137:         prp_path: Path to PRP markdown file
138:         implementation_paths: Files to analyze; auto-detected if None via:
139:             1. Parse PRP IMPLEMENTATION BLUEPRINT for file references
140:                (searches for patterns: "Modify: path/file.py", "Create: path/file.py")
141:             2. Fallback: git diff --name-only main...HEAD
142:             3. Fallback: Interactive prompt for user to specify files
143: 
144:     Returns:
145:         {
146:             "success": bool,
147:             "drift_score": float,
148:             "threshold_action": str,  # auto_accept | auto_fix | escalate
149:             "decision": Optional[str],  # if escalated: accepted | rejected | examples_updated
150:             "justification": Optional[str],
151:             "duration": float,
152:             "level": 4
153:         }
154: 
155:     Raises:
156:         RuntimeError: If PRP parsing fails or Serena MCP unavailable
157: 
158:     Process:
159:         1. Extract patterns from PRP EXAMPLES
160:         2. Analyze implementation with Serena MCP
161:         3. Calculate drift score
162:         4. Apply threshold logic (auto-accept/fix/escalate)
163:         5. If escalated: prompt user, persist decision
164:         6. Return validation result
165:     """
166:     start_time = time.time()
167: 
168:     try:
169:         # Step 1: Auto-detect implementation paths if not provided
170:         if implementation_paths is None:
171:             implementation_paths = _auto_detect_implementation_paths(prp_path)
172: 
173:         # Step 2: Extract expected patterns from PRP
174:         expected_patterns = extract_patterns_from_prp(prp_path)
175: 
176:         # Step 3: Analyze implementation
177:         analysis_result = analyze_implementation(prp_path, implementation_paths)
178:         detected_patterns = analysis_result["detected_patterns"]
179: 
180:         # Step 4: Calculate drift score
181:         drift_result = calculate_drift_score(expected_patterns, detected_patterns)
182:         drift_score = drift_result["drift_score"]
183:         threshold_action = drift_result["threshold_action"]
184: 
185:         # Step 5: Handle based on threshold
186:         decision = None
187:         justification = None
188: 
189:         if threshold_action == "auto_accept":
190:             # Drift < 10%: Auto-accept
191:             success = True
192:         elif threshold_action == "auto_fix":
193:             # Drift 10-30%: Display suggestions (MVP: no auto-apply)
194:             success = True
195:             suggestions = get_auto_fix_suggestions(drift_result["mismatches"])
196:             print("\n‚ö†Ô∏è  MODERATE DRIFT DETECTED - SUGGESTIONS:")
197:             for suggestion in suggestions:
198:                 print(f"   {suggestion}")
199:             print()
200:         else:
201:             # Drift >= 30%: Escalate to user
202:             success, decision, justification = _handle_user_escalation(
203:                 prp_path,
204:                 drift_result,
205:                 implementation_paths
206:             )
207: 
208:             # Persist decision to PRP if user decided
209:             if decision:
210:                 _persist_drift_decision(prp_path, drift_result, decision, justification)
211: 
212:         duration = time.time() - start_time
213: 
214:         return {
215:             "success": success,
216:             "drift_score": drift_score,
217:             "threshold_action": threshold_action,
218:             "decision": decision,
219:             "justification": justification,
220:             "duration": round(duration, 2),
221:             "level": 4,
222:             "files_analyzed": analysis_result["files_analyzed"],
223:             "category_scores": drift_result["category_scores"]
224:         }
225: 
226:     except Exception as e:
227:         duration = time.time() - start_time
228:         return {
229:             "success": False,
230:             "drift_score": 100.0,
231:             "threshold_action": "escalate",
232:             "decision": None,
233:             "justification": None,
234:             "duration": round(duration, 2),
235:             "level": 4,
236:             "error": str(e)
237:         }
238: 
239: 
240: def _auto_detect_implementation_paths(prp_path: str) -> List[str]:
241:     """Auto-detect implementation file paths from PRP or git."""
242:     # Strategy 1: Parse PRP IMPLEMENTATION BLUEPRINT
243:     paths = _parse_prp_blueprint_paths(prp_path)
244:     if paths:
245:         return paths
246: 
247:     # Strategy 2: Git diff (changed files)
248:     result = run_cmd("git diff --name-only main...HEAD", capture_output=True)
249:     if result["success"] and result["stdout"].strip():
250:         paths = [p.strip() for p in result["stdout"].strip().split("\n")]
251:         # Filter for code files only
252:         code_extensions = {".py", ".ts", ".tsx", ".js", ".jsx", ".go", ".rs", ".java"}
253:         paths = [p for p in paths if Path(p).suffix in code_extensions]
254:         if paths:
255:             return paths
256: 
257:     # Strategy 3: Interactive prompt
258:     print("\nüîç Unable to auto-detect implementation files.")
259:     print("Please specify file paths to analyze (comma-separated):")
260:     user_input = input("> ").strip()
261:     if user_input:
262:         return [p.strip() for p in user_input.split(",")]
263: 
264:     raise RuntimeError(
265:         "No implementation files specified\n"
266:         "üîß Troubleshooting: Specify --files flag or add file references to PRP"
267:     )
268: 
269: 
270: def _parse_prp_blueprint_paths(prp_path: str) -> List[str]:
271:     """Parse implementation file paths from PRP IMPLEMENTATION BLUEPRINT section."""
272:     content = Path(prp_path).read_text()
273: 
274:     # Find IMPLEMENTATION BLUEPRINT section
275:     blueprint_match = re.search(
276:         r"##\s+.*?IMPLEMENTATION\s+BLUEPRINT.*?\n(.*?)(?=\n##|\Z)",
277:         content,
278:         re.DOTALL | re.IGNORECASE
279:     )
280: 
281:     if not blueprint_match:
282:         return []
283: 
284:     blueprint_text = blueprint_match.group(1)
285: 
286:     # Extract file paths from patterns like "Modify: path/file.py", "Create: path/file.py"
287:     file_patterns = re.findall(
288:         r"(?:Modify|Create|Update|Add):\s*([`]?)([a-zA-Z0-9_/\.\-]+\.(py|ts|tsx|js|jsx|go|rs|java))\1",
289:         blueprint_text,
290:         re.IGNORECASE
291:     )
292: 
293:     paths = [match[1] for match in file_patterns]
294:     return list(set(paths))  # Deduplicate
295: 
296: 
297: def _handle_user_escalation(
298:     prp_path: str,
299:     drift_result: Dict[str, Any],
300:     implementation_paths: List[str]
301: ) -> tuple[bool, Optional[str], Optional[str]]:
302:     """Interactive CLI for high-drift cases requiring human decision.
303: 
304:     Returns:
305:         (success, decision, justification) tuple
306:     """
307:     from .drift import get_drift_history, drift_summary
308:     import logging
309: 
310:     logger = logging.getLogger(__name__)
311:     drift_score = drift_result["drift_score"]
312:     category_scores = drift_result["category_scores"]
313:     mismatches = drift_result["mismatches"]
314: 
315:     print("\n" + "=" * 80)
316:     print(f"üö® HIGH DRIFT DETECTED: {drift_score:.1f}%")
317:     print("=" * 80)
318:     print(f"\nPRP: {prp_path}")
319:     print(f"Implementation: {', '.join(implementation_paths)}")
320: 
321:     # NEW: Show drift history for context
322:     try:
323:         history = get_drift_history(last_n=5)
324:         if history:
325:             print("\nüìä RECENT DRIFT HISTORY (for context):\n")
326:             print(f"{'PRP':<12} {'Score':<8} {'Action':<18} {'Date':<12}")
327:             print("‚îÄ" * 50)
328:             for h in history:
329:                 dd = h["drift_decision"]
330:                 prp_id = h["prp_id"]
331:                 score = dd["score"]
332:                 action = dd["action"]
333:                 timestamp = dd.get("timestamp", "N/A")[:10]
334:                 print(f"{prp_id:<12} {score:<8.2f} {action:<18} {timestamp:<12}")
335:             print()
336: 
337:             # Show summary stats
338:             summary = drift_summary()
339:             print(f"Historical Average: {summary['avg_drift_score']:.2f}%")
340:             print(f"Accepted: {summary['decisions'].get('accepted', 0)} | "
341:                   f"Rejected: {summary['decisions'].get('rejected', 0)}\n")
342:     except Exception as e:
343:         logger.warning(f"Could not load drift history: {e}")
344: 
345:     print("\nDRIFT BREAKDOWN:")
346:     print("‚îÅ" * 80)
347:     print(f"{'Category':<25} {'Expected':<20} {'Detected':<20} {'Drift':<10}")
348:     print("‚îÄ" * 80)
349: 
350:     for category, score in category_scores.items():
351:         print(f"{category:<25} {'(see PRP)':<20} {'(varies)':<20} {score:.1f}%")
352: 
353:     print("‚îÅ" * 80)
354: 
355:     # Show affected patterns
356:     if mismatches:
357:         print("\nAFFECTED PATTERNS:")
358:         for mismatch in mismatches[:5]:  # Show first 5
359:             expected = mismatch["expected"]
360:             detected = mismatch.get("detected", "None")
361:             category = mismatch["category"]
362:             print(f"‚Ä¢ {category}: Expected '{expected}', Detected {detected}")
363: 
364:     print("\nOPTIONS:")
365:     print("[A] Accept drift (add DRIFT_JUSTIFICATION to PRP)")
366:     print("[R] Reject and halt (requires manual refactoring)")
367:     print("[U] Update EXAMPLES in PRP (update specification)")
368:     print("[Q] Quit without saving")
369:     print()
370: 
371:     while True:
372:         choice = input("Your choice (A/R/U/Q): ").strip().upper()
373: 
374:         if choice == "A":
375:             justification = input("Justification for accepting drift: ").strip()
376:             if not justification:
377:                 print("‚ö†Ô∏è  Justification required. Try again.")
378:                 continue
379:             return (True, "accepted", justification)
380: 
381:         elif choice == "R":
382:             print("\n‚ùå L4 validation REJECTED - Manual refactoring required")
383:             return (False, "rejected", "User rejected high drift")
384: 
385:         elif choice == "U":
386:             print("\n‚ÑπÔ∏è  Update EXAMPLES section in PRP manually, then re-run validation")
387:             return (False, "examples_updated", "User chose to update PRP EXAMPLES")
388: 
389:         elif choice == "Q":
390:             print("\n‚ùå L4 validation aborted")
391:             return (False, None, None)
392: 
393:         else:
394:             print("‚ö†Ô∏è  Invalid choice. Please enter A, R, U, or Q.")
395: 
396: 
397: def _persist_drift_decision(
398:     prp_path: str,
399:     drift_result: Dict[str, Any],
400:     decision: str,
401:     justification: Optional[str]
402: ):
403:     """Persist DRIFT_JUSTIFICATION to PRP YAML header."""
404:     content = Path(prp_path).read_text()
405: 
406:     # Build drift decision YAML
407:     drift_yaml = f"""drift_decision:
408:   score: {drift_result['drift_score']}
409:   action: "{decision}"
410:   justification: "{justification or 'N/A'}"
411:   timestamp: "{datetime.now(timezone.utc).isoformat()}"
412:   category_breakdown:
413: """
414: 
415:     for category, score in drift_result["category_scores"].items():
416:         drift_yaml += f"    {category}: {score}\n"
417: 
418:     drift_yaml += f'  reviewer: "human"\n'
419: 
420:     # Insert into YAML header (after last YAML field before ---)
421:     yaml_end_match = re.search(r"(---\s*\n)", content)
422:     if yaml_end_match:
423:         # Insert before closing ---
424:         insert_pos = yaml_end_match.start()
425:         new_content = content[:insert_pos] + drift_yaml + content[insert_pos:]
426:         Path(prp_path).write_text(new_content)
427:         print(f"\n‚úÖ Drift decision persisted to {prp_path}")
428:     else:
429:         print(f"\n‚ö†Ô∏è  Warning: Could not find YAML header in {prp_path}")
430: 
431: 
432: def calculate_confidence(results: Dict[int, Dict[str, Any]]) -> int:
433:     """Calculate confidence score (1-10) based on validation results.
434: 
435:     Scoring breakdown:
436:     - Baseline: 6 (untested code)
437:     - Level 1 (Syntax & Style): +1
438:     - Level 2 (Unit Tests): +2 (with >80% coverage)
439:     - Level 3 (Integration): +1
440:     - Level 4 (Pattern Conformance): +1 (NEW)
441:     - Max: 10/10 (production-ready)
442: 
443:     Requirements for +1 from L4:
444:     - drift_score < 10% (auto-accept threshold)
445:     - OR drift_score < 30% AND decision = "accepted" with justification
446: 
447:     Args:
448:         results: Dict mapping level (1-4) to validation results
449: 
450:     Returns:
451:         Confidence score 1-10
452: 
453:     Examples:
454:         >>> results = {1: {"success": True}, 2: {"success": True, "coverage": 0.85}}
455:         >>> calculate_confidence(results)
456:         9  # Without L3, L4
457: 
458:         >>> results = {
459:         ...     1: {"success": True},
460:         ...     2: {"success": True, "coverage": 0.85},
461:         ...     3: {"success": True},
462:         ...     4: {"success": True, "drift_score": 8.5}
463:         ... }
464:         >>> calculate_confidence(results)
465:         10  # All gates pass
466:     """
467:     score = 6  # Baseline
468: 
469:     if results.get(1, {}).get("success"):
470:         score += 1
471: 
472:     if results.get(2, {}).get("success") and results.get(2, {}).get("coverage", 0) > 0.8:
473:         score += 2
474: 
475:     if results.get(3, {}).get("success"):
476:         score += 1
477: 
478:     # Level 4: Pattern conformance (NEW)
479:     l4_result = results.get(4, {})
480:     if l4_result.get("success"):
481:         drift_score = l4_result.get("drift_score", 100)
482:         decision = l4_result.get("decision")
483: 
484:         # Pass L4 if:
485:         # 1. drift < 10% (auto-accept)
486:         # 2. drift < 30% AND explicitly accepted with justification
487:         if drift_score < 10.0 or (drift_score < 30.0 and decision == "accepted"):
488:             score += 1
489: 
490:     return min(score, 10)
491: 
492: 
493: def validate_all() -> Dict[str, Any]:
494:     """Run all validation levels sequentially.
495: 
496:     Returns:
497:         Dict with: success (bool), results (Dict[int, Dict]),
498:                    total_duration (float), confidence_score (int)
499: 
500:     Note: Runs all levels even if early ones fail (for comprehensive report).
501:     """
502:     results = {}
503:     total_duration = 0.0
504: 
505:     # Level 1
506:     try:
507:         results[1] = validate_level_1()
508:         total_duration += results[1]["duration"]
509:     except Exception as e:
510:         results[1] = {
511:             "success": False,
512:             "errors": [f"Level 1 exception: {str(e)}"],
513:             "duration": 0.0,
514:             "level": 1
515:         }
516: 
517:     # Level 2
518:     try:
519:         results[2] = validate_level_2()
520:         total_duration += results[2]["duration"]
521:     except Exception as e:
522:         results[2] = {
523:             "success": False,
524:             "errors": [f"Level 2 exception: {str(e)}"],
525:             "duration": 0.0,
526:             "level": 2
527:         }
528: 
529:     # Level 3
530:     try:
531:         results[3] = validate_level_3()
532:         total_duration += results[3]["duration"]
533:     except Exception as e:
534:         results[3] = {
535:             "success": False,
536:             "errors": [f"Level 3 exception: {str(e)}"],
537:             "duration": 0.0,
538:             "level": 3
539:         }
540: 
541:     # Overall success: all levels must pass
542:     overall_success = all(r["success"] for r in results.values())
543: 
544:     # Calculate confidence score
545:     confidence_score = calculate_confidence(results)
546: 
547:     return {
548:         "success": overall_success,
549:         "results": results,
550:         "total_duration": total_duration,
551:         "confidence_score": confidence_score
552:     }
</file>

<file path="tools/ce/validation_loop.py">
  1: """Validation loop with self-healing capabilities.
  2: 
  3: Orchestrates L1-L4 validation levels with automatic error detection,
  4: parsing, and self-healing fixes. Includes escalation triggers for
  5: human intervention when automated fixes are insufficient.
  6: """
  7: 
  8: import re
  9: from typing import Dict, Any, List
 10: from pathlib import Path
 11: 
 12: from .exceptions import EscalationRequired
 13: 
 14: 
 15: def run_validation_loop(
 16:     phase: Dict[str, Any],
 17:     prp_path: str,
 18:     max_attempts: int = 3
 19: ) -> Dict[str, Any]:
 20:     """Run L1-L4 validation loop with self-healing.
 21: 
 22:     Args:
 23:         phase: Phase dict with validation_command
 24:         prp_path: Path to PRP file (for L4 validation)
 25:         max_attempts: Max self-healing attempts (default: 3)
 26: 
 27:     Returns:
 28:         {
 29:             "success": True,
 30:             "validation_levels": {
 31:                 "L1": {"passed": True, "attempts": 1, "errors": []},
 32:                 "L2": {"passed": True, "attempts": 2, "errors": ["..."]},
 33:                 "L3": {"passed": True, "attempts": 1, "errors": []},
 34:                 "L4": {"passed": True, "attempts": 1, "errors": []}
 35:             },
 36:             "self_healed": ["L2: Fixed import error"],
 37:             "escalated": [],
 38:             "attempts": 1
 39:         }
 40: 
 41:     Raises:
 42:         EscalationRequired: If validation fails after max_attempts or trigger hit
 43: 
 44:     Process:
 45:         1. Run L1 (Syntax): validate_level_1() with self-healing
 46:         2. Run L2 (Unit Tests): Custom validation from phase with self-healing
 47:         3. Run L3 (Integration): validate_level_3() with self-healing
 48:         4. Run L4 (Pattern Conformance): validate_level_4(prp_path)
 49: 
 50:         For each level:
 51:         - If pass: continue to next level
 52:         - If fail: enter self-healing loop (max 3 attempts)
 53:           1. Parse error
 54:           2. Check escalation triggers
 55:           3. Apply fix
 56:           4. Re-run validation
 57:         - If still failing after max_attempts: escalate to human
 58:     """
 59:     from .validate import validate_level_1, validate_level_3, validate_level_4
 60: 
 61:     print(f"  üß™ Running validation...")
 62: 
 63:     validation_levels = {}
 64:     self_healed = []
 65:     escalated = []
 66:     all_passed = True
 67: 
 68:     # L1: Syntax & Style (with self-healing)
 69:     print(f"    L1: Syntax & Style...")
 70:     l1_passed = False
 71:     l1_attempts = 0
 72:     l1_errors = []
 73:     error_history = []
 74: 
 75:     for attempt in range(1, max_attempts + 1):
 76:         l1_attempts = attempt
 77:         try:
 78:             l1_result = validate_level_1()
 79:             if not l1_result["success"]:
 80:                 # Validation failed - try self-healing
 81:                 l1_errors = l1_result.get("errors", [])
 82:                 print(f"    ‚ùå L1 failed (attempt {attempt}/{max_attempts}): {len(l1_errors)} errors")
 83:                 combined_error = "\n".join(l1_errors)
 84:                 _try_self_heal(combined_error, "L1", attempt, max_attempts, error_history)
 85:                 continue
 86: 
 87:             # Success path
 88:             l1_passed = True
 89:             print(f"    ‚úÖ L1 passed ({l1_result['duration']:.2f}s)")
 90:             if attempt > 1:
 91:                 self_healed.append(f"L1: Fixed after {attempt} attempts")
 92:             break
 93: 
 94:         except EscalationRequired:
 95:             raise  # Propagate escalation
 96:         except Exception as e:
 97:             l1_errors = [str(e)]
 98:             print(f"    ‚ùå L1 exception (attempt {attempt}): {str(e)}")
 99:             if attempt == max_attempts:
100:                 break
101: 
102:     validation_levels["L1"] = {
103:         "passed": l1_passed,
104:         "attempts": l1_attempts,
105:         "errors": l1_errors
106:     }
107:     if not l1_passed:
108:         all_passed = False
109:         print(f"    ‚ùå L1 failed after {l1_attempts} attempts - escalating")
110:         error = parse_validation_error("\n".join(l1_errors), "L1")
111:         escalate_to_human(error, "persistent_error")
112: 
113:     # L2: Unit Tests (with self-healing)
114:     l2_passed = False
115:     l2_attempts = 0
116:     l2_errors = []
117:     error_history_l2 = []
118: 
119:     if phase.get("validation_command"):
120:         print(f"    L2: Running {phase['validation_command']}...")
121:         from .core import run_cmd
122: 
123:         for attempt in range(1, max_attempts + 1):
124:             l2_attempts = attempt
125:             try:
126:                 l2_result = run_cmd(phase["validation_command"])
127:                 if not l2_result["success"]:
128:                     # Validation failed - try self-healing
129:                     l2_errors = [l2_result.get("stderr", "Test failed")]
130:                     print(f"    ‚ùå L2 failed (attempt {attempt}/{max_attempts})")
131:                     print(f"       {l2_result.get('stderr', 'Unknown error')[:200]}")
132:                     error_output = l2_result.get("stderr", "")
133:                     _try_self_heal(error_output, "L2", attempt, max_attempts, error_history_l2)
134:                     continue
135: 
136:                 # Success path
137:                 l2_passed = True
138:                 print(f"    ‚úÖ L2 passed ({l2_result['duration']:.2f}s)")
139:                 if attempt > 1:
140:                     self_healed.append(f"L2: Fixed after {attempt} attempts")
141:                 break
142: 
143:             except EscalationRequired:
144:                 raise
145:             except Exception as e:
146:                 l2_errors = [str(e)]
147:                 print(f"    ‚ùå L2 exception (attempt {attempt}): {str(e)}")
148:                 if attempt == max_attempts:
149:                     break
150: 
151:         validation_levels["L2"] = {
152:             "passed": l2_passed,
153:             "attempts": l2_attempts,
154:             "errors": l2_errors
155:         }
156:         if not l2_passed:
157:             all_passed = False
158:             print(f"    ‚ùå L2 failed after {l2_attempts} attempts - escalating")
159:             error = parse_validation_error("\n".join(l2_errors), "L2")
160:             escalate_to_human(error, "persistent_error")
161: 
162:     else:
163:         # No validation command - skip L2
164:         print(f"    ‚ö†Ô∏è  L2 skipped: No validation command specified")
165:         validation_levels["L2"] = {"passed": True, "attempts": 1, "errors": [], "skipped": True}
166: 
167:     # L3: Integration Tests (MVP: no self-healing for integration tests)
168:     try:
169:         print(f"    L3: Integration Tests...")
170:         l3_result = validate_level_3()
171:         validation_levels["L3"] = {
172:             "passed": l3_result["success"],
173:             "attempts": 1,
174:             "errors": l3_result.get("errors", [])
175:         }
176:         if l3_result["success"]:
177:             print(f"    ‚úÖ L3 passed ({l3_result['duration']:.2f}s)")
178:         else:
179:             print(f"    ‚ùå L3 failed - integration tests require manual review")
180:             all_passed = False
181:             # Integration test failures typically require architectural changes
182:             error = parse_validation_error(str(l3_result.get("errors", [])), "L3")
183:             escalate_to_human(error, "architectural")
184:     except EscalationRequired:
185:         raise
186:     except Exception as e:
187:         print(f"    ‚ö†Ô∏è  L3 skipped: {str(e)}")
188:         validation_levels["L3"] = {"passed": True, "attempts": 1, "errors": [], "skipped": True}
189: 
190:     # L4: Pattern Conformance
191:     try:
192:         print(f"    L4: Pattern Conformance...")
193:         l4_result = validate_level_4(prp_path)
194:         validation_levels["L4"] = {
195:             "passed": l4_result["success"],
196:             "attempts": 1,
197:             "errors": [],
198:             "drift_score": l4_result.get("drift_score", 0)
199:         }
200:         if l4_result["success"]:
201:             print(f"    ‚úÖ L4 passed (drift: {l4_result.get('drift_score', 0):.1f}%)")
202:         else:
203:             print(f"    ‚ùå L4 failed (drift: {l4_result.get('drift_score', 100):.1f}%)")
204:             all_passed = False
205:     except Exception as e:
206:         print(f"    ‚ö†Ô∏è  L4 skipped: {str(e)}")
207:         validation_levels["L4"] = {"passed": True, "attempts": 1, "errors": [], "skipped": True}
208: 
209:     print(f"  {'‚úÖ' if all_passed else '‚ùå'} Validation {'complete' if all_passed else 'failed'}")
210: 
211:     return {
212:         "success": all_passed,
213:         "validation_levels": validation_levels,
214:         "self_healed": self_healed,
215:         "escalated": escalated,
216:         "attempts": 1
217:     }
218: 
219: 
220: def _try_self_heal(
221:     error_output: str,
222:     level: str,
223:     attempt: int,
224:     max_attempts: int,
225:     error_history: List[str]
226: ) -> bool:
227:     """Try self-healing for validation error.
228: 
229:     Args:
230:         error_output: Raw error output to parse
231:         level: Validation level (L1, L2, etc.)
232:         attempt: Current attempt number
233:         max_attempts: Maximum attempts allowed
234:         error_history: List of previous error messages
235: 
236:     Returns:
237:         True if should continue trying, False if should stop
238: 
239:     Raises:
240:         EscalationRequired: If escalation triggered
241:     """
242:     if attempt >= max_attempts:
243:         return False
244: 
245:     error = parse_validation_error(error_output, level)
246:     error_history.append(error["message"])
247: 
248:     # Check escalation triggers
249:     if check_escalation_triggers(error, attempt, error_history):
250:         escalate_to_human(error, "persistent_error")
251: 
252:     # Apply self-healing
253:     print(f"      üîß Attempting self-heal...")
254:     fix_result = apply_self_healing_fix(error, attempt)
255:     if fix_result["success"]:
256:         print(f"      ‚úÖ Applied fix: {fix_result['description']}")
257:     else:
258:         print(f"      ‚ö†Ô∏è  Auto-fix failed: {fix_result['description']}")
259: 
260:     return True
261: 
262: 
263: def calculate_confidence_score(validation_results: Dict[str, Any]) -> str:
264:     """Calculate confidence score (1-10) based on validation results.
265: 
266:     Args:
267:         validation_results: Dict with L1-L4 results per phase
268: 
269:     Returns:
270:         "8/10" or "10/10"
271: 
272:     Scoring:
273:         - All L1-L4 passed on first attempt: 10/10
274:         - All passed, 1-2 self-heals: 9/10
275:         - All passed, 3+ self-heals: 8/10
276:         - L1-L3 passed, L4 skipped: 7/10
277:         - L1-L2 passed, L3-L4 skipped: 5/10
278:     """
279:     if not validation_results:
280:         return "6/10"  # No validation = baseline
281: 
282:     total_attempts = 0
283:     all_passed = True
284: 
285:     for _, phase_result in validation_results.items():
286:         if not phase_result.get("success"):
287:             all_passed = False
288: 
289:         # Count total attempts across all levels
290:         for _, level_result in phase_result.get("validation_levels", {}).items():
291:             total_attempts += level_result.get("attempts", 1) - 1  # -1 because first attempt doesn't count as retry
292: 
293:     if not all_passed:
294:         return "5/10"  # Validation failures
295: 
296:     # All passed - score by attempts
297:     if total_attempts == 0:
298:         return "10/10"  # Perfect
299:     elif total_attempts <= 2:
300:         return "9/10"  # Minor issues
301:     else:
302:         return "8/10"  # Multiple retries
303: 
304: 
305: def parse_validation_error(output: str, _level: str) -> Dict[str, Any]:
306:     """Parse validation error output into structured format.
307: 
308:     Args:
309:         output: Raw error output (stderr + stdout)
310:         _level: Validation level (L1, L2, L3, L4) - reserved for future use
311: 
312:     Returns:
313:         {
314:             "type": "assertion_error",  # assertion_error, import_error, syntax_error, etc.
315:             "file": "src/auth.py",
316:             "line": 42,
317:             "function": "authenticate",
318:             "message": "Expected User, got None",
319:             "traceback": "<full traceback>",
320:             "suggested_fix": "Check return value"
321:         }
322: 
323:     Process:
324:         1. Detect error type (assertion, import, syntax, type, etc.)
325:         2. Extract file:line location
326:         3. Extract function/class context
327:         4. Extract error message
328:         5. Generate suggested fix hint
329:     """
330:     error = {
331:         "type": "unknown_error",
332:         "file": "unknown",
333:         "line": 0,
334:         "function": None,
335:         "message": output[:200] if output else "Unknown error",
336:         "traceback": output,
337:         "suggested_fix": "Manual review required"
338:     }
339: 
340:     # Detect error type from output patterns
341:     if "ImportError" in output or "ModuleNotFoundError" in output or "cannot import" in output:
342:         error["type"] = "import_error"
343:         error["suggested_fix"] = "Add missing import statement"
344: 
345:         # Extract module name: "No module named 'jwt'" or "cannot import name 'User'"
346:         import_match = re.search(r"No module named '([^']+)'", output)
347:         if import_match:
348:             error["message"] = f"No module named '{import_match.group(1)}'"
349:             error["suggested_fix"] = f"Install or import {import_match.group(1)}"
350:         else:
351:             name_match = re.search(r"cannot import name '([^']+)'", output)
352:             if name_match:
353:                 error["message"] = f"cannot import name '{name_match.group(1)}'"
354:                 error["suggested_fix"] = f"Check import of {name_match.group(1)}"
355: 
356:     elif "AssertionError" in output or "assert" in output.lower():
357:         error["type"] = "assertion_error"
358:         error["suggested_fix"] = "Check assertion logic"
359: 
360:     elif "SyntaxError" in output:
361:         error["type"] = "syntax_error"
362:         error["suggested_fix"] = "Fix syntax error"
363: 
364:     elif "TypeError" in output:
365:         error["type"] = "type_error"
366:         error["suggested_fix"] = "Check type annotations and conversions"
367: 
368:     elif "NameError" in output or "is not defined" in output:
369:         error["type"] = "name_error"
370:         error["suggested_fix"] = "Define missing variable or import"
371: 
372:     elif "AttributeError" in output:
373:         error["type"] = "attribute_error"
374:         error["suggested_fix"] = "Check attribute exists on object"
375: 
376:     # Extract file:line location (common patterns)
377:     # Pattern 1: File "path/to/file.py", line 42
378:     file_match = re.search(r'File "([^"]+)", line (\d+)', output)
379:     if file_match:
380:         error["file"] = file_match.group(1)
381:         error["line"] = int(file_match.group(2))
382: 
383:     # Pattern 2: path/to/file.py:42:
384:     location_match = re.search(r'([^:\s]+\.py):(\d+):', output)
385:     if location_match:
386:         error["file"] = location_match.group(1)
387:         error["line"] = int(location_match.group(2))
388: 
389:     # Extract function/class context
390:     func_match = re.search(r'in (\w+)', output)
391:     if func_match:
392:         error["function"] = func_match.group(1)
393: 
394:     return error
395: 
396: 
397: def check_escalation_triggers(
398:     error: Dict[str, Any],
399:     attempt: int,
400:     error_history: List[str]
401: ) -> bool:
402:     """Check if error triggers human escalation.
403: 
404:     Args:
405:         error: Parsed error dict
406:         attempt: Current attempt number
407:         error_history: List of previous error messages for this validation
408: 
409:     Returns:
410:         True if escalation required, False to continue self-healing
411: 
412:     Escalation Triggers:
413:         1. Same error after 3 attempts (error message unchanged)
414:         2. Ambiguous error messages (generic "something went wrong")
415:         3. Architectural changes required (detected by keywords: "refactor", "redesign")
416:         4. External dependency issues (network errors, API failures, missing packages)
417:         5. Security concerns (vulnerability, secret exposure, permission escalation)
418:     """
419:     # Trigger 1: Same error after 3 attempts
420:     if attempt >= 3 and len(error_history) >= 3:
421:         # Check if all 3 error messages are identical
422:         if len(set(error_history[-3:])) == 1:
423:             return True
424: 
425:     # Trigger 2: Ambiguous error messages
426:     ambiguous_patterns = [
427:         "something went wrong",
428:         "unexpected error",
429:         "failed",
430:         "error occurred",
431:         "unknown error"
432:     ]
433:     error_msg = error.get("message", "").lower()
434:     if any(pattern in error_msg for pattern in ambiguous_patterns):
435:         # Only escalate if also no file/line info
436:         if error.get("file") == "unknown" and error.get("line") == 0:
437:             return True
438: 
439:     # Trigger 3: Architectural changes required
440:     architecture_keywords = [
441:         "refactor",
442:         "redesign",
443:         "architecture",
444:         "restructure",
445:         "circular import",
446:         "coupling"
447:     ]
448:     full_error = error.get("traceback", "") + error.get("message", "")
449:     if any(keyword in full_error.lower() for keyword in architecture_keywords):
450:         return True
451: 
452:     # Trigger 4: External dependency issues
453:     dependency_keywords = [
454:         "connection refused",
455:         "network error",
456:         "timeout",
457:         "api error",
458:         "http error",
459:         "could not resolve host",
460:         "package not found",
461:         "pypi",
462:         "npm error"
463:     ]
464:     if any(keyword in full_error.lower() for keyword in dependency_keywords):
465:         return True
466: 
467:     # Trigger 5: Security concerns
468:     security_keywords = [
469:         "cve-",
470:         "vulnerability",
471:         "secret",
472:         "password",
473:         "api key",
474:         "token",
475:         "credential",
476:         "permission denied",
477:         "access denied",
478:         "unauthorized",
479:         "security"
480:     ]
481:     if any(keyword in full_error.lower() for keyword in security_keywords):
482:         return True
483: 
484:     return False
485: 
486: 
487: def apply_self_healing_fix(error: Dict[str, Any], _attempt: int) -> Dict[str, Any]:
488:     """Apply self-healing fix based on error type.
489: 
490:     Args:
491:         error: Parsed error dict from parse_validation_error()
492:         _attempt: Current attempt number (1-3) - reserved for future use
493: 
494:     Returns:
495:         {
496:             "success": True,
497:             "fix_type": "import_added",
498:             "location": "src/auth.py:3",
499:             "description": "Added missing import: from models import User"
500:         }
501: 
502:     Process:
503:         1. Check escalation triggers first (done in run_validation_loop)
504:         2. Match error type to fix strategy:
505:            - import_error ‚Üí add_missing_import()
506:            - assertion_error ‚Üí Manual review (escalate)
507:            - type_error ‚Üí Manual review (escalate)
508:            - syntax_error ‚Üí Manual review (escalate)
509:            - name_error ‚Üí Manual review (escalate)
510:         3. Apply fix using file operations
511:         4. Log fix for debugging
512:     """
513:     error_type = error.get("type", "unknown_error")
514: 
515:     # Import errors - can auto-fix by adding import statement
516:     if error_type == "import_error":
517:         try:
518:             filepath = error.get("file", "unknown")
519:             message = error.get("message", "")
520: 
521:             # Extract module/class name
522:             if "No module named" in message:
523:                 match = re.search(r"No module named '([^']+)'", message)
524:                 if match:
525:                     module = match.group(1)
526:                     return _add_import_statement(filepath, f"import {module}")
527:             elif "cannot import name" in message:
528:                 match = re.search(r"cannot import name '([^']+)'", message)
529:                 if match:
530:                     name = match.group(1)
531:                     # Try common import patterns
532:                     return _add_import_statement(filepath, f"from . import {name}")
533: 
534:         except Exception as e:
535:             return {
536:                 "success": False,
537:                 "fix_type": "import_error_failed",
538:                 "description": f"Failed to fix import: {str(e)}"
539:             }
540: 
541:     # Other error types - require manual intervention or more complex logic
542:     # These will be handled by escalation triggers
543:     return {
544:         "success": False,
545:         "fix_type": f"{error_type}_not_implemented",
546:         "description": f"Auto-fix not implemented for {error_type} - escalate to human"
547:     }
548: 
549: 
550: def _add_import_statement(filepath: str, import_stmt: str) -> Dict[str, Any]:
551:     """Add import statement to file.
552: 
553:     Args:
554:         filepath: Path to Python file
555:         import_stmt: Import statement to add (e.g., "import jwt" or "from models import User")
556: 
557:     Returns:
558:         Fix result dict
559:     """
560:     try:
561:         file_path = Path(filepath)
562:         if not file_path.exists():
563:             return {
564:                 "success": False,
565:                 "fix_type": "import_add_failed",
566:                 "description": f"File not found: {filepath}"
567:             }
568: 
569:         # Read current content
570:         content = file_path.read_text()
571:         lines = content.split("\n")
572: 
573:         # Find position to insert import (after existing imports or at top)
574:         insert_pos = 0
575:         for i, line in enumerate(lines):
576:             if line.startswith("import ") or line.startswith("from "):
577:                 insert_pos = i + 1
578:             elif line.strip() and not line.startswith("#") and not line.startswith('"""'):
579:                 # Found first non-import, non-comment line
580:                 break
581: 
582:         # Insert import statement
583:         lines.insert(insert_pos, import_stmt)
584: 
585:         # Write back
586:         file_path.write_text("\n".join(lines))
587: 
588:         return {
589:             "success": True,
590:             "fix_type": "import_added",
591:             "location": f"{filepath}:{insert_pos + 1}",
592:             "description": f"Added import: {import_stmt}"
593:         }
594: 
595:     except Exception as e:
596:         return {
597:             "success": False,
598:             "fix_type": "import_add_failed",
599:             "description": f"Error adding import: {str(e)}"
600:         }
601: 
602: 
603: def escalate_to_human(error: Dict[str, Any], reason: str) -> None:
604:     """Escalate to human with detailed error report.
605: 
606:     Args:
607:         error: Parsed error dict
608:         reason: Escalation trigger reason
609: 
610:     Raises:
611:         EscalationRequired: Always (signals need for human intervention)
612: 
613:     Process:
614:         1. Format error report with type and location
615:         2. Include full error message and traceback
616:         3. Provide escalation reason
617:         4. Generate troubleshooting guidance based on error type
618:     """
619:     # Build context-specific troubleshooting guidance
620:     troubleshooting_lines = ["Steps to resolve:"]
621: 
622:     if reason == "persistent_error":
623:         troubleshooting_lines.extend([
624:             "1. Review error details - same error occurred 3 times",
625:             "2. Check if fix logic matches error type",
626:             "3. Consider if architectural change needed",
627:             "4. Review validation command output manually"
628:         ])
629: 
630:     elif reason == "ambiguous_error":
631:         troubleshooting_lines.extend([
632:             "1. Run validation command manually for full context",
633:             "2. Check logs for additional error details",
634:             "3. Add debug print statements if needed",
635:             "4. Review recent code changes"
636:         ])
637: 
638:     elif reason == "architectural":
639:         troubleshooting_lines.extend([
640:             "1. Review error for architectural keywords (refactor, redesign, circular)",
641:             "2. Consider if code structure needs reorganization",
642:             "3. Check for circular dependencies",
643:             "4. May require human design decision"
644:         ])
645: 
646:     elif reason == "dependencies":
647:         troubleshooting_lines.extend([
648:             "1. Check network connectivity",
649:             "2. Verify package repository access (PyPI, npm, etc.)",
650:             "3. Review dependency versions in requirements",
651:             "4. Check for transitive dependency conflicts"
652:         ])
653: 
654:     elif reason == "security":
655:         troubleshooting_lines.extend([
656:             "1. DO NOT auto-fix security-related errors",
657:             "2. Review error for exposed secrets/credentials",
658:             "3. Check for permission/access issues",
659:             "4. Consult security documentation if CVE mentioned"
660:         ])
661: 
662:     else:
663:         troubleshooting_lines.extend([
664:             "1. Review error details above",
665:             "2. Check file and line number for context",
666:             "3. Run validation command manually",
667:             "4. Consult documentation for error type"
668:         ])
669: 
670:     # Add error-type-specific guidance
671:     error_type = error.get("type", "unknown")
672:     if error_type == "import_error":
673:         troubleshooting_lines.append("5. Check if module is installed: pip list | grep <module>")
674:     elif error_type == "assertion_error":
675:         troubleshooting_lines.append("5. Review test logic and expected vs actual values")
676:     elif error_type == "type_error":
677:         troubleshooting_lines.append("5. Check type annotations and ensure type compatibility")
678: 
679:     troubleshooting = "\n".join(troubleshooting_lines)
680: 
681:     raise EscalationRequired(
682:         reason=reason,
683:         error=error,
684:         troubleshooting=troubleshooting
685:     )
</file>

<file path="tools/bootstrap.sh">
 1: #!/bin/bash
 2: # Bootstrap script for Context Engineering CLI Tools
 3: # One-command setup
 4: 
 5: set -euo pipefail
 6: 
 7: echo "=== Context Engineering CLI Tools Bootstrap ==="
 8: echo ""
 9: 
10: # Check prerequisites
11: echo "üìã Checking prerequisites..."
12: 
13: if ! command -v uv &> /dev/null; then
14:     echo "‚ùå UV package manager not found"
15:     echo "üîß Install with: curl -LsSf https://astral.sh/uv/install.sh | sh"
16:     exit 1
17: fi
18: 
19: if ! command -v git &> /dev/null; then
20:     echo "‚ùå Git not found"
21:     echo "üîß Install git first"
22:     exit 1
23: fi
24: 
25: echo "‚úÖ Prerequisites OK"
26: echo ""
27: 
28: # Create virtual environment
29: echo "üîß Creating virtual environment..."
30: if [ -d ".venv" ]; then
31:     echo "   .venv already exists, skipping"
32: else
33:     uv venv
34:     echo "‚úÖ Virtual environment created"
35: fi
36: echo ""
37: 
38: # Install package
39: echo "üì¶ Installing ce-tools..."
40: uv pip install -e .
41: echo "‚úÖ ce-tools installed"
42: echo ""
43: 
44: # Run tests to verify installation
45: echo "üß™ Running tests to verify installation..."
46: if uv run pytest tests/ -v --tb=short; then
47:     echo "‚úÖ All tests passed"
48: else
49:     echo "‚ö†Ô∏è  Some tests failed (this is OK if npm commands not available)"
50: fi
51: echo ""
52: 
53: # Final instructions
54: echo "=== Bootstrap Complete ==="
55: echo ""
56: echo "CLI is ready to use:"
57: echo "  ce --help"
58: echo "  ce validate --level all"
59: echo "  ce git status"
60: echo "  ce context health"
61: echo ""
62: echo "To activate virtual environment manually:"
63: echo "  source .venv/bin/activate"
64: echo ""
65: echo "Or use directly with uv run:"
66: echo "  uv run ce --help"
67: echo ""
</file>

<file path="tools/README.md">
  1: # Context Engineering CLI Tools
  2: 
  3: Minimal, efficient tooling for Context Engineering framework operations.
  4: 
  5: ## Features
  6: 
  7: ‚úÖ **PRP Generation**: Automated PRP creation from INITIAL.md
  8: ‚úÖ **File Operations**: Read/write with security validation
  9: ‚úÖ **Git Integration**: Status, diff, checkpoints
 10: ‚úÖ **3-Level Validation**: Syntax, unit tests, integration tests
 11: ‚úÖ **Context Management**: Sync and health checks
 12: ‚úÖ **Zero Dependencies**: Pure stdlib implementation
 13: ‚úÖ **JSON Output**: Scriptable for CI/CD pipelines
 14: 
 15: ## Installation
 16: 
 17: ### Quick Install
 18: ```bash
 19: cd tools
 20: ./bootstrap.sh
 21: ```
 22: 
 23: ### Manual Install
 24: ```bash
 25: cd tools
 26: uv venv
 27: uv pip install -e .
 28: ```
 29: 
 30: ## Commands Reference
 31: 
 32: ### PRP Generation
 33: 
 34: **Generate PRP from INITIAL.md**
 35: ```bash
 36: ce prp generate <initial-md-path> [-o OUTPUT_DIR] [--json]
 37: 
 38: # Example
 39: ce prp generate ../feature-requests/user-auth/INITIAL.md
 40: # Output: ../PRPs/feature-requests/PRP-6-user-authentication-system.md
 41: 
 42: # Custom output directory
 43: ce prp generate feature.md -o /tmp/prps
 44: 
 45: # JSON output for scripting
 46: ce prp generate feature.md --json
 47: ```
 48: 
 49: **What it does**:
 50: 1. Parses INITIAL.md (FEATURE, EXAMPLES, DOCUMENTATION, OTHER CONSIDERATIONS)
 51: 2. Researches codebase using Serena MCP (pattern search, symbol analysis)
 52: 3. Fetches documentation using Context7 MCP (library docs, external links)
 53: 4. Generates complete 6-section PRP with YAML header
 54: 5. Auto-assigns next PRP ID (PRP-N+1)
 55: 6. Validates completeness (all required sections present)
 56: 
 57: **INITIAL.md structure** (see `.claude/commands/generate-prp.md` for details):
 58: ```markdown
 59: # Feature: <Feature Name>
 60: 
 61: ## FEATURE
 62: <What to build - user story, acceptance criteria>
 63: 
 64: ## EXAMPLES
 65: <Code examples, file references>
 66: 
 67: ## DOCUMENTATION
 68: <Library docs, external resources>
 69: 
 70: ## OTHER CONSIDERATIONS
 71: <Security, constraints, edge cases>
 72: ```
 73: 
 74: **Graceful degradation**: Works without MCP servers (reduced functionality)
 75: 
 76: See also: `/generate-prp` slash command
 77: 
 78: ---
 79: 
 80: ### Validation Gates
 81: 
 82: **Level 1: Syntax & Style**
 83: ```bash
 84: ce validate --level 1
 85: # Runs: npm run lint && npm run type-check
 86: ```
 87: 
 88: **Level 2: Unit Tests**
 89: ```bash
 90: ce validate --level 2
 91: # Runs: npm test
 92: ```
 93: 
 94: **Level 3: Integration Tests**
 95: ```bash
 96: ce validate --level 3
 97: # Runs: npm run test:integration
 98: ```
 99: 
100: **All Levels**
101: ```bash
102: ce validate --level all
103: # Runs all validation levels sequentially
104: ```
105: 
106: ---
107: 
108: ### Markdown & Mermaid Linting
109: 
110: **Markdown Linting** (integrated in Level 1 validation)
111: ```bash
112: # Lint markdown files
113: npm run lint:md
114: 
115: # Auto-fix markdown issues
116: npm run lint:md:fix
117: ```
118: 
119: **What it does**:
120: - Validates markdown syntax using markdownlint-cli2
121: - Checks for common issues: trailing spaces, missing blank lines, inconsistent headings
122: - Auto-fixes formatting issues automatically
123: - Integrated into Level 1 validation gate
124: 
125: **Configuration**: `.markdownlint.json` in project root
126: ```json
127: {
128:   "default": true,
129:   "MD013": false,  // Line length disabled (allow long code examples)
130:   "MD033": {       // Inline HTML allowed for badges
131:     "allowed_elements": ["img", "br", "sub", "sup", "User"]
132:   },
133:   "MD046": {       // Fenced code blocks required
134:     "style": "fenced"
135:   }
136: }
137: ```
138: 
139: **Mermaid Diagram Validation** (integrated in Level 1 validation)
140: 
141: The mermaid validator automatically checks and fixes diagram syntax issues:
142: 
143: **Features**:
144: - ‚úÖ Validates node text for unquoted special characters
145: - ‚úÖ Checks style statements have color specified (theme compatibility)
146: - ‚úÖ Auto-fixes common issues (renaming nodes, adding colors)
147: - ‚úÖ HTML tag support (`<br/>`, `<sub/>`, `<sup/>`)
148: - ‚úÖ Smart detection of problematic characters only
149: 
150: **Safe Characters** (no quoting needed):
151: - Colons `:` - "Level 0: CLAUDE.md" ‚úÖ
152: - Question marks `?` - "Why? Because!" ‚úÖ
153: - Exclamation marks `!` - "Important!" ‚úÖ
154: - Slashes `/` `\` - "path/to/file" ‚úÖ
155: - HTML tags - "Line 1<br/>Line 2" ‚úÖ
156: 
157: **Problematic Characters** (require quoting or node renaming):
158: - Brackets `[]` `{}` - used for node shape syntax
159: - Parentheses `()` - used for node shape syntax
160: - Pipes `|` - used for subgraph syntax
161: - Unbalanced quotes `"` `'` - break parsing
162: 
163: **Example Issues Detected**:
164: ```mermaid
165: graph TD
166:     N1[Text with (parentheses)]    # ‚ùå Will be flagged
167:     style B fill:#ff0000,color:#fff           # ‚ùå Missing color specification
168: ```
169: 
170: **Auto-Fixed Output**:
171: ```mermaid
172: graph TD
173:     N1[Text with parentheses renamed]
174:     style B fill:#ff0000,color:#fff  # ‚úÖ Color added
175: ```
176: 
177: **Standalone Usage** (if needed):
178: ```bash
179: # Validate all markdown/mermaid in docs/
180: cd tools
181: python ce/mermaid_validator.py ../docs
182: 
183: # Auto-fix issues
184: python ce/mermaid_validator.py --fix ../docs
185: ```
186: 
187: **Results**:
188: - Files checked: 14
189: - Diagrams checked: 73
190: - Issues auto-fixed: varies based on file state
191: 
192: **Style Color Determination**:
193: 
194: The validator automatically determines appropriate text color based on background luminance:
195: - Light backgrounds (luminance > 0.5) ‚Üí black text `#000`
196: - Dark backgrounds (luminance ‚â§ 0.5) ‚Üí white text `#fff`
197: 
198: Uses W3C WCAG 2.0 relative luminance formula for accurate color contrast.
199: 
200: ---
201: 
202: ### Git Operations
203: 
204: **Check Status**
205: ```bash
206: ce git status [--json]
207: # Shows: staged, unstaged, untracked files
208: ```
209: 
210: **Create Checkpoint**
211: ```bash
212: ce git checkpoint "Phase 1 complete"
213: # Creates annotated git tag: checkpoint-<timestamp>
214: ```
215: 
216: **View Changes**
217: ```bash
218: ce git diff [--since HEAD~5] [--json]
219: # Shows changed files since specified ref
220: ```
221: 
222: ### Context Management
223: 
224: **Fast Drift Analysis** (NEW - PRP-17)
225: ```bash
226: ce analyze-context [--json] [--force] [--cache-ttl N]
227: # Fast drift check without metadata updates (2-3s vs 10-15s)
228: # Exit codes: 0 (ok), 1 (warning), 2 (critical)
229: # Uses smart caching (5-min TTL by default)
230: 
231: # Examples:
232: ce analyze-context                        # Quick check with cache
233: ce analyze-context --force                # Force re-analysis
234: ce analyze-context --json                 # CI/CD integration
235: ce analyze-context --cache-ttl 10         # Custom TTL (minutes)
236: ce analyse-context                        # UK spelling alias
237: ```
238: 
239: **What it does**:
240: - Detects pattern drift (code violations + missing examples)
241: - Generates/updates `.ce/drift-report.md` with violations
242: - Returns CI/CD-friendly exit codes (0/1/2)
243: - Reuses cache if fresh (avoids redundant analysis)
244: - No PRP metadata updates (faster than update-context)
245: 
246: **When to use**:
247: - Quick drift checks in CI/CD pipelines
248: - Pre-commit validation
249: - Before running update-context (shares cache)
250: 
251: **Sync Context**
252: ```bash
253: ce context sync [--json]
254: # Detects git diff, reports files needing reindex
255: # Returns: reindexed_count, files, drift_score, drift_level
256: ```
257: 
258: **Health Check**
259: ```bash
260: ce context health [--json]
261: # Comprehensive health report:
262: # - Compilation status (Level 1)
263: # - Git cleanliness
264: # - Tests passing (Level 2)
265: # - Context drift score
266: # - Actionable recommendations
267: ```
268: 
269: **Prune Memories** (placeholder)
270: ```bash
271: ce context prune [--age 7] [--dry-run]
272: # Requires Serena MCP integration
273: ```
274: 
275: ### Drift History Tracking
276: 
277: **View Drift History**
278: ```bash
279: ce drift history [--last N] [--prp-id ID] [--action-filter TYPE] [--json]
280: # Shows drift decisions from all PRPs sorted by timestamp
281: 
282: # Examples:
283: ce drift history --last 5
284: ce drift history --prp-id PRP-001
285: ce drift history --action-filter accepted
286: ```
287: 
288: **Show Drift Decision**
289: ```bash
290: ce drift show <prp-id> [--json]
291: # Detailed view of drift decision for specific PRP
292: 
293: # Example:
294: ce drift show PRP-001
295: ```
296: 
297: **Drift Summary**
298: ```bash
299: ce drift summary [--json]
300: # Aggregate statistics across all drift decisions
301: # Shows: total PRPs, average score, distribution, category breakdown
302: ```
303: 
304: **Compare Drift Decisions**
305: ```bash
306: ce drift compare <prp-id-1> <prp-id-2> [--json]
307: # Compare drift decisions between two PRPs
308: # Shows: score difference, common/divergent categories
309: 
310: # Example:
311: ce drift compare PRP-001 PRP-002
312: ```
313: 
314: **What it tracks**:
315: - Drift score (0-100%)
316: - Action taken (accepted, rejected, examples_updated)
317: - Justification for decisions
318: - Category breakdown (code_structure, error_handling, etc.)
319: - Reviewer (human, auto_accept, auto_fix)
320: - Historical patterns and trends
321: 
322: **Integration**: Drift history is displayed during Level 4 validation escalation to provide context for high-drift decisions.
323: 
324: ## JSON Output
325: 
326: All commands support `--json` flag for programmatic use:
327: 
328: ```bash
329: ce validate --level all --json > validation-report.json
330: ce git status --json | jq '.clean'
331: ce context health --json | jq '.drift_score'
332: ```
333: 
334: ## Exit Codes
335: 
336: **Standard Commands**:
337: - **0**: Success
338: - **1**: Failure
339: 
340: **analyze-context** (drift-aware):
341: - **0**: OK - drift score < 5%
342: - **1**: WARNING - drift score 5-15%
343: - **2**: CRITICAL - drift score >= 15%
344: 
345: Use in scripts:
346: ```bash
347: # Standard validation
348: if ce validate --level 1; then
349:     echo "Validation passed"
350: else
351:     echo "Validation failed"
352:     exit 1
353: fi
354: 
355: # Drift analysis for CI/CD
356: ce analyze-context --json
357: EXIT_CODE=$?
358: if [ $EXIT_CODE -eq 2 ]; then
359:     echo "CRITICAL drift - blocking merge"
360:     exit 1
361: elif [ $EXIT_CODE -eq 1 ]; then
362:     echo "WARNING drift - review recommended"
363: fi
364: ```
365: 
366: ## Architecture
367: 
368: ```
369: ce/
370: ‚îú‚îÄ‚îÄ __init__.py       # Package metadata
371: ‚îú‚îÄ‚îÄ __main__.py       # CLI entry point
372: ‚îú‚îÄ‚îÄ core.py           # File, git, shell operations
373: ‚îú‚îÄ‚îÄ validate.py       # 3-level validation gates
374: ‚îú‚îÄ‚îÄ context.py        # Context management
375: ‚îú‚îÄ‚îÄ generate.py       # PRP generation from INITIAL.md
376: ‚îî‚îÄ‚îÄ prp.py            # PRP state management
377: ```
378: 
379: **Design Principles**:
380: - **KISS**: Single responsibility per function
381: - **SOLID**: Clear interfaces, dependency injection
382: - **DRY**: Shared utilities
383: - **No Fishy Fallbacks**: Exceptions thrown, not caught silently
384: - **Real Testing**: Actual functionality, no mocks
385: 
386: ## Development
387: 
388: ### Run Tests
389: ```bash
390: uv run pytest tests/ -v
391: 
392: # With coverage
393: uv run pytest tests/ --cov=ce --cov-report=term-missing
394: ```
395: 
396: ### Add Dependencies
397: ```bash
398: # Never edit pyproject.toml directly!
399: uv add package-name              # Production
400: uv add --dev package-name        # Development
401: ```
402: 
403: ### Test Locally
404: ```bash
405: # Install in editable mode
406: uv pip install -e .
407: 
408: # Use anywhere
409: ce --help
410: ```
411: 
412: ## Integration with Context Engineering Framework
413: 
414: This CLI complements the Context Engineering framework documented in `/docs/`:
415: 
416: - **Validation Gates**: Implements 3-level validation from `08-validation-testing.md`
417: - **Context Sync**: Implements drift detection from `04-self-healing-framework.md`
418: - **Git Operations**: Supports checkpoint pattern from `06-workflow-patterns.md`
419: 
420: ## Troubleshooting
421: 
422: **Command not found: ce**
423: ```bash
424: # Ensure you're in tools directory
425: cd tools
426: 
427: # Reinstall
428: uv pip install -e .
429: 
430: # Or use directly
431: uv run python -m ce --help
432: ```
433: 
434: **Tests failing**
435: ```bash
436: # Install dev dependencies
437: uv sync
438: 
439: # Run specific test
440: uv run pytest tests/test_core.py::test_run_cmd_success -v
441: ```
442: 
443: **npm commands not available**
444: ```bash
445: # Some tests/commands require npm scripts
446: # Ensure package.json has required scripts:
447: npm run lint
448: npm run type-check
449: npm test
450: ```
451: 
452: ## License
453: 
454: Part of the Context Engineering Framework.
455: 
456: ## Version
457: 
458: Current: 0.1.0
</file>

<file path=".ce/PRPs/executed/system/PRP-0-CONTEXT-ENGINEERING.md">
  1: ---
  2: prp_id: PRP-0
  3: title: Context Engineering Framework Installation
  4: status: executed
  5: created: {TIMESTAMP}
  6: executed: {TIMESTAMP}
  7: batch: 0
  8: phase: 0
  9: order: 0
 10: estimated_hours: {INSTALL_TIME}
 11: complexity: low
 12: risk_level: low
 13: ce_version: {CE_VERSION}
 14: installation_method: {METHOD}
 15: ---
 16: 
 17: # PRP-0: Context Engineering Framework Installation
 18: 
 19: ## üìã TL;DR
 20: 
 21: **What**: Installed Context Engineering (CE) framework version {CE_VERSION} into {PROJECT_NAME}
 22: 
 23: **When**: {TIMESTAMP}
 24: 
 25: **How**: {INSTALLATION_METHOD}
 26: 
 27: **Result**: CE 1.1 framework fully installed with /system/ organization
 28: 
 29: ---
 30: 
 31: ## Installation Details
 32: 
 33: ### CE Version
 34: 
 35: **Version**: {CE_VERSION}
 36: **Release Date**: {CE_RELEASE_DATE}
 37: **Distribution Package**: `ce-infrastructure.xml` ({SIZE}KB)
 38: 
 39: ### Installation Method
 40: 
 41: **Method**: {METHOD}
 42: 
 43: **Options**:
 44: - `greenfield` - New project, no existing CE
 45: - `mature-project` - Existing code, added CE
 46: - `existing-ce` - Upgraded CE 1.0 ‚Üí CE 1.1
 47: - `partial-ce` - Completed incomplete installation
 48: 
 49: ### Installation Date
 50: 
 51: **Started**: {START_TIMESTAMP}
 52: **Completed**: {END_TIMESTAMP}
 53: **Duration**: {DURATION} minutes
 54: 
 55: ---
 56: 
 57: ## Components Installed
 58: 
 59: ### Framework Memories (23 files)
 60: 
 61: **Location**: `.serena/memories/system/`
 62: 
 63: **Critical Memories (6)**:
 64: - code-style-conventions.md
 65: - suggested-commands.md
 66: - task-completion-checklist.md
 67: - testing-standards.md
 68: - tool-usage-syntropy.md
 69: - use-syntropy-tools-not-bash.md
 70: 
 71: **Regular Memories (17)**:
 72: - batch-generation-patterns.md
 73: - ce-tool-patterns.md
 74: - complexity-estimation.md
 75: - context-drift-monitoring.md
 76: - file-structure-best-practices.md
 77: - git-worktree-patterns.md
 78: - prp-execution-checklist.md
 79: - prp-sizing-thresholds.md
 80: - sequential-thinking-usage.md
 81: - syntropy-mcp-architecture.md
 82: - syntropy-tool-management.md
 83: - testing-strategy-pattern.md
 84: - token-optimization-strategies.md
 85: - tool-selection-decision-tree.md
 86: - validation-levels.md
 87: - worktree-conflict-resolution.md
 88: - xml-package-generation.md
 89: 
 90: ### Framework Examples (21 files)
 91: 
 92: **Location**: `.ce/examples/system/`
 93: 
 94: **Files Installed**:
 95: {LIST_OF_EXAMPLE_FILES}
 96: 
 97: ### Framework Commands (11 files)
 98: 
 99: **Location**: `.claude/commands/`
100: 
101: **Commands Installed**:
102: - batch-exe-prp.md
103: - batch-gen-prp.md
104: - denoise.md
105: - execute-prp.md
106: - generate-prp.md
107: - peer-review.md
108: - sync-with-syntropy.md
109: - syntropy-health.md
110: - tools-misuse-scan.md
111: - update-context.md
112: - vacuum.md
113: 
114: ### Tool Source Code (33 files)
115: 
116: **Location**: `tools/`
117: 
118: **Modules Installed**:
119: {LIST_OF_TOOL_MODULES}
120: 
121: ### CLAUDE.md Sections
122: 
123: **Framework Sections Added**:
124: - Communication
125: - Core Principles
126: - UV Package Management
127: - Ad-Hoc Code Policy
128: - Quick Commands
129: - Tool Naming Convention
130: - Allowed Tools Summary
131: - Command Permissions
132: - Quick Tool Selection
133: - Project Structure
134: - Testing Standards
135: - Code Quality
136: 
137: ---
138: 
139: ## Validation Results
140: 
141: ### Component Verification
142: 
143: ```bash
144: # Run validation checklist
145: {VALIDATION_COMMAND_OUTPUT}
146: ```
147: 
148: **Results**:
149: - ‚úÖ 23 system memories installed
150: - ‚úÖ 21 system examples installed
151: - ‚úÖ 11 framework commands installed
152: - ‚úÖ 33 tool files installed
153: - ‚úÖ CLAUDE.md framework sections added
154: - ‚úÖ CE 1.1 /system/ organization verified
155: 
156: ### Token Counts
157: 
158: **Framework Documentation**:
159: - System memories: ~{MEMORY_TOKENS}k tokens
160: - System examples: ~{EXAMPLE_TOKENS}k tokens
161: - Commands: ~{COMMAND_TOKENS}k tokens
162: - **Total**: ~{TOTAL_TOKENS}k tokens
163: 
164: ---
165: 
166: ## Project Integration
167: 
168: ### Existing Project Context
169: 
170: **Project Name**: {PROJECT_NAME}
171: **Project Type**: {PROJECT_TYPE}
172: **Primary Language**: {PRIMARY_LANGUAGE}
173: **Repository**: {REPO_URL}
174: 
175: **Pre-Installation State**:
176: {PRE_INSTALL_STATE}
177: 
178: ### Post-Installation Configuration
179: 
180: **Syntropy MCP**:
181: - Status: {MCP_STATUS}
182: - Servers: {MCP_SERVERS}
183: 
184: **Linear Integration**:
185: - Status: {LINEAR_STATUS}
186: - Project: {LINEAR_PROJECT}
187: - Team: {LINEAR_TEAM}
188: 
189: **UV Tools**:
190: - Status: {UV_STATUS}
191: - Python Version: {PYTHON_VERSION}
192: 
193: ---
194: 
195: ## Next Steps
196: 
197: ### Immediate Actions
198: 
199: 1. **Test framework commands**:
200:    ```bash
201:    /generate-prp
202:    /vacuum --dry-run
203:    /syntropy-health
204:    ```
205: 
206: 2. **Configure project settings**:
207:    - Update `.claude/settings.local.json` with project-specific permissions
208:    - Update `CLAUDE.md` with user sections
209: 
210: 3. **Create first user PRP**:
211:    - Document initial feature or bug fix
212:    - Practice PRP workflow
213: 
214: ### Recommended Reading
215: 
216: **Essential Documentation**:
217: - `.ce/examples/system/TOOL-USAGE-GUIDE.md` - Tool selection reference
218: - `.ce/examples/system/prp-decomposition-patterns.md` - PRP sizing
219: - `.serena/memories/system/task-completion-checklist.md` - Workflow checklist
220: 
221: **Migration Guide**:
222: - `examples/INITIALIZATION.md` - Complete CE 1.1 initialization guide (5-phase workflow, scenario-aware)
223: 
224: ---
225: 
226: ## Troubleshooting
227: 
228: **Issue**: Commands not found
229: 
230: **Solution**:
231: ```bash
232: # Verify commands directory
233: ls .claude/commands/
234: 
235: # Check Claude Code settings
236: cat .claude/settings.local.json | grep commands
237: ```
238: 
239: **Issue**: MCP servers not connected
240: 
241: **Solution**:
242: ```bash
243: # Remove auth cache
244: rm -rf ~/.mcp-auth
245: 
246: # Restart Claude Code
247: /mcp
248: ```
249: 
250: ---
251: 
252: ## Changelog
253: 
254: ### {TIMESTAMP} - Initial Installation
255: 
256: - Installed CE {CE_VERSION} via {INSTALLATION_METHOD}
257: - Added 23 framework memories to `.serena/memories/system/`
258: - Added 21 framework examples to `.ce/examples/system/`
259: - Added 11 framework commands to `.claude/commands/`
260: - Configured CLAUDE.md with framework sections
261: 
262: ---
263: 
264: ## References
265: 
266: - **CE Framework Documentation**: `.ce/examples/system/`
267: - **Migration Guides**: `examples/workflows/migration-*.md`
268: - **Master Initialization Guide**: `examples/INITIALIZATION.md`
269: - **Repomix Packages**: `.ce/ce-infrastructure.xml`, `.ce/ce-workflow-docs.xml`
270: 
271: ---
272: 
273: **Installation Completed**: {TIMESTAMP}
274: **Installed By**: {INSTALLER_NAME}
275: **CE Version**: {CE_VERSION}
</file>

<file path=".claude/commands/batch-exe-prp.md">
   1: # /batch-exe-prp - Parallel PRP Execution with Monitoring
   2: 
   3: Execute multiple PRPs in parallel using subagents with health checks, real-time monitoring, and coordinated error handling.
   4: 
   5: ## Usage
   6: 
   7: ```
   8: /batch-exe-prp [options] <prp-1> <prp-2> ... <prp-n>
   9: /batch-exe-prp --batch <batch-id> [options]
  10: ```
  11: 
  12: **Examples:**
  13: ```bash
  14: # Execute 3 PRPs in parallel (auto-selects model per PRP)
  15: /batch-exe-prp PRP-A PRP-B PRP-C
  16: 
  17: # Execute entire batch by ID (auto-review enabled by default)
  18: /batch-exe-prp --batch 34
  19: 
  20: # Execute batch without automatic execution review
  21: /batch-exe-prp --batch 34 --no-auto-review
  22: 
  23: # Execute specific stage only (auto-review enabled by default)
  24: /batch-exe-prp --batch 34 --stage 2
  25: 
  26: # Resume interrupted batch execution
  27: /batch-exe-prp --batch 34 --resume
  28: 
  29: # Force all PRPs to use Haiku (override auto-selection)
  30: /batch-exe-prp --model haiku PRP-A PRP-B PRP-C
  31: 
  32: # Limit parallelism (useful for resource constraints)
  33: /batch-exe-prp --max-parallel 2 PRP-A PRP-B PRP-C PRP-D
  34: 
  35: # Dry run (parse all PRPs, show model assignments)
  36: /batch-exe-prp --dry-run PRP-A PRP-B PRP-C
  37: 
  38: # Continue on failure (don't abort if one PRP fails)
  39: /batch-exe-prp --continue-on-error PRP-A PRP-B PRP-C
  40: ```
  41: 
  42: ## What It Does
  43: 
  44: **Core Design**: Cohesive architecture with `/execute-prp`
  45: 
  46: - **Batch-exe-prp** (this command): Coordinator - analyzes PRPs, assigns models, launches parallel agents, monitors health, aggregates results
  47: - **Execute-prp** (single PRP engine): Execution logic - phases, validation gates, self-healing, checkpoints (reused by batch agents)
  48: - **No duplication**: Batch delegates to execute-prp via Task agents, doesn't reimplement PRP logic
  49: 
  50: ## Execution Modes
  51: 
  52: ### Mode 1: Individual PRPs (Original Behavior)
  53: 
  54: Execute specific PRPs in parallel, all at once:
  55: 
  56: ```bash
  57: /batch-exe-prp PRP-A PRP-B PRP-C
  58: ```
  59: 
  60: **Workflow**: Steps 1-8 below (analyze ‚Üí validate ‚Üí execute all ‚Üí merge ‚Üí cleanup)
  61: 
  62: ### Mode 2: Batch-Aware Execution (NEW)
  63: 
  64: Execute all PRPs in a batch, with stage-by-stage execution and automatic peer review:
  65: 
  66: ```bash
  67: /batch-exe-prp --batch 34
  68: ```
  69: 
  70: **Key Features**:
  71: 1. **Stage-aware**: Discovers all PRPs in batch, groups by stage, executes stages sequentially
  72: 2. **Auto-review**: Runs `/batch-peer-review --exe` after each stage completes (default, use `--no-auto-review` to disable)
  73: 3. **Quality gates**: Only proceeds to next stage if execution review passes
  74: 4. **Checkpointing**: Saves state after each stage for resume capability
  75: 5. **Resume support**: Continue from last checkpoint if interrupted
  76: 
  77: **Batch Execution Workflow**:
  78: 
  79: ```
  80: Stage 1: Execute PRPs
  81:    ‚Üì
  82: Stage 1: Auto-review execution (/batch-peer-review --batch 34 --exe --stage 1)
  83:    ‚Üì
  84:    ‚îú‚îÄ Issues found (minor) ‚Üí Apply fixes automatically ‚Üí Continue
  85:    ‚îú‚îÄ Issues found (critical) ‚Üí Pause, report, wait for user approval
  86:    ‚îî‚îÄ No issues ‚Üí Continue
  87:    ‚Üì
  88: Stage 1: Merge worktrees
  89:    ‚Üì
  90: Checkpoint saved (.ce/tmp/batch-execution-34.json)
  91:    ‚Üì
  92: Stage 2: Execute PRPs
  93:    ‚Üì
  94: [Repeat review/merge/checkpoint cycle]
  95:    ‚Üì
  96: ... Stage 3, 4, etc.
  97:    ‚Üì
  98: Final cleanup
  99: ```
 100: 
 101: **Stage Discovery** (automatic):
 102: 1. Read master plan: `PRPs/feature-requests/PRP-34-INITIAL.md`
 103: 2. Scan for all batch PRPs: `PRPs/feature-requests/PRP-34.*.*.md`
 104: 3. Parse YAML headers, extract `stage` field
 105: 4. Group by stage: `{1: [PRP-34.1.1], 2: [PRP-34.2.1, PRP-34.2.2, ...], ...}`
 106: 5. Execute stages in order (1 ‚Üí 2 ‚Üí 3 ‚Üí 4)
 107: 
 108: **Execution Review** (automatic after each stage):
 109: - Calls `/batch-peer-review --batch 34 --exe --stage N` internally
 110: - Reviews all PRPs in stage N that just executed
 111: - Checks: Implementation matches specs, code quality, no guideline violations, etc. (9 checks)
 112: - Minor issues (typos, style): Auto-fix, continue
 113: - Critical issues (logic errors, security): Pause, escalate to user
 114: 
 115: **Pause/Resume**:
 116: ```bash
 117: # If batch execution paused (review failed, conflict, error)
 118: # Resume from last checkpoint
 119: /batch-exe-prp --batch 34 --resume
 120: 
 121: # Reads: .ce/tmp/batch-execution-34.json
 122: # Determines: Last completed stage = 2
 123: # Resumes: Execute Stage 3 (skips Stages 1-2)
 124: ```
 125: 
 126: **Benefits**:
 127: - üõ°Ô∏è Quality gate between stages (catches errors early)
 128: - üîÑ Resume from interruptions (don't lose progress)
 129: - üìä Stage-level validation (validate integration within stage)
 130: - üöÄ Parallel within stages (max speed for independent PRPs)
 131: - üéØ Sequential across stages (respects dependencies)
 132: 
 133: ### 1. Analyze PRP Complexity & Auto-Assign Models (Sequential)
 134: **Time**: ~5-10 seconds per PRP
 135: 
 136: For each PRP, analyzes complexity and assigns optimal model (unless `--model` overrides):
 137: 
 138: **Complexity Analysis**:
 139: ```python
 140: def complexity_weight(complexity):
 141:     """Convert complexity label to numeric weight"""
 142:     weights = {"low": 0.5, "medium": 1.0, "high": 1.5}
 143:     return weights.get(complexity, 1.0)  # Default to medium
 144: 
 145: def analyze_prp_complexity(prp_path):
 146:     # Read PRP file
 147:     prp = read_prp(prp_path)
 148: 
 149:     # Extract metadata
 150:     complexity = prp.yaml_header.get('complexity', 'medium')  # low/medium/high
 151:     estimated_hours = prp.yaml_header.get('estimated_hours', 1.0)
 152:     files_modified = len(prp.yaml_header.get('files_modified', []))
 153:     phases = count_phases(prp.implementation_blueprint)
 154: 
 155:     # Calculate complexity score (0-100)
 156:     # Score breakdown:
 157:     # - Complexity weight: 0-60 points (low=20, medium=40, high=60)
 158:     # - Hours: 0-30 points (capped at 3 hours)
 159:     # - Files: 0-20 points (capped at 4 files)
 160:     # - Phases: 0-10 points (capped at 3+ phases)
 161:     score = (
 162:         complexity_weight(complexity) * 40 +  # 20/40/60 points
 163:         min(estimated_hours * 10, 30) +       # 30 points max
 164:         min(files_modified * 5, 20) +         # 20 points max
 165:         min(phases * 3, 10)                   # 10 points max
 166:     )
 167: 
 168:     return score
 169: 
 170: def assign_model(score):
 171:     if score < 40:
 172:         return "haiku"    # Simple: single-file edits, <0.5h, low complexity
 173:     elif score < 70:
 174:         return "sonnet"   # Medium: multi-file, 0.5-2h, some judgment
 175:     else:
 176:         return "opus"     # High: architectural, >2h, critical decisions
 177: ```
 178: 
 179: **Model Assignment Report**:
 180: ```
 181: üß† Model Assignment (Auto-Selected)
 182: ============================================================
 183: PRP-A: Tool Deny List Implementation
 184:   Complexity: low | Hours: 0.25-0.33 | Files: 1 | Phases: 3
 185:   Score: 37/100 ‚Üí Model: haiku ‚úì
 186:   Rationale: Simple JSON edit, single file, no architectural decisions
 187:   Calculation: (0.5*40) + (0.29*10) + (1*5) + (3*3) = 20+2.9+5+9 = 37
 188: 
 189: PRP-B: Tool Usage Guide Creation
 190:   Complexity: low | Hours: 0.33-0.42 | Files: 1 | Phases: 3
 191:   Score: 38/100 ‚Üí Model: haiku ‚úì
 192:   Rationale: Straightforward doc creation, clear structure
 193:   Calculation: (0.5*40) + (0.38*10) + (1*5) + (3*3) = 20+3.8+5+9 = 38
 194: 
 195: PRP-C: Worktree Migration
 196:   Complexity: medium | Hours: 0.42-0.50 | Files: 3 | Phases: 3
 197:   Score: 69/100 ‚Üí Model: sonnet ‚úì
 198:   Rationale: Multi-file, doc structuring requires judgment
 199:   Calculation: (1.0*40) + (0.46*10) + (3*5) + (3*3) = 40+4.6+15+9 = 69
 200: 
 201: ============================================================
 202: Thresholds: Haiku <40, Sonnet 40-69, Opus ‚â•70
 203: Cost estimate: $0.05 (vs $0.25 all-sonnet = 80% savings)
 204: ```
 205: 
 206: ### 2. Pre-Flight Validation (Sequential)
 207: **Time**: ~10-30 seconds depending on PRP count
 208: 
 209: For each PRP:
 210: - ‚úì Validate PRP file exists and is readable
 211: - ‚úì Parse YAML headers (extract stage, worktree_path, conflict_potential)
 212: - ‚úì Check git worktree availability (if worktree_path specified)
 213: - ‚úì Verify no conflicting worktrees already active
 214: - ‚úì Run health check: `mcp__syntropy__healthcheck(detailed=True)`
 215: - ‚úì Estimate total execution time from PRP metadata
 216: 
 217: **Validation Report**:
 218: ```
 219: üìã Pre-Flight Validation
 220: ============================================================
 221: PRPs to execute: 3
 222: Parallelism: 3 (max)
 223: Model assignment: auto (haiku: 2, sonnet: 1)
 224: Total estimated time: 20-25 minutes (45m sequential, 55% savings)
 225: 
 226: PRP-A: Tool Deny List Implementation [HAIKU]
 227:   ‚úì File exists: PRPs/feature-requests/PRP-A-tool-deny-list.md
 228:   ‚úì Stage: stage-1-parallel
 229:   ‚úì Worktree: ../ctx-eng-plus-prp-a (available)
 230:   ‚úì Conflict potential: MEDIUM
 231:   ‚è± Estimated: 15-20 minutes
 232: 
 233: PRP-B: Tool Usage Guide Creation [HAIKU]
 234:   ‚úì File exists: PRPs/feature-requests/PRP-B-tool-usage-guide.md
 235:   ‚úì Stage: stage-1-parallel
 236:   ‚úì Worktree: ../ctx-eng-plus-prp-b (available)
 237:   ‚úì Conflict potential: NONE
 238:   ‚è± Estimated: 20-25 minutes
 239: 
 240: PRP-C: Worktree Migration [SONNET]
 241:   ‚úì File exists: PRPs/feature-requests/PRP-C-gitbutler-worktree-migration.md
 242:   ‚úì Stage: stage-1-parallel
 243:   ‚úì Worktree: ../ctx-eng-plus-prp-c (available)
 244:   ‚úì Conflict potential: LOW
 245:   ‚è± Estimated: 25-30 minutes
 246: 
 247: ‚úÖ All validations passed
 248: üöÄ Ready to execute
 249: ============================================================
 250: ```
 251: 
 252: **Abort conditions**:
 253: - ‚ùå Any PRP file not found
 254: - ‚ùå Conflicting stages (e.g., stage-1 + stage-2 PRPs)
 255: - ‚ùå Worktree path conflicts
 256: - ‚ùå MCP server health check failed
 257: - ‚ùå Git repo not clean (uncommitted changes)
 258: 
 259: ### 3. Create Git Worktrees (Sequential)
 260: **Time**: ~5-10 seconds
 261: 
 262: If PRPs specify `worktree_path` in YAML headers:
 263: ```bash
 264: git worktree add ../ctx-eng-plus-prp-a -b prp-a-tool-deny
 265: git worktree add ../ctx-eng-plus-prp-b -b prp-b-usage-guide
 266: git worktree add ../ctx-eng-plus-prp-c -b prp-c-worktree-migration
 267: ```
 268: 
 269: **Output**:
 270: ```
 271: üå≥ Creating worktrees...
 272:   ‚úì Created: ../ctx-eng-plus-prp-a (branch: prp-a-tool-deny)
 273:   ‚úì Created: ../ctx-eng-plus-prp-b (branch: prp-b-usage-guide)
 274:   ‚úì Created: ../ctx-eng-plus-prp-c (branch: prp-c-worktree-migration)
 275: ```
 276: 
 277: ### 4. Launch Parallel Execution (Parallel)
 278: **Time**: Variable (depends on PRP complexity)
 279: 
 280: **Delegates to `/execute-prp` via Task agents** (in single message with multiple Task calls):
 281: 
 282: ```python
 283: # Parallel agent launch (sent in single message)
 284: # Agent A: PRP-A (Haiku)
 285: Task(
 286:   subagent_type="general-purpose",
 287:   model="haiku",  # Auto-assigned based on complexity score
 288:   description="Execute PRP-A",
 289:   prompt=f"""
 290: You are a PRP execution agent. Execute the following PRP using /execute-prp logic.
 291: 
 292: **PRP**: PRPs/feature-requests/PRP-A-tool-deny-list.md
 293: **Worktree**: /Users/bprzybysz/nc-src/ctx-eng-plus-prp-a
 294: **Branch**: prp-a-tool-deny
 295: **Model**: haiku (auto-assigned: complexity score 25/100)
 296: 
 297: **Execution Protocol**:
 298: 1. Change working directory to worktree: `cd /Users/bprzybysz/nc-src/ctx-eng-plus-prp-a`
 299: 2. Read PRP file completely to understand all implementation steps
 300: 3. Execute /execute-prp logic (phases, validation gates, self-healing, checkpoints):
 301:    - Parse implementation blueprint (extract phases)
 302:    - For each phase:
 303:      a. Execute implementation steps
 304:      b. Run L1 validation (syntax & style)
 305:      c. Run L2 validation (unit tests)
 306:      d. Run L3 validation (integration tests)
 307:      e. Run L4 validation (pattern conformance, drift <30%)
 308:      f. Create checkpoint: git commit
 309:    - Self-heal L1-L2 errors (max 3 attempts)
 310:    - Escalate if persistent/architectural/security errors
 311: 4. Commit all changes to branch with message format: "PRP-A: <summary>"
 312: 5. Return execution report (see format below)
 313: 
 314: **Health Check Protocol** (output every 5 minutes):
 315: ```
 316: HEALTH:OK
 317: HEALTH:ERROR:<reason>
 318: ```
 319: 
 320: **Progress Updates** (output on phase completion):
 321: ```
 322: STATUS:PHASE_COMPLETE:1/3
 323: STATUS:VALIDATION:L2
 324: STATUS:SELF_HEAL:attempt_2
 325: ```
 326: 
 327: **Completion Signal**:
 328: ```
 329: STATUS:COMPLETE:10/10        # Success (confidence score)
 330: STATUS:FAILED:L3_timeout     # Failure reason
 331: STATUS:PARTIAL:2/3           # Partial completion
 332: ```
 333: 
 334: **Return Format** (final report):
 335: {{
 336:   "prp_id": "PRP-A",
 337:   "status": "SUCCESS|FAILED|PARTIAL",
 338:   "phases_completed": 3,
 339:   "phases_total": 3,
 340:   "confidence_score": 10,
 341:   "validation_results": {{
 342:     "L1": {{"passed": true, "attempts": 1}},
 343:     "L2": {{"passed": true, "attempts": 1}},
 344:     "L3": {{"passed": true, "attempts": 1}},
 345:     "L4": {{"passed": true, "drift_score": 4.2}}
 346:   }},
 347:   "self_heals": 0,
 348:   "commit_hash": "ab3118f",
 349:   "execution_time": "6m 12s",
 350:   "files_modified": [".claude/settings.local.json"],
 351:   "errors": []
 352: }}
 353: 
 354: **Error Handling**:
 355: - L1-L2: Attempt self-healing (max 3 attempts per error)
 356: - L3-L4: Escalate, no auto-healing
 357: - Persistent errors (same error 3x): Escalate
 358: - Architectural errors: Escalate immediately
 359: - Security errors: Escalate immediately, DO NOT auto-fix
 360: 
 361: **Constraints**:
 362: - Work only in worktree, never touch main repo
 363: - All validation gates must pass (L1-L4)
 364: - Drift score must be <30%
 365: - Create checkpoint (git commit) after each phase
 366: """
 367: )
 368: 
 369: # Agent B: PRP-B (Haiku)
 370: Task(
 371:   subagent_type="general-purpose",
 372:   model="haiku",
 373:   description="Execute PRP-B",
 374:   prompt="<same structure as PRP-A, with PRP-B details>"
 375: )
 376: 
 377: # Agent C: PRP-C (Sonnet - higher complexity)
 378: Task(
 379:   subagent_type="general-purpose",
 380:   model="sonnet",
 381:   description="Execute PRP-C",
 382:   prompt="<same structure as PRP-A, with PRP-C details>"
 383: )
 384: ```
 385: 
 386: **Key Points**:
 387: - **No logic duplication**: Agents follow /execute-prp protocol (phases, validation, self-healing)
 388: - **Model assignment**: Auto-selected based on complexity analysis (Step 1)
 389: - **Health monitoring**: Agents output health signals every 5 minutes
 390: - **Parallel launch**: All Task calls in single message for true parallelism
 391: 
 392: ### 5. Monitor Execution (Git Log Polling)
 393: **Time**: Continuous during execution
 394: 
 395: **Monitoring Mechanism**: Poll git logs every 60 seconds for checkpoint commits
 396: 
 397: **Polling Logic**:
 398: ```bash
 399: # For each worktree
 400: for worktree in worktrees:
 401:     latest_commit=$(git -C $worktree log -1 --oneline)
 402:     commit_time=$(git -C $worktree log -1 --format=%ct)
 403:     current_time=$(date +%s)
 404:     age=$((current_time - commit_time))
 405: 
 406:     if [ $age -lt 300 ]; then
 407:         echo "HEALTHY: Latest commit ${age}s ago"
 408:     elif [ $age -lt 600 ]; then
 409:         echo "WARNING: Last commit ${age}s ago (may be stalled)"
 410:     else
 411:         echo "STALLED: No commits for ${age}s (likely hung)"
 412:     fi
 413: done
 414: ```
 415: 
 416: **Monitoring Dashboard** (updates every 60 seconds):
 417: ```
 418: üìä Batch Execution Status (Updated: 10:45:23)
 419: ============================================================
 420: Elapsed: 12m 34s / Estimated: 45m (28% complete)
 421: 
 422: PRP-A: Tool Deny List Implementation [HEALTHY]
 423:   Last commit: 2m ago "Phase 3: Validation complete"
 424:   Branch: prp-a-tool-deny-list
 425:   Status: Likely completing final phase
 426: 
 427: PRP-B: Tool Usage Guide Creation [HEALTHY]
 428:   Last commit: 1m ago "Phase 2: Implementation complete"
 429:   Branch: prp-b-tool-usage-guide
 430:   Status: Moving to Phase 3
 431: 
 432: PRP-C: Worktree Migration [WARNING]
 433:   Last commit: 8m ago "Phase 1: Preparation complete"
 434:   Branch: prp-c-worktree-migration
 435:   Status: May be stalled on Phase 2 (long-running step)
 436: 
 437: ============================================================
 438: Active: 3 | HEALTHY: 2 | WARNING: 1 | STALLED: 0
 439: Timeout: 60m per PRP (fallback if no commits for >10m)
 440: ```
 441: 
 442: **Health Status Criteria**:
 443: - **HEALTHY**: Last commit <5m ago
 444: - **WARNING**: Last commit 5-10m ago (may be long-running phase)
 445: - **STALLED**: Last commit >10m ago (likely hung, check agent timeout)
 446: - **FAILED**: Agent returned error or timeout exceeded
 447: 
 448: **Stall Handling**:
 449: ```
 450: ‚ö†Ô∏è PRP-B stalled (no commits for 12m)
 451: Actions:
 452:   1. Check Task agent timeout (60m default)
 453:   2. Agent still running ‚Üí wait for timeout
 454:   3. Agent timed out ‚Üí mark FAILED, preserve worktree
 455:   4. Review partial work: cd ../ctx-eng-plus-prp-b && git log
 456: 
 457: Note: Cannot forcibly abort Task agents mid-execution
 458: Rely on Task timeout mechanism for automatic abort
 459: ```
 460: 
 461: ### 6. Aggregate Results (Sequential)
 462: **Time**: ~10-30 seconds
 463: 
 464: After all agents complete (or timeout):
 465: 
 466: ```python
 467: # Collect results from all agents
 468: results = {
 469:   "PRP-A": agent_a.result,
 470:   "PRP-B": agent_b.result,
 471:   "PRP-C": agent_c.result
 472: }
 473: 
 474: # Calculate aggregate metrics
 475: total_phases = sum(r.phases_completed for r in results.values())
 476: total_errors = sum(len(r.errors) for r in results.values())
 477: avg_confidence = mean(r.confidence_score for r in results.values())
 478: ```
 479: 
 480: **Aggregate Report**:
 481: ```
 482: üìä Batch Execution Complete
 483: ============================================================
 484: Total Time: 18m 42s (estimated: 45m, 58% faster)
 485: PRPs Executed: 3 | Succeeded: 3 | Failed: 0
 486: 
 487: PRP-A: Tool Deny List Implementation [‚úÖ SUCCESS]
 488:   Phases: 3/3 completed
 489:   Validation: L1-L4 passed
 490:   Confidence: 10/10
 491:   Commit: ab3118f "Add 55 MCP tools to deny list, clean duplicates"
 492:   Execution time: 6m 12s
 493: 
 494: PRP-B: Tool Usage Guide Creation [‚úÖ SUCCESS]
 495:   Phases: 3/3 completed
 496:   Validation: L1-L4 passed
 497:   Confidence: 10/10
 498:   Commit: 43051cb "Create comprehensive tool usage guide"
 499:   Execution time: 8m 35s
 500: 
 501: PRP-C: Worktree Migration [‚úÖ SUCCESS]
 502:   Phases: 3/3 completed
 503:   Validation: L1-L4 passed
 504:   Confidence: 9/10 (1 self-heal: remove duplicate hooks)
 505:   Commit: 388508b "Migrate from GitButler to git worktree documentation"
 506:   Execution time: 10m 18s
 507: 
 508: ============================================================
 509: Aggregate Metrics:
 510:   Total phases: 9/9
 511:   Avg confidence: 9.7/10
 512:   Total errors: 0
 513:   Self-heals: 1
 514:   Time savings: 26m 18s (58%)
 515: 
 516: ‚úÖ All PRPs executed successfully
 517: ```
 518: 
 519: ### 7. Merge Worktrees (Sequential)
 520: **Time**: ~30-60 seconds
 521: 
 522: If PRPs executed in worktrees, merge in `merge_order` from YAML headers:
 523: 
 524: ```bash
 525: # Switch to main branch
 526: git checkout main
 527: 
 528: # Merge in order (A ‚Üí B ‚Üí C)
 529: git merge prp-a-tool-deny --no-ff -m "Merge PRP-A: Tool Deny List Implementation"
 530: git merge prp-b-usage-guide --no-ff -m "Merge PRP-B: Tool Usage Guide Creation"
 531: git merge prp-c-worktree-migration --no-ff -m "Merge PRP-C: GitButler to Worktree Migration"
 532: ```
 533: 
 534: **Conflict Detection**:
 535: ```
 536: ‚ö†Ô∏è Merge conflict detected: PRP-D
 537: File: .claude/settings.local.json
 538: Conflict markers found at lines 145-152
 539: 
 540: Conflict Resolution Required:
 541:   1. Read file to view conflict markers (<<<<<<< HEAD)
 542:   2. Decide on resolution strategy (keep both, prefer incoming, prefer current)
 543:   3. Use Edit tool to remove markers and merge changes
 544:   4. Stage resolved file: git add .claude/settings.local.json
 545:   5. Complete merge: git commit -m "Merge PRP-D: Resolve settings conflict"
 546: 
 547: Pausing batch execution until conflict resolved.
 548: ```
 549: 
 550: **If `--continue-on-error` flag set**:
 551: - Skip failed PRP merge
 552: - Log conflict details
 553: - Continue with remaining PRPs
 554: 
 555: ### 8. Cleanup (Sequential)
 556: **Time**: ~5-10 seconds
 557: 
 558: ```bash
 559: # Remove worktrees
 560: git worktree remove ../ctx-eng-plus-prp-a
 561: git worktree remove ../ctx-eng-plus-prp-b
 562: git worktree remove ../ctx-eng-plus-prp-c
 563: 
 564: # Prune stale references
 565: git worktree prune
 566: 
 567: # Update context (sync PRPs with codebase)
 568: cd tools && uv run ce update-context
 569: ```
 570: 
 571: **Output**:
 572: ```
 573: üßπ Cleanup complete
 574:   ‚úì Removed 3 worktrees
 575:   ‚úì Pruned stale references
 576:   ‚úì Updated context (drift: 3.2%)
 577: ```
 578: 
 579: ## Options
 580: 
 581: | Flag | Description | Default |
 582: |------|-------------|---------|
 583: | `--batch <N>` | Execute all PRPs in batch N (stages sequentially) | `none` |
 584: | `--stage <N>` | Execute specific stage only (requires --batch) | `all` |
 585: | `--resume` | Resume from last checkpoint (requires --batch) | `false` |
 586: | `--no-auto-review` | Disable automatic execution review after each stage | `false` (review enabled by default for --batch) |
 587: | `--model <sonnet\|haiku\|opus>` | Model for subagents (overrides auto-selection) | `auto` |
 588: | `--max-parallel <N>` | Max concurrent PRPs within stage | `unlimited` |
 589: | `--dry-run` | Parse PRPs without execution | `false` |
 590: | `--continue-on-error` | Don't abort if one PRP fails | `false` |
 591: | `--no-merge` | Skip worktree merge step | `false` |
 592: | `--no-cleanup` | Keep worktrees after execution | `false` |
 593: | `--timeout <minutes>` | Max execution time per PRP | `60` |
 594: | `--health-check-interval <seconds>` | Health check frequency | `30` |
 595: | `--json` | Output results as JSON | `false` |
 596: 
 597: ## Model Selection Guidelines
 598: 
 599: **Default Behavior**: Automatic model assignment based on complexity analysis (see Step 1)
 600: 
 601: The batch executor analyzes each PRP and assigns the optimal model:
 602: - **Haiku**: Score <40 (simple, single-file, <0.5h)
 603: - **Sonnet**: Score 40-69 (medium, multi-file, 0.5-2h)
 604: - **Opus**: Score ‚â•70 (complex, architectural, >2h)
 605: 
 606: ### When to Override with `--model` Flag
 607: 
 608: **Scenario 1: Force Haiku (maximize cost savings)**
 609: ```bash
 610: /batch-exe-prp --model haiku PRP-A PRP-B PRP-C
 611: ```
 612: 
 613: **Use when**:
 614: - All PRPs are simple (you've verified manually)
 615: - Cost is priority over execution quality
 616: - You're willing to manually fix if Haiku makes mistakes
 617: 
 618: **Risk**: Haiku may struggle with:
 619: - Ambiguous requirements (needs clarification)
 620: - Multi-file coordination (may miss dependencies)
 621: - Complex judgment calls (doc structure, error handling)
 622: 
 623: **Example**: 3 simple PRPs (JSON edits, straightforward docs)
 624: - Auto (Haiku: 2, Sonnet: 1): ~$0.05
 625: - Forced Haiku: ~$0.03 (40% cheaper, but higher risk)
 626: 
 627: **Scenario 2: Force Sonnet (reliability priority)**
 628: ```bash
 629: /batch-exe-prp --model sonnet PRP-A PRP-B PRP-C
 630: ```
 631: 
 632: **Use when**:
 633: - Some PRPs auto-assigned Haiku, but you want consistency
 634: - You don't trust Haiku with your codebase
 635: - Budget allows, prefer reliability
 636: 
 637: **Benefit**: Higher success rate, fewer escalations, better judgment
 638: 
 639: **Example**: 3 mixed PRPs (auto: Haiku: 2, Sonnet: 1)
 640: - Auto: ~$0.05 (optimal)
 641: - Forced Sonnet: ~$0.25 (5x cost, minimal quality gain for simple PRPs)
 642: 
 643: **Scenario 3: Force Opus (critical changes)**
 644: ```bash
 645: /batch-exe-prp --model opus PRP-D PRP-E
 646: ```
 647: 
 648: **Use when**:
 649: - Database migrations, security patches, infrastructure changes
 650: - Confidence score must be 10/10
 651: - Rollback would be costly/dangerous
 652: 
 653: **Cost**: ~$1.50-3.00 per PRP (10-15x more than Sonnet)
 654: 
 655: ### Recommendation
 656: 
 657: **Trust auto-selection** unless you have specific reasons:
 658: - ‚úÖ Optimizes cost/quality trade-off per PRP
 659: - ‚úÖ Analyzes complexity objectively (not based on gut feeling)
 660: - ‚úÖ Saves 50-80% vs all-Sonnet (typical batch)
 661: 
 662: **Override only when**:
 663: - üîß You've manually reviewed all PRPs and disagree with assignments
 664: - üîß You have budget constraints (force Haiku)
 665: - üîß You have reliability requirements (force Sonnet/Opus)
 666: 
 667: ## Monitoring Protocol
 668: 
 669: ### Real-Time Status Updates
 670: 
 671: **Agent Health Signals** (every 30 seconds):
 672: ```
 673: HEALTH:OK                           # Agent running normally
 674: HEALTH:ERROR:timeout                # Validation timeout
 675: HEALTH:ERROR:import_error:foo.py    # Import error detected
 676: ```
 677: 
 678: **Progress Updates** (every phase completion):
 679: ```
 680: STATUS:PHASE_COMPLETE:1/3           # Phase 1 of 3 done
 681: STATUS:VALIDATION:L2                # Running L2 validation
 682: STATUS:SELF_HEAL:attempt_2          # Self-healing (attempt 2/3)
 683: ```
 684: 
 685: **Completion Signals**:
 686: ```
 687: STATUS:COMPLETE:10/10               # Success (confidence: 10/10)
 688: STATUS:FAILED:L3_timeout            # Failed at L3 validation
 689: STATUS:PARTIAL:2/3                  # Partial (2 of 3 phases done)
 690: ```
 691: 
 692: ### Error Aggregation
 693: 
 694: All errors logged to `.ce/batch-execution-{timestamp}.log`:
 695: 
 696: ```
 697: [2025-10-29 10:45:23] PRP-A | ERROR | L2 | ImportError: No module named 'foo'
 698: [2025-10-29 10:45:25] PRP-A | HEAL  | L2 | Added import foo at line 12
 699: [2025-10-29 10:45:30] PRP-A | OK    | L2 | Passed after 1 self-heal
 700: [2025-10-29 10:52:18] PRP-B | ERROR | L4 | Drift score 35% exceeds threshold
 701: [2025-10-29 10:52:20] PRP-B | ABORT | L4 | User acceptance required
 702: ```
 703: 
 704: ### Escalation Triggers
 705: 
 706: **Immediate Escalation** (pause batch execution):
 707: 1. **Security Error**: CVE detected, credentials exposed
 708: 2. **Critical Failure**: Agent crashed, worktree corrupted
 709: 3. **Merge Conflict**: Automatic merge failed (see Conflict Resolution)
 710: 4. **Drift Spike**: L4 drift >30% (user acceptance required)
 711: 5. **Resource Exhaustion**: Disk full, memory exhausted
 712: 
 713: **Deferred Escalation** (complete batch, report at end):
 714: 1. **Low Confidence**: Confidence score <7/10
 715: 2. **Self-Heal Excessive**: >5 self-healing attempts
 716: 3. **Partial Success**: Some phases completed, others skipped
 717: 
 718: ## Conflict Resolution
 719: 
 720: When merge conflicts occur:
 721: 
 722: **Step 1: Detect Conflict**
 723: ```bash
 724: git merge prp-d-command-perms --no-ff
 725: # Auto-merging .claude/settings.local.json
 726: # CONFLICT (content): Merge conflict in .claude/settings.local.json
 727: # Automatic merge failed; fix conflicts and then commit the result.
 728: ```
 729: 
 730: **Step 2: Analyze Conflict**
 731: ```python
 732: # Read conflicted file
 733: Read(file_path=".claude/settings.local.json")
 734: # Look for conflict markers: <<<<<<< HEAD, =======, >>>>>>>
 735: ```
 736: 
 737: **Step 3: Resolution Strategy**
 738: 
 739: **Option A: Keep Both Changes** (most common)
 740: ```python
 741: Edit(
 742:   file_path=".claude/settings.local.json",
 743:   old_string="""<<<<<<< HEAD
 744:   "deny": ["tool-a", "tool-b"]
 745: =======
 746:   "deny": ["tool-c", "tool-d"]
 747: >>>>>>> prp-d-command-perms""",
 748:   new_string="""  "deny": ["tool-a", "tool-b", "tool-c", "tool-d"]"""
 749: )
 750: ```
 751: 
 752: **Option B: Prefer Incoming** (last-merged wins)
 753: ```python
 754: Edit(
 755:   file_path=".claude/settings.local.json",
 756:   old_string="""<<<<<<< HEAD
 757:   "allow": ["Bash(git:*)"]
 758: =======
 759:   "allow": ["Bash(gh:*)"]
 760: >>>>>>> prp-d-command-perms""",
 761:   new_string="""  "allow": ["Bash(gh:*)"]"""  # Prefer incoming
 762: )
 763: ```
 764: 
 765: **Option C: Manual Decision** (conflicting logic)
 766: - User chooses which change to keep
 767: - Update PRP priority/dependency order
 768: - Re-run batch with adjusted order
 769: 
 770: **Step 4: Complete Merge**
 771: ```bash
 772: git add .claude/settings.local.json
 773: git commit -m "Merge PRP-D: Resolve settings conflict (kept both)"
 774: ```
 775: 
 776: ## Error Handling
 777: 
 778: ### Agent Failures
 779: 
 780: **Scenario 1: Agent Timeout**
 781: ```
 782: ‚ùå PRP-B agent timeout (60 minutes exceeded)
 783: Last status: L3 integration tests (running for 45m)
 784: 
 785: Actions taken:
 786:   1. Marked PRP-B as FAILED
 787:   2. Preserved worktree: ../ctx-eng-plus-prp-b
 788:   3. Logged partial results to .ce/batch-execution.log
 789: 
 790: Manual intervention required:
 791:   cd ../ctx-eng-plus-prp-b
 792:   # Review partial work, continue manually if needed
 793: ```
 794: 
 795: **Resolution**:
 796: - Review agent log for bottleneck
 797: - Increase timeout: `--timeout 90`
 798: - Split PRP into smaller phases
 799: 
 800: **Scenario 2: Agent Crash**
 801: ```
 802: ‚ùå PRP-C agent crashed (unexpected error)
 803: Stack trace: [shows error details]
 804: 
 805: Actions taken:
 806:   1. Marked PRP-C as FAILED
 807:   2. Worktree preserved for debugging
 808:   3. Other agents continue execution
 809: 
 810: Manual intervention:
 811:   Review stack trace for root cause
 812:   Check if bug in /execute-prp logic
 813:   Re-run individually: /execute-prp PRP-C
 814: ```
 815: 
 816: ### Validation Failures
 817: 
 818: **Scenario 3: L4 Drift Threshold Exceeded**
 819: ```
 820: ‚ö†Ô∏è PRP-B validation failed: L4 drift 35% (threshold: 30%)
 821: 
 822: User decision required:
 823:   1. [A]ccept drift (update threshold in PRP)
 824:   2. [R]eject and rollback (restore to checkpoint)
 825:   3. [M]anually fix and re-validate
 826: 
 827: Pausing batch execution for user input...
 828: ```
 829: 
 830: **With `--continue-on-error`**:
 831: - Mark PRP-B as PARTIAL
 832: - Skip to next PRP (C)
 833: - Report failure in aggregate results
 834: 
 835: ### Checkpoint & Resume
 836: 
 837: **Purpose**: Save execution state after each stage for recovery from interruptions, failures, or manual pauses.
 838: 
 839: **Checkpoint Format**
 840: 
 841: **Location**: `.ce/tmp/batch-execution-{batch_id}.json`
 842: 
 843: **Example**: `.ce/tmp/batch-execution-34.json`
 844: 
 845: ```json
 846: {
 847:   "batch_id": 34,
 848:   "started": "2025-11-05T10:00:00Z",
 849:   "updated": "2025-11-05T11:30:00Z",
 850:   "master_plan": "PRPs/feature-requests/PRP-34-INITIAL.md",
 851:   "total_stages": 4,
 852:   "last_stage_completed": 2,
 853:   "current_stage": 3,
 854:   "status": "IN_PROGRESS",
 855:   "stages": {
 856:     "1": {
 857:       "status": "COMPLETED",
 858:       "started": "2025-11-05T10:00:00Z",
 859:       "completed": "2025-11-05T10:25:00Z",
 860:       "prps": ["PRP-34.1.1"],
 861:       "results": {
 862:         "PRP-34.1.1": {
 863:           "status": "SUCCESS",
 864:           "phases_completed": 4,
 865:           "confidence_score": 10,
 866:           "commit_hash": "ab3118f",
 867:           "execution_time": "18m 32s"
 868:         }
 869:       },
 870:       "review_status": "PASSED",
 871:       "review_findings": [],
 872:       "merged": true,
 873:       "merge_commits": ["920edd4"]
 874:     },
 875:     "2": {
 876:       "status": "COMPLETED",
 877:       "started": "2025-11-05T10:26:00Z",
 878:       "completed": "2025-11-05T11:12:00Z",
 879:       "prps": ["PRP-34.2.1", "PRP-34.2.2", "PRP-34.2.3", "PRP-34.2.4", "PRP-34.2.5", "PRP-34.2.6"],
 880:       "results": { /* 6 PRP results */ },
 881:       "review_status": "PASSED_WITH_FIXES",
 882:       "review_findings": [
 883:         "Minor: Fixed typo in classification.py line 45",
 884:         "Minor: Added missing type hint in blending.py line 102"
 885:       ],
 886:       "merged": true,
 887:       "merge_commits": ["9af5bcc", "b53e184", "c64f295", "d75e3a6", "e86f4b7", "f97g5c8"]
 888:     },
 889:     "3": {
 890:       "status": "IN_PROGRESS",
 891:       "started": "2025-11-05T11:15:00Z",
 892:       "completed": null,
 893:       "prps": ["PRP-34.3.1", "PRP-34.3.2", "PRP-34.3.3"],
 894:       "results": null,
 895:       "review_status": "PENDING",
 896:       "review_findings": [],
 897:       "merged": false,
 898:       "merge_commits": []
 899:     },
 900:     "4": {
 901:       "status": "PENDING",
 902:       "started": null,
 903:       "completed": null,
 904:       "prps": ["PRP-34.4.1"],
 905:       "results": null,
 906:       "review_status": "PENDING",
 907:       "review_findings": [],
 908:       "merged": false,
 909:       "merge_commits": []
 910:     }
 911:   },
 912:   "aggregate_metrics": {
 913:     "prps_completed": 7,
 914:     "prps_total": 11,
 915:     "stages_completed": 2,
 916:     "total_execution_time": "72m 18s",
 917:     "total_self_heals": 3,
 918:     "review_fixes_applied": 2
 919:   }
 920: }
 921: ```
 922: 
 923: ### When Checkpoints Are Created
 924: 
 925: **Checkpoint triggers** (automatic):
 926: 1. **After stage execution completes** (before review)
 927: 2. **After execution review completes** (with review results)
 928: 3. **After stage merge completes** (with merge commits)
 929: 4. **On manual interruption** (Ctrl+C, user abort)
 930: 5. **On error** (agent failure, validation failure, conflict)
 931: 
 932: **Checkpoint updates** (incremental):
 933: - Only changed fields updated
 934: - Preserves history of completed stages
 935: - Timestamp updated on every write
 936: 
 937: ### Resume Behavior
 938: 
 939: **Automatic detection**:
 940: ```bash
 941: # Resume from last checkpoint
 942: /batch-exe-prp --batch 34 --resume
 943: ```
 944: 
 945: **Resume workflow**:
 946: 1. **Read checkpoint**: `.ce/tmp/batch-execution-34.json`
 947: 2. **Validate checkpoint**:
 948:    - Check batch ID matches
 949:    - Verify master plan exists
 950:    - Confirm PRPs still exist
 951: 3. **Determine resume point**:
 952:    - `current_stage`: Stage that was executing when interrupted
 953:    - `last_stage_completed`: Last fully completed stage
 954:    - Resume from: `last_stage_completed + 1` OR `current_stage` (if partially done)
 955: 4. **Skip completed stages**:
 956:    - Stages 1-2: COMPLETED ‚Üí Skip
 957:    - Stage 3: IN_PROGRESS ‚Üí Check if PRPs executed, review pending
 958:    - Stage 4: PENDING ‚Üí Execute normally
 959: 5. **Resume execution**:
 960:    - If stage 3 PRPs all executed ‚Üí Run review only
 961:    - If stage 3 PRPs partially executed ‚Üí Resume execution, then review
 962:    - If stage 3 not started ‚Üí Execute from beginning
 963: 
 964: **Resume scenarios**:
 965: 
 966: **Scenario 1: Interrupted during execution**
 967: ```
 968: Status: Stage 2 executing, interrupted by Ctrl+C
 969: Checkpoint: stage 2 IN_PROGRESS, last_stage_completed = 1
 970: 
 971: Resume behavior:
 972:   1. Skip Stage 1 (already merged)
 973:   2. Check Stage 2 worktrees for partial progress
 974:   3. If PRPs executed: Run review, merge
 975:   4. If PRPs not executed: Re-execute Stage 2 from start
 976:   5. Continue to Stage 3
 977: ```
 978: 
 979: **Scenario 2: Paused during review (critical issue found)**
 980: ```
 981: Status: Stage 2 execution complete, review found critical issue
 982: Checkpoint: stage 2 COMPLETED (execution), review_status = FAILED
 983: 
 984: Resume behavior:
 985:   1. Skip Stage 1 (already merged)
 986:   2. Show Stage 2 review findings
 987:   3. Ask user: [F]ix manually and continue, [R]e-review, [S]kip stage
 988:   4. If fixed: Re-run review on Stage 2
 989:   5. If passed: Merge Stage 2, continue to Stage 3
 990: ```
 991: 
 992: **Scenario 3: Merge conflict**
 993: ```
 994: Status: Stage 3 execution complete, review passed, merge conflict on PRP-34.3.2
 995: Checkpoint: stage 3 COMPLETED (execution + review), merged = false (conflict)
 996: 
 997: Resume behavior:
 998:   1. Skip Stages 1-2 (already merged)
 999:   2. Detect unresolved merge conflict in Stage 3
1000:   3. Show conflict files, prompt user to resolve
1001:   4. After resolution: Complete Stage 3 merge
1002:   5. Continue to Stage 4
1003: ```
1004: 
1005: **Resume output**:
1006: ```
1007: üìÇ Resuming batch 34 from checkpoint
1008: ============================================================
1009: Checkpoint: .ce/tmp/batch-execution-34.json
1010: Last updated: 2025-11-05T11:30:00Z (15 minutes ago)
1011: 
1012: Progress:
1013:   ‚úÖ Stage 1: Completed (1 PRP, merged)
1014:   ‚úÖ Stage 2: Completed (6 PRPs, merged)
1015:   ‚è∏Ô∏è  Stage 3: Execution completed, review pending
1016:   ‚è≥ Stage 4: Not started
1017: 
1018: Resuming from: Stage 3 execution review
1019: ============================================================
1020: 
1021: Running execution review for Stage 3...
1022: /batch-peer-review --batch 34 --exe --stage 3
1023: ```
1024: 
1025: **Checkpoint Cleanup**
1026: 
1027: **Automatic cleanup** (on successful batch completion):
1028: - Checkpoint kept for 7 days
1029: - Moved to: `.ce/tmp/batch-execution-archive/batch-34-{timestamp}.json`
1030: 
1031: **Manual cleanup**:
1032: ```bash
1033: # Remove checkpoint (abort batch, can't resume)
1034: rm .ce/tmp/batch-execution-34.json
1035: 
1036: # Archive checkpoint (preserve history)
1037: mkdir -p .ce/tmp/batch-execution-archive
1038: mv .ce/tmp/batch-execution-34.json .ce/tmp/batch-execution-archive/
1039: ```
1040: 
1041: ## Performance Metrics
1042: 
1043: ### Sequential vs Parallel
1044: 
1045: **3 PRPs (15m each)**:
1046: - Sequential: 45 minutes
1047: - Parallel (3 agents): ~18 minutes (60% faster)
1048: - Parallel (Haiku): ~15 minutes (67% faster, 90% cheaper)
1049: 
1050: **6 PRPs (15m each)**:
1051: - Sequential: 90 minutes
1052: - Parallel (unlimited): ~20 minutes (78% faster)
1053: - Parallel (--max-parallel 3): ~35 minutes (61% faster)
1054: 
1055: ### Resource Usage
1056: 
1057: **Per Agent** (Sonnet):
1058: - CPU: ~30-50% (1 core)
1059: - Memory: ~500MB-1GB
1060: - Tokens: ~10k-30k per PRP
1061: 
1062: **Total** (3 agents):
1063: - CPU: ~100-150% (1.5 cores)
1064: - Memory: ~1.5-3GB
1065: - Tokens: ~30k-90k total
1066: 
1067: **Recommendation**: Limit to 3-4 parallel agents on typical laptop (4-core, 16GB RAM)
1068: 
1069: ## Output Formats
1070: 
1071: ### Standard Output (default)
1072: 
1073: Human-readable dashboard with progress bars, health status, and aggregate report (shown in sections 4-5 above).
1074: 
1075: ### JSON Output (`--json`)
1076: 
1077: ```json
1078: {
1079:   "success": true,
1080:   "prps_total": 3,
1081:   "prps_succeeded": 3,
1082:   "prps_failed": 0,
1083:   "prps_partial": 0,
1084:   "execution_time": "18m 42s",
1085:   "estimated_time": "45m",
1086:   "time_savings": "26m 18s (58%)",
1087:   "model": "sonnet",
1088:   "max_parallel": 3,
1089:   "results": {
1090:     "PRP-A": {
1091:       "prp_id": "PRP-A",
1092:       "status": "SUCCESS",
1093:       "phases_completed": 3,
1094:       "phases_total": 3,
1095:       "confidence_score": 10,
1096:       "validation_results": {
1097:         "L1": {"passed": true, "attempts": 1},
1098:         "L2": {"passed": true, "attempts": 1},
1099:         "L3": {"passed": true, "attempts": 1},
1100:         "L4": {"passed": true, "drift_score": 4.2}
1101:       },
1102:       "self_heals": 0,
1103:       "commit_hash": "ab3118f",
1104:       "execution_time": "6m 12s",
1105:       "files_modified": [".claude/settings.local.json"],
1106:       "errors": []
1107:     },
1108:     "PRP-B": { /* similar structure */ },
1109:     "PRP-C": { /* similar structure */ }
1110:   },
1111:   "aggregate_metrics": {
1112:     "total_phases": 9,
1113:     "total_phases_completed": 9,
1114:     "avg_confidence_score": 9.7,
1115:     "total_self_heals": 1,
1116:     "total_errors": 0
1117:   },
1118:   "merge_status": {
1119:     "success": true,
1120:     "conflicts": [],
1121:     "commits": [
1122:       "920edd4 Merge PRP-A: Tool Deny List Implementation",
1123:       "9af5bcc Merge PRP-B: Tool Usage Guide Creation",
1124:       "b53e184 Merge PRP-C: GitButler to Worktree Migration"
1125:     ]
1126:   },
1127:   "cleanup_status": {
1128:     "worktrees_removed": 3,
1129:     "context_drift": 3.2
1130:   }
1131: }
1132: ```
1133: 
1134: ## Common Workflows
1135: 
1136: ### Workflow 1: Full Batch Execution (Recommended)
1137: 
1138: Execute entire batch with automatic quality gates:
1139: 
1140: ```bash
1141: # Execute all stages sequentially with auto-review (default)
1142: /batch-exe-prp --batch 34
1143: 
1144: # What happens:
1145: # 1. Discovers all PRP-34.*.* files
1146: # 2. Groups by stage (1, 2, 3, 4)
1147: # 3. For each stage:
1148: #    a. Execute PRPs in parallel
1149: #    b. Run execution review (auto-fix minor issues)
1150: #    c. Merge worktrees
1151: #    d. Save checkpoint
1152: # 4. If interrupted: resume with --resume flag
1153: 
1154: # Time: ~60-90 minutes (for 11-PRP batch)
1155: # Quality: High (peer review after each stage)
1156: ```
1157: 
1158: ### Workflow 2: Stage-by-Stage Manual Execution
1159: 
1160: Execute one stage at a time with manual validation:
1161: 
1162: ```bash
1163: # Stage 1: Foundation
1164: /batch-exe-prp --batch 34 --stage 1
1165: # Manual test, validate output
1166: 
1167: # Stage 2: Core modules (6 PRPs in parallel)
1168: /batch-exe-prp --batch 34 --stage 2
1169: # Manual test, validate integration
1170: 
1171: # Stage 3: Domain strategies
1172: /batch-exe-prp --batch 34 --stage 3
1173: 
1174: # Stage 4: Integration
1175: /batch-exe-prp --batch 34 --stage 4
1176: ```
1177: 
1178: ### Workflow 3: Resume from Interruption
1179: 
1180: Continue batch execution after pause/error:
1181: 
1182: ```bash
1183: # Scenario: Stage 2 execution review found critical issue
1184: # You fixed it manually, now resume
1185: 
1186: # Resume from checkpoint (skips completed stages)
1187: /batch-exe-prp --batch 34 --resume
1188: 
1189: # Reads checkpoint: .ce/tmp/batch-execution-34.json
1190: # Determines: Stage 1 completed, Stage 2 needs re-review
1191: # Runs: Stage 2 execution review ‚Üí merge ‚Üí Stage 3 ‚Üí Stage 4
1192: ```
1193: 
1194: ### Workflow 4: Individual PRPs (Legacy Mode)
1195: 
1196: Execute specific PRPs without batch-awareness:
1197: 
1198: ```bash
1199: # Extract stage-1 PRPs manually
1200: # Execute in parallel
1201: /batch-exe-prp PRP-34.1.1
1202: 
1203: # After completion, proceed to Stage 2
1204: /batch-exe-prp PRP-34.2.1 PRP-34.2.2 PRP-34.2.3 PRP-34.2.4 PRP-34.2.5 PRP-34.2.6
1205: 
1206: # Note: No auto-review, no checkpointing, manual merge management
1207: ```
1208: 
1209: ### Workflow 5: Cost-Optimized Execution
1210: 
1211: Use Haiku for simple PRPs, Sonnet for complex:
1212: 
1213: ```bash
1214: # Force Haiku for simple batch (if auto-selection assigns Sonnet)
1215: /batch-exe-prp --batch 34 --model haiku
1216: 
1217: # Cost savings: ~70% vs all-Sonnet
1218: # Trade-off: Lower quality, may need manual fixes
1219: ```
1220: 
1221: ### Workflow 6: Batch without Auto-Review
1222: 
1223: Execute batch quickly without peer review (risky):
1224: 
1225: ```bash
1226: # Disable auto-review (not recommended)
1227: /batch-exe-prp --batch 34 --no-auto-review
1228: 
1229: # Use when:
1230: # - Already peer-reviewed PRPs manually
1231: # - Low-risk changes (docs, comments)
1232: # - Time-critical deployment
1233: 
1234: # Risk: May merge code with quality issues
1235: ```
1236: 
1237: ### Workflow 7: Dry Run + Review
1238: 
1239: Preview batch execution plan before committing:
1240: 
1241: ```bash
1242: # Dry run: Show stages, PRPs, model assignments
1243: /batch-exe-prp --batch 34 --dry-run
1244: 
1245: # Output:
1246: # - Stage breakdown (which PRPs in each stage)
1247: # - Model assignments (Haiku vs Sonnet per PRP)
1248: # - Estimated time and cost
1249: # - Dependency graph
1250: 
1251: # Review output, adjust PRP complexities if needed
1252: 
1253: # Execute for real
1254: /batch-exe-prp --batch 34
1255: ```
1256: 
1257: ## Troubleshooting
1258: 
1259: ### Issue: "Worktree path conflicts"
1260: 
1261: **Symptom**: Pre-flight validation fails with "worktree path already exists"
1262: 
1263: **Cause**: Previous worktree not cleaned up
1264: 
1265: **Solution**:
1266: ```bash
1267: git worktree list  # Check existing worktrees
1268: git worktree remove ../ctx-eng-plus-prp-a
1269: git worktree prune
1270: ```
1271: 
1272: ### Issue: "Agent stalled (no health signal)"
1273: 
1274: **Symptom**: Agent shows "STALLED" after 2+ minutes
1275: 
1276: **Cause**: Long-running validation (e.g., integration tests)
1277: 
1278: **Solution**: Increase health check interval
1279: ```bash
1280: /batch-exe-prp --health-check-interval 60 PRP-A PRP-B
1281: ```
1282: 
1283: ### Issue: "All agents failed immediately"
1284: 
1285: **Symptom**: All agents marked FAILED within seconds
1286: 
1287: **Cause**: MCP server disconnected or permission denied
1288: 
1289: **Solution**:
1290: ```bash
1291: # Check MCP servers
1292: mcp__syntropy__healthcheck(detailed=True)
1293: 
1294: # Reconnect if needed
1295: /mcp
1296: 
1297: # Verify permissions in .claude/settings.local.json
1298: ```
1299: 
1300: ### Issue: "Merge conflicts on every PRP"
1301: 
1302: **Symptom**: Every merge attempt results in conflicts
1303: 
1304: **Cause**: PRPs modifying same file sections, wrong merge order
1305: 
1306: **Solution**:
1307: 1. Review PRP YAML headers: check `files_modified` and `conflict_potential`
1308: 2. Adjust `merge_order` to sequence conflicting PRPs
1309: 3. Re-run batch with adjusted order
1310: 4. Consider splitting conflicting PRPs into separate batches
1311: 
1312: ### Issue: "Batch not found" or "No PRPs discovered"
1313: 
1314: **Symptom**: `/batch-exe-prp --batch 34` reports "No PRPs found for batch 34"
1315: 
1316: **Cause**: PRPs not in expected location or naming format
1317: 
1318: **Solution**:
1319: ```bash
1320: # Check if master plan exists
1321: ls PRPs/feature-requests/PRP-34-INITIAL.md
1322: 
1323: # Check if batch PRPs exist
1324: ls PRPs/feature-requests/PRP-34.*.md
1325: 
1326: # Verify naming format: PRP-34.1.1.md, PRP-34.2.1.md, etc.
1327: # NOT: PRP-34-1-1.md or PRP34.1.1.md
1328: ```
1329: 
1330: ### Issue: "Checkpoint corrupted" or "Invalid checkpoint format"
1331: 
1332: **Symptom**: `/batch-exe-prp --batch 34 --resume` fails to read checkpoint
1333: 
1334: **Cause**: Checkpoint JSON malformed or manually edited
1335: 
1336: **Solution**:
1337: ```bash
1338: # Validate checkpoint JSON
1339: cat .ce/tmp/batch-execution-34.json | jq .
1340: 
1341: # If invalid: Start fresh (lose resume capability)
1342: rm .ce/tmp/batch-execution-34.json
1343: /batch-exe-prp --batch 34
1344: 
1345: # If valid but wrong state: Manually fix checkpoint
1346: # Example: Set last_stage_completed to correct value
1347: ```
1348: 
1349: ### Issue: "Execution review paused" with no clear reason
1350: 
1351: **Symptom**: Batch pauses after stage execution, says "review found critical issues" but doesn't show what
1352: 
1353: **Cause**: Review findings not displayed, need to check checkpoint
1354: 
1355: **Solution**:
1356: ```bash
1357: # Read checkpoint to see review findings
1358: cat .ce/tmp/batch-execution-34.json | jq '.stages["2"].review_findings'
1359: 
1360: # Output: ["Critical: Logic error in classification.py line 45"]
1361: 
1362: # Fix issue manually, then resume
1363: vim tools/ce/blending/classification.py
1364: # Fix line 45
1365: 
1366: # Resume (will re-run review)
1367: /batch-exe-prp --batch 34 --resume
1368: ```
1369: 
1370: ### Issue: "Stage already completed but PRPs not merged"
1371: 
1372: **Symptom**: Stage marked COMPLETED in checkpoint but changes not in main branch
1373: 
1374: **Cause**: Merge step failed or was skipped, checkpoint not updated
1375: 
1376: **Solution**:
1377: ```bash
1378: # Check if worktrees still exist
1379: git worktree list
1380: 
1381: # If worktrees exist: Manually merge
1382: git checkout main
1383: git merge prp-34-2-1 --no-ff
1384: git merge prp-34-2-2 --no-ff
1385: # ... (merge all stage PRPs)
1386: 
1387: # Update checkpoint manually
1388: # Set stages.2.merged = true
1389: 
1390: # Resume to next stage
1391: /batch-exe-prp --batch 34 --resume
1392: ```
1393: 
1394: ## CLI Command
1395: 
1396: ```bash
1397: # From project root
1398: cd tools
1399: 
1400: # Individual PRPs
1401: uv run ce batch-exe [options] <prp-1> <prp-2> ...
1402: 
1403: # Batch-aware execution
1404: uv run ce batch-exe --batch <N> [options]
1405: 
1406: # Options (same as slash command)
1407: --batch <N>              # Execute all PRPs in batch N
1408: --stage <N>              # Execute specific stage only
1409: --resume                 # Resume from checkpoint
1410: --no-auto-review         # Disable auto-review (not recommended)
1411: --model <sonnet|haiku|opus>  # Override auto-selection
1412: --max-parallel <N>       # Limit parallelism within stage
1413: --dry-run                # Preview execution plan
1414: --continue-on-error      # Don't abort on failure
1415: --no-merge               # Skip merge step
1416: --no-cleanup             # Keep worktrees
1417: --timeout <minutes>      # Max execution time per PRP
1418: --json                   # JSON output
1419: ```
1420: 
1421: ## Implementation Details
1422: 
1423: - **Module**: `tools/ce/batch_execute.py` (to be implemented)
1424: - **Tests**: `tools/tests/test_batch_execute.py` (integration tests)
1425: - **Dependencies**: `execute.py` (PRP execution logic), `core.py` (git operations)
1426: - **Agent Launch**: Uses Task tool with `subagent_type="general-purpose"`
1427: - **Monitoring**: Polling agent output every 30 seconds for health signals
1428: - **Error Recovery**: Preserves worktrees on failure for debugging
1429: 
1430: ### Checkpoint Implementation Requirements
1431: 
1432: **Critical**: When implementing `--batch` mode, checkpoint handling is MANDATORY:
1433: 
1434: 1. **Checkpoint Creation** (after each stage):
1435:    ```python
1436:    import json
1437:    from pathlib import Path
1438: 
1439:    checkpoint_path = Path(".ce/tmp/batch-execution-{batch_id}.json")
1440:    checkpoint_path.parent.mkdir(parents=True, exist_ok=True)
1441: 
1442:    checkpoint = {
1443:        "batch_id": batch_id,
1444:        "started": start_time.isoformat(),
1445:        "updated": datetime.now(timezone.utc).isoformat(),
1446:        "master_plan": master_plan_path,
1447:        "total_stages": len(stages),
1448:        "last_stage_completed": completed_stage_num,
1449:        "current_stage": current_stage_num,
1450:        "status": "IN_PROGRESS",  # or "COMPLETED", "FAILED", "PAUSED"
1451:        "stages": { ... }
1452:    }
1453: 
1454:    with open(checkpoint_path, 'w') as f:
1455:        json.dump(checkpoint, f, indent=2)
1456:    ```
1457: 
1458: 2. **Checkpoint Resume** (on `--resume` flag):
1459:    ```python
1460:    checkpoint_path = Path(f".ce/tmp/batch-execution-{batch_id}.json")
1461: 
1462:    if not checkpoint_path.exists():
1463:        print(f"‚ùå No checkpoint found: {checkpoint_path}")
1464:        print(f"Start fresh: /batch-exe-prp --batch {batch_id}")
1465:        return
1466: 
1467:    with open(checkpoint_path) as f:
1468:        checkpoint = json.load(f)
1469: 
1470:    # Validate checkpoint
1471:    if checkpoint["batch_id"] != batch_id:
1472:        raise ValueError(f"Checkpoint batch ID mismatch")
1473: 
1474:    # Determine resume point
1475:    last_completed = checkpoint["last_stage_completed"]
1476:    resume_from_stage = last_completed + 1
1477: 
1478:    # Skip completed stages, resume from next
1479:    for stage_num in range(resume_from_stage, checkpoint["total_stages"] + 1):
1480:        execute_stage(stage_num)
1481:    ```
1482: 
1483: 3. **Auto-Review Integration** (after stage execution):
1484:    ```python
1485:    # After stage N execution completes
1486:    if not args.no_auto_review:
1487:        print(f"\nüîç Running execution review for Stage {stage_num}...")
1488: 
1489:        # Call /batch-peer-review internally via SlashCommand tool
1490:        review_result = SlashCommand(
1491:            command=f"/batch-peer-review --batch {batch_id} --exe --stage {stage_num}"
1492:        )
1493: 
1494:        # Update checkpoint with review results
1495:        checkpoint["stages"][str(stage_num)]["review_status"] = review_result.status
1496:        checkpoint["stages"][str(stage_num)]["review_findings"] = review_result.findings
1497: 
1498:        # Handle review failures
1499:        if review_result.status == "FAILED":
1500:            print(f"‚ùå Stage {stage_num} execution review FAILED")
1501:            print(f"Findings: {review_result.findings}")
1502:            print(f"\nPausing batch execution.")
1503:            print(f"Fix issues and resume: /batch-exe-prp --batch {batch_id} --resume")
1504: 
1505:            checkpoint["status"] = "PAUSED"
1506:            save_checkpoint(checkpoint)
1507:            return  # Exit, don't proceed to next stage
1508: 
1509:        # Minor issues auto-fixed, continue
1510:        if review_result.fixes_applied:
1511:            print(f"‚úÖ Auto-fixed {len(review_result.fixes_applied)} minor issues")
1512:    ```
1513: 
1514: 4. **Stage Discovery** (for `--batch` mode):
1515:    ```python
1516:    def discover_batch_prps(batch_id):
1517:        """Find all PRPs in batch, group by stage"""
1518: 
1519:        # Find master plan
1520:        master_plan = Path(f"PRPs/feature-requests/PRP-{batch_id}-INITIAL.md")
1521:        if not master_plan.exists():
1522:            raise FileNotFoundError(f"Master plan not found: {master_plan}")
1523: 
1524:        # Find all batch PRPs: PRP-34.1.1.md, PRP-34.2.1.md, etc.
1525:        prp_files = list(Path("PRPs/feature-requests").glob(f"PRP-{batch_id}.*.*.md"))
1526: 
1527:        # Parse YAML headers, extract stage field
1528:        stages = {}
1529:        for prp_file in prp_files:
1530:            yaml_header = parse_yaml_header(prp_file)
1531:            stage = yaml_header.get("stage", 1)  # Default to stage 1
1532: 
1533:            if stage not in stages:
1534:                stages[stage] = []
1535:            stages[stage].append(prp_file)
1536: 
1537:        # Sort stages
1538:        return dict(sorted(stages.items()))
1539:    ```
1540: 
1541: 5. **Checkpoint Cleanup** (on success):
1542:    ```python
1543:    # After all stages complete successfully
1544:    checkpoint["status"] = "COMPLETED"
1545:    save_checkpoint(checkpoint)
1546: 
1547:    # Archive checkpoint
1548:    archive_dir = Path(".ce/tmp/batch-execution-archive")
1549:    archive_dir.mkdir(parents=True, exist_ok=True)
1550: 
1551:    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
1552:    archive_path = archive_dir / f"batch-{batch_id}-{timestamp}.json"
1553: 
1554:    checkpoint_path.rename(archive_path)
1555:    print(f"‚úÖ Checkpoint archived: {archive_path}")
1556:    ```
1557: 
1558: ## Related Commands
1559: 
1560: - `/execute-prp <prp>` - Execute single PRP
1561: - `/generate-prp <initial>` - Generate PRP with parallel metadata
1562: - `/peer-review exe <prp>` - Review executed PRP quality
1563: - `ce prp restore <prp-id> [phase]` - Rollback PRP to checkpoint
1564: 
1565: ## Security Notes
1566: 
1567: 1. **Isolation**: Each agent works in isolated worktree, cannot affect main repo
1568: 2. **Permissions**: Agents inherit same permissions as main session
1569: 3. **Secrets**: Never log credentials or API keys in batch execution logs
1570: 4. **Rollback**: All changes committed to branches, easy rollback via `git reset`
1571: 
1572: ## Future Enhancements
1573: 
1574: - **Auto-retry**: Retry failed PRPs with adjusted parameters
1575: - **Smart scheduling**: Prioritize high-conflict PRPs first
1576: - **Resource management**: Auto-throttle based on CPU/memory usage
1577: - **Dependency resolution**: Parse PRP dependencies, auto-sequence
1578: - **Web UI**: Real-time dashboard with progress graphs
</file>

<file path=".claude/commands/peer-review.md">
  1: # /peer-review - Context-Naive Peer Review Command
  2: 
  3: Perform context-naive peer review of specified PRP work with optional execution review.
  4: 
  5: ## Usage
  6: 
  7: ```bash
  8: /peer-review [prp-reference] [exe|execution]
  9: ```
 10: 
 11: ## Parameters
 12: 
 13: ### 1. prp-reference (optional, default: latest)
 14: Specify which PRP to review using one of these formats:
 15: 
 16: - **PRP ID**: `PRP-8.8` or `8.8` or `34.2.1` (batch notation)
 17: - **File path**: `context-engineering/PRPs/PRP-8.8-web-ui-ux-improvements.md`
 18: - **Natural language**: `"shift pattern logic"` or `"web ui ux improvements"`
 19: - **Keyword**: `latest` (most recent PRP from conversation)
 20: - **Batch context**: If batch active, searches within batch PRPs first
 21: 
 22: ### 2. exe|execution (optional)
 23: Control review mode:
 24: 
 25: - **Absent**: Document review only (default mode)
 26: - **exe**: Review PRP execution results (assumes PRP already executed)
 27: - **execution**: Alias for `exe` (same behavior, clearer intent)
 28: 
 29: ## Batch Context Integration
 30: 
 31: When working within a batch workflow, `/peer-review` automatically understands batch context:
 32: 
 33: ```bash
 34: # After /batch-gen-prp or during /batch-exe-prp
 35: /peer-review 34.2.1              # Review specific PRP from batch 34
 36: /peer-review latest              # Review latest PRP in batch context
 37: /peer-review "classification"    # Search within batch 34 PRPs first
 38: 
 39: # Execution review within batch
 40: /peer-review 34.2.1 exe          # Review execution of one PRP in batch
 41: /peer-review latest execution    # Review execution of latest PRP in batch
 42: ```
 43: 
 44: **Batch Context Detection**:
 45: - Checks for active batch in conversation (from `/batch-gen-prp` or `/batch-exe-prp`)
 46: - Prioritizes PRPs within active batch when searching
 47: - Falls back to all PRPs if not found in batch
 48: - **For full batch review**: Use `/batch-peer-review` for systematic review of all PRPs with inter-PRP consistency checks
 49: 
 50: **When to Use Single vs Batch Review**:
 51: - **Use `/peer-review`** (this command): Deep dive into one specific PRP, detailed analysis, spot checks
 52: - **Use `/batch-peer-review`**: Systematic review of all PRPs, inter-PRP consistency, parallel efficiency
 53: 
 54: ## Examples
 55: 
 56: ```bash
 57: # Review latest PRP document (no execution review)
 58: /peer-review
 59: 
 60: # Review specific PRP by ID
 61: /peer-review PRP-8.8
 62: 
 63: # Review specific PRP in batch (batch notation)
 64: /peer-review 34.2.1
 65: 
 66: # Find PRP by natural language description
 67: /peer-review "shift pattern logic"
 68: 
 69: # Review execution of already-executed PRP
 70: /peer-review PRP-8.8 exe
 71: 
 72: # Review execution of batch PRP
 73: /peer-review 34.2.1 execution
 74: 
 75: # Review execution of most recent PRP
 76: /peer-review latest execution
 77: 
 78: # Batch workflow examples
 79: /peer-review 34.2.1              # Document review (before execution)
 80: /peer-review 34.2.1 exe          # Execution review (after execution)
 81: /peer-review latest              # Latest PRP in active batch
 82: ```
 83: 
 84: ## Review Process
 85: 
 86: ### Phase 1: Document Review (Always Performed)
 87: 
 88: 1. **Locate PRP**: Find PRP file from reference parameter
 89: 2. **Read Fresh**: Read PRP as standalone artifact, ignoring generation conversation
 90: 3. **Evaluate Quality**:
 91:    - ‚úÖ Completeness: All sections present and detailed?
 92:    - ‚úÖ Clarity: Technical requirements unambiguous?
 93:    - ‚úÖ Feasibility: Implementation approach sound?
 94:    - ‚úÖ Testability: Acceptance criteria measurable?
 95:    - ‚úÖ Edge Cases: Potential issues identified?
 96:    - ‚úÖ Alignment with CLAUDE.md guidelines.
 97:    - ‚úÖ Existing patterns (ce examples) and architecture respectation
 98:    - ‚úÖ Existing code reuse
 99:    - ‚úÖ Check also serena memories for more guidelines
100: 
101: 4. **Provide Recommendations**: Actionable improvements
102: 5. **Apply Improvements**: Update PRP unless profound questions arise
103: 6. **Document Review**: Add review notes to PRP appendix
104: 
105: ### Phase 2: Execution Review (Only if exe|execution flag present)
106: 
107: **Prerequisite**: PRP must already be executed via `/execute-prp` or manual implementation
108: 
109: 1. **Read PRP Requirements**: Review what was supposed to be implemented
110: 2. **Read Changed Files Fresh**: Read implementation as standalone artifacts, ignoring implementation conversation
111: 3. **Evaluate Execution**:
112:    - ‚úÖ Implementation matches PRP requirements?
113:    - ‚úÖ Code quality meets project standards?
114:    - ‚úÖ Acceptance criteria satisfied?
115:    - ‚úÖ Unintended side effects detected?
116:    - ‚úÖ Edge cases handled?
117:    - ‚úÖ No implementation violating guidelines specified in Document Review (CLAUDE.md)
118:    - ‚úÖ No implementation violating existing patterns (ce examples) and architecture respectation
119:    - ‚úÖ No implementation duplicating existing code (should extend existing code)
120:    - ‚úÖ Check also serena memories for more guidelines not to violate
121: 
122: 4. **Provide Recommendations**: Actionable fixes
123: 5. **Apply Fixes**: Update code unless profound questions arise
124: 6. **Document Execution Review**: Add notes to PRP execution section
125: 
126: ## Output Format
127: 
128: ### Document Review Output
129: 
130: ```markdown
131: ## Context-Naive Peer Review: Document
132: 
133: **PRP**: PRP-8.8-web-ui-ux-improvements.md
134: **Reviewed**: 2025-10-02T19:55:00Z
135: 
136: ### Findings
137: - ‚úÖ Strength 1: Clear structure with before/after code examples
138: - ‚úÖ Strength 2: Specific line numbers for all changes
139: - ‚ö†Ô∏è Issue 1: React Hooks violation in Change #4
140: - ‚ö†Ô∏è Issue 2: className inconsistency in documentation
141: 
142: ### Recommendations Applied
143: 1. Fixed React Hooks pattern: Moved to Set-based state management
144: 2. Removed gap-2 from button className
145: 3. Added explicit useState import statement
146: 4. Fixed target state diagram to show flat ZIP structure
147: 
148: ### Questions for User
149: (None - all issues resolved)
150: ```
151: 
152: ### Execution Review Output (if exe|execution flag used)
153: 
154: ```markdown
155: ## Context-Naive Peer Review: Execution
156: 
157: **PRP**: PRP-8.8-web-ui-ux-improvements.md
158: **Execution Reviewed**: 2025-10-02T20:00:00Z
159: 
160: ### Implementation Findings
161: - ‚úÖ Change #1: ZIP structure simplified correctly (main.py:609)
162: - ‚úÖ Change #2: Section numbering removed (App.tsx:91,100,146)
163: - ‚úÖ Change #3: Download button text updated (App.tsx:202)
164: - ‚ö†Ô∏è Issue: useState import missing from App.tsx
165: - ‚ùå Critical: expandedJobs state not initialized at component level
166: 
167: ### Fixes Applied
168: 1. Added useState import to App.tsx line 2
169: 2. Initialized expandedJobs Set state at component level
170: 3. Tested expanded state persistence during polling
171: 
172: ### Questions for User
173: (None - all issues resolved)
174: ```
175: 
176: ## Context-Naive Definition
177: 
178: **What It Means**:
179: - **IGNORE**: Conversation that generated PRP or implementation code
180: - **USE**: Project context (CLAUDE.md, codebase structure, ce examples, existing PRPs, serena memories)
181: - **GOAL**: Fresh perspective as if first time reading the artifact
182: 
183: **Why It Matters**:
184: - Catches inconsistencies between plan and code
185: - Identifies assumptions made during rapid development
186: - Validates documentation matches implementation
187: - Ensures artifacts are self-documenting
188: 
189: ## Error Handling
190: 
191: ### PRP Not Found
192: ```
193: ‚ùå PRP not found: "shift pattern logic"
194: 
195: Available PRPs matching search:
196: - PRP-5.2: Shift Patterns Logic Implementation
197: - PRP-5.3: Critical Validation Enum Fixes
198: 
199: Please specify: /peer-review PRP-5.2
200: ```
201: 
202: ### Multiple Matches
203: ```
204: ‚ö†Ô∏è Multiple PRPs match "shift pattern":
205: 1. PRP-5.2: Shift Patterns Logic Implementation
206: 2. PRP-6.9.3: Shift Pattern Hour Ranges
207: 
208: Please clarify which PRP to review.
209: ```
210: 
211: ### Execution Review Without Execution
212: ```
213: ‚ö†Ô∏è Execution review requested but PRP not executed: PRP-8.8
214: 
215: Please execute PRP first:
216: /execute-prp PRP-8.8
217: 
218: Then review execution:
219: /peer-review PRP-8.8 exe
220: ```
221: 
222: ## Integration with Workflow
223: 
224: ### Single PRP Workflow
225: 
226: #### After Generate PRP (Document Review)
227: ```bash
228: # Generate PRP
229: /generate-prp "Web UI UX improvements"
230: 
231: # Immediately review document quality
232: /peer-review latest
233: 
234: # Result: PRP improved before any coding starts
235: ```
236: 
237: #### After Execute PRP (Execution Review)
238: ```bash
239: # Execute PRP implementation
240: /execute-prp PRP-8.8
241: 
242: # Review execution results with fresh eyes
243: /peer-review PRP-8.8 execution
244: 
245: # Result: Catches implementation issues vs spec
246: ```
247: 
248: #### Complete Single PRP Workflow
249: ```bash
250: # Step 1: Generate and review PRP document
251: /generate-prp "simplify ZIP structure"
252: /peer-review latest
253: 
254: # Step 2: Execute PRP
255: /execute-prp latest
256: 
257: # Step 3: Review execution
258: /peer-review latest exe
259: 
260: # Result: High-quality PRP + implementation
261: ```
262: 
263: ### Batch PRP Workflow
264: 
265: #### During Batch Execution (Individual PRP Review)
266: ```bash
267: # Generate batch
268: /batch-gen-prp PRP-34-INITIAL.md
269: 
270: # Review individual PRP in batch (if needed)
271: /peer-review 34.2.1              # Review doc before batch execution
272: /peer-review "classification"    # Search by keyword in batch
273: 
274: # Execute batch
275: /batch-exe-prp --batch 34
276: 
277: # Review individual PRP execution (if issues found)
278: /peer-review 34.2.1 exe          # Review execution of one PRP
279: /peer-review latest execution    # Review latest executed PRP in batch
280: ```
281: 
282: #### When to Use Single vs Batch Review
283: 
284: **Use `/peer-review` (single) when**:
285: - Reviewing one specific PRP in detail
286: - Fixing issues in one PRP during batch workflow
287: - Iterating on one PRP before batch execution
288: - Deep dive into one PRP's execution
289: 
290: **Use `/batch-peer-review` (batch) when**:
291: - Reviewing all PRPs in batch systematically
292: - Checking inter-PRP consistency (deps, terminology, file conflicts)
293: - Quality gate before/after batch execution
294: - Parallel review of multiple PRPs
295: 
296: ### Quality Gate Before Merge
297: ```bash
298: # Review completed PRP execution (single)
299: /peer-review PRP-8.8 execution
300: 
301: # Review completed batch execution (batch)
302: /batch-peer-review --batch 34 --exe
303: 
304: # Validates:
305: # - All changes implemented correctly
306: # - No unintended side effects
307: # - Acceptance criteria met
308: # - (Batch only) Inter-PRP integration correct
309: ```
310: 
311: ## Command Implementation
312: 
313: This command should:
314: 1. **Detect Batch Context**:
315:    - Search conversation for recent `/batch-gen-prp` or `/batch-exe-prp` calls (last 10 messages)
316:    - Extract batch ID from command: `/batch-gen-prp PRP-34-INITIAL.md` ‚Üí batch 34
317:    - OR detect batch notation in PRP reference: `34.2.1` ‚Üí batch 34
318:    - OR scan for batch PRP files in recent context (e.g., `PRP-34.2.1.md`)
319:    - Use detected batch ID to prioritize batch PRPs when searching (search `PRPs/feature-requests/PRP-{batch_id}.*` first)
320: 2. **Parse prp-reference parameter**:
321:    - ID formats: `PRP-8.8`, `8.8`, `34.2.1` (batch notation)
322:    - Filepath: `context-engineering/PRPs/PRP-8.8.md` or `PRPs/feature-requests/PRP-34.2.1.md`
323:    - Natural language: Search within batch PRPs first (if batch active), then all PRPs
324:    - "latest": Most recent PRP from conversation (prioritize batch context)
325: 3. **Locate PRP file**:
326:    - If batch notation (e.g., `34.2.1`): Search `PRPs/feature-requests/PRP-34.2.1*.md`
327:    - If standard notation: Search `.ce/PRPs/` or `PRPs/` or `context-engineering/PRPs/`
328:    - If NL search: Prioritize batch PRPs (if batch active), then search all PRPs
329: 4. **Phase 1 - Document Review**:
330:    - Read PRP document (ignoring generation conversation)
331:    - Perform systematic quality review (9 checks)
332:    - Apply recommendations to PRP file
333: 5. **Phase 2 - Execution Review** (if exe|execution flag):
334:    - Check PRP has been executed (look for changed files from PRP specs)
335:    - Read implementation files (ignoring implementation conversation)
336:    - Validate implementation vs PRP requirements (9 checks)
337:    - Apply fixes to code
338: 6. **Output review summary** with findings + fixes
339: 
340: ## Best Practices
341: 
342: **When to Use Document Review Only**:
343: - After generating new PRP (validate quality before execution)
344: - Reviewing PRPs created by others
345: - Planning phase - want to improve docs before coding
346: 
347: **When to Use Execution Review**:
348: - After `/execute-prp` completes (validate implementation matches spec)
349: - Before marking PRP as complete
350: - Quality gate before merging to main branch
351: - Troubleshooting implementation issues
352: 
353: **Command Efficiency**:
354: - Use natural language for quick PRP lookup
355: - Use `latest` to review most recent work
356: - Separate document and execution reviews for focused feedback
357: 
358: ## Related Commands
359: 
360: **Single PRP Commands**:
361: - `/generate-prp` - Generate new PRP from natural language
362: - `/execute-prp` - Execute PRP implementation
363: - `/peer-review` - Review single PRP (document or execution)
364: 
365: **Batch PRP Commands**:
366: - `/batch-gen-prp` - Generate multiple PRPs from master plan
367: - `/batch-exe-prp` - Execute batch of PRPs in parallel
368: - `/batch-peer-review` - Review entire batch (document or execution)
369: 
370: **Context Management**:
371: - `/update-context` - Update project context after PRP execution
372: 
373: ## Workflow Integration
374: 
375: ### Single PRP Workflow
376: 
377: ```
378: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
379: ‚îÇ  /generate-prp  ‚îÇ  Create PRP
380: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
381:          ‚îÇ
382:          ‚Üì
383: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
384: ‚îÇ  /peer-review   ‚îÇ  Review document quality
385: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
386:          ‚îÇ
387:          ‚Üì
388: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
389: ‚îÇ  /execute-prp   ‚îÇ  Implement changes
390: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
391:          ‚îÇ
392:          ‚Üì
393: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
394: ‚îÇ /peer-review exe‚îÇ  Review execution results
395: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
396: ```
397: 
398: ### Batch PRP Workflow (with single peer-review for spot checks)
399: 
400: ```
401: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
402: ‚îÇ  /batch-gen-prp      ‚îÇ  Create batch of PRPs
403: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
404:            ‚îÇ
405:            ‚Üì
406: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
407: ‚îÇ /batch-peer-review   ‚îÇ  Review all PRPs (document)
408: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
409:            ‚îÇ
410:            ‚Üì  (optional spot checks)
411:     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
412:     ‚îÇ             ‚îÇ
413:     ‚Üì             ‚Üì
414: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê Continue
415: ‚îÇ /peer-review ‚îÇ (individual PRP fixes)
416: ‚îÇ   34.2.1     ‚îÇ
417: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
418:            ‚îÇ
419:            ‚Üì
420: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
421: ‚îÇ  /batch-exe-prp      ‚îÇ  Execute batch (parallel)
422: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
423:            ‚îÇ
424:            ‚Üì
425: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
426: ‚îÇ /batch-peer-review   ‚îÇ  Review all executions
427: ‚îÇ       --exe          ‚îÇ
428: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
429:            ‚îÇ
430:            ‚Üì  (optional spot checks)
431:     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
432:     ‚îÇ             ‚îÇ
433:     ‚Üì             ‚Üì
434: ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê Continue
435: ‚îÇ /peer-review ‚îÇ (individual execution fixes)
436: ‚îÇ  34.2.1 exe  ‚îÇ
437: ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
438:            ‚îÇ
439:            ‚Üì
440:       ‚úÖ Done
441: ```
</file>

<file path=".claude/commands/tools-misuse-scan.md">
  1: # Tool Misuse Scan Command
  2: 
  3: **Purpose**: Detect and categorize tool misuse patterns in Claude Code sessions
  4: 
  5: **Target**: AI agents working with Context Engineering codebase
  6: 
  7: **Last Updated**: 2025-10-17
  8: 
  9: ---
 10: 
 11: ## Command Usage
 12: 
 13: ```bash
 14: /tools-misuse-scan
 15: ```
 16: 
 17: **What it does**:
 18: - Scans conversation history for denied tool errors
 19: - Categorizes into: (1) Bash anti-patterns, (2) Denied tools without substitutes
 20: - Provides remediation suggestions with proper tool alternatives
 21: - Generates structured report for debugging and improvement
 22: 
 23: ---
 24: 
 25: ## Detection Patterns
 26: 
 27: ### Category 1: Bash Anti-patterns
 28: 
 29: **Pattern**: Bash text processing operations that should use designated tools
 30: 
 31: **Detection Rules**:
 32: ```regex
 33: # Bash head/tail with file piping
 34: Bash\(.*\|\s*head\s+-\d+
 35: Bash\(.*\|\s*tail\s+-\d+
 36: 
 37: # Bash grep with file piping
 38: Bash\(.*\|\s*grep\s+
 39: 
 40: # Direct head/tail commands
 41: Bash\(head\s+-\d+
 42: Bash\(tail\s+-\d+
 43: 
 44: # Python subprocess without uv
 45: Bash\(python3?\s+
 46: Bash\(python3?\s+-m
 47: ```
 48: 
 49: **Remediation Mappings**:
 50: | Anti-pattern | Correct Tool | Example |
 51: |-------------|--------------|---------|
 52: | `Bash("cat file \| head -50")` | `Read(file, limit=50)` | Read first 50 lines |
 53: | `Bash("cat file \| tail -100")` | `Read(file, offset=-100)` | Read last 100 lines |
 54: | `Bash("grep pattern file")` | `shell_utils.grep_text()` | Search with context |
 55: | `Bash("python script.py")` | `uv run python script.py` | Proper env management |
 56: 
 57: ### Category 2: Denied Tools
 58: 
 59: **Pattern**: MCP tools explicitly denied with no direct substitute
 60: 
 61: **Detection Rules**:
 62: ```regex
 63: # Tool denied error messages
 64: has been denied
 65: permission denied.*mcp__
 66: Tool.*not available
 67: ```
 68: 
 69: **Known Denied Tools**:
 70: | Tool | Reason | Alternative |
 71: |------|--------|-------------|
 72: | `mcp__serena__replace_symbol_body` (old) | Legacy tool (pre-Syntropy aggregator) | `mcp__syntropy__serena_replace_symbol_body` (new, allowed) or `Edit` (native) |
 73: 
 74: ---
 75: 
 76: ## Analysis Workflow
 77: 
 78: ### Step 1: Error Collection
 79: 
 80: Scan conversation for:
 81: - "has been denied" messages
 82: - "permission denied" errors
 83: - Failed tool invocations with error responses
 84: 
 85: ### Step 2: Categorization
 86: 
 87: **Group 1: Bash Misuse**
 88: - Extract Bash command from error context
 89: - Match against anti-pattern regex
 90: - Identify correct tool replacement
 91: - Generate remediation suggestion
 92: 
 93: **Group 2: Denied Tools**
 94: - Extract tool name from error message
 95: - Check known alternatives table
 96: - Provide workaround guidance
 97: 
 98: ### Step 3: Report Generation
 99: 
100: **Output Format**:
101: ```markdown
102: ## Tool Misuse Analysis Report
103: 
104: **Session**: {timestamp}
105: **Total Errors Found**: {count}
106: 
107: ### Category 1: Bash Anti-patterns ({count})
108: 
109: 1. **Error**: Bash("cat file | head -50")
110:    - **Issue**: Text processing with piping
111:    - **Remedy**: Use `Read(file, limit=50)`
112:    - **Location**: Message #{n}
113:    - **Performance Impact**: 10-50x slower (subprocess overhead)
114: 
115: ### Category 2: Denied Tools ({count})
116: 
117: 1. **Error**: mcp__serena__replace_symbol_body (old naming)
118:    - **Issue**: Legacy tool (pre-Syntropy aggregator)
119:    - **Remedy**: Use `mcp__syntropy__serena_replace_symbol_body` (new, allowed) or `Edit` (native)
120:    - **Location**: Message #{n}
121:    - **Documentation**: See examples/TOOL-USAGE-GUIDE.md
122: ```
123: 
124: ---
125: 
126: ## Implementation Instructions
127: 
128: When running this command:
129: 
130: 1. **Scan Phase** (2 passes for thoroughness):
131:    - Pass 1: Search for "denied" and "permission" keywords
132:    - Pass 2: Validate each error against detection patterns
133: 
134: 2. **Categorization Phase**:
135:    - Apply regex patterns to extract tool names and commands
136:    - Match against known anti-patterns and denied tools
137:    - Generate remediation suggestions
138: 
139: 3. **Validation Phase**:
140:    - Cross-reference with `examples/tool-usage-patterns.md`
141:    - Verify remediation suggestions are accurate
142:    - Check that alternatives exist and are allowed
143: 
144: 4. **Report Phase**:
145:    - Generate structured markdown report
146:    - Include location references (message numbers)
147:    - Add performance impact notes where applicable
148:    - Link to relevant documentation
149: 
150: ---
151: 
152: ## Quality Checks
153: 
154: **Before finalizing report**:
155: 
156: - ‚úÖ All errors categorized correctly
157: - ‚úÖ Remediation suggestions are actionable
158: - ‚úÖ Alternatives verified in tool-usage-patterns.md
159: - ‚úÖ Location references are accurate
160: - ‚úÖ No false positives (legitimate tool uses)
161: - ‚úÖ Performance impact documented (bash subprocess overhead)
162: 
163: ---
164: 
165: ## Expected Output Example
166: 
167: ```markdown
168: ## Tool Misuse Analysis Report
169: 
170: **Session**: 2025-10-17
171: **Total Errors Found**: 7
172: 
173: ### Category 1: Bash Anti-patterns (6 errors)
174: 
175: 1. **Error**: Bash("head -50 file")
176:    - **Issue**: Direct head command without tool
177:    - **Remedy**: Use `Read(file, limit=50)` or `shell_utils.head(file, 50)`
178:    - **Performance**: 10-50x faster with Python utilities
179: 
180: 2. **Error**: Bash("tail -100 file")
181:    - **Issue**: Direct tail command without tool
182:    - **Remedy**: Use `Read(file, offset=-100)` or `shell_utils.tail(file, 100)`
183:    - **Performance**: 10-50x faster with Python utilities
184: 
185: 3. **Error**: Bash("grep pattern file")
186:    - **Issue**: Text search with subprocess
187:    - **Remedy**: Use `shell_utils.grep_text(pattern, file_content)`
188:    - **Performance**: No subprocess fork overhead
189: 
190: 4-6. [Similar patterns...]
191: 
192: ### Category 2: Denied Tools (1 error)
193: 
194: 1. **Error**: mcp__serena__replace_symbol_body (old naming)
195:    - **Issue**: Legacy tool (pre-Syntropy aggregator)
196:    - **Remedy**: Use `mcp__syntropy__serena_replace_symbol_body` (new, allowed)
197:    - **Alternative**: Use `Edit` (native) for direct file modifications
198:    - **Documentation**: examples/TOOL-USAGE-GUIDE.md
199: 
200: ---
201: 
202: ## Recommendations
203: 
204: 1. **Update Documentation**: Ensure examples/tool-usage-patterns.md covers all anti-patterns
205: 2. **Add Pre-commit Validation**: Consider tool usage linting in CI/CD
206: 3. **Agent Training**: Review remediation patterns with agents
207: 4. **Performance Monitoring**: Track subprocess overhead reduction after fixes
208: ```
209: 
210: ---
211: 
212: ## References
213: 
214: - **Tool Usage Patterns**: `examples/tool-usage-patterns.md`
215: - **Serena MCP Restrictions**: `.serena/memories/serena-mcp-tool-restrictions.md`
216: - **CLAUDE.md**: Text processing anti-patterns section
217: - **shell_utils Module**: `tools/ce/shell_utils.py`
218: 
219: ---
220: 
221: ## Notes
222: 
223: - This command is retrospective analysis, not real-time enforcement
224: - Helps identify patterns for documentation updates
225: - Useful for agent training and tool usage improvement
226: - Run periodically after major development sessions
</file>

<file path=".claude/commands/vacuum.md">
  1: # Vacuum
  2: 
  3: Clean up project noise: temp files, obsolete docs, unreferenced code, orphaned tests, dead links, commented code blocks.
  4: 
  5: ## Usage
  6: ```bash
  7: /vacuum [--path DIR] [--execute|--force|--auto|--nuclear] [--exclude-strategy STRATEGY] [--min-confidence N]
  8: ```
  9: 
 10: ## Modes
 11: 
 12: **Default (dry-run)**: Generate report only, no deletions
 13: ```bash
 14: /vacuum
 15: ```
 16: 
 17: **Scoped cleanup**: Clean specific directory only
 18: ```bash
 19: /vacuum --path syntropy-mcp
 20: /vacuum --path tools/ce --execute
 21: ```
 22: 
 23: **Execute mode**: Delete HIGH confidence items only (100% safe: temp files, backups)
 24: ```bash
 25: /vacuum --execute
 26: ```
 27: 
 28: **Force/Auto mode**: Delete HIGH + MEDIUM confidence items (includes obsolete docs, orphan tests, dead links)
 29: ```bash
 30: /vacuum --force
 31: # or
 32: /vacuum --auto
 33: ```
 34: 
 35: **Nuclear mode**: Delete ALL items including LOW confidence (unreferenced code, commented blocks) - requires confirmation
 36: ```bash
 37: /vacuum --nuclear
 38: ```
 39: 
 40: ## Parameters
 41: 
 42: - `--path DIR`: Directory to scan (relative to project root). Defaults to entire project
 43: - `--execute`: Delete HIGH confidence items (‚â•100%)
 44: - `--force`: Delete MEDIUM + HIGH confidence items (‚â•60%)
 45: - `--auto`: Automatically delete MEDIUM + HIGH confidence items (same as --force)
 46: - `--nuclear`: Delete ALL items including LOW confidence (<60%) - requires "yes" confirmation
 47: - `--min-confidence N`: Set custom confidence threshold (0-100)
 48: - `--exclude-strategy STRATEGY`: Skip specific strategy (use multiple times for multiple strategies)
 49: 
 50: ## Strategies
 51: 
 52: ### 1. temp-files (HIGH: 100%)
 53: - `*.pyc`, `__pycache__/`, `.DS_Store`, `*.swp`, `.pytest_cache/`, `*.log`, `*.tmp`
 54: - **Auto-delete with --execute**
 55: 
 56: ### 2. backup-files (HIGH: 100%)
 57: - `*.bak`, `*~`, `*.orig`, `*.rej` (git merge artifacts)
 58: - **Auto-delete with --execute**
 59: 
 60: ### 3. obsolete-docs (MEDIUM: 70%)
 61: **Filename Patterns**:
 62: - Versioned docs: `*-v1.md`, `*-old.md`, `*-deprecated.md`
 63: - Temporary analysis docs: `ANALYSIS-*`, `CHANGELIST-*`, `REPORT-*`, `IMPLEMENTATION-*`, `DEPLOYMENT*`
 64: - Planning docs: `*-PLAN.md`, `*-REPLAN.md`, `*-SUMMARY-*.md`, `*_SOLUTION.md`
 65: - Root-level all-caps files (e.g., `VERSION` - with exceptions for LICENSE, MAKEFILE)
 66: 
 67: **Content Analysis** (reads first 20 lines):
 68: - Status markers: "**Status**: PENDING", "**Status**: READY FOR EXECUTION"
 69: - Execution tracking: "**Completed**:", "**Remaining Work**:"
 70: - Planning markers: "## üìã EXECUTIVE SUMMARY", "## üéØ EXECUTION SUMMARY"
 71: - Solution patterns: "**Problem**:", "**Solution**:", "## Solution Options"
 72: - Date indicators: "**Date**: 2025-10-*" (time-bound docs)
 73: - Workflow markers: "**Workflow:**", "Source Plan:", "This plan addresses"
 74: 
 75: **Protected Patterns**:
 76: - `.ce/**`, `.claude/**`, `.serena/**` (framework/config dirs)
 77: - `syntropy-mcp/**` (MCP server)
 78: - `tmp/finalizing/**` (work-in-progress)
 79: - `PRPs/**/*.md` with YAML headers (real PRPs)
 80: 
 81: - **Delete with --force/--auto**
 82: 
 83: ### 4. orphan-tests (MEDIUM: 60%)
 84: - `test_foo.py` where `foo.py` doesn't exist
 85: - **Delete with --force/--auto**
 86: 
 87: ### 5. unreferenced-code (LOW: 40%)
 88: - Python files where:
 89:   - ALL definitions (functions/classes) are unreferenced elsewhere
 90:   - AND the file itself is not imported by any other module
 91: - Uses Serena MCP for symbol analysis (~15s)
 92: - **Manual review only** (use --nuclear with caution)
 93: - Common false positives: CLI entry points, scripts meant to be run directly
 94: 
 95: ### 6. commented-code (LOW: 30%)
 96: - Commented code blocks ‚â•20 lines
 97: - Excludes docstrings, license headers, teaching examples
 98: - **Manual review only**
 99: 
100: ## Safety Mechanisms
101: 
102: **NEVER_DELETE Paths**:
103: - `.ce/**` - Framework boilerplate
104: - `.claude/**` - Claude Code configuration (all commands, settings)
105: - `.serena/**` - Serena memories and configuration
106: - `syntropy-mcp/**` - Syntropy MCP server directory
107: - `tmp/**` files < 2 days old - Recent work-in-progress (older files deleted)
108: - `PRPs/**/*.md` - All PRP files with YAML headers (managed by update-context)
109: - `pyproject.toml`, `README.md`, `CLAUDE.md`, `WARP.md`, `VERSION`
110: - `examples/**` - Pattern documentation
111: - `**/__init__.py`, `**/cli.py`, `**/__main__.py` - Entry points
112: 
113: **Note**: Analysis files (CHANGELIST-*, ANALYSIS-*, REPORT-*, IMPLEMENTATION-*) are **NOT** protected and can be cleaned up if obsolete, as they're temporary artifacts of PRP execution.
114: 
115: **Protection Rules**:
116: 1. Files modified in last 30 days get lower confidence scores
117: 2. Files referenced in markdown docs are flagged
118: 3. Files with git activity are protected
119: 4. Dry-run by default (explicit --execute required)
120: 
121: ## Output
122: 
123: **Report**: `.ce/vacuum-report.md`
124: 
125: Sections:
126: 1. **Summary**: Candidate count, bytes reclaimable, confidence breakdown
127: 2. **HIGH Confidence**: Safe to delete (path, reason, size, last modified)
128: 3. **MEDIUM Confidence**: Review recommended (path, reason, confidence, git history)
129: 4. **LOW Confidence**: Manual verification required (path, reason, confidence, references)
130: 
131: ## Examples
132: 
133: **Basic cleanup** (temp files + backups):
134: ```bash
135: /vacuum --execute
136: ```
137: 
138: **Deep cleanup** (temp files + backups + obsolete docs + orphan tests):
139: ```bash
140: /vacuum --force
141: ```
142: 
143: **Skip slow strategy**:
144: ```bash
145: /vacuum --exclude-strategy unreferenced-code
146: ```
147: 
148: **Custom threshold** (only delete 80%+ confidence):
149: ```bash
150: /vacuum --execute --min-confidence 80
151: ```
152: 
153: **Review only** (generate report without deleting):
154: ```bash
155: /vacuum
156: # Or explicitly:
157: cd tools && uv run ce vacuum --dry-run
158: ```
159: 
160: ## Exit Codes
161: 
162: - `0`: No candidates found (clean)
163: - `1`: Candidates found (check report)
164: - `2`: Error occurred
165: 
166: ## Performance
167: 
168: **Parallel execution** for fast strategies (temp-files, backup-files, obsolete-docs, orphan-tests, commented-code):
169: - ~5 seconds total
170: 
171: **Sequential execution** for slow strategy (unreferenced-code via Serena):
172: - ~15 seconds
173: 
174: **Total time**: ~20 seconds for all strategies
175: 
176: **Skip slow strategy** for quick cleanup:
177: ```bash
178: /vacuum --execute --exclude-strategy unreferenced-code
179: # ~5 seconds
180: ```
181: 
182: ## Workflow Integration
183: 
184: **After /update-context**:
185: ```bash
186: # 1. Sync context
187: /update-context
188: 
189: # 2. Clean up project
190: /vacuum --execute
191: 
192: # 3. Review report
193: cat .ce/vacuum-report.md
194: ```
195: 
196: **Before commits**:
197: ```bash
198: # Clean temp files before committing
199: /vacuum --execute
200: git add .
201: git commit -m "Implement feature X"
202: ```
203: 
204: ## Troubleshooting
205: 
206: **"Strategy not found"**: Check spelling, available strategies:
207: - `temp-files`, `backup-files`, `obsolete-docs`, `unreferenced-code`, `orphan-tests`, `commented-code`
208: 
209: **"Serena unavailable"**: unreferenced-code strategy will skip gracefully, other strategies run normally
210: 
211: **"Permission denied"**: File in use or protected by OS - check .ce/vacuum-report.md for details
212: 
213: **False positive**: File flagged incorrectly? Add to PROTECTED_PATTERNS in `tools/ce/vacuum_strategies/base.py` or use lower confidence threshold
214: 
215: ## Advanced Usage
216: 
217: **Review LOW confidence candidates** before nuclear mode:
218: ```bash
219: # 1. Generate report
220: /vacuum
221: 
222: # 2. Review .ce/vacuum-report.md LOW confidence section
223: 
224: # 3. If satisfied, run nuclear mode
225: /vacuum --nuclear
226: # (type "yes" to confirm)
227: ```
228: 
229: **Chain with git commit**:
230: ```bash
231: /vacuum --execute && git add . && git commit -m "Clean up project noise"
232: ```
233: 
234: ## Goal
235: 
236: Maintain clean project state by identifying and safely removing noise while preserving all vital content through confidence-based safety mechanisms.
</file>

<file path=".claude/settings.local.json">
  1: {
  2:   "permissions": {
  3:     "allow": [
  4:       "Bash(git:*)",
  5:       "Bash(uv:*)",
  6:       "Bash(uvx:*)",
  7:       "Bash(pytest:*)",
  8:       "Bash(python:*)",
  9:       "Bash(python3:*)",
 10:       "Bash(gh:*)",
 11:       "Bash(rg:*)",
 12:       "Bash(env:*)",
 13:       "Bash(head:*)",
 14:       "Bash(tail:*)",
 15:       "Bash(cat:*)",
 16:       "Bash(grep:*)",
 17:       "Bash(wc:*)",
 18:       "Bash(echo:*)",
 19:       "Bash(ps:*)",
 20:       "Bash(rm -rf ~/.mcp-auth)",
 21:       "Bash(ls:*)",
 22:       "Bash(cd:*)",
 23:       "Bash(pwd:*)",
 24:       "Bash(find:*)",
 25:       "Bash(tree:*)",
 26:       "Bash(which:*)",
 27:       "Bash(whereis:*)",
 28:       "Bash(file:*)",
 29:       "Bash(stat:*)",
 30:       "Bash(less:*)",
 31:       "Bash(more:*)",
 32:       "Bash(sed:*)",
 33:       "Bash(awk:*)",
 34:       "Bash(sort:*)",
 35:       "Bash(uniq:*)",
 36:       "Bash(cut:*)",
 37:       "Bash(diff:*)",
 38:       "Bash(comm:*)",
 39:       "Read(//Users/bprzybysz/nc-src/**)",
 40:       "Read(//Users/bprzybysz/.claude/**)",
 41:       "WebFetch(domain:github.com)",
 42:       "SlashCommand(/generate-prp:*)",
 43:       "SlashCommand(/execute-prp:*)",
 44:       "SlashCommand(/peer-review:*)",
 45:       "SlashCommand(/mcp:*)",
 46:       "WebSearch",
 47:       "mcp__syntropy__context7_get_library_docs",
 48:       "mcp__syntropy__context7_resolve_library_id",
 49:       "mcp__syntropy__linear_create_issue",
 50:       "mcp__syntropy__linear_get_issue",
 51:       "mcp__syntropy__linear_list_issues",
 52:       "mcp__syntropy__linear_list_projects",
 53:       "mcp__syntropy__linear_update_issue",
 54:       "mcp__syntropy__linear_list_teams",
 55:       "mcp__syntropy__linear_get_team",
 56:       "mcp__syntropy__linear_list_users",
 57:       "mcp__syntropy__linear_create_project",
 58:       "mcp__syntropy__serena_activate_project",
 59:       "mcp__syntropy__serena_create_text_file",
 60:       "mcp__syntropy__serena_find_referencing_symbols",
 61:       "mcp__syntropy__serena_find_symbol",
 62:       "mcp__syntropy__serena_get_symbols_overview",
 63:       "mcp__syntropy__serena_list_dir",
 64:       "mcp__syntropy__serena_read_file",
 65:       "mcp__syntropy__serena_replace_symbol_body",
 66:       "mcp__syntropy__serena_search_for_pattern",
 67:       "mcp__syntropy__serena_write_memory",
 68:       "mcp__syntropy__serena_read_memory",
 69:       "mcp__syntropy__serena_list_memories",
 70:       "mcp__syntropy__serena_delete_memory",
 71:       "mcp__syntropy__thinking_sequentialthinking",
 72:       "mcp__syntropy__healthcheck",
 73:       "mcp__syntropy__enable_tools",
 74:       "mcp__syntropy__list_all_tools"
 75:     ],
 76:     "deny": [
 77:       "mcp__syntropy__serena_think_about_collected_information",
 78:       "mcp__syntropy__serena_think_about_task_adherence",
 79:       "mcp__syntropy__serena_think_about_whether_you_are_done",
 80:       "mcp__syntropy__filesystem_read_media_file",
 81:       "mcp__syntropy__filesystem_read_multiple_files",
 82:       "mcp__syntropy__filesystem_create_directory",
 83:       "mcp__syntropy__filesystem_move_file",
 84:       "mcp__syntropy__filesystem_list_directory_with_sizes",
 85:       "mcp__syntropy__git_git_branch",
 86:       "mcp__syntropy__git_git_checkout",
 87:       "mcp__syntropy__git_git_show",
 88:       "mcp__syntropy__git_git_create_branch",
 89:       "mcp__syntropy__git_git_reset",
 90:       "mcp__syntropy__git_git_diff_staged",
 91:       "mcp__syntropy__git_git_diff_unstaged",
 92:       "mcp__syntropy__filesystem_read_file",
 93:       "mcp__syntropy__filesystem_read_text_file",
 94:       "mcp__syntropy__filesystem_write_file",
 95:       "mcp__syntropy__filesystem_edit_file",
 96:       "mcp__syntropy__filesystem_list_directory",
 97:       "mcp__syntropy__filesystem_search_files",
 98:       "mcp__syntropy__filesystem_directory_tree",
 99:       "mcp__syntropy__filesystem_get_file_info",
100:       "mcp__syntropy__git_git_status",
101:       "mcp__syntropy__git_git_diff",
102:       "mcp__syntropy__git_git_log",
103:       "mcp__syntropy__git_git_add",
104:       "mcp__syntropy__git_git_commit",
105:       "mcp__syntropy__github_create_or_update_file",
106:       "mcp__syntropy__github_search_repositories",
107:       "mcp__syntropy__github_create_repository",
108:       "mcp__syntropy__github_get_file_contents",
109:       "mcp__syntropy__github_push_files",
110:       "mcp__syntropy__github_create_issue",
111:       "mcp__syntropy__github_create_pull_request",
112:       "mcp__syntropy__github_fork_repository",
113:       "mcp__syntropy__github_create_branch",
114:       "mcp__syntropy__github_list_commits",
115:       "mcp__syntropy__github_list_issues",
116:       "mcp__syntropy__github_update_issue",
117:       "mcp__syntropy__github_add_issue_comment",
118:       "mcp__syntropy__github_search_code",
119:       "mcp__syntropy__github_search_issues",
120:       "mcp__syntropy__github_search_users",
121:       "mcp__syntropy__github_get_issue",
122:       "mcp__syntropy__github_get_pull_request",
123:       "mcp__syntropy__github_list_pull_requests",
124:       "mcp__syntropy__github_create_pull_request_review",
125:       "mcp__syntropy__github_merge_pull_request",
126:       "mcp__syntropy__github_get_pull_request_files",
127:       "mcp__syntropy__github_get_pull_request_status",
128:       "mcp__syntropy__github_update_pull_request_branch",
129:       "mcp__syntropy__github_get_pull_request_comments",
130:       "mcp__syntropy__github_get_pull_request_reviews",
131:       "mcp__syntropy__repomix_pack_codebase",
132:       "mcp__syntropy__repomix_grep_repomix_output",
133:       "mcp__syntropy__repomix_read_repomix_output",
134:       "mcp__syntropy__repomix_pack_remote_repository",
135:       "mcp__syntropy__playwright_navigate",
136:       "mcp__syntropy__playwright_screenshot",
137:       "mcp__syntropy__playwright_click",
138:       "mcp__syntropy__playwright_fill",
139:       "mcp__syntropy__playwright_evaluate",
140:       "mcp__syntropy__playwright_get_visible_text",
141:       "mcp__syntropy__perplexity_perplexity_ask",
142:       "mcp__syntropy__init_project",
143:       "mcp__syntropy__get_system_doc",
144:       "mcp__syntropy__get_user_doc",
145:       "mcp__syntropy__get_summary",
146:       "mcp__syntropy__denoise"
147:     ],
148:     "ask": [
149:       "Bash(rm:*)",
150:       "Bash(mv:*)",
151:       "Bash(cp:*)",
152:       "Bash(curl:*)",
153:       "Bash(wget:*)",
154:       "Bash(nc:*)",
155:       "Bash(telnet:*)",
156:       "Bash(ssh:*)",
157:       "Bash(scp:*)",
158:       "Bash(rsync:*)",
159:       "Bash(sudo:*)",
160:       "Bash(brew install:*)",
161:       "Bash(npm install:*)",
162:       "Bash(pip install:*)",
163:       "Bash(gem install:*)"
164:     ],
165:     "additionalDirectories": [
166:       "/Users/bprzybyszi"
167:     ]
168:   },
169:   "hooks": {
170:     "SessionStart": [
171:       {
172:         "hooks": [
173:           {
174:             "type": "command",
175:             "command": "PROJECT_ROOT=$(git rev-parse --show-toplevel 2>/dev/null || pwd) && cd \"$PROJECT_ROOT/tools\" && bash scripts/session-startup.sh",
176:             "timeout": 12
177:           }
178:         ]
179:       }
180:     ]
181:   },
182:   "enabledPlugins": {}
183: }
</file>

<file path="examples/ce-blend-usage.md">
  1: # CE Blend Command - Usage Examples
  2: 
  3: **Command**: `ce blend`
  4: **Purpose**: Merge CE framework files with target project files using intelligent blending strategies
  5: **PRP**: PRP-34 (Blending Framework)
  6: 
  7: ---
  8: 
  9: ## Overview
 10: 
 11: The `blend` command implements a 4-phase pipeline to merge Context Engineering framework with target project customizations:
 12: 
 13: 1. **DETECT**: Scan target project for existing CE files
 14: 2. **CLASSIFY**: Validate CE patterns and file structure
 15: 3. **BLEND**: Apply domain-specific merge strategies
 16: 4. **CLEANUP**: Remove legacy directories after migration
 17: 
 18: ---
 19: 
 20: ## Basic Usage
 21: 
 22: ### Blend All Domains
 23: 
 24: ```bash
 25: cd tools
 26: uv run ce blend --all
 27: ```
 28: 
 29: Blends all 6 domains: PRPs, examples, CLAUDE.md, settings, commands, memories
 30: 
 31: ### Blend Specific Domain
 32: 
 33: ```bash
 34: # Blend only CLAUDE.md
 35: uv run ce blend --claude-md
 36: 
 37: # Blend only memories
 38: uv run ce blend --memories
 39: 
 40: # Blend multiple domains
 41: uv run ce blend --settings --commands
 42: ```
 43: 
 44: ### Target Different Project
 45: 
 46: ```bash
 47: # Blend into different project directory
 48: uv run ce blend --all --target-dir ~/projects/my-app
 49: 
 50: # With custom config
 51: uv run ce blend --all --target-dir ~/projects/my-app --config ~/.ce/custom-blend-config.yml
 52: ```
 53: 
 54: ---
 55: 
 56: ## Common Scenarios
 57: 
 58: ### Scenario 1: Initial Framework Installation
 59: 
 60: **Goal**: Install CE framework in new project
 61: 
 62: ```bash
 63: cd tools
 64: uv run ce blend --all --target-dir ~/projects/new-project
 65: 
 66: # Expected output:
 67: # üîÄ Running Phase: DETECT
 68: # ‚úì Detected 0 files (new installation)
 69: # üîÄ Running Phase: BLEND
 70: # ‚úì Blending complete (6 domains processed)
 71: ```
 72: 
 73: **Result**: Framework files copied to target project
 74: 
 75: ---
 76: 
 77: ### Scenario 2: Update Existing Installation
 78: 
 79: **Goal**: Update CE framework in project with customizations
 80: 
 81: ```bash
 82: cd tools
 83: uv run ce blend --all --target-dir ~/projects/existing-project
 84: 
 85: # Expected output:
 86: # üîÄ Running Phase: DETECT
 87: # ‚úì Detected 134 files across 6 domains
 88: # üîÄ Running Phase: CLASSIFY
 89: # ‚úì Classified 128 valid files
 90: # üîÄ Running Phase: BLEND
 91: #   Blending claude_md (1 files)...
 92: #   ‚úì Merged 5 sections, preserved 3 custom sections
 93: #   Blending memories (24 files)...
 94: #   ‚úì Merged 23 framework + 5 user memories
 95: # ‚úì Blending complete
 96: ```
 97: 
 98: **Result**: Framework updated, user customizations preserved
 99: 
100: ---
101: 
102: ### Scenario 3: Re-blend After Framework Changes
103: 
104: **Goal**: Re-apply blending after updating memories/examples
105: 
106: ```bash
107: # 1. Update framework files
108: vim .serena/memories/new-pattern.md
109: 
110: # 2. Rebuild packages
111: .ce/build-and-distribute.sh
112: 
113: # 3. Re-blend
114: cd tools
115: uv run ce blend --all
116: ```
117: 
118: ---
119: 
120: ## Domain-Specific Blending
121: 
122: ### CLAUDE.md Blending
123: 
124: **Strategy**: Section-level merge with Sonnet LLM
125: 
126: ```bash
127: uv run ce blend --claude-md
128: 
129: # Behavior:
130: # - Detects framework sections (## Communication, ## Core Principles)
131: # - Detects user sections (## Project-Specific Notes)
132: # - Merges overlapping sections using Sonnet
133: # - Preserves user-only sections verbatim
134: ```
135: 
136: **Example Output**:
137: ```
138: Blending claude_md (1 files)...
139:   ‚úì Merged section: Core Principles (framework + user changes)
140:   ‚úì Preserved section: Project-Specific Notes (user only)
141:   ‚úì Added section: New Framework Section
142: ```
143: 
144: ---
145: 
146: ### Memories Blending
147: 
148: **Strategy**: YAML header merge + Haiku similarity check
149: 
150: ```bash
151: uv run ce blend --memories
152: 
153: # Behavior:
154: # - Merges YAML headers (tags, categories)
155: # - Checks content similarity with Haiku
156: # - Merges if >80% similar, keeps separate otherwise
157: # - Preserves user memories (type: user)
158: ```
159: 
160: **Example Output**:
161: ```
162: Blending memories (24 files)...
163:   ‚úì Merged: code-style-conventions.md (framework + user tags)
164:   ‚úì Kept separate: my-project-patterns.md (user only)
165:   ‚úì 23 framework + 5 user memories = 28 total
166: ```
167: 
168: ---
169: 
170: ### Settings Blending
171: 
172: **Strategy**: JSON deep merge with conflict resolution
173: 
174: ```bash
175: uv run ce blend --settings
176: 
177: # Behavior:
178: # - Deep merges .claude/settings.local.json
179: # - Preserves user customizations
180: # - Adds new framework keys
181: # - Conflict resolution: user value wins
182: ```
183: 
184: **Example**:
185: ```json
186: // Framework: {"allow": ["Bash(git:*)"]}
187: // User: {"allow": ["Bash(npm:*)"], "custom_key": "value"}
188: // Result: {"allow": ["Bash(git:*)", "Bash(npm:*)"], "custom_key": "value"}
189: ```
190: 
191: ---
192: 
193: ### Examples Blending
194: 
195: **Strategy**: Semantic deduplication with natural language similarity
196: 
197: ```bash
198: uv run ce blend --examples
199: 
200: # Behavior:
201: # - Compares examples by content similarity (not filename)
202: # - Deduplicates if >85% similar
203: # - Preserves both if semantically different
204: ```
205: 
206: ---
207: 
208: ## Advanced Usage
209: 
210: ### Dry Run Mode
211: 
212: ```bash
213: # See what would be blended without executing
214: uv run ce blend --all --dry-run
215: 
216: # Output shows planned actions:
217: # [DRY-RUN] Would blend:
218: #   - CLAUDE.md: 5 sections to merge
219: #   - memories: 23 framework + 5 user
220: #   - settings: 12 keys to add
221: ```
222: 
223: ### Verbose Output
224: 
225: ```bash
226: # See detailed blending decisions
227: uv run ce blend --all --verbose
228: 
229: # Shows:
230: # - File-by-file processing
231: # - Similarity scores
232: # - Merge decisions
233: # - Conflict resolutions
234: ```
235: 
236: ### Custom Configuration
237: 
238: ```bash
239: # Use custom blend config
240: uv run ce blend --all --config ~/.ce/my-blend-config.yml
241: ```
242: 
243: **Config Structure** (`.ce/blend-config.yml`):
244: ```yaml
245: strategies:
246:   claude_md:
247:     model: sonnet
248:     temperature: 0.3
249:   memories:
250:     similarity_threshold: 0.8
251:   examples:
252:     dedupe_threshold: 0.85
253: ```
254: 
255: ---
256: 
257: ## Troubleshooting
258: 
259: ### Issue: "Config file not found"
260: 
261: ```bash
262: # Error: Config file not found: .ce/blend-config.yml
263: 
264: # Solution: Copy default config
265: cp ~/ctx-eng-plus/.ce/blend-config.yml .ce/
266: ```
267: 
268: ---
269: 
270: ### Issue: "Cannot cleanup PRPs/: 87 unmigrated files"
271: 
272: ```bash
273: # Error during cleanup phase
274: 
275: # Cause: Old PRPs not yet migrated to .ce/PRPs/
276: # Solution: Run migration or skip cleanup
277: uv run ce blend --all --skip-cleanup
278: ```
279: 
280: ---
281: 
282: ### Issue: "LLM call failed"
283: 
284: ```bash
285: # Error: Anthropic API error
286: 
287: # Cause: Missing ANTHROPIC_API_KEY
288: # Solution: Set environment variable
289: export ANTHROPIC_API_KEY=your-key-here
290: uv run ce blend --all
291: ```
292: 
293: ---
294: 
295: ## Integration with Other Commands
296: 
297: ### With init-project
298: 
299: ```bash
300: # init-project automatically calls blend
301: uv run ce init-project ~/projects/target
302: 
303: # Internally runs:
304: # 1. Extract framework files
305: # 2. uv run ce blend --all --target-dir ~/projects/target
306: # 3. Initialize Python environment
307: # 4. Verify installation
308: ```
309: 
310: ### With update-context
311: 
312: ```bash
313: # After updating memories, blend automatically triggers
314: uv run ce update-context
315: 
316: # If framework files changed:
317: # 1. Updates context sync
318: # 2. Rebuilds repomix packages
319: # 3. Auto-blends (if configured)
320: ```
321: 
322: ---
323: 
324: ## Best Practices
325: 
326: ### 1. Always Use --all for Initial Setup
327: 
328: ```bash
329: # ‚úÖ Good: Blend all domains at once
330: uv run ce blend --all
331: 
332: # ‚ùå Avoid: Blending domains piecemeal
333: uv run ce blend --claude-md
334: uv run ce blend --memories
335: # ... (easy to miss domains)
336: ```
337: 
338: ### 2. Backup Before Major Updates
339: 
340: ```bash
341: # Before major framework update
342: cp -r .ce .ce.backup
343: uv run ce blend --all
344: 
345: # Rollback if needed
346: rm -rf .ce
347: mv .ce.backup .ce
348: ```
349: 
350: ### 3. Use Dry Run First
351: 
352: ```bash
353: # See impact before committing
354: uv run ce blend --all --dry-run
355: 
356: # Review output, then execute
357: uv run ce blend --all
358: ```
359: 
360: ### 4. Review Merged Sections
361: 
362: ```bash
363: # After blending CLAUDE.md
364: git diff CLAUDE.md
365: 
366: # Check for:
367: # - Preserved custom sections
368: # - Correct merge of overlapping sections
369: # - No lost user content
370: ```
371: 
372: ---
373: 
374: ## Exit Codes
375: 
376: - `0`: Blend successful
377: - `1`: Blend failed (check error output)
378: - `2`: Cleanup failed (partial success)
379: 
380: ---
381: 
382: ## Related Commands
383: 
384: - `ce init-project` - Full framework initialization (includes blend)
385: - `ce update-context` - Auto-triggers blend if framework files changed
386: - `ce vacuum` - Cleanup temporary blend artifacts
387: 
388: ---
389: 
390: ## See Also
391: 
392: - [INITIALIZATION.md](INITIALIZATION.md) - Full CE framework setup guide
393: - [PRP-34-INITIAL.md](../PRPs/feature-requests/PRP-34-INITIAL.md) - Blend tool design
394: - `.ce/blend-config.yml` - Blend configuration reference
</file>

<file path="examples/ce-init-project-usage.md">
  1: # CE Init-Project Command - Usage Examples
  2: 
  3: **Command**: `ce init-project`
  4: **Purpose**: Initialize CE Framework in target project with 4-phase pipeline
  5: **PRP**: PRP-36 (Project Initializer)
  6: 
  7: ---
  8: 
  9: ## Overview
 10: 
 11: The `init-project` command automates complete CE framework installation using a 4-phase pipeline:
 12: 
 13: 1. **EXTRACT**: Unpack framework files from ce-infrastructure.xml
 14: 2. **BLEND**: Merge framework + user customizations
 15: 3. **INITIALIZE**: Install Python dependencies with uv
 16: 4. **VERIFY**: Validate installation and report status
 17: 
 18: ---
 19: 
 20: ## Basic Usage
 21: 
 22: ### Initialize Current Directory
 23: 
 24: ```bash
 25: cd tools
 26: uv run ce init-project ~/projects/my-app
 27: ```
 28: 
 29: ### With Dry Run
 30: 
 31: ```bash
 32: # See what would be done without executing
 33: uv run ce init-project ~/projects/my-app --dry-run
 34: 
 35: # Output shows planned actions:
 36: # [DRY-RUN] Extract: Would extract 52 files
 37: # [DRY-RUN] Blend: Would run ce blend --all
 38: # [DRY-RUN] Initialize: Would run uv sync
 39: # [DRY-RUN] Verify: Would check installation
 40: ```
 41: 
 42: ---
 43: 
 44: ## Common Scenarios
 45: 
 46: ### Scenario 1: Fresh Project Setup (Greenfield)
 47: 
 48: **Goal**: Install CE framework in new project with no existing .ce/
 49: 
 50: ```bash
 51: cd tools
 52: uv run ce init-project ~/projects/new-app
 53: 
 54: # Expected output:
 55: # ============================================================
 56: # Phase: extract
 57: # ============================================================
 58: # ‚úÖ Extracted 52 files to /Users/you/projects/new-app/.ce
 59: #
 60: # ============================================================
 61: # Phase: blend
 62: # ============================================================
 63: # ‚úÖ Blend phase completed
 64: #
 65: # ============================================================
 66: # Phase: initialize
 67: # ============================================================
 68: # ‚úÖ Python environment initialized (.ce/tools/.venv created)
 69: #
 70: # ============================================================
 71: # Phase: verify
 72: # ============================================================
 73: # ‚úÖ Installation complete
 74: ```
 75: 
 76: **Result**: Full CE framework installed and ready to use
 77: 
 78: ---
 79: 
 80: ### Scenario 2: Existing .ce/ Directory
 81: 
 82: **Goal**: Upgrade CE framework without losing customizations
 83: 
 84: ```bash
 85: cd tools
 86: uv run ce init-project ~/projects/existing-app
 87: 
 88: # Expected output:
 89: # ============================================================
 90: # Phase: extract
 91: # ============================================================
 92: # ‚ÑπÔ∏è  Renamed existing .ce/ to .ce.old/
 93: # üí° .ce.old/ will be included as additional context source during blend
 94: # ‚úÖ Extracted 52 files to /Users/you/projects/existing-app/.ce
 95: #
 96: # ============================================================
 97: # Phase: blend
 98: # ============================================================
 99: # ‚úÖ Blend phase completed
100: # üí° Note: .ce.old/ detected - blend tool will include it as additional source
101: ```
102: 
103: **Result**:
104: - Old `.ce/` preserved as `.ce.old/`
105: - New framework extracted
106: - Blend merges `.ce.old/` customizations with new framework
107: 
108: ---
109: 
110: ### Scenario 3: Phase-Specific Execution
111: 
112: **Goal**: Run only specific phase (e.g., re-blend after manual changes)
113: 
114: ```bash
115: # Run only extract phase
116: uv run ce init-project ~/projects/app --phase extract
117: 
118: # Run only blend phase
119: uv run ce init-project ~/projects/app --phase blend
120: 
121: # Run only initialize phase
122: uv run ce init-project ~/projects/app --phase initialize
123: 
124: # Run only verify phase
125: uv run ce init-project ~/projects/app --phase verify
126: ```
127: 
128: **Use Cases**:
129: - `--phase extract`: Re-extract after framework update
130: - `--phase blend`: Re-blend after editing .ce.old/ files
131: - `--phase initialize`: Reinstall Python deps after adding packages
132: - `--phase verify`: Quick health check
133: 
134: ---
135: 
136: ### Scenario 4: Blend-Only Mode (Re-initialization)
137: 
138: **Goal**: Skip extraction, only re-blend existing files
139: 
140: ```bash
141: # You already have .ce/ directory with framework files
142: # Just want to re-blend with customizations
143: uv run ce init-project ~/projects/app --blend-only
144: 
145: # Skips: extract, initialize phases
146: # Runs: blend, verify phases
147: ```
148: 
149: **Use Case**: After manually editing framework files in `.ce/`, want to re-merge with target project
150: 
151: ---
152: 
153: ## Phase Details
154: 
155: ### Phase 1: Extract
156: 
157: **What it does**:
158: 1. Checks for ce-infrastructure.xml package
159: 2. Renames existing `.ce/` ‚Üí `.ce.old/` (if exists)
160: 3. Extracts 52 framework files to `.ce/`
161: 4. Reorganizes: `tools/` ‚Üí `.ce/tools/`
162: 5. Copies ce-workflow-docs.xml reference package
163: 
164: **Files Extracted**:
165: - System directories: `.serena/memories/`, `.claude/commands/`, `examples/`
166: - Tool files: `.ce/tools/ce/*.py`, `pyproject.toml`, `bootstrap.sh`
167: - Config files: `.claude/settings.local.json`, `.ce/config.yml`
168: 
169: **Output Example**:
170: ```
171: ‚úÖ Extracted 52 files to /Users/you/projects/app/.ce
172: 
173: Directory structure:
174:   .ce/
175:   ‚îú‚îÄ‚îÄ PRPs/
176:   ‚îú‚îÄ‚îÄ tools/ce/
177:   ‚îú‚îÄ‚îÄ tools/pyproject.toml
178:   ‚îî‚îÄ‚îÄ .serena/memories/
179: ```
180: 
181: ---
182: 
183: ### Phase 2: Blend
184: 
185: **What it does**:
186: 1. Runs `ce blend --all --target-dir <target>`
187: 2. Merges framework files with target project
188: 3. Includes `.ce.old/` as additional source (if exists)
189: 4. Applies domain-specific strategies
190: 
191: **Blended Domains**:
192: - CLAUDE.md: Section-level merge
193: - Memories: YAML + content merge
194: - Examples: Semantic deduplication
195: - Settings: JSON deep merge
196: - Commands: File-level merge
197: - PRPs: Migration to `.ce/PRPs/`
198: 
199: **Output Example**:
200: ```
201: ‚úÖ Blend phase completed
202: üí° Note: .ce.old/ detected - blend tool will include it as additional source
203: 
204: Blended:
205:   - CLAUDE.md: 5 sections merged
206:   - memories: 23 framework + 5 user
207:   - settings: 12 keys added
208: ```
209: 
210: ---
211: 
212: ### Phase 3: Initialize
213: 
214: **What it does**:
215: 1. Navigates to `.ce/tools/`
216: 2. Runs `uv sync` to install Python dependencies
217: 3. Creates `.venv` virtual environment
218: 4. Installs ce-tools package in editable mode
219: 
220: **Dependencies Installed** (from `pyproject.toml`):
221: - anthropic>=0.40.0
222: - deepdiff>=6.0
223: - jsonschema>=4.25.1
224: - python-frontmatter>=1.1.0
225: - pyyaml>=6.0
226: 
227: **Output Example**:
228: ```
229: ‚úÖ Python environment initialized (.ce/tools/.venv created)
230: 
231: Installed packages:
232:   - anthropic 0.40.0
233:   - deepdiff 6.0
234:   - pyyaml 6.0
235:   ... (8 packages total)
236: ```
237: 
238: ---
239: 
240: ### Phase 4: Verify
241: 
242: **What it does**:
243: 1. Checks critical files exist
244: 2. Validates JSON files (settings.local.json)
245: 3. Verifies Python environment
246: 4. Reports installation summary
247: 
248: **Verification Checks**:
249: - ‚úÖ `.ce/tools/` exists
250: - ‚úÖ `.venv/` created
251: - ‚úÖ `ce` command available
252: - ‚úÖ Critical memories present
253: - ‚úÖ settings.local.json valid JSON
254: 
255: **Output Example**:
256: ```
257: ‚úÖ Installation complete
258: 
259: Summary:
260:   Framework files: 52
261:   Python packages: 8
262:   Commands available: 15
263:   Memories loaded: 28
264: ```
265: 
266: ---
267: 
268: ## Advanced Usage
269: 
270: ### Custom Package Location
271: 
272: If ce-infrastructure.xml is in non-standard location:
273: 
274: ```bash
275: # init-project expects packages in ctx-eng-plus/.ce/
276: # If in different location, copy first:
277: cp ~/custom-location/*.xml ~/ctx-eng-plus/.ce/
278: uv run ce init-project ~/projects/app
279: ```
280: 
281: ---
282: 
283: ### Handling Existing .ce.old/
284: 
285: If you already have `.ce.old/` from previous run:
286: 
287: ```bash
288: # Old .ce.old/ will be deleted automatically
289: # New .ce/ ‚Üí .ce.old/, then fresh extract
290: uv run ce init-project ~/projects/app
291: 
292: # To preserve multiple backups:
293: mv .ce.old .ce.backup-$(date +%Y%m%d)
294: uv run ce init-project ~/projects/app
295: ```
296: 
297: ---
298: 
299: ### Troubleshooting Failed Phases
300: 
301: If a phase fails, you can re-run from that point:
302: 
303: ```bash
304: # Example: Blend failed due to missing config
305: # Fix: Add config file
306: cp ~/ctx-eng-plus/.ce/blend-config.yml .ce/
307: 
308: # Re-run from blend phase
309: uv run ce init-project ~/projects/app --phase blend
310: 
311: # Or continue with full pipeline
312: uv run ce init-project ~/projects/app
313: ```
314: 
315: ---
316: 
317: ## Integration with Other Commands
318: 
319: ### Before init-project: Build Packages
320: 
321: ```bash
322: # Ensure you have latest framework packages
323: cd ~/ctx-eng-plus
324: .ce/build-and-distribute.sh
325: 
326: # Verify packages exist
327: ls -lh .ce/*.xml
328: 
329: # Now run init-project
330: cd tools
331: uv run ce init-project ~/projects/app
332: ```
333: 
334: ---
335: 
336: ### After init-project: Verify Installation
337: 
338: ```bash
339: # Navigate to target project
340: cd ~/projects/app
341: 
342: # Verify ce command works
343: cd .ce/tools
344: uv run ce --help
345: 
346: # Check framework files
347: ls -la ~/.ce/PRPs/
348: ls -la ~/.ce/.serena/memories/
349: ```
350: 
351: ---
352: 
353: ### With Git Workflow
354: 
355: ```bash
356: # Initialize project on feature branch
357: git checkout -b feature/add-ce-framework
358: cd tools
359: uv run ce init-project ~/projects/app
360: 
361: # Review changes in target project
362: cd ~/projects/app
363: git status
364: git diff CLAUDE.md
365: 
366: # Commit framework installation
367: git add .ce/ CLAUDE.md .serena/ .claude/
368: git commit -m "Add CE framework via init-project"
369: ```
370: 
371: ---
372: 
373: ## Error Handling
374: 
375: ### Error 1: Missing XML Packages
376: 
377: ```
378: ‚ùå ce-infrastructure.xml not found at /Users/you/ctx-eng-plus/.ce/ce-infrastructure.xml
379: üîß Ensure you're running from ctx-eng-plus repo root
380: ```
381: 
382: **Solution**:
383: ```bash
384: cd ~/ctx-eng-plus
385: .ce/build-and-distribute.sh
386: cp ce-32/builds/*.xml .ce/
387: ```
388: 
389: ---
390: 
391: ### Error 2: Blend Config Not Found
392: 
393: ```
394: ‚ùå Blending failed: Config file not found: .ce/blend-config.yml
395: üîß Create .ce/blend-config.yml (see PRP-34.1.1)
396: ```
397: 
398: **Solution**:
399: ```bash
400: # In target project
401: cp ~/ctx-eng-plus/.ce/blend-config.yml .ce/
402: uv run ce init-project . --phase blend
403: ```
404: 
405: ---
406: 
407: ### Error 3: UV Not Installed
408: 
409: ```
410: ‚ùå uv not found in PATH
411: üîß Install UV: curl -LsSf https://astral.sh/uv/install.sh | sh
412: ```
413: 
414: **Solution**:
415: ```bash
416: # Install UV
417: curl -LsSf https://astral.sh/uv/install.sh | sh
418: 
419: # Retry initialize
420: uv run ce init-project ~/projects/app --phase initialize
421: ```
422: 
423: ---
424: 
425: ### Error 4: Corrupted Extracted Files
426: 
427: ```
428: ‚ùå UV sync failed (exit code 2)
429: TOML parse error at line 1, column 5
430:   1 |  1: [project]
431: ```
432: 
433: **Cause**: Line numbers not stripped during extraction (fixed in latest version)
434: 
435: **Solution**:
436: ```bash
437: # Update to latest ctx-eng-plus (includes fix)
438: cd ~/ctx-eng-plus
439: git pull
440: 
441: # Re-extract
442: uv run ce init-project ~/projects/app --phase extract
443: ```
444: 
445: ---
446: 
447: ## Best Practices
448: 
449: ### 1. Always Run from ctx-eng-plus/tools/
450: 
451: ```bash
452: # ‚úÖ Good: Run from tools/ directory
453: cd ~/ctx-eng-plus/tools
454: uv run ce init-project ~/projects/app
455: 
456: # ‚ùå Avoid: Running from other locations
457: cd ~/projects/app
458: uv run ce init-project .  # May not find packages
459: ```
460: 
461: ---
462: 
463: ### 2. Use Dry Run for First Time
464: 
465: ```bash
466: # See what will happen before committing
467: uv run ce init-project ~/projects/app --dry-run
468: 
469: # Review output, then execute
470: uv run ce init-project ~/projects/app
471: ```
472: 
473: ---
474: 
475: ### 3. Commit .ce.old/ for Rollback
476: 
477: ```bash
478: # Before major upgrade
479: cd ~/projects/app
480: git add .ce.old/
481: git commit -m "Backup CE before upgrade"
482: 
483: # Now upgrade
484: uv run ce init-project . --phase all
485: 
486: # Rollback if needed
487: git checkout HEAD~1 -- .ce.old/
488: ```
489: 
490: ---
491: 
492: ### 4. Verify After Each Phase
493: 
494: ```bash
495: # Run phase by phase with verification
496: uv run ce init-project ~/projects/app --phase extract
497: ls -la ~/projects/app/.ce/  # Check files
498: 
499: uv run ce init-project ~/projects/app --phase blend
500: git diff ~/projects/app/CLAUDE.md  # Check merge
501: 
502: uv run ce init-project ~/projects/app --phase initialize
503: source ~/projects/app/.ce/tools/.venv/bin/activate
504: which ce  # Check command
505: 
506: uv run ce init-project ~/projects/app --phase verify
507: # Final check
508: ```
509: 
510: ---
511: 
512: ## Exit Codes
513: 
514: - `0`: All phases successful
515: - `1`: User error (invalid directory, missing files)
516: - `2`: Initialization error (phase failed)
517: 
518: ---
519: 
520: ## Performance
521: 
522: **Timing** (typical project):
523: - Extract: ~5 seconds (52 files)
524: - Blend: ~30 seconds (6 domains)
525: - Initialize: ~20 seconds (8 packages)
526: - Verify: ~2 seconds
527: 
528: **Total**: ~1 minute for full initialization
529: 
530: ---
531: 
532: ## Related Commands
533: 
534: - `ce blend` - Manual blending (called by init-project)
535: - `ce update-context` - Updates context after installation
536: - `ce validate` - Validates installed framework
537: 
538: ---
539: 
540: ## See Also
541: 
542: - [INITIALIZATION.md](INITIALIZATION.md) - Complete CE setup guide (manual process)
543: - [ce-blend-usage.md](ce-blend-usage.md) - Blend command details
544: - [PRP-36-INITIAL.md](../PRPs/feature-requests/PRP-36-INITIAL.md) - Init-project design
545: - `tmp/prp36test/PRP-36-TEST-SUMMARY.md` - E2E test results and known issues
</file>

<file path="tools/ce/blending/strategies/base.py">
 1: """Base classes for blending strategies."""
 2: 
 3: from abc import ABC, abstractmethod
 4: from typing import Any, Dict, Optional
 5: from pathlib import Path
 6: 
 7: 
 8: class BlendStrategy(ABC):
 9:     """
10:     Base class for all blending strategies.
11: 
12:     Each domain (settings, CLAUDE.md, memories, etc.) implements this interface
13:     to define how framework and target content should be blended.
14: 
15:     Philosophy: "Copy ours (framework), import theirs (target) where not contradictory"
16:     """
17: 
18:     @abstractmethod
19:     def can_handle(self, domain: str) -> bool:
20:         """
21:         Check if strategy can handle this domain.
22: 
23:         Args:
24:             domain: Domain name (settings, claude_md, memories, etc.)
25: 
26:         Returns:
27:             True if strategy can handle this domain
28: 
29:         Example:
30:             >>> strategy = SettingsBlendStrategy()
31:             >>> strategy.can_handle("settings")
32:             True
33:             >>> strategy.can_handle("claude_md")
34:             False
35:         """
36:         pass
37: 
38:     @abstractmethod
39:     def blend(
40:         self,
41:         framework_content: Any,
42:         target_content: Optional[Any],
43:         context: Dict[str, Any]
44:     ) -> Any:
45:         """
46:         Blend framework and target content.
47: 
48:         Args:
49:             framework_content: Framework version (always present, authoritative)
50:             target_content: Target version (may be None)
51:             context: Additional context:
52:                 - file_path: Path to output file
53:                 - dry_run: bool (if True, return result without writing)
54:                 - interactive: bool (if True, ask user for conflicts)
55:                 - backup_path: Path to backup (if created)
56:                 - rules_content: str (framework rules, e.g., RULES.md)
57:                 - llm_client: Anthropic client (for NL-based strategies)
58: 
59:         Returns:
60:             Blended content (type varies by domain)
61: 
62:         Raises:
63:             ValueError: If framework_content invalid
64:             RuntimeError: If blending fails
65: 
66:         Note: No fishy fallbacks - exceptions bubble up with actionable messages
67:         """
68:         pass
69: 
70:     @abstractmethod
71:     def validate(self, blended_content: Any, context: Dict[str, Any]) -> bool:
72:         """
73:         Validate blended content integrity.
74: 
75:         Args:
76:             blended_content: Result from blend()
77:             context: Additional context (same as blend())
78: 
79:         Returns:
80:             True if valid, False otherwise
81: 
82:         Raises:
83:             ValueError: If validation detects critical errors
84: 
85:         Example validation checks:
86:         - Settings: Valid JSON, all CE tools in one list
87:         - CLAUDE.md: Valid markdown, all framework sections present
88:         - Memories: Valid YAML headers, >= 23 files
89:         """
90:         pass
91: 
92:     def get_domain_name(self) -> str:
93:         """
94:         Get human-readable domain name.
95: 
96:         Returns:
97:             Domain name (e.g., "Settings JSON", "CLAUDE.md")
98:         """
99:         return self.__class__.__name__.replace("BlendStrategy", "")
</file>

<file path="tools/ce/blending/classification.py">
  1: """
  2: Classification Module (Phase B) - Bucket Initialization Pipeline
  3: 
  4: Validates collected files using Claude 4.5 Haiku for fast, cheap classification.
  5: 
  6: Functions:
  7: - validate_prp(file_path: str) -> ClassificationResult
  8: - validate_example(file_path: str) -> ClassificationResult
  9: - validate_memory(file_path: str) -> ClassificationResult
 10: - classify_with_haiku(file_path: str, file_type: str) -> ClassificationResult
 11: - is_garbage(file_path: str) -> bool
 12: 
 13: Classification Result:
 14: {
 15:   "valid": bool,
 16:   "confidence": float,  # 0.0-1.0
 17:   "issues": List[str],
 18:   "file_type": str,     # "prp", "example", "memory", "garbage"
 19: }
 20: """
 21: 
 22: import re
 23: import json
 24: from pathlib import Path
 25: from typing import Dict, List, Optional
 26: from dataclasses import dataclass
 27: import anthropic
 28: import os
 29: 
 30: 
 31: @dataclass
 32: class ClassificationResult:
 33:     """Result of file classification."""
 34:     valid: bool
 35:     confidence: float
 36:     issues: List[str]
 37:     file_type: str  # "prp", "example", "memory", "garbage"
 38: 
 39:     def to_dict(self) -> Dict:
 40:         """Convert to dictionary for JSON serialization."""
 41:         return {
 42:             "valid": self.valid,
 43:             "confidence": self.confidence,
 44:             "issues": self.issues,
 45:             "file_type": self.file_type
 46:         }
 47: 
 48: 
 49: def is_garbage(file_path: str) -> bool:
 50:     """
 51:     Check if file matches garbage patterns.
 52: 
 53:     Garbage patterns:
 54:     - *REPORT*.md
 55:     - *INITIAL*.md
 56:     - *summary*.md
 57:     - *analysis*.md
 58:     - *PLAN*.md
 59:     - *TODO*.md
 60: 
 61:     Args:
 62:         file_path: Path to file
 63: 
 64:     Returns:
 65:         True if file is garbage, False otherwise
 66:     """
 67:     filename = Path(file_path).name.lower()
 68:     garbage_patterns = [
 69:         "report", "initial", "summary",
 70:         "analysis", "plan", "todo"
 71:     ]
 72: 
 73:     return any(pattern in filename for pattern in garbage_patterns)
 74: 
 75: 
 76: def validate_prp(file_path: str) -> ClassificationResult:
 77:     """
 78:     Validate PRP file.
 79: 
 80:     Validation checks (in priority order):
 81:     1. File is not garbage (required)
 82:     2. PRP ID exists in content or YAML (required)
 83:     3. YAML header exists (optional, +0.2 confidence)
 84:     4. Standard sections exist (optional, +0.1 confidence)
 85: 
 86:     Args:
 87:         file_path: Path to PRP file
 88: 
 89:     Returns:
 90:         ClassificationResult with validation outcome
 91:     """
 92:     issues = []
 93:     confidence = 0.6  # Base confidence
 94: 
 95:     # Check garbage first
 96:     if is_garbage(file_path):
 97:         return ClassificationResult(
 98:             valid=False,
 99:             confidence=1.0,
100:             issues=["File matches garbage pattern"],
101:             file_type="garbage"
102:         )
103: 
104:     # Read file content
105:     try:
106:         with open(file_path, 'r', encoding='utf-8') as f:
107:             content = f.read()
108:     except Exception as e:
109:         return ClassificationResult(
110:             valid=False,
111:             confidence=1.0,
112:             issues=[f"Failed to read file: {e}"],
113:             file_type="unknown"
114:         )
115: 
116:     # Check for PRP ID in content (required)
117:     prp_id_pattern = r'(?:PRP-\d+(?:\.\d+)*|prp_id:\s*["\']?[\d.]+["\']?)'
118:     if not re.search(prp_id_pattern, content, re.IGNORECASE):
119:         issues.append("No PRP ID found in content or YAML")
120:         return ClassificationResult(
121:             valid=False,
122:             confidence=0.9,
123:             issues=issues,
124:             file_type="unknown"
125:         )
126: 
127:     # Check for YAML header (optional, +0.2 confidence)
128:     if re.match(r'^---\s*\n', content):
129:         confidence += 0.2
130:     else:
131:         issues.append("No YAML header (optional but recommended)")
132: 
133:     # Check for standard sections (optional, +0.1 confidence)
134:     standard_sections = ["TL;DR", "Context", "Implementation", "Validation"]
135:     found_sections = sum(1 for section in standard_sections if section in content)
136:     if found_sections >= 3:
137:         confidence += 0.1
138:     else:
139:         issues.append(f"Found {found_sections}/4 standard sections (optional)")
140: 
141:     # Cap confidence at 0.9 for deterministic validation
142:     confidence = min(confidence, 0.9)
143: 
144:     return ClassificationResult(
145:         valid=True,
146:         confidence=confidence,
147:         issues=issues,
148:         file_type="prp"
149:     )
150: 
151: 
152: def validate_example(file_path: str) -> ClassificationResult:
153:     """
154:     Validate example file.
155: 
156:     Validation checks:
157:     1. File is not garbage (required)
158:     2. H1 title exists (required)
159:     3. H2 sections exist (optional, +0.2 confidence)
160:     4. Code blocks exist (optional, +0.2 confidence)
161:     5. Substantial content (>500 chars, optional, +0.1 confidence)
162: 
163:     Args:
164:         file_path: Path to example file
165: 
166:     Returns:
167:         ClassificationResult with validation outcome
168:     """
169:     issues = []
170:     confidence = 0.5  # Base confidence
171: 
172:     # Check garbage first
173:     if is_garbage(file_path):
174:         return ClassificationResult(
175:             valid=False,
176:             confidence=1.0,
177:             issues=["File matches garbage pattern"],
178:             file_type="garbage"
179:         )
180: 
181:     # Read file content
182:     try:
183:         with open(file_path, 'r', encoding='utf-8') as f:
184:             content = f.read()
185:     except Exception as e:
186:         return ClassificationResult(
187:             valid=False,
188:             confidence=1.0,
189:             issues=[f"Failed to read file: {e}"],
190:             file_type="unknown"
191:         )
192: 
193:     # Check for H1 title (required)
194:     if not re.search(r'^#\s+.+', content, re.MULTILINE):
195:         issues.append("No H1 title found")
196:         return ClassificationResult(
197:             valid=False,
198:             confidence=0.9,
199:             issues=issues,
200:             file_type="unknown"
201:         )
202: 
203:     # Check for H2 sections (optional, +0.2 confidence)
204:     h2_sections = re.findall(r'^##\s+.+', content, re.MULTILINE)
205:     if len(h2_sections) >= 2:
206:         confidence += 0.2
207:     else:
208:         issues.append(f"Found {len(h2_sections)} H2 sections (recommended: ‚â•2)")
209: 
210:     # Check for code blocks (optional, +0.2 confidence)
211:     code_blocks = re.findall(r'```.*?```', content, re.DOTALL)
212:     if len(code_blocks) >= 1:
213:         confidence += 0.2
214:     else:
215:         issues.append("No code blocks found (recommended for examples)")
216: 
217:     # Check substantial content (optional, +0.1 confidence)
218:     if len(content) > 500:
219:         confidence += 0.1
220:     else:
221:         issues.append(f"Short content ({len(content)} chars, recommended: >500)")
222: 
223:     return ClassificationResult(
224:         valid=True,
225:         confidence=confidence,
226:         issues=issues,
227:         file_type="example"
228:     )
229: 
230: 
231: def validate_memory(file_path: str) -> ClassificationResult:
232:     """
233:     Validate memory file.
234: 
235:     Validation checks:
236:     1. File is not garbage (required)
237:     2. YAML frontmatter exists (required)
238:     3. Type field in YAML (regular|critical|user, required)
239:     4. Created/updated timestamps (optional, +0.1 confidence)
240:     5. Category field (optional, +0.1 confidence)
241: 
242:     Args:
243:         file_path: Path to memory file
244: 
245:     Returns:
246:         ClassificationResult with validation outcome
247:     """
248:     issues = []
249:     confidence = 0.6  # Base confidence
250: 
251:     # Check garbage first
252:     if is_garbage(file_path):
253:         return ClassificationResult(
254:             valid=False,
255:             confidence=1.0,
256:             issues=["File matches garbage pattern"],
257:             file_type="garbage"
258:         )
259: 
260:     # Read file content
261:     try:
262:         with open(file_path, 'r', encoding='utf-8') as f:
263:             content = f.read()
264:     except Exception as e:
265:         return ClassificationResult(
266:             valid=False,
267:             confidence=1.0,
268:             issues=[f"Failed to read file: {e}"],
269:             file_type="unknown"
270:         )
271: 
272:     # Check for YAML frontmatter (required)
273:     if not re.match(r'^---\s*\n', content):
274:         issues.append("No YAML frontmatter found")
275:         return ClassificationResult(
276:             valid=False,
277:             confidence=0.9,
278:             issues=issues,
279:             file_type="unknown"
280:         )
281: 
282:     # Extract YAML header
283:     yaml_match = re.match(r'^---\s*\n(.*?)\n---', content, re.DOTALL)
284:     if not yaml_match:
285:         issues.append("Malformed YAML frontmatter")
286:         return ClassificationResult(
287:             valid=False,
288:             confidence=0.9,
289:             issues=issues,
290:             file_type="unknown"
291:         )
292: 
293:     yaml_content = yaml_match.group(1)
294: 
295:     # Check for type field (required)
296:     type_match = re.search(r'type:\s*(regular|critical|user)', yaml_content)
297:     if not type_match:
298:         issues.append("No valid type field (must be: regular, critical, or user)")
299:         return ClassificationResult(
300:             valid=False,
301:             confidence=0.9,
302:             issues=issues,
303:             file_type="unknown"
304:         )
305: 
306:     confidence += 0.2  # Valid type field
307: 
308:     # Check for created/updated timestamps (optional, +0.1 confidence)
309:     if re.search(r'created:\s*["\']?\d{4}-\d{2}-\d{2}', yaml_content):
310:         confidence += 0.05
311:     else:
312:         issues.append("No created timestamp (optional)")
313: 
314:     if re.search(r'updated:\s*["\']?\d{4}-\d{2}-\d{2}', yaml_content):
315:         confidence += 0.05
316:     else:
317:         issues.append("No updated timestamp (optional)")
318: 
319:     # Check for category field (optional, +0.1 confidence)
320:     if re.search(r'category:\s*\w+', yaml_content):
321:         confidence += 0.1
322:     else:
323:         issues.append("No category field (optional)")
324: 
325:     return ClassificationResult(
326:         valid=True,
327:         confidence=confidence,
328:         issues=issues,
329:         file_type="memory"
330:     )
331: 
332: 
333: def classify_with_haiku(file_path: str, file_type: str) -> ClassificationResult:
334:     """
335:     Classify file using Claude 4.5 Haiku for uncertain cases.
336: 
337:     Used when deterministic validation has confidence < 0.9.
338: 
339:     Args:
340:         file_path: Path to file
341:         file_type: Expected file type ("prp", "example", "memory")
342: 
343:     Returns:
344:         ClassificationResult from Haiku analysis
345:     """
346:     # Read file content
347:     try:
348:         with open(file_path, 'r', encoding='utf-8') as f:
349:             content = f.read()
350:     except Exception as e:
351:         return ClassificationResult(
352:             valid=False,
353:             confidence=1.0,
354:             issues=[f"Failed to read file: {e}"],
355:             file_type="unknown"
356:         )
357: 
358:     # Prepare system prompt based on file type
359:     system_prompts = {
360:         "prp": """You are a PRP (Product Requirements Prompt) validator.
361: 
362: Analyze the file and determine:
363: 1. Is this a valid PRP? (contains PRP ID, describes a feature/task)
364: 2. Confidence level (0.0-1.0)
365: 3. Any issues found
366: 
367: Respond in JSON format:
368: {
369:   "valid": true/false,
370:   "confidence": 0.0-1.0,
371:   "issues": ["issue1", "issue2", ...]
372: }
373: 
374: Valid PRP indicators:
375: - Contains PRP ID (PRP-X or prp_id: X)
376: - Describes a feature or task
377: - Has implementation steps or acceptance criteria
378: 
379: Invalid indicators:
380: - Garbage file (REPORT, INITIAL, summary, analysis)
381: - No clear feature description
382: - Empty or minimal content""",
383: 
384:         "example": """You are an example file validator.
385: 
386: Analyze the file and determine:
387: 1. Is this a valid example? (demonstrates code patterns, has explanations)
388: 2. Confidence level (0.0-1.0)
389: 3. Any issues found
390: 
391: Respond in JSON format:
392: {
393:   "valid": true/false,
394:   "confidence": 0.0-1.0,
395:   "issues": ["issue1", "issue2", ...]
396: }
397: 
398: Valid example indicators:
399: - Contains code blocks
400: - Has explanatory text
401: - Demonstrates patterns or best practices
402: 
403: Invalid indicators:
404: - Garbage file (REPORT, INITIAL, summary)
405: - No code or minimal content
406: - Just a file listing""",
407: 
408:         "memory": """You are a Serena memory validator.
409: 
410: Analyze the file and determine:
411: 1. Is this a valid memory? (has YAML frontmatter with type field)
412: 2. Confidence level (0.0-1.0)
413: 3. Any issues found
414: 
415: Respond in JSON format:
416: {
417:   "valid": true/false,
418:   "confidence": 0.0-1.0,
419:   "issues": ["issue1", "issue2", ...]
420: }
421: 
422: Valid memory indicators:
423: - Has YAML frontmatter with --- delimiters
424: - Contains type field (regular, critical, or user)
425: - Has meaningful content
426: 
427: Invalid indicators:
428: - No YAML frontmatter
429: - No type field
430: - Garbage file (REPORT, INITIAL)"""
431:     }
432: 
433:     system_prompt = system_prompts.get(file_type, system_prompts["prp"])
434: 
435:     # Call Haiku API
436:     try:
437:         client = anthropic.Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))
438: 
439:         message = client.messages.create(
440:             model="claude-haiku-4-5",
441:             max_tokens=1024,
442:             temperature=0.0,
443:             system=system_prompt,
444:             messages=[
445:                 {
446:                     "role": "user",
447:                     "content": f"Classify this file:\n\n{content[:4000]}"  # Limit to 4000 chars
448:                 }
449:             ]
450:         )
451: 
452:         # Parse JSON response
453:         response_text = message.content[0].text
454: 
455:         # Extract JSON from response (may have markdown code blocks)
456:         json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
457:         if not json_match:
458:             return ClassificationResult(
459:                 valid=False,
460:                 confidence=0.5,
461:                 issues=["Haiku returned invalid JSON"],
462:                 file_type="unknown"
463:             )
464: 
465:         result = json.loads(json_match.group(0))
466: 
467:         return ClassificationResult(
468:             valid=result.get("valid", False),
469:             confidence=result.get("confidence", 0.5),
470:             issues=result.get("issues", []),
471:             file_type=file_type if result.get("valid") else "unknown"
472:         )
473: 
474:     except anthropic.APIError as e:
475:         return ClassificationResult(
476:             valid=False,
477:             confidence=0.5,
478:             issues=[f"Haiku API error: {e}"],
479:             file_type="unknown"
480:         )
481:     except Exception as e:
482:         return ClassificationResult(
483:             valid=False,
484:             confidence=0.5,
485:             issues=[f"Classification error: {e}"],
486:             file_type="unknown"
487:         )
488: 
489: 
490: def classify_file(file_path: str, expected_type: Optional[str] = None) -> ClassificationResult:
491:     """
492:     Classify a file using deterministic validation + Haiku fallback.
493: 
494:     Process:
495:     1. Run deterministic validator based on expected_type or file location
496:     2. If confidence < 0.9, call classify_with_haiku()
497:     3. Return final classification result
498: 
499:     Args:
500:         file_path: Path to file
501:         expected_type: Expected file type ("prp", "example", "memory"), or None to infer
502: 
503:     Returns:
504:         ClassificationResult with final classification
505:     """
506:     # Infer expected type from file path if not provided
507:     if expected_type is None:
508:         path_lower = file_path.lower()
509:         if "prp" in path_lower:
510:             expected_type = "prp"
511:         elif "example" in path_lower:
512:             expected_type = "example"
513:         elif "memor" in path_lower or "serena" in path_lower:
514:             expected_type = "memory"
515:         else:
516:             expected_type = "prp"  # Default to PRP
517: 
518:     # Run deterministic validator
519:     validators = {
520:         "prp": validate_prp,
521:         "example": validate_example,
522:         "memory": validate_memory
523:     }
524: 
525:     validator = validators.get(expected_type, validate_prp)
526:     result = validator(file_path)
527: 
528:     # If garbage, return immediately
529:     if result.file_type == "garbage":
530:         return result
531: 
532:     # If confidence < 0.9, use Haiku for additional validation
533:     if result.confidence < 0.9:
534:         haiku_result = classify_with_haiku(file_path, expected_type)
535: 
536:         # Combine results (use Haiku's validation, average confidence)
537:         return ClassificationResult(
538:             valid=haiku_result.valid,
539:             confidence=(result.confidence + haiku_result.confidence) / 2,
540:             issues=result.issues + haiku_result.issues,
541:             file_type=haiku_result.file_type if haiku_result.valid else "unknown"
542:         )
543: 
544:     return result
545: 
546: 
547: def main():
548:     """CLI interface for classification module."""
549:     import sys
550: 
551:     if len(sys.argv) < 2:
552:         print("Usage: python classification.py <file-path> [expected-type]")
553:         print("Expected types: prp, example, memory")
554:         sys.exit(1)
555: 
556:     file_path = sys.argv[1]
557:     expected_type = sys.argv[2] if len(sys.argv) > 2 else None
558: 
559:     result = classify_file(file_path, expected_type)
560: 
561:     print(json.dumps(result.to_dict(), indent=2))
562: 
563:     sys.exit(0 if result.valid else 1)
564: 
565: 
566: if __name__ == "__main__":
567:     main()
</file>

<file path="tools/ce/blending/llm_client.py">
  1: """LLM client for blending operations with Haiku + Sonnet hybrid support."""
  2: 
  3: import os
  4: import logging
  5: from typing import Dict, Any, Optional, List
  6: from anthropic import Anthropic
  7: 
  8: logger = logging.getLogger(__name__)
  9: 
 10: # Model configurations
 11: HAIKU_MODEL = "claude-3-5-haiku-20241022"
 12: SONNET_MODEL = "claude-sonnet-4-5-20250929"
 13: 
 14: # Blending philosophy system prompt
 15: BLENDING_PHILOSOPHY = """
 16: Blending Philosophy: "Copy ours (framework), import theirs (target) where not contradictory"
 17: 
 18: Rules:
 19: 1. Framework content is authoritative - preserve all framework sections
 20: 2. Target customizations that don't contradict framework are preserved
 21: 3. When conflict exists, framework wins (with explanation comment)
 22: 4. Additive merging preferred - combine rather than replace
 23: 5. User-specific content (names, paths, project details) always preserved
 24: """
 25: 
 26: 
 27: class BlendingLLM:
 28:     """
 29:     Claude SDK wrapper for blending operations.
 30: 
 31:     Provides hybrid model support:
 32:     - Haiku: Fast/cheap classification and similarity checks
 33:     - Sonnet: High-quality document blending
 34: 
 35:     Usage:
 36:         >>> llm = BlendingLLM()
 37:         >>> result = llm.blend_content(framework, target, rules)
 38:         >>> similarity = llm.check_similarity(text1, text2)
 39:         >>> classification = llm.classify_file(content)
 40:     """
 41: 
 42:     def __init__(
 43:         self,
 44:         api_key: Optional[str] = None,
 45:         timeout: int = 60,
 46:         max_retries: int = 3
 47:     ):
 48:         """
 49:         Initialize LLM client.
 50: 
 51:         Args:
 52:             api_key: Anthropic API key (defaults to ANTHROPIC_API_KEY env var)
 53:             timeout: Request timeout in seconds (default: 60)
 54:             max_retries: Max retry attempts for rate limits (default: 3)
 55: 
 56:         Raises:
 57:             ValueError: If API key not provided and not in environment
 58:         """
 59:         # Get API key from parameter or environment
 60:         self.api_key = api_key or os.environ.get("ANTHROPIC_API_KEY")
 61: 
 62:         if not self.api_key:
 63:             raise ValueError(
 64:                 "Anthropic API key required\n"
 65:                 "üîß Troubleshooting:\n"
 66:                 "  1. Set ANTHROPIC_API_KEY environment variable\n"
 67:                 "  2. Or pass api_key parameter to BlendingLLM()\n"
 68:                 "  3. Get key from: https://console.anthropic.com/settings/keys"
 69:             )
 70: 
 71:         self.timeout = timeout
 72:         self.max_retries = max_retries
 73: 
 74:         # Initialize Anthropic client
 75:         try:
 76:             self.client = Anthropic(
 77:                 api_key=self.api_key,
 78:                 timeout=timeout,
 79:                 max_retries=max_retries
 80:             )
 81:             logger.debug("‚úì BlendingLLM initialized")
 82:         except Exception as e:
 83:             raise RuntimeError(
 84:                 f"Failed to initialize Anthropic client: {e}\n"
 85:                 f"üîß Troubleshooting:\n"
 86:                 f"  1. Check API key is valid\n"
 87:                 f"  2. Verify network connectivity\n"
 88:                 f"  3. Check Anthropic status: https://status.anthropic.com"
 89:             ) from e
 90: 
 91:         # Token usage tracking
 92:         self.total_input_tokens = 0
 93:         self.total_output_tokens = 0
 94: 
 95:     def get_token_usage(self) -> Dict[str, int]:
 96:         """
 97:         Get cumulative token usage.
 98: 
 99:         Returns:
100:             Dict with input_tokens, output_tokens, total_tokens
101:         """
102:         return {
103:             "input_tokens": self.total_input_tokens,
104:             "output_tokens": self.total_output_tokens,
105:             "total_tokens": self.total_input_tokens + self.total_output_tokens
106:         }
107: 
108:     def _track_tokens(self, usage: Any) -> None:
109:         """Track tokens from API response usage object."""
110:         if usage:
111:             self.total_input_tokens += getattr(usage, 'input_tokens', 0)
112:             self.total_output_tokens += getattr(usage, 'output_tokens', 0)
113: 
114:     def blend_content(
115:         self,
116:         framework_content: str,
117:         target_content: Optional[str],
118:         rules_content: Optional[str] = None,
119:         domain: str = "unknown"
120:     ) -> Dict[str, Any]:
121:         """
122:         Blend framework and target content using Sonnet.
123: 
124:         Uses high-quality model for semantic understanding of blending philosophy.
125: 
126:         Args:
127:             framework_content: Framework version (authoritative)
128:             target_content: Target version (may be None)
129:             rules_content: Framework rules (e.g., RULES.md content)
130:             domain: Domain name for context (e.g., "claude_md", "memories")
131: 
132:         Returns:
133:             Dict with:
134:                 - blended: Blended content (str)
135:                 - model: Model used (str)
136:                 - tokens: Token usage dict
137:                 - confidence: Blend confidence 0.0-1.0 (float)
138: 
139:         Raises:
140:             RuntimeError: If API call fails after retries
141:         """
142:         logger.info(f"Blending {domain} content with Sonnet...")
143: 
144:         # Build prompt
145:         prompt_parts = [BLENDING_PHILOSOPHY]
146: 
147:         if rules_content:
148:             prompt_parts.append(f"\n## Framework Rules\n\n{rules_content}")
149: 
150:         prompt_parts.append(f"\n## Framework Content\n\n{framework_content}")
151: 
152:         if target_content:
153:             prompt_parts.append(f"\n## Target Content\n\n{target_content}")
154:         else:
155:             # No target content - just validate framework
156:             prompt_parts.append("\n## Target Content\n\n(No existing content)")
157: 
158:         prompt_parts.append(
159:             f"\n## Task\n\n"
160:             f"Blend the framework and target content for the '{domain}' domain. "
161:             f"Follow the blending philosophy above. Output ONLY the blended content, "
162:             f"no explanations or markdown code blocks."
163:         )
164: 
165:         prompt = "".join(prompt_parts)
166: 
167:         try:
168:             # Call Sonnet
169:             response = self.client.messages.create(
170:                 model=SONNET_MODEL,
171:                 max_tokens=8192,
172:                 messages=[
173:                     {"role": "user", "content": prompt}
174:                 ]
175:             )
176: 
177:             # Track tokens
178:             self._track_tokens(response.usage)
179: 
180:             # Extract blended content
181:             blended = response.content[0].text
182: 
183:             # Calculate confidence (heuristic: presence of both framework and target markers)
184:             confidence = 1.0
185:             if target_content and target_content.strip():
186:                 # Check if blend includes elements from both
187:                 has_framework = any(
188:                     marker in blended
189:                     for marker in ["## Core Principles", "## Quick Commands"]
190:                 )
191:                 has_target = len(blended) > len(framework_content) * 0.8
192:                 confidence = 0.9 if (has_framework and has_target) else 0.7
193: 
194:             logger.info(
195:                 f"‚úì Blended {domain} "
196:                 f"({response.usage.input_tokens} in, "
197:                 f"{response.usage.output_tokens} out)"
198:             )
199: 
200:             return {
201:                 "blended": blended,
202:                 "model": SONNET_MODEL,
203:                 "tokens": {
204:                     "input": response.usage.input_tokens,
205:                     "output": response.usage.output_tokens
206:                 },
207:                 "confidence": confidence
208:             }
209: 
210:         except Exception as e:
211:             logger.error(f"‚ùå Blending failed: {e}")
212:             raise RuntimeError(
213:                 f"Sonnet blend_content() failed for {domain}: {e}\n"
214:                 f"üîß Troubleshooting:\n"
215:                 f"  1. Check API key is valid\n"
216:                 f"  2. Check network connectivity\n"
217:                 f"  3. Verify content size < 200k tokens\n"
218:                 f"  4. Check rate limits: https://console.anthropic.com/settings/limits"
219:             ) from e
220: 
221:     def check_similarity(
222:         self,
223:         text1: str,
224:         text2: str,
225:         threshold: float = 0.9
226:     ) -> Dict[str, Any]:
227:         """
228:         Check semantic similarity between two texts using Haiku.
229: 
230:         Fast, cheap operation for similarity scoring.
231: 
232:         Args:
233:             text1: First text
234:             text2: Second text
235:             threshold: Similarity threshold 0.0-1.0 (default: 0.9)
236: 
237:         Returns:
238:             Dict with:
239:                 - similar: Boolean (similarity >= threshold)
240:                 - score: Similarity score 0.0-1.0 (float)
241:                 - model: Model used (str)
242:                 - tokens: Token usage dict
243: 
244:         Raises:
245:             RuntimeError: If API call fails after retries
246:         """
247:         logger.debug("Checking similarity with Haiku...")
248: 
249:         prompt = f"""Compare these two texts for semantic similarity.
250: 
251: Text 1:
252: {text1[:1000]}...
253: 
254: Text 2:
255: {text2[:1000]}...
256: 
257: Rate similarity on scale 0.0-1.0 where:
258: - 0.0 = Completely different topics/purposes
259: - 0.5 = Related but distinct content
260: - 0.9 = Very similar, likely duplicates
261: - 1.0 = Identical or nearly identical
262: 
263: Output ONLY a number between 0.0 and 1.0, nothing else."""
264: 
265:         try:
266:             response = self.client.messages.create(
267:                 model=HAIKU_MODEL,
268:                 max_tokens=10,
269:                 messages=[
270:                     {"role": "user", "content": prompt}
271:                 ]
272:             )
273: 
274:             # Track tokens
275:             self._track_tokens(response.usage)
276: 
277:             # Parse similarity score
278:             score_text = response.content[0].text.strip()
279:             try:
280:                 score = float(score_text)
281:                 score = max(0.0, min(1.0, score))  # Clamp to [0.0, 1.0]
282:             except ValueError:
283:                 # Failed to parse - default to low similarity
284:                 logger.warning(f"Failed to parse similarity score: {score_text}")
285:                 score = 0.0
286: 
287:             similar = score >= threshold
288: 
289:             logger.debug(
290:                 f"Similarity: {score:.2f} ({'similar' if similar else 'different'})"
291:             )
292: 
293:             return {
294:                 "similar": similar,
295:                 "score": score,
296:                 "model": HAIKU_MODEL,
297:                 "tokens": {
298:                     "input": response.usage.input_tokens,
299:                     "output": response.usage.output_tokens
300:                 }
301:             }
302: 
303:         except Exception as e:
304:             logger.error(f"‚ùå Similarity check failed: {e}")
305:             raise RuntimeError(
306:                 f"Haiku check_similarity() failed: {e}\n"
307:                 f"üîß Troubleshooting:\n"
308:                 f"  1. Check API key is valid\n"
309:                 f"  2. Check network connectivity\n"
310:                 f"  3. Verify text size reasonable (< 1000 chars per text)\n"
311:                 f"  4. Check rate limits"
312:             ) from e
313: 
314:     def classify_file(
315:         self,
316:         content: str,
317:         expected_patterns: List[str],
318:         file_path: str = "unknown"
319:     ) -> Dict[str, Any]:
320:         """
321:         Classify file content for CE pattern compliance using Haiku.
322: 
323:         Fast validation for Phase B classification.
324: 
325:         Args:
326:             content: File content to classify
327:             expected_patterns: List of expected CE patterns (e.g., ["YAML header", "## sections"])
328:             file_path: File path for context
329: 
330:         Returns:
331:             Dict with:
332:                 - valid: Boolean (passes CE pattern checks)
333:                 - confidence: Confidence score 0.0-1.0 (float)
334:                 - issues: List of validation issues (List[str])
335:                 - model: Model used (str)
336:                 - tokens: Token usage dict
337: 
338:         Raises:
339:             RuntimeError: If API call fails after retries
340:         """
341:         logger.debug(f"Classifying {file_path} with Haiku...")
342: 
343:         patterns_text = "\n".join(f"- {p}" for p in expected_patterns)
344: 
345:         prompt = f"""Validate this file against CE (Context Engineering) patterns.
346: 
347: File: {file_path}
348: 
349: Expected patterns:
350: {patterns_text}
351: 
352: Content (first 2000 chars):
353: {content[:2000]}...
354: 
355: Respond in this format:
356: VALID: yes/no
357: CONFIDENCE: 0.0-1.0
358: ISSUES: comma-separated list of issues (or "none")
359: 
360: Example:
361: VALID: yes
362: CONFIDENCE: 0.95
363: ISSUES: none"""
364: 
365:         try:
366:             response = self.client.messages.create(
367:                 model=HAIKU_MODEL,
368:                 max_tokens=100,
369:                 messages=[
370:                     {"role": "user", "content": prompt}
371:                 ]
372:             )
373: 
374:             # Track tokens
375:             self._track_tokens(response.usage)
376: 
377:             # Parse classification result
378:             result_text = response.content[0].text.strip()
379: 
380:             valid = False
381:             confidence = 0.0
382:             issues = []
383: 
384:             for line in result_text.split('\n'):
385:                 line = line.strip()
386:                 if line.startswith('VALID:'):
387:                     valid = 'yes' in line.lower()
388:                 elif line.startswith('CONFIDENCE:'):
389:                     try:
390:                         confidence = float(line.split(':')[1].strip())
391:                         confidence = max(0.0, min(1.0, confidence))
392:                     except (ValueError, IndexError):
393:                         confidence = 0.5
394:                 elif line.startswith('ISSUES:'):
395:                     issues_text = line.split(':', 1)[1].strip()
396:                     if issues_text.lower() != 'none':
397:                         issues = [i.strip() for i in issues_text.split(',')]
398: 
399:             logger.debug(
400:                 f"Classification: {'valid' if valid else 'invalid'} "
401:                 f"(confidence: {confidence:.2f})"
402:             )
403: 
404:             return {
405:                 "valid": valid,
406:                 "confidence": confidence,
407:                 "issues": issues,
408:                 "model": HAIKU_MODEL,
409:                 "tokens": {
410:                     "input": response.usage.input_tokens,
411:                     "output": response.usage.output_tokens
412:                 }
413:             }
414: 
415:         except Exception as e:
416:             logger.error(f"‚ùå Classification failed: {e}")
417:             raise RuntimeError(
418:                 f"Haiku classify_file() failed for {file_path}: {e}\n"
419:                 f"üîß Troubleshooting:\n"
420:                 f"  1. Check API key is valid\n"
421:                 f"  2. Check network connectivity\n"
422:                 f"  3. Verify content size < 200k tokens\n"
423:                 f"  4. Check rate limits"
424:             ) from e
</file>

<file path="tools/ce/blending/validation.py">
  1: """Post-blend validation framework."""
  2: 
  3: import json
  4: import logging
  5: from pathlib import Path
  6: from typing import Dict, Any, List, Tuple
  7: 
  8: logger = logging.getLogger(__name__)
  9: 
 10: 
 11: def validate_settings_json(file_path: Path) -> Tuple[bool, List[str]]:
 12:     """
 13:     Validate settings.local.json after blending.
 14: 
 15:     Checks:
 16:     1. Valid JSON
 17:     2. Has allow/deny/ask lists
 18:     3. Each CE tool in ONE list only
 19: 
 20:     Args:
 21:         file_path: Path to settings.local.json
 22: 
 23:     Returns:
 24:         (is_valid, errors) tuple
 25:     """
 26:     errors = []
 27: 
 28:     if not file_path.exists():
 29:         return (False, [f"Settings file not found: {file_path}"])
 30: 
 31:     try:
 32:         with open(file_path) as f:
 33:             settings = json.load(f)
 34:     except json.JSONDecodeError as e:
 35:         return (False, [f"Invalid JSON: {e}"])
 36: 
 37:     # Check required lists exist
 38:     for list_name in ['allow', 'deny', 'ask']:
 39:         if list_name not in settings:
 40:             errors.append(f"Missing '{list_name}' list")
 41: 
 42:     if errors:
 43:         return (False, errors)
 44: 
 45:     # Check single membership (each tool in ONE list only)
 46:     allow_set = set(settings.get('allow', []))
 47:     deny_set = set(settings.get('deny', []))
 48:     ask_set = set(settings.get('ask', []))
 49: 
 50:     # Find tools in multiple lists
 51:     all_tools = allow_set | deny_set | ask_set
 52:     for tool in all_tools:
 53:         count = 0
 54:         if tool in allow_set:
 55:             count += 1
 56:         if tool in deny_set:
 57:             count += 1
 58:         if tool in ask_set:
 59:             count += 1
 60: 
 61:         if count > 1:
 62:             errors.append(f"Tool in multiple lists: {tool}")
 63: 
 64:     return (len(errors) == 0, errors)
 65: 
 66: 
 67: def validate_claude_md(file_path: Path, required_sections: List[str]) -> Tuple[bool, List[str]]:
 68:     """
 69:     Validate CLAUDE.md after blending.
 70: 
 71:     Checks:
 72:     1. File exists
 73:     2. Valid markdown
 74:     3. All framework sections present
 75: 
 76:     Args:
 77:         file_path: Path to CLAUDE.md
 78:         required_sections: List of required section headers (e.g., ["## Quick Commands"])
 79: 
 80:     Returns:
 81:         (is_valid, errors) tuple
 82:     """
 83:     errors = []
 84: 
 85:     if not file_path.exists():
 86:         return (False, [f"CLAUDE.md not found: {file_path}"])
 87: 
 88:     try:
 89:         with open(file_path) as f:
 90:             content = f.read()
 91:     except OSError as e:
 92:         return (False, [f"Cannot read CLAUDE.md: {e}"])
 93: 
 94:     # Check required sections present
 95:     for section in required_sections:
 96:         if section not in content:
 97:             errors.append(f"Missing section: {section}")
 98: 
 99:     # Basic markdown validation
100:     if len(content) < 100:
101:         errors.append("CLAUDE.md too short (< 100 chars)")
102: 
103:     return (len(errors) == 0, errors)
104: 
105: 
106: def validate_memories(memories_dir: Path, min_count: int = 23) -> Tuple[bool, List[str]]:
107:     """
108:     Validate Serena memories after blending.
109: 
110:     Checks:
111:     1. Directory exists
112:     2. At least min_count memory files
113:     3. All have valid YAML headers
114: 
115:     Args:
116:         memories_dir: Path to .serena/memories/
117:         min_count: Minimum expected memory files (default: 23 framework memories)
118: 
119:     Returns:
120:         (is_valid, errors) tuple
121:     """
122:     errors = []
123: 
124:     if not memories_dir.exists():
125:         return (False, [f"Memories directory not found: {memories_dir}"])
126: 
127:     # Count memory files
128:     memory_files = list(memories_dir.glob('*.md'))
129: 
130:     if len(memory_files) < min_count:
131:         errors.append(f"Too few memories: {len(memory_files)} (expected >= {min_count})")
132: 
133:     # Check YAML headers (sample first 5)
134:     import yaml
135: 
136:     for memory_file in memory_files[:5]:
137:         try:
138:             with open(memory_file) as f:
139:                 content = f.read()
140: 
141:             if not content.startswith('---'):
142:                 errors.append(f"Missing YAML header: {memory_file.name}")
143:                 continue
144: 
145:             # Parse YAML frontmatter
146:             yaml_end = content.find('---', 3)
147:             if yaml_end == -1:
148:                 errors.append(f"Invalid YAML header: {memory_file.name}")
149:                 continue
150: 
151:             yaml_content = content[3:yaml_end]
152:             yaml_data = yaml.safe_load(yaml_content)
153: 
154:             if not yaml_data or 'type' not in yaml_data:
155:                 errors.append(f"YAML missing 'type': {memory_file.name}")
156: 
157:         except Exception as e:
158:             errors.append(f"Cannot parse {memory_file.name}: {e}")
159: 
160:     return (len(errors) == 0, errors)
161: 
162: 
163: def validate_all_domains(target_dir: Path, config: Dict[str, Any]) -> Dict[str, Tuple[bool, List[str]]]:
164:     """
165:     Run validation on all domains.
166: 
167:     Args:
168:         target_dir: Target project directory
169:         config: Blend configuration
170: 
171:     Returns:
172:         Dict of {domain: (is_valid, errors)}
173:     """
174:     results = {}
175: 
176:     # Validate settings
177:     settings_path = target_dir / '.claude' / 'settings.local.json'
178:     results['settings'] = validate_settings_json(settings_path)
179: 
180:     # Validate CLAUDE.md
181:     claude_md_path = target_dir / 'CLAUDE.md'
182:     required_sections = [
183:         "## Core Principles",
184:         "## Quick Commands",
185:         "## Project Structure"
186:     ]
187:     results['claude_md'] = validate_claude_md(claude_md_path, required_sections)
188: 
189:     # Validate memories
190:     memories_dir = target_dir / '.serena' / 'memories'
191:     results['memories'] = validate_memories(memories_dir, min_count=23)
192: 
193:     return results
</file>

<file path="tools/ce/examples/syntropy/filesystem_patterns.py">
  1: """
  2: Filesystem MCP File Operations Patterns
  3: 
  4: Filesystem provides file read/write/list operations through Syntropy.
  5: Pattern: mcp__syntropy__filesystem__<operation>
  6: """
  7: 
  8: 
  9: # ‚úÖ PATTERN 1: Read Text File
 10: # Use when: Reading config files, markdown, non-code text
 11: def example_read_text_file():
 12:     """Read configuration or text files."""
 13:     from mcp__syntropy import syntropy_filesystem_read_text_file
 14:     
 15:     # Read YAML config
 16:     result = syntropy_filesystem_read_text_file(
 17:         path="/Users/bprzybysz/nc-src/ctx-eng-plus/.ce/linear-defaults.yml"
 18:     )
 19:     # Returns: File contents as string
 20:     
 21:     # Read markdown documentation
 22:     result = syntropy_filesystem_read_text_file(
 23:         path="/Users/bprzybysz/nc-src/ctx-eng-plus/README.md"
 24:     )
 25:     # Returns: Full markdown content
 26: 
 27: 
 28: # ‚úÖ PATTERN 2: Write File
 29: # Use when: Creating new files or overwriting existing
 30: def example_write_file():
 31:     """Write file contents (creates or overwrites)."""
 32:     from mcp__syntropy import syntropy_filesystem_write_file
 33:     
 34:     # Create new config file
 35:     syntropy_filesystem_write_file(
 36:         path="/Users/bprzybysz/nc-src/ctx-eng-plus/config.yml",
 37:         content="""
 38:         project: Context Engineering
 39:         version: 1.0.0
 40:         features:
 41:           - prp-system
 42:           - mcp-integration
 43:         """
 44:     )
 45:     
 46:     # Overwrite existing file
 47:     syntropy_filesystem_write_file(
 48:         path="/Users/bprzybysz/nc-src/ctx-eng-plus/temp.txt",
 49:         content="Updated content"
 50:     )
 51:     # ‚ö†Ô∏è Warning: Completely replaces file, no backup
 52: 
 53: 
 54: # ‚úÖ PATTERN 3: Edit File (Surgical Changes)
 55: # Use when: Making precise line-by-line edits
 56: def example_edit_file():
 57:     """Make targeted edits to existing files."""
 58:     from mcp__syntropy import syntropy_filesystem_edit_file
 59:     
 60:     # Single edit
 61:     result = syntropy_filesystem_edit_file(
 62:         path="/Users/bprzybysz/nc-src/ctx-eng-plus/config.py",
 63:         edits=[
 64:             {
 65:                 "oldText": "DEBUG = False",
 66:                 "newText": "DEBUG = True"
 67:             }
 68:         ]
 69:     )
 70:     
 71:     # Multiple edits in one call
 72:     result = syntropy_filesystem_edit_file(
 73:         path="/Users/bprzybysz/nc-src/ctx-eng-plus/config.py",
 74:         edits=[
 75:             {
 76:                 "oldText": "DEBUG = False",
 77:                 "newText": "DEBUG = True"
 78:             },
 79:             {
 80:                 "oldText": "LOG_LEVEL = 'info'",
 81:                 "newText": "LOG_LEVEL = 'debug'"
 82:             }
 83:         ]
 84:     )
 85:     # ‚úÖ Safer than write: Only changes specified portions
 86: 
 87: 
 88: # ‚úÖ PATTERN 4: List Directory Contents
 89: # Use when: Exploring directory structure
 90: def example_list_directory():
 91:     """List files and subdirectories."""
 92:     from mcp__syntropy import syntropy_filesystem_list_directory
 93:     
 94:     # List project root
 95:     result = syntropy_filesystem_list_directory(
 96:         path="/Users/bprzybysz/nc-src/ctx-eng-plus"
 97:     )
 98:     # Returns: [FILE] name.ext, [DIR] dirname
 99:     
100:     # List specific subdirectory
101:     result = syntropy_filesystem_list_directory(
102:         path="/Users/bprzybysz/nc-src/ctx-eng-plus/tools/ce"
103:     )
104:     # Returns: All Python modules in ce/
105: 
106: 
107: # ‚úÖ PATTERN 5: Search Files by Pattern
108: # Use when: Finding files matching glob pattern
109: def example_search_files():
110:     """Find files matching name pattern."""
111:     from mcp__syntropy import syntropy_filesystem_search_files
112:     
113:     # Find all test files
114:     result = syntropy_filesystem_search_files(
115:         directory="/Users/bprzybysz/nc-src/ctx-eng-plus/tools",
116:         pattern="test_*.py"
117:     )
118:     # Returns: List of matching file paths
119:     
120:     # Find all markdown files
121:     result = syntropy_filesystem_search_files(
122:         directory="/Users/bprzybysz/nc-src/ctx-eng-plus",
123:         pattern="*.md"
124:     )
125:     
126:     # Find PRPs
127:     result = syntropy_filesystem_search_files(
128:         directory="/Users/bprzybysz/nc-src/ctx-eng-plus/PRPs",
129:         pattern="PRP-*.md"
130:     )
131: 
132: 
133: # ‚úÖ PATTERN 6: Get Directory Tree
134: # Use when: Visualizing project structure
135: def example_directory_tree():
136:     """Get hierarchical directory structure."""
137:     from mcp__syntropy import syntropy_filesystem_directory_tree
138:     
139:     # Full tree
140:     result = syntropy_filesystem_directory_tree(
141:         path="/Users/bprzybysz/nc-src/ctx-eng-plus/tools/ce",
142:         max_depth=3
143:     )
144:     # Returns: JSON tree structure
145:     
146:     # Shallow tree (just immediate children)
147:     result = syntropy_filesystem_directory_tree(
148:         path="/Users/bprzybysz/nc-src/ctx-eng-plus",
149:         max_depth=1
150:     )
151: 
152: 
153: # ‚úÖ PATTERN 7: Get File Info
154: # Use when: Getting metadata (size, timestamp, permissions)
155: def example_get_file_info():
156:     """Get file metadata."""
157:     from mcp__syntropy import syntropy_filesystem_get_file_info
158:     
159:     result = syntropy_filesystem_get_file_info(
160:         path="/Users/bprzybysz/nc-src/ctx-eng-plus/tools/ce/core.py"
161:     )
162:     # Returns: {size, modified_time, permissions, is_directory, ...}
163: 
164: 
165: # ‚úÖ PATTERN 8: List Allowed Directories
166: # Use when: Checking sandbox boundaries
167: def example_list_allowed_directories():
168:     """Check which directories are accessible."""
169:     from mcp__syntropy import syntropy_filesystem_list_allowed_directories
170:     
171:     result = syntropy_filesystem_list_allowed_directories()
172:     # Returns: List of accessible directory paths
173: 
174: 
175: # üìä PERFORMANCE CHARACTERISTICS
176: # - Read file: O(file_size) - sequential read
177: # - Write file: O(content_size) - sequential write
178: # - List directory: O(n) where n = items in directory
179: # - Search files: O(n*pattern_complexity)
180: # - Directory tree: O(n*depth) where n = items per level
181: 
182: 
183: # üéØ WORKFLOW EXAMPLE: Configuration Management
184: def config_workflow():
185:     """Typical workflow: Read config, modify, write back."""
186:     from mcp__syntropy import (
187:         syntropy_filesystem_read_text_file,
188:         syntropy_filesystem_edit_file
189:     )
190:     
191:     # Step 1: Read current config
192:     config = syntropy_filesystem_read_text_file(
193:         path="/Users/bprzybysz/nc-src/ctx-eng-plus/.ce/config.yml"
194:     )
195:     print(f"Current config:\n{config}")
196:     
197:     # Step 2: Make targeted edit
198:     result = syntropy_filesystem_edit_file(
199:         path="/Users/bprzybysz/nc-src/ctx-eng-plus/.ce/config.yml",
200:         edits=[
201:             {
202:                 "oldText": "cache_ttl: 5",
203:                 "newText": "cache_ttl: 10"
204:             }
205:         ]
206:     )
207:     print(f"Edit successful: {result['success']}")
208: 
209: 
210: # üéØ WORKFLOW EXAMPLE: Project Exploration
211: def exploration_workflow():
212:     """Typical workflow: Explore project structure."""
213:     from mcp__syntropy import (
214:         syntropy_filesystem_list_directory,
215:         syntropy_filesystem_directory_tree,
216:         syntropy_filesystem_search_files
217:     )
218:     
219:     # Step 1: List root directory
220:     root_items = syntropy_filesystem_list_directory(
221:         path="/Users/bprzybysz/nc-src/ctx-eng-plus"
222:     )
223:     
224:     # Step 2: Get tree view of specific subtree
225:     tree = syntropy_filesystem_directory_tree(
226:         path="/Users/bprzybysz/nc-src/ctx-eng-plus/tools/ce",
227:         max_depth=2
228:     )
229:     
230:     # Step 3: Find all Python test files
231:     tests = syntropy_filesystem_search_files(
232:         directory="/Users/bprzybysz/nc-src/ctx-eng-plus/tools",
233:         pattern="test_*.py"
234:     )
235:     print(f"Found {len(tests)} test files")
236: 
237: 
238: # üîß ERROR HANDLING PATTERNS
239: 
240: # ‚úÖ PATTERN: Safe Edit with Validation
241: def safe_edit_pattern():
242:     """Edit file safely with validation."""
243:     from mcp__syntropy import (
244:         syntropy_filesystem_read_text_file,
245:         syntropy_filesystem_edit_file
246:     )
247:     
248:     # Step 1: Read current content
249:     current = syntropy_filesystem_read_text_file(
250:         path="/Users/bprzybysz/nc-src/ctx-eng-plus/config.py"
251:     )
252:     
253:     # Step 2: Verify old_string exists
254:     if "old_value = 'X'" not in current:
255:         raise ValueError("Expected text not found - file may have changed\nüîß Troubleshooting: Check inputs and system state")
256:     
257:     # Step 3: Make edit
258:     result = syntropy_filesystem_edit_file(
259:         path="/Users/bprzybysz/nc-src/ctx-eng-plus/config.py",
260:         edits=[
261:             {
262:                 "oldText": "old_value = 'X'",
263:                 "newText": "old_value = 'Y'"
264:             }
265:         ]
266:     )
267:     
268:     if not result.get("success"):
269:         raise RuntimeError(f"Edit failed: {result.get('error')}\nüîß Troubleshooting: Check inputs and system state")
270: 
271: 
272: # ‚úÖ PATTERN: Batch File Operations
273: def batch_operations():
274:     """Process multiple files efficiently."""
275:     from mcp__syntropy import (
276:         syntropy_filesystem_search_files,
277:         syntropy_filesystem_read_text_file
278:     )
279:     
280:     # Find all config files
281:     configs = syntropy_filesystem_search_files(
282:         directory="/Users/bprzybysz/nc-src/ctx-eng-plus",
283:         pattern="*.yml"
284:     )
285:     
286:     # Read each one
287:     results = {}
288:     for config_path in configs:
289:         content = syntropy_filesystem_read_text_file(path=config_path)
290:         results[config_path] = content
291:     
292:     return results
293: 
294: 
295: # üîß TROUBLESHOOTING
296: def troubleshooting():
297:     """Common issues and solutions."""
298:     # Issue: File not found
299:     # Solution: Use absolute paths, verify directory exists
300:     
301:     # Issue: Edit oldText not found
302:     # Solution: Text must match exactly (whitespace, line endings)
303:     # Use read_text_file first to verify exact content
304:     
305:     # Issue: Permission denied
306:     # Solution: Check file permissions, ensure path is in allowed directories
307:     # Use list_allowed_directories() to verify access
308:     
309:     # Issue: Large file operations slow
310:     # Solution: Use edit_file for surgical changes (faster than write)
311:     # Batch multiple edits in single call when possible
</file>

<file path="tools/ce/toml_formats/__init__.py">
 1: """TOML format handlers for multi-format TOML merging."""
 2: 
 3: from .version_resolver import VersionResolver
 4: from .pep621_handler import PEP621Handler
 5: from .poetry_handler import PoetryHandler
 6: from .setuptools_handler import SetuptoolsHandler
 7: 
 8: __all__ = [
 9:     'VersionResolver',
10:     'PEP621Handler',
11:     'PoetryHandler',
12:     'SetuptoolsHandler'
13: ]
</file>

<file path="tools/ce/toml_formats/pep621_handler.py">
 1: """PEP 621 format handler."""
 2: 
 3: from typing import Dict
 4: 
 5: 
 6: class PEP621Handler:
 7:     """Handles PEP 621 format TOML files."""
 8: 
 9:     @staticmethod
10:     def detect(toml_data: Dict) -> bool:
11:         """Check if TOML uses PEP 621 format."""
12:         return "project" in toml_data
13: 
14:     @staticmethod
15:     def extract_dependencies(toml_data: Dict) -> Dict[str, list]:
16:         """
17:         Extract dependencies from PEP 621 TOML.
18: 
19:         Returns:
20:             Dict with 'prod' and 'dev' keys containing dependency lists
21:         """
22:         result = {"prod": [], "dev": []}
23: 
24:         # Production dependencies
25:         if "project" in toml_data and "dependencies" in toml_data["project"]:
26:             result["prod"] = toml_data["project"]["dependencies"]
27: 
28:         # Dev dependencies (PEP 735 dependency groups)
29:         if "dependency-groups" in toml_data:
30:             if "dev" in toml_data["dependency-groups"]:
31:                 result["dev"] = toml_data["dependency-groups"]["dev"]
32: 
33:         return result
34: 
35:     @staticmethod
36:     def preserve_metadata(target_data: Dict, merged_data: Dict) -> Dict:
37:         """
38:         Preserve target's extra metadata in merged TOML.
39: 
40:         Args:
41:             target_data: Original target TOML
42:             merged_data: Merged TOML data
43: 
44:         Returns:
45:             Merged data with preserved metadata
46:         """
47:         if "project" not in target_data or "project" not in merged_data:
48:             return merged_data
49: 
50:         # Preserve these fields from target if present
51:         preserve_keys = ["authors", "maintainers", "urls", "license", "keywords", "classifiers"]
52: 
53:         for key in preserve_keys:
54:             if key in target_data["project"] and key not in merged_data["project"]:
55:                 merged_data["project"][key] = target_data["project"][key]
56: 
57:         return merged_data
</file>

<file path="tools/ce/toml_formats/poetry_handler.py">
 1: """Poetry format handler."""
 2: 
 3: from typing import Dict
 4: 
 5: 
 6: class PoetryHandler:
 7:     """Handles Poetry format TOML files."""
 8: 
 9:     @staticmethod
10:     def detect(toml_data: Dict) -> bool:
11:         """Check if TOML uses Poetry format."""
12:         return "tool" in toml_data and "poetry" in toml_data.get("tool", {})
13: 
14:     @staticmethod
15:     def convert_to_pep621(poetry_data: Dict) -> Dict:
16:         """
17:         Convert Poetry format to PEP 621 format.
18: 
19:         Args:
20:             poetry_data: Poetry-formatted TOML data
21: 
22:         Returns:
23:             PEP 621-formatted TOML data
24:         """
25:         pep621 = {"project": {}, "dependency-groups": {}}
26:         poetry = poetry_data["tool"]["poetry"]
27: 
28:         # Convert basic metadata
29:         pep621["project"]["name"] = poetry.get("name", "")
30:         pep621["project"]["version"] = poetry.get("version", "0.1.0")
31:         pep621["project"]["description"] = poetry.get("description", "")
32: 
33:         # Convert dependencies
34:         if "dependencies" in poetry:
35:             pep621["project"]["dependencies"] = []
36:             for pkg, version in poetry["dependencies"].items():
37:                 if pkg == "python":
38:                     pep621["project"]["requires-python"] = version
39:                 else:
40:                     pep621["project"]["dependencies"].append(
41:                         PoetryHandler._convert_dep(pkg, version)
42:                     )
43: 
44:         # Convert dev-dependencies
45:         if "dev-dependencies" in poetry:
46:             pep621["dependency-groups"]["dev"] = [
47:                 PoetryHandler._convert_dep(pkg, version)
48:                 for pkg, version in poetry["dev-dependencies"].items()
49:             ]
50: 
51:         return pep621
52: 
53:     @staticmethod
54:     def _convert_dep(package: str, version) -> str:
55:         """
56:         Convert Poetry dependency to PEP 621 format.
57: 
58:         Args:
59:             package: Package name
60:             version: Poetry version specifier (string or dict)
61: 
62:         Returns:
63:             PEP 621 dependency string
64:         """
65:         if isinstance(version, dict):
66:             version_str = version.get("version", "")
67:         else:
68:             version_str = str(version)
69: 
70:         # Convert Poetry caret (^) to PEP 440
71:         # ^1.2.3 ‚Üí >=1.2.3,<2.0.0
72:         if version_str.startswith("^"):
73:             base = version_str[1:]
74:             major = base.split(".")[0]
75:             version_str = f">={base},<{int(major)+1}.0.0"
76: 
77:         # Convert Poetry tilde (~) to PEP 440
78:         # ~1.2.3 ‚Üí >=1.2.3,<1.3.0
79:         elif version_str.startswith("~"):
80:             base = version_str[1:]
81:             parts = base.split(".")
82:             if len(parts) >= 2:
83:                 version_str = f">={base},<{parts[0]}.{int(parts[1])+1}.0"
84: 
85:         return f"{package}{version_str}" if version_str else package
</file>

<file path="tools/ce/toml_formats/setuptools_handler.py">
 1: """Setuptools format handler."""
 2: 
 3: from typing import Dict
 4: 
 5: 
 6: class SetuptoolsHandler:
 7:     """Handles Setuptools format TOML files."""
 8: 
 9:     @staticmethod
10:     def detect(toml_data: Dict) -> bool:
11:         """
12:         Check if TOML uses Setuptools format.
13: 
14:         Setuptools format: Has build-system but no [project] or [tool.poetry]
15:         """
16:         has_build_system = "build-system" in toml_data
17:         has_project = "project" in toml_data
18:         has_poetry = "tool" in toml_data and "poetry" in toml_data.get("tool", {})
19: 
20:         return has_build_system and not has_project and not has_poetry
21: 
22:     @staticmethod
23:     def convert_to_pep621(setuptools_data: Dict) -> Dict:
24:         """
25:         Convert Setuptools format to PEP 621 format.
26: 
27:         Args:
28:             setuptools_data: Setuptools-formatted TOML data
29: 
30:         Returns:
31:             PEP 621-formatted TOML data with minimal structure
32:         """
33:         pep621 = {"project": {}}
34: 
35:         # Preserve build-system if present
36:         if "build-system" in setuptools_data:
37:             pep621["build-system"] = setuptools_data["build-system"]
38: 
39:         # Setuptools typically doesn't have dependencies in pyproject.toml
40:         # Dependencies are usually in setup.py or setup.cfg
41:         # Return minimal structure
42:         return pep621
</file>

<file path="tools/ce/vacuum_strategies/backup_files.py">
 1: """Strategy for finding backup files."""
 2: 
 3: from pathlib import Path
 4: from typing import List
 5: 
 6: from .base import BaseStrategy, CleanupCandidate
 7: 
 8: 
 9: class BackupFileStrategy(BaseStrategy):
10:     """Find backup files from editors and git."""
11: 
12:     BACKUP_PATTERNS = [
13:         "**/*.bak",
14:         "**/*~",
15:         "**/*.orig",
16:         "**/*.rej",
17:     ]
18: 
19:     def find_candidates(self) -> List[CleanupCandidate]:
20:         """Find all backup files.
21: 
22:         Returns:
23:             List of CleanupCandidate objects with HIGH confidence (100%)
24:         """
25:         candidates = []
26: 
27:         for pattern in self.BACKUP_PATTERNS:
28:             for path in self.scan_path.glob(pattern):
29:                 if not path.exists():
30:                     continue
31: 
32:                 # Skip if protected
33:                 if self.is_protected(path):
34:                     continue
35: 
36:                 candidate = CleanupCandidate(
37:                     path=path,
38:                     reason=f"Backup file: {pattern}",
39:                     confidence=100,  # HIGH confidence - safe to delete
40:                     size_bytes=self.get_file_size(path),
41:                     last_modified=self.get_last_modified(path),
42:                     git_history=self.get_git_history(path),
43:                 )
44:                 candidates.append(candidate)
45: 
46:         return candidates
</file>

<file path="tools/ce/vacuum_strategies/base.py">
  1: """Base strategy for vacuum cleanup operations."""
  2: 
  3: from abc import ABC, abstractmethod
  4: from dataclasses import dataclass
  5: from pathlib import Path
  6: from typing import List
  7: import subprocess
  8: 
  9: 
 10: @dataclass
 11: class CleanupCandidate:
 12:     """Represents a file/directory candidate for cleanup."""
 13: 
 14:     path: Path
 15:     reason: str
 16:     confidence: int  # 0-100
 17:     size_bytes: int
 18:     last_modified: str
 19:     git_history: str = ""
 20:     references: List[str] = None
 21:     report_only: bool = False  # If True, never delete (only report)
 22: 
 23:     def __post_init__(self):
 24:         if self.references is None:
 25:             self.references = []
 26: 
 27: 
 28: class BaseStrategy(ABC):
 29:     """Abstract base class for cleanup strategies."""
 30: 
 31:     # Patterns that should NEVER be deleted
 32:     PROTECTED_PATTERNS = [
 33:         ".ce/**",
 34:         ".claude/**",  # Protect ALL Claude Code configuration
 35:         ".serena/**",  # Protect Serena memories and configuration
 36:         "syntropy-mcp/**",  # Protect syntropy MCP server directory
 37:         # Note: tmp/** files are conditionally protected by age (see is_protected_by_age)
 38:         # Note: PRP files are protected by PRP naming convention check in is_protected()
 39:         "pyproject.toml",
 40:         "README.md",
 41:         "CLAUDE.md",
 42:         "WARP.md",
 43:         "examples/**",
 44:         "**/__init__.py",
 45:         "**/cli.py",
 46:         "**/__main__.py",
 47:         "**/bootstrap.sh",
 48:     ]
 49: 
 50:     def __init__(self, project_root: Path, scan_path: Path = None):
 51:         """Initialize strategy with project root.
 52: 
 53:         Args:
 54:             project_root: Path to project root directory
 55:             scan_path: Optional path to scan (defaults to project_root)
 56:         """
 57:         self.project_root = project_root
 58:         self.scan_path = scan_path if scan_path else project_root
 59: 
 60:     @abstractmethod
 61:     def find_candidates(self) -> List[CleanupCandidate]:
 62:         """Find cleanup candidates using this strategy.
 63: 
 64:         Returns:
 65:             List of CleanupCandidate objects
 66:         """
 67:         pass
 68: 
 69:     def is_protected(self, path: Path) -> bool:
 70:         """Check if path matches protected patterns.
 71: 
 72:         Args:
 73:             path: Path to check
 74: 
 75:         Returns:
 76:             True if path is protected, False otherwise
 77:         """
 78:         relative_path = path.relative_to(self.project_root)
 79:         relative_str = str(relative_path)
 80: 
 81:         # Special case: tmp/ files - protect if modified less than 2 days ago
 82:         if relative_str.startswith("tmp/"):
 83:             # Files < 2 days old (by filesystem mtime) are protected
 84:             if not self.is_recently_modified(path, days=2):
 85:                 # File is older than 2 days - not protected
 86:                 return False
 87:             # File is recent (< 2 days) - protected
 88:             return True
 89: 
 90:         # Special case: Markdown files in PRPs/ - check if they have YAML header
 91:         # Match both "/PRPs/" and "PRPs/" (start of path)
 92:         if (("/PRPs/" in relative_str or relative_str.startswith("PRPs/")) and path.suffix == ".md"):
 93:             if self._has_yaml_frontmatter(path):
 94:                 return True
 95:             # If in PRPs/ but no YAML header, not a real PRP - can be cleaned
 96:             return False
 97: 
 98:         for pattern in self.PROTECTED_PATTERNS:
 99:             # Simple glob-like matching
100:             if self._matches_pattern(relative_str, pattern):
101:                 return True
102: 
103:         return False
104: 
105:     def _has_yaml_frontmatter(self, path: Path) -> bool:
106:         """Check if markdown file has YAML frontmatter header.
107: 
108:         Args:
109:             path: Path to markdown file
110: 
111:         Returns:
112:             True if file starts with YAML frontmatter (---), False otherwise
113:         """
114:         try:
115:             with open(path, "r", encoding="utf-8") as f:
116:                 first_line = f.readline().strip()
117:                 return first_line == "---"
118:         except Exception:
119:             return False
120: 
121:     def _matches_pattern(self, path: str, pattern: str) -> bool:
122:         """Simple pattern matching for protected paths.
123: 
124:         Args:
125:             path: Path string to check
126:             pattern: Pattern with ** and * wildcards
127: 
128:         Returns:
129:             True if path matches pattern
130:         """
131:         from fnmatch import fnmatch
132: 
133:         # Handle ** for recursive directory matching
134:         if "**" in pattern:
135:             pattern_parts = pattern.split("**")
136:             if len(pattern_parts) == 2:
137:                 prefix, suffix = pattern_parts
138:                 prefix = prefix.rstrip("/")
139:                 suffix = suffix.lstrip("/")
140: 
141:                 if prefix and not path.startswith(prefix):
142:                     return False
143:                 if suffix and not fnmatch(path, f"*{suffix}"):
144:                     return False
145:                 return True
146: 
147:         return fnmatch(path, pattern)
148: 
149:     def get_file_size(self, path: Path) -> int:
150:         """Get file or directory size in bytes.
151: 
152:         Args:
153:             path: Path to file or directory
154: 
155:         Returns:
156:             Size in bytes
157:         """
158:         if path.is_file():
159:             return path.stat().st_size
160:         elif path.is_dir():
161:             return sum(f.stat().st_size for f in path.rglob("*") if f.is_file())
162:         return 0
163: 
164:     def get_last_modified(self, path: Path) -> str:
165:         """Get last modified timestamp.
166: 
167:         Args:
168:             path: Path to file or directory
169: 
170:         Returns:
171:             ISO format timestamp string
172:         """
173:         from datetime import datetime
174: 
175:         timestamp = path.stat().st_mtime
176:         return datetime.fromtimestamp(timestamp).isoformat()
177: 
178:     def get_git_history(self, path: Path, days: int = 30) -> str:
179:         """Get git history summary for file.
180: 
181:         Args:
182:             path: Path to file
183:             days: Number of days to look back
184: 
185:         Returns:
186:             Git history summary or empty string
187:         """
188:         try:
189:             relative_path = path.relative_to(self.project_root)
190:             result = subprocess.run(
191:                 ["git", "log", "--oneline", f"--since={days} days ago", "--", str(relative_path)],
192:                 cwd=self.project_root,
193:                 capture_output=True,
194:                 text=True,
195:                 timeout=5,
196:             )
197: 
198:             if result.returncode == 0 and result.stdout.strip():
199:                 lines = result.stdout.strip().split("\n")
200:                 return f"{len(lines)} commits in last {days} days"
201:             return f"No commits in last {days} days"
202: 
203:         except Exception:
204:             return "Git history unavailable"
205: 
206:     def is_recently_active(self, path: Path, days: int = 30) -> bool:
207:         """Check if file has recent git activity.
208: 
209:         Args:
210:             path: Path to file
211:             days: Number of days to consider recent
212: 
213:         Returns:
214:             True if file has commits in last N days
215:         """
216:         history = self.get_git_history(path, days)
217:         return "commits in last" in history and not history.startswith("0 commits")
218: 
219:     def is_recently_modified(self, path: Path, days: int = 30) -> bool:
220:         """Check if file was modified recently (filesystem mtime).
221: 
222:         Args:
223:             path: Path to file
224:             days: Number of days to consider recent
225: 
226:         Returns:
227:             True if file was modified in last N days (based on filesystem mtime)
228:         """
229:         from datetime import datetime, timedelta
230: 
231:         try:
232:             mtime = datetime.fromtimestamp(path.stat().st_mtime)
233:             cutoff = datetime.now() - timedelta(days=days)
234:             return mtime > cutoff
235:         except Exception:
236:             return False
</file>

<file path="tools/ce/vacuum_strategies/commented_code.py">
  1: """Strategy for finding large commented code blocks."""
  2: 
  3: import re
  4: from pathlib import Path
  5: from typing import List
  6: 
  7: from .base import BaseStrategy, CleanupCandidate
  8: 
  9: 
 10: class CommentedCodeStrategy(BaseStrategy):
 11:     """Find large commented-out code blocks."""
 12: 
 13:     MIN_COMMENTED_LINES = 20  # Minimum consecutive commented lines to flag
 14: 
 15:     def find_candidates(self) -> List[CleanupCandidate]:
 16:         """Find files with large commented code blocks.
 17: 
 18:         Returns:
 19:             List of CleanupCandidate objects with LOW confidence (30%)
 20:         """
 21:         candidates = []
 22: 
 23:         # Find all Python files
 24:         for py_file in self.scan_path.glob("**/*.py"):
 25:             if not py_file.exists() or py_file.is_dir():
 26:                 continue
 27: 
 28:             # Skip if protected
 29:             if self.is_protected(py_file):
 30:                 continue
 31: 
 32:             # Find commented code blocks
 33:             blocks = self._find_commented_code_blocks(py_file)
 34: 
 35:             if blocks:
 36:                 total_lines = sum(block["lines"] for block in blocks)
 37:                 block_summary = ", ".join(
 38:                     f"Line {b['start']}: {b['lines']} lines" for b in blocks
 39:                 )
 40: 
 41:                 candidate = CleanupCandidate(
 42:                     path=py_file,
 43:                     reason=f"Contains {len(blocks)} commented code block(s) ({total_lines} lines total)",
 44:                     confidence=30,  # LOW confidence - manual review only
 45:                     size_bytes=self.get_file_size(py_file),
 46:                     last_modified=self.get_last_modified(py_file),
 47:                     git_history=self.get_git_history(py_file),
 48:                     references=[block_summary],
 49:                 )
 50:                 candidates.append(candidate)
 51: 
 52:         return candidates
 53: 
 54:     def _find_commented_code_blocks(self, py_file: Path) -> List[dict]:
 55:         """Find large commented code blocks in Python file.
 56: 
 57:         Args:
 58:             py_file: Path to Python file
 59: 
 60:         Returns:
 61:             List of dicts with block info: {"start": line_num, "lines": count}
 62:         """
 63:         blocks = []
 64: 
 65:         try:
 66:             content = py_file.read_text(encoding="utf-8")
 67:         except Exception:
 68:             return blocks
 69: 
 70:         lines = content.split("\n")
 71:         in_block = False
 72:         block_start = 0
 73:         block_lines = 0
 74: 
 75:         # Code patterns that indicate commented-out code (not prose)
 76:         code_patterns = [
 77:             r"^\s*#\s*(def|class|if|for|while|try|except|import|from)\s",
 78:             r"^\s*#\s*[a-zA-Z_][a-zA-Z0-9_]*\s*=",  # assignments
 79:             r"^\s*#\s*return\s",
 80:         ]
 81: 
 82:         for i, line in enumerate(lines, start=1):
 83:             stripped = line.strip()
 84: 
 85:             # Skip docstrings and module headers
 86:             if '"""' in line or "'''" in line:
 87:                 continue
 88: 
 89:             # Check if line is a comment with code
 90:             is_code_comment = any(re.search(pattern, line) for pattern in code_patterns)
 91: 
 92:             if is_code_comment:
 93:                 if not in_block:
 94:                     in_block = True
 95:                     block_start = i
 96:                     block_lines = 1
 97:                 else:
 98:                     block_lines += 1
 99:             else:
100:                 # End of block
101:                 if in_block and block_lines >= self.MIN_COMMENTED_LINES:
102:                     blocks.append({"start": block_start, "lines": block_lines})
103: 
104:                 in_block = False
105:                 block_lines = 0
106: 
107:         # Check final block
108:         if in_block and block_lines >= self.MIN_COMMENTED_LINES:
109:             blocks.append({"start": block_start, "lines": block_lines})
110: 
111:         return blocks
</file>

<file path="tools/ce/vacuum_strategies/obsolete_docs.py">
  1: """Strategy for finding obsolete documentation."""
  2: 
  3: import re
  4: from pathlib import Path
  5: from typing import List
  6: 
  7: from .base import BaseStrategy, CleanupCandidate
  8: 
  9: 
 10: class ObsoleteDocStrategy(BaseStrategy):
 11:     """Find obsolete documentation files."""
 12: 
 13:     OBSOLETE_SUFFIXES = [
 14:         "-v1",
 15:         "-v2",
 16:         "-old",
 17:         "-deprecated",
 18:         "-backup",
 19:         ".old",
 20:         ".bak",
 21:     ]
 22: 
 23:     # Temporary analysis doc prefixes (PRP execution artifacts)
 24:     TEMP_DOC_PREFIXES = [
 25:         "ANALYSIS-",
 26:         "CHANGELIST-",
 27:         "REPORT-",
 28:         "IMPLEMENTATION-",
 29:         "DEPLOYMENT",  # DEPLOYMENT.md, DEPLOYMENT_*.md
 30:         "VERIFICATION-",
 31:         "VALIDATION-",
 32:         "DENOISE_",  # DENOISE_*.md artifacts
 33:     ]
 34: 
 35:     # Temporary analysis doc suffixes
 36:     TEMP_DOC_SUFFIXES = [
 37:         "-PLAN",  # e.g., TOOL-PERMISSION-LOCKDOWN-PLAN.md
 38:         "_SUMMARY",  # e.g., DEPLOYMENT_SUMMARY.md
 39:         "_SOLUTION",  # e.g., CMD_V_IMAGE_PASTE_SOLUTION.md
 40:     ]
 41: 
 42:     # Temporary analysis doc infix patterns (match anywhere in name)
 43:     TEMP_DOC_INFIXES = [
 44:         "-SUMMARY-",  # e.g., PEER-REVIEW-IMPROVEMENTS-SUMMARY-PRP-29-SERIES.md
 45:         "-REPL",  # Matches REPLAN, REPLKAN, etc.
 46:     ]
 47: 
 48:     # Content markers indicating temporary/analysis docs
 49:     # Note: Avoid markers that are common in PRPs (like "**status**:")
 50:     TEMP_CONTENT_MARKERS = [
 51:         "temporary analysis",
 52:         "work in progress",
 53:         "wip",
 54:         "draft",
 55:         "todo:",
 56:         "fixme:",
 57:         "execution artifact",
 58:         "analysis for prp-",
 59:         "generated for prp-",
 60:         "artifact from",
 61:         "this document was created to",
 62:         "planning document",
 63:         "**status**: pending approval",  # More specific than just "**status**:"
 64:         "**status**: ready for execution",
 65:         "**status:** pending approval",
 66:         "**completed**:",
 67:         "**remaining work**:",
 68:         "## üìã executive summary",  # With emoji - more likely to be temp doc
 69:         "## üéØ execution summary",  # With emoji - more likely to be temp doc
 70:         "**problem**:",
 71:         "**solution**:",
 72:         "## solution options",
 73:         "source plan:",
 74:         "**workflow:**",
 75:         "## improvement statistics",
 76:         "**date**: 20",  # Matches "**Date**: 2025-10-29" in root-level planning docs
 77:         "**date:** 20",  # Matches "**Date:** 2025-10-22"
 78:         "this plan addresses",
 79:         "revised execution plan",
 80:         "replkan",  # REPLAN/REPLKAN pattern
 81:     ]
 82: 
 83:     def find_candidates(self) -> List[CleanupCandidate]:
 84:         """Find obsolete documentation files.
 85: 
 86:         Returns:
 87:             List of CleanupCandidate objects with MEDIUM confidence (70%)
 88:         """
 89:         candidates = []
 90: 
 91:         # Check for root-level garbage files (all-caps, no extension)
 92:         candidates.extend(self._find_root_garbage_files())
 93: 
 94:         # Find all markdown files
 95:         for md_file in self.scan_path.glob("**/*.md"):
 96:             if not md_file.exists() or md_file.is_dir():
 97:                 continue
 98: 
 99:             # Skip if protected
100:             if self.is_protected(md_file):
101:                 continue
102: 
103:             stem = md_file.stem
104:             name = md_file.name
105: 
106:             # Check for temporary analysis docs (ANALYSIS-*, DEPLOYMENT*, etc.)
107:             temp_doc_match = self._is_temp_analysis_doc(name, stem)
108:             content_marker = None
109: 
110:             # If filename doesn't match, check content for temporary markers
111:             if not temp_doc_match:
112:                 content_marker = self._check_doc_content(md_file)
113: 
114:             if temp_doc_match or content_marker:
115:                 # Combine filename and content analysis
116:                 reason_parts = []
117:                 if temp_doc_match:
118:                     reason_parts.append(f"filename {temp_doc_match}")
119:                 if content_marker:
120:                     reason_parts.append(f"content: {content_marker}")
121: 
122:                 reason = f"Temporary analysis doc: {', '.join(reason_parts)}"
123: 
124:                 # Recently active files get lower confidence
125:                 confidence = 70
126:                 if self.is_recently_active(md_file, days=30):
127:                     confidence = 55
128: 
129:                 candidate = CleanupCandidate(
130:                     path=md_file,
131:                     reason=reason,
132:                     confidence=confidence,  # MEDIUM confidence
133:                     size_bytes=self.get_file_size(md_file),
134:                     last_modified=self.get_last_modified(md_file),
135:                     git_history=self.get_git_history(md_file),
136:                 )
137:                 candidates.append(candidate)
138:                 continue
139: 
140:             # Check for versioned/obsolete suffixes
141:             for suffix in self.OBSOLETE_SUFFIXES:
142:                 if stem.endswith(suffix):
143:                     # Check if newer version exists
144:                     newer_version = self._find_newer_version(md_file, suffix)
145: 
146:                     # Recently active files get lower confidence
147:                     confidence = 70
148:                     if self.is_recently_active(md_file, days=30):
149:                         confidence = 50
150: 
151:                     reason = f"Versioned/obsolete doc: {suffix} suffix"
152:                     if newer_version:
153:                         reason += f" (newer: {newer_version.name})"
154: 
155:                     candidate = CleanupCandidate(
156:                         path=md_file,
157:                         reason=reason,
158:                         confidence=confidence,  # MEDIUM confidence
159:                         size_bytes=self.get_file_size(md_file),
160:                         last_modified=self.get_last_modified(md_file),
161:                         git_history=self.get_git_history(md_file),
162:                     )
163:                     candidates.append(candidate)
164:                     break
165: 
166:         return candidates
167: 
168:     def _is_temp_analysis_doc(self, name: str, stem: str) -> str | None:
169:         """Check if filename matches temporary analysis doc patterns.
170: 
171:         Args:
172:             name: Full filename (e.g., DEPLOYMENT.md)
173:             stem: Filename without extension (e.g., DEPLOYMENT)
174: 
175:         Returns:
176:             Pattern description if match, None otherwise
177:         """
178:         # Check prefixes
179:         for prefix in self.TEMP_DOC_PREFIXES:
180:             if stem.startswith(prefix):
181:                 return f"prefix '{prefix}'"
182: 
183:         # Check suffixes
184:         for suffix in self.TEMP_DOC_SUFFIXES:
185:             if stem.endswith(suffix):
186:                 return f"suffix '{suffix}'"
187: 
188:         # Check infixes (match anywhere in name)
189:         for infix in self.TEMP_DOC_INFIXES:
190:             if infix in stem:
191:                 return f"infix '{infix}'"
192: 
193:         return None
194: 
195:     def _check_doc_content(self, file: Path) -> str | None:
196:         """Check document content for temporary markers.
197: 
198:         Reads first 20 lines and looks for markers like "WIP", "DRAFT", "TODO", etc.
199: 
200:         Args:
201:             file: Path to markdown file
202: 
203:         Returns:
204:             Marker description if found, None otherwise
205:         """
206:         try:
207:             with open(file, "r", encoding="utf-8", errors="ignore") as f:
208:                 # Read first 20 lines
209:                 lines = []
210:                 for i, line in enumerate(f):
211:                     if i >= 20:
212:                         break
213:                     lines.append(line.lower())
214: 
215:                 content = " ".join(lines)
216: 
217:                 # Check for temporary markers
218:                 for marker in self.TEMP_CONTENT_MARKERS:
219:                     if marker in content:
220:                         return f"'{marker}' found"
221: 
222:         except Exception:
223:             # If we can't read the file, skip content check
224:             pass
225: 
226:         return None
227: 
228:     def _find_root_garbage_files(self) -> List[CleanupCandidate]:
229:         """Find garbage files in project root.
230: 
231:         Looks for:
232:         - All-caps files with no extension (e.g., VERSION)
233:         - Other temporary tracking files
234: 
235:         Returns:
236:             List of CleanupCandidate objects
237:         """
238:         candidates = []
239: 
240:         # Scan only project root (not subdirectories)
241:         for file in self.project_root.iterdir():
242:             if not file.is_file():
243:                 continue
244: 
245:             # Skip if protected
246:             if self.is_protected(file):
247:                 continue
248: 
249:             name = file.name
250:             stem = file.stem
251: 
252:             # Check for all-caps files with no extension
253:             if "." not in name and name.isupper() and len(name) > 1:
254:                 # Skip known good all-caps files (should be in PROTECTED_PATTERNS but double-check)
255:                 if name in ["LICENSE", "MAKEFILE", "VERSION"]:
256:                     continue
257: 
258:                 confidence = 65  # MEDIUM confidence
259:                 if self.is_recently_active(file, days=30):
260:                     confidence = 50
261: 
262:                 candidate = CleanupCandidate(
263:                     path=file,
264:                     reason=f"Root-level all-caps file with no extension",
265:                     confidence=confidence,
266:                     size_bytes=self.get_file_size(file),
267:                     last_modified=self.get_last_modified(file),
268:                     git_history=self.get_git_history(file),
269:                 )
270:                 candidates.append(candidate)
271: 
272:         return candidates
273: 
274:     def _find_newer_version(self, old_file: Path, suffix: str) -> Path | None:
275:         """Try to find newer version of file.
276: 
277:         Args:
278:             old_file: Path to old versioned file
279:             suffix: The version suffix (e.g., '-v1')
280: 
281:         Returns:
282:             Path to newer version if found, None otherwise
283:         """
284:         # Remove suffix to get base name
285:         base_name = old_file.stem.replace(suffix, "")
286:         parent = old_file.parent
287: 
288:         # Look for file without suffix
289:         newer_file = parent / f"{base_name}.md"
290:         if newer_file.exists() and newer_file != old_file:
291:             return newer_file
292: 
293:         # Look for higher version numbers
294:         if re.match(r".*-v\d+$", old_file.stem):
295:             current_version = int(re.search(r"-v(\d+)$", old_file.stem).group(1))
296:             for i in range(current_version + 1, current_version + 10):
297:                 newer_file = parent / f"{base_name}-v{i}.md"
298:                 if newer_file.exists():
299:                     return newer_file
300: 
301:         return None
</file>

<file path="tools/ce/vacuum_strategies/temp_files.py">
 1: """Strategy for finding temporary files."""
 2: 
 3: from pathlib import Path
 4: from typing import List
 5: 
 6: from .base import BaseStrategy, CleanupCandidate
 7: 
 8: 
 9: class TempFileStrategy(BaseStrategy):
10:     """Find temporary files that can be safely deleted."""
11: 
12:     TEMP_PATTERNS = [
13:         "**/*.pyc",
14:         "**/__pycache__",
15:         "**/.DS_Store",
16:         "**/*.swp",
17:         "**/*.swo",
18:         "**/.pytest_cache",
19:         "**/.coverage",
20:         "**/*.log",
21:         "**/*.tmp",
22:     ]
23: 
24:     def find_candidates(self) -> List[CleanupCandidate]:
25:         """Find all temporary files.
26: 
27:         Returns:
28:             List of CleanupCandidate objects with HIGH confidence (100%)
29:         """
30:         candidates = []
31: 
32:         for pattern in self.TEMP_PATTERNS:
33:             for path in self.scan_path.glob(pattern):
34:                 if not path.exists():
35:                     continue
36: 
37:                 # Skip if protected (shouldn't happen but safety check)
38:                 if self.is_protected(path):
39:                     continue
40: 
41:                 candidate = CleanupCandidate(
42:                     path=path,
43:                     reason=f"Temporary file: {pattern}",
44:                     confidence=100,  # HIGH confidence - safe to delete
45:                     size_bytes=self.get_file_size(path),
46:                     last_modified=self.get_last_modified(path),
47:                     git_history=self.get_git_history(path),
48:                 )
49:                 candidates.append(candidate)
50: 
51:         return candidates
</file>

<file path="tools/ce/vacuum_strategies/unreferenced_code.py">
  1: """Strategy for finding unreferenced code files using Serena."""
  2: 
  3: from pathlib import Path
  4: from typing import List, Set
  5: import subprocess
  6: import json
  7: 
  8: from .base import BaseStrategy, CleanupCandidate
  9: 
 10: 
 11: class UnreferencedCodeStrategy(BaseStrategy):
 12:     """Find Python files where ALL definitions are unreferenced.
 13: 
 14:     A file is considered unreferenced if:
 15:     1. The file/module itself is not imported anywhere
 16:     2. (Future enhancement: Check individual symbols with Serena)
 17: 
 18:     Note: Currently only checks #1 for performance. Full Serena integration
 19:     would check if each function/class is referenced, but this is slower.
 20:     """
 21: 
 22:     def __init__(self, project_root: Path, scan_path: Path = None):
 23:         """Initialize strategy and activate Serena project.
 24: 
 25:         Args:
 26:             project_root: Path to project root directory
 27:             scan_path: Optional path to scan (defaults to project_root)
 28:         """
 29:         super().__init__(project_root, scan_path)
 30:         self.serena_available = False
 31: 
 32:     def find_candidates(self) -> List[CleanupCandidate]:
 33:         """Find unreferenced code files.
 34: 
 35:         Returns:
 36:             List of CleanupCandidate objects with LOW confidence (40%)
 37:         """
 38:         candidates = []
 39: 
 40:         # Check if Serena is available via mcp CLI
 41:         if not self._check_serena_available():
 42:             return candidates
 43: 
 44:         # Get all Python files
 45:         py_files = list(self.scan_path.glob("**/*.py"))
 46: 
 47:         # Get all import statements to find which files are referenced
 48:         referenced_modules = self._get_all_imports()
 49: 
 50:         for py_file in py_files:
 51:             if not py_file.exists() or py_file.is_dir():
 52:                 continue
 53: 
 54:             # Skip if protected
 55:             if self.is_protected(py_file):
 56:                 continue
 57: 
 58:             # Check if this file is imported anywhere
 59:             relative_path = py_file.relative_to(self.project_root)
 60:             module_name = str(relative_path).replace("/", ".").replace(".py", "")
 61: 
 62:             # Also check stem (e.g., "core" from "ce/core.py")
 63:             stem = py_file.stem
 64: 
 65:             is_referenced = False
 66:             for ref_module in referenced_modules:
 67:                 if module_name in ref_module or stem in ref_module:
 68:                     is_referenced = True
 69:                     break
 70: 
 71:             if not is_referenced:
 72:                 # Recently active files get lower confidence
 73:                 confidence = 40
 74:                 if self.is_recently_active(py_file, days=30):
 75:                     confidence = 30
 76: 
 77:                 candidate = CleanupCandidate(
 78:                     path=py_file,
 79:                     reason="File not imported anywhere (all defs potentially unreferenced)",
 80:                     confidence=confidence,  # LOW confidence
 81:                     size_bytes=self.get_file_size(py_file),
 82:                     last_modified=self.get_last_modified(py_file),
 83:                     git_history=self.get_git_history(py_file),
 84:                     references=list(referenced_modules) if referenced_modules else [],
 85:                 )
 86:                 candidates.append(candidate)
 87: 
 88:         return candidates
 89: 
 90:     def _check_serena_available(self) -> bool:
 91:         """Check if Serena MCP is available.
 92: 
 93:         Returns:
 94:             True if Serena is available, False otherwise
 95:         """
 96:         try:
 97:             result = subprocess.run(
 98:                 ["mcp", "list"],
 99:                 capture_output=True,
100:                 text=True,
101:                 timeout=5,
102:             )
103:             self.serena_available = "serena" in result.stdout.lower()
104:             return self.serena_available
105:         except Exception:
106:             return False
107: 
108:     def _get_all_imports(self) -> Set[str]:
109:         """Get all import statements in the project.
110: 
111:         Uses grep to find all import statements efficiently.
112: 
113:         Returns:
114:             Set of imported module names
115:         """
116:         imports = set()
117: 
118:         try:
119:             # Find all "from X import" and "import X" statements
120:             result = subprocess.run(
121:                 ["grep", "-rh", "--include=*.py", "-E", r"^(from|import)\s+", str(self.project_root)],
122:                 capture_output=True,
123:                 text=True,
124:                 timeout=10,
125:             )
126: 
127:             if result.returncode == 0:
128:                 for line in result.stdout.split("\n"):
129:                     line = line.strip()
130:                     if not line:
131:                         continue
132: 
133:                     # Parse "from foo.bar import baz" -> "foo.bar"
134:                     if line.startswith("from "):
135:                         parts = line.split()
136:                         if len(parts) >= 2:
137:                             imports.add(parts[1])
138: 
139:                     # Parse "import foo.bar" -> "foo.bar"
140:                     elif line.startswith("import "):
141:                         parts = line.split()
142:                         if len(parts) >= 2:
143:                             # Handle "import foo, bar, baz"
144:                             for module in parts[1:]:
145:                                 module = module.strip(",")
146:                                 if module and not module.startswith("#"):
147:                                     imports.add(module)
148: 
149:         except Exception:
150:             pass
151: 
152:         return imports
</file>

<file path="tools/ce/core.py">
  1: """Core operations: file, git, and shell utilities."""
  2: 
  3: import subprocess
  4: import time
  5: import shlex
  6: from pathlib import Path
  7: from typing import Dict, List, Any, Optional, Union
  8: 
  9: 
 10: def run_cmd(
 11:     cmd: Union[str, List[str]],
 12:     cwd: Optional[str] = None,
 13:     timeout: int = 60,
 14:     capture_output: bool = True
 15: ) -> Dict[str, Any]:
 16:     """Execute shell command with timeout and error handling.
 17: 
 18:     Args:
 19:         cmd: Shell command (str will be safely split) or list of args
 20:         cwd: Working directory (default: current)
 21:         timeout: Command timeout in seconds
 22:         capture_output: Whether to capture stdout/stderr
 23: 
 24:     Returns:
 25:         Dict with: success (bool), stdout (str), stderr (str),
 26:                    exit_code (int), duration (float)
 27: 
 28:     Raises:
 29:         ValueError: If command is empty
 30:         subprocess.TimeoutExpired: If command exceeds timeout
 31: 
 32:     Security: Uses shell=False to prevent command injection (CWE-78).
 33:               String commands are safely parsed with shlex.split().
 34:     
 35:     Note: No fishy fallbacks - exceptions are thrown to troubleshoot quickly.
 36:     """
 37:     start = time.time()
 38: 
 39:     # Convert string to safe list
 40:     if isinstance(cmd, str):
 41:         cmd_list = shlex.split(cmd)  # Safe parsing with proper escaping
 42:     else:
 43:         cmd_list = cmd
 44: 
 45:     # Handle empty command
 46:     if not cmd_list:
 47:         raise ValueError(
 48:             "Empty command provided\n"
 49:             "üîß Troubleshooting: Provide a valid command string or list"
 50:         )
 51: 
 52:     try:
 53:         result = subprocess.run(
 54:             cmd_list,  # ‚úÖ List format
 55:             shell=False,  # ‚úÖ SAFE - no shell interpretation (CWE-78 fix)
 56:             cwd=cwd,
 57:             timeout=timeout,
 58:             capture_output=capture_output,
 59:             text=True
 60:         )
 61: 
 62:         duration = time.time() - start
 63: 
 64:         return {
 65:             "success": result.returncode == 0,
 66:             "stdout": result.stdout if capture_output else "",
 67:             "stderr": result.stderr if capture_output else "",
 68:             "exit_code": result.returncode,
 69:             "duration": duration
 70:         }
 71: 
 72:     except subprocess.TimeoutExpired as e:
 73:         duration = time.time() - start
 74:         raise TimeoutError(
 75:             f"Command timed out after {timeout}s: {' '.join(cmd_list)}\n"
 76:             f"üîß Troubleshooting: Increase timeout or check for hanging process"
 77:         ) from e
 78: 
 79:     except Exception as e:
 80:         duration = time.time() - start
 81:         raise RuntimeError(
 82:             f"Command failed: {' '.join(cmd_list)}\n"
 83:             f"Error: {str(e)}\n"
 84:             f"üîß Troubleshooting: Check command syntax and permissions"
 85:         ) from e
 86: 
 87: 
 88: def count_git_files() -> int:
 89:     """Count total tracked files in git repository.
 90: 
 91:     Replaces shell pattern: git ls-files | wc -l
 92: 
 93:     Returns:
 94:         Number of tracked files
 95: 
 96:     Raises:
 97:         RuntimeError: If not in git repository
 98: 
 99:     Security: Uses subprocess.run with shell=False (CWE-78 safe).
100:     """
101:     try:
102:         result = subprocess.run(
103:             ["git", "ls-files"],
104:             capture_output=True,
105:             text=True,
106:             shell=False,  # ‚úÖ SAFE
107:             timeout=30
108:         )
109: 
110:         if result.returncode != 0:
111:             raise RuntimeError(
112:                 "Failed to list git files\n"
113:                 "üîß Troubleshooting: Ensure you're in a git repository"
114:             )
115: 
116:         files = result.stdout.strip().split('\n') if result.stdout.strip() else []
117:         return len(files)
118: 
119:     except subprocess.TimeoutExpired:
120:         raise RuntimeError(
121:             "Git ls-files timed out\n"
122:             "üîß Troubleshooting: Repository may be too large"
123:         )
124: 
125: 
126: def count_git_diff_lines(
127:     ref: str = "HEAD~5",
128:     files: Optional[List[str]] = None
129: ) -> int:
130:     """Count lines changed in git diff.
131: 
132:     Replaces shell pattern: git diff HEAD~5 -- file1 file2 | wc -l
133: 
134:     Args:
135:         ref: Git reference to diff against (default: HEAD~5)
136:         files: Optional list of files to diff
137: 
138:     Returns:
139:         Number of changed lines
140: 
141:     Security: Uses subprocess.run with shell=False (CWE-78 safe).
142:     Note: Returns 0 on error (graceful degradation for health checks).
143:     """
144:     cmd = ["git", "diff", ref]
145:     if files:
146:         cmd.extend(["--"] + files)
147: 
148:     try:
149:         result = subprocess.run(
150:             cmd,
151:             capture_output=True,
152:             text=True,
153:             shell=False,  # ‚úÖ SAFE
154:             timeout=30
155:         )
156: 
157:         if result.returncode != 0:
158:             return 0
159: 
160:         return len(result.stdout.split('\n')) if result.stdout else 0
161: 
162:     except subprocess.TimeoutExpired:
163:         return 0
164: 
165: 
166: def read_file(path: str, encoding: str = "utf-8") -> str:
167:     """Read file with validation."""
168:     file_path = Path(path)
169:     if not file_path.exists():
170:         raise FileNotFoundError(f"File not found: {path}\nüîß Troubleshooting: Check path spelling")
171:     if not file_path.is_file():
172:         raise ValueError(f"Path is not a file: {path}\nüîß Troubleshooting: Use different method")
173:     return file_path.read_text(encoding=encoding)
174: 
175: 
176: def write_file(path: str, content: str, encoding: str = "utf-8", create_dirs: bool = True) -> None:
177:     """Write file with security validation."""
178:     file_path = Path(path)
179:     sensitive_patterns = [("API_KEY", "API keys"), ("SECRET", "Secrets"), ("PASSWORD", "Passwords")]
180:     for pattern, msg in sensitive_patterns:
181:         if pattern in content.upper():
182:             raise ValueError(f"Sensitive data: {msg}\nüîß Use environment variables")
183:     if create_dirs:
184:         file_path.parent.mkdir(parents=True, exist_ok=True)
185:     file_path.write_text(content, encoding=encoding)
186: 
187: 
188: def git_status() -> Dict[str, Any]:
189:     """Get git repository status."""
190:     check_result = run_cmd("git rev-parse --git-dir", capture_output=True)
191:     if not check_result["success"]:
192:         raise RuntimeError("Not in git repository\nüîß Troubleshooting: Check inputs and system state")
193:     result = run_cmd("git status --porcelain", capture_output=True)
194:     if not result["success"]:
195:         raise RuntimeError(f"Git status failed: {result['stderr']}\nüîß Troubleshooting: Check inputs and system state")
196:     
197:     staged, unstaged, untracked = [], [], []
198:     lines = result["stdout"].strip().split("\n") if result["stdout"].strip() else []
199:     
200:     for line in lines:
201:         if not line:
202:             continue
203:         status, filepath = line[:2], line[3:]
204:         if status[0] != " " and status[0] != "?":
205:             staged.append(filepath)
206:         if status[1] != " " and status[1] != "?":
207:             unstaged.append(filepath)
208:         if status == "??":
209:             untracked.append(filepath)
210:     
211:     return {"clean": len(staged) == 0 and len(unstaged) == 0 and len(untracked) == 0,
212:             "staged": staged, "unstaged": unstaged, "untracked": untracked}
213: 
214: 
215: def git_diff(since: str = "HEAD~5", name_only: bool = True) -> List[str]:
216:     """Get changed files since specified ref."""
217:     flag = "--name-only" if name_only else "--stat"
218:     result = run_cmd(f"git diff {flag} {since}", capture_output=True)
219:     if not result["success"]:
220:         raise RuntimeError(f"Git diff failed: {result['stderr']}\nüîß Troubleshooting: Check inputs and system state")
221:     return [f.strip() for f in result["stdout"].strip().split("\n") if f.strip()]
222: 
223: 
224: def git_checkpoint(message: str = "Context Engineering checkpoint") -> str:
225:     """Create git tag checkpoint for recovery."""
226:     import datetime
227:     timestamp = int(datetime.datetime.now().timestamp())
228:     checkpoint_id = f"checkpoint-{timestamp}"
229:     result = run_cmd(["git", "tag", "-a", checkpoint_id, "-m", message], capture_output=True)
230:     if not result["success"]:
231:         raise RuntimeError(f"Failed to create checkpoint: {result['stderr']}\nüîß Troubleshooting: Check inputs and system state")
232:     return checkpoint_id
233: 
234: 
235: def run_py(code: Optional[str] = None, file: Optional[str] = None, args: str = "", auto: Optional[str] = None) -> Dict[str, Any]:
236:     """Execute Python code using uv with strict LOC limits."""
237:     if auto is not None:
238:         if code is not None or file is not None:
239:             raise ValueError("Cannot use 'auto' with 'code' or 'file'\nüîß Troubleshooting: Check inputs and system state")
240:         file = auto if "/" in auto or auto.endswith(".py") else None
241:         code = auto if file is None else None
242: 
243:     if code is None and file is None:
244:         raise ValueError("Either 'code', 'file', or 'auto' must be provided")
245:     if code is not None and file is not None:
246:         raise ValueError("Cannot provide both 'code' and 'file'\nüîß Troubleshooting: Check inputs and system state")
247: 
248:     if code is not None:
249:         lines = [line for line in code.split('\n') if line.strip()]
250:         if len(lines) > 3:
251:             raise ValueError(f"Ad-hoc code exceeds 3 LOC limit\nüîß Troubleshooting: Check inputs and system state")
252:         cmd = ["uv", "run", "python", "-c", code]
253:         if args:
254:             cmd.extend(args.split())
255:         return run_cmd(cmd, timeout=120)
256: 
257:     if file is not None:
258:         file_path = Path(file)
259:         if not any(part == "tmp" for part in file_path.parts):
260:             raise ValueError(f"File must be in tmp/ folder")
261:         if not file_path.exists():
262:             raise FileNotFoundError(f"Python file not found: {file}\nüîß Troubleshooting: Check inputs and system state")
263:         cmd = ["uv", "run", "python", file]
264:         if args:
265:             cmd.extend(args.split())
266:         return run_cmd(cmd, timeout=300)
</file>

<file path="tools/ce/linear_mcp_resilience.py">
  1: """Linear MCP resilience layer with automatic auth recovery.
  2: 
  3: Ensures Linear MCP within Syntropy calls handles authentication failures gracefully:
  4: 1. Detects auth failures (401, "Not connected", "unauthorized")
  5: 2. Attempts auth reset (rm -rf ~/.mcp-auth)
  6: 3. Retries operation with retry/backoff logic
  7: 4. Falls back gracefully if auth recovery fails
  8: 
  9: Design:
 10: - Circuit breaker prevents repeated auth attempts after threshold
 11: - Retry with exponential backoff (1s, 2s, 4s)
 12: - Detailed error messages with troubleshooting guidance
 13: - No silent failures - all auth issues surfaced
 14: """
 15: 
 16: import subprocess
 17: from typing import Callable, Any, Optional, Dict
 18: from pathlib import Path
 19: from datetime import datetime
 20: 
 21: from ce.resilience import CircuitBreaker, retry_with_backoff, CircuitBreakerOpenError
 22: from ce.logging_config import get_logger
 23: 
 24: logger = get_logger(__name__)
 25: 
 26: # Circuit breaker for Linear MCP operations
 27: linear_breaker = CircuitBreaker(
 28:     name="linear-mcp",
 29:     failure_threshold=3,
 30:     recovery_timeout=300  # 5 minutes between recovery attempts
 31: )
 32: 
 33: # Circuit breaker specifically for auth recovery
 34: auth_recovery_breaker = CircuitBreaker(
 35:     name="linear-mcp-auth-recovery",
 36:     failure_threshold=2,
 37:     recovery_timeout=600  # 10 minutes between recovery attempts
 38: )
 39: 
 40: # Auth cache to avoid repeated resets
 41: _auth_reset_cache: Dict[str, datetime] = {}
 42: AUTH_RESET_COOLDOWN = 60  # Minimum seconds between auth resets
 43: 
 44: 
 45: def _is_auth_error(error: Exception, error_msg: str = "") -> bool:
 46:     """Detect if error is authentication-related.
 47: 
 48:     Patterns:
 49:     - "Not connected" (Linear MCP disconnected)
 50:     - "401" or "unauthorized" (HTTP auth failure)
 51:     - "authentication" (generic auth failure)
 52:     - "permission denied" (auth permission issue)
 53:     """
 54:     error_text = str(error).lower() + error_msg.lower()
 55: 
 56:     auth_patterns = [
 57:         "not connected",
 58:         "401",
 59:         "unauthorized",
 60:         "authentication",
 61:         "permission denied",
 62:         "auth failed",
 63:         "invalid credentials",
 64:         "access denied"
 65:     ]
 66: 
 67:     return any(pattern in error_text for pattern in auth_patterns)
 68: 
 69: 
 70: def _can_reset_auth() -> bool:
 71:     """Check if auth reset is allowed (respects cooldown).
 72: 
 73:     Returns:
 74:         True if enough time has passed since last reset
 75:     """
 76:     last_reset = _auth_reset_cache.get("linear_mcp_last_reset")
 77:     if last_reset is None:
 78:         return True
 79: 
 80:     elapsed = (datetime.now() - last_reset).total_seconds()
 81:     return elapsed >= AUTH_RESET_COOLDOWN
 82: 
 83: 
 84: def _reset_linear_mcp_auth() -> bool:
 85:     """Reset Linear MCP auth by clearing MCP auth cache.
 86: 
 87:     Executes: rm -rf ~/.mcp-auth
 88: 
 89:     Returns:
 90:         True if reset succeeded, False otherwise
 91: 
 92:     Side Effects:
 93:         - Clears ~/.mcp-auth directory
 94:         - Updates auth reset cache timestamp
 95:     """
 96:     if not _can_reset_auth():
 97:         logger.debug("Auth reset on cooldown - skipping")
 98:         return False
 99: 
100:     try:
101:         auth_dir = Path.home() / ".mcp-auth"
102: 
103:         if not auth_dir.exists():
104:             logger.debug("Auth directory already cleared")
105:             _auth_reset_cache["linear_mcp_last_reset"] = datetime.now()
106:             return True
107: 
108:         # Use subprocess for safe deletion
109:         result = subprocess.run(
110:             ["rm", "-rf", str(auth_dir)],
111:             capture_output=True,
112:             timeout=5
113:         )
114: 
115:         if result.returncode != 0:
116:             logger.warning(f"Auth reset command failed: {result.stderr.decode()}")
117:             return False
118: 
119:         logger.info("Linear MCP auth reset successfully")
120:         _auth_reset_cache["linear_mcp_last_reset"] = datetime.now()
121:         return True
122: 
123:     except subprocess.TimeoutExpired:
124:         logger.error("Auth reset command timed out")
125:         return False
126:     except Exception as e:
127:         logger.error(f"Failed to reset Linear MCP auth: {e}")
128:         return False
129: 
130: 
131: @retry_with_backoff(
132:     max_attempts=3,
133:     base_delay=1.0,
134:     max_delay=10.0,
135:     exceptions=(RuntimeError, ConnectionError, IOError, OSError)
136: )
137: def _call_linear_mcp_with_retry(func: Callable, *args, **kwargs) -> Any:
138:     """Call Linear MCP function with retry logic.
139: 
140:     Args:
141:         func: Linear MCP function to call
142:         *args: Positional arguments
143:         **kwargs: Keyword arguments
144: 
145:     Returns:
146:         Function result
147: 
148:     Raises:
149:         RuntimeError: If all retries exhausted
150:     """
151:     try:
152:         return func(*args, **kwargs)
153:     except Exception as e:
154:         error_msg = str(e)
155: 
156:         # Check if auth error
157:         if _is_auth_error(e, error_msg):
158:             logger.warning(f"Auth error detected: {error_msg}")
159:             logger.info("Attempting auth recovery...")
160: 
161:             # Try to recover auth
162:             if _reset_linear_mcp_auth():
163:                 logger.info("Auth reset succeeded - will retry operation")
164:                 # Retry is handled by decorator
165:                 raise RuntimeError(f"Auth recovered, retrying: {error_msg}\nüîß Troubleshooting: Check inputs and system state") from e
166:             else:
167:                 logger.error("Auth reset failed - operation cannot proceed")
168:                 raise RuntimeError(
169:                     f"Linear MCP auth failed and recovery failed\n"
170:                     f"Error: {error_msg}\n"
171:                     f"üîß Troubleshooting:\n"
172:                     f"  1. Manually run: rm -rf ~/.mcp-auth\n"
173:                     f"  2. Verify Linear MCP is properly configured\n"
174:                     f"  3. Check network connectivity to Linear service\n"
175:                     f"  4. Restart Claude Code and retry"
176:                 ) from e
177: 
178:         # Non-auth error - propagate
179:         raise
180: 
181: 
182: def call_linear_mcp_resilient(
183:     func: Callable,
184:     *args,
185:     operation_name: str = "Linear MCP operation",
186:     **kwargs
187: ) -> Dict[str, Any]:
188:     """Call Linear MCP function with full resilience (retry + circuit breaker + auth recovery).
189: 
190:     Args:
191:         func: Linear MCP function to call
192:         *args: Positional arguments
193:         operation_name: Human-readable operation name for logging
194:         **kwargs: Keyword arguments
195: 
196:     Returns:
197:         {
198:             "success": True,
199:             "result": <function result>,
200:             "method": "direct_call",
201:             "attempts": 1,
202:             "error": None
203:         }
204:         OR
205:         {
206:             "success": False,
207:             "result": None,
208:             "method": "failed",
209:             "attempts": N,
210:             "error": "<error message>",
211:             "recovery_attempted": True/False
212:         }
213: 
214:     Process:
215:         1. Check circuit breaker state
216:         2. Call function with retry + auth recovery
217:         3. On auth error: attempt auth reset, retry
218:         4. On persistent failure: open circuit breaker
219:         5. Return detailed result
220: 
221:     Side Effects:
222:         - May reset ~/.mcp-auth on auth failure
223:         - Updates circuit breaker state
224:     """
225:     logger.info(f"Starting resilient Linear MCP call: {operation_name}")
226: 
227:     attempt = 0
228:     recovery_attempted = False
229: 
230:     try:
231:         # Check circuit breaker
232:         if linear_breaker.state == "open":
233:             if not linear_breaker._should_attempt_reset():
234:                 raise CircuitBreakerOpenError(
235:                     f"Circuit breaker '{linear_breaker.name}' is OPEN\n"
236:                     f"Failures: {linear_breaker.failure_count}/{linear_breaker.failure_threshold}\n"
237:                     f"üîß Troubleshooting: Wait {linear_breaker.recovery_timeout}s or check Linear service health"
238:                 )
239:             # Attempt recovery from half-open state
240:             linear_breaker._transition_to_half_open()
241: 
242:         # Call with retry + auth recovery
243:         attempt = 1
244:         try:
245:             result = _call_linear_mcp_with_retry(func, *args, **kwargs)
246:             linear_breaker._on_success()
247: 
248:             return {
249:                 "success": True,
250:                 "result": result,
251:                 "method": "direct_call",
252:                 "attempts": attempt,
253:                 "error": None,
254:                 "recovery_attempted": False
255:             }
256: 
257:         except RuntimeError as retry_error:
258:             # Check if it's auth recovery retry
259:             if "Auth recovered" in str(retry_error):
260:                 recovery_attempted = True
261:                 attempt += 1
262:                 logger.info(f"Retrying after auth recovery (attempt {attempt})")
263:                 result = _call_linear_mcp_with_retry(func, *args, **kwargs)
264:                 linear_breaker._on_success()
265: 
266:                 return {
267:                     "success": True,
268:                     "result": result,
269:                     "method": "after_auth_recovery",
270:                     "attempts": attempt,
271:                     "error": None,
272:                     "recovery_attempted": True
273:                 }
274:             raise
275: 
276:     except CircuitBreakerOpenError as e:
277:         linear_breaker._on_failure()
278:         logger.error(f"Circuit breaker open: {e}")
279: 
280:         return {
281:             "success": False,
282:             "result": None,
283:             "method": "circuit_breaker_open",
284:             "attempts": attempt,
285:             "error": str(e),
286:             "recovery_attempted": False
287:         }
288: 
289:     except Exception as e:
290:         linear_breaker._on_failure()
291:         error_msg = str(e)
292: 
293:         logger.error(f"Linear MCP operation failed: {error_msg}")
294: 
295:         # Provide actionable error with troubleshooting
296:         is_auth = _is_auth_error(e, error_msg)
297: 
298:         return {
299:             "success": False,
300:             "result": None,
301:             "method": "auth_recovery" if is_auth else "failed",
302:             "attempts": attempt,
303:             "error": f"{error_msg}\n"
304:                     f"üîß Troubleshooting:\n"
305:                     f"  1. Check Linear MCP connectivity\n"
306:                     f"  2. Run: rm -rf ~/.mcp-auth\n"
307:                     f"  3. Verify API credentials are valid\n"
308:                     f"  4. Check network connectivity\n",
309:             "recovery_attempted": recovery_attempted
310:         }
311: 
312: 
313: def create_issue_resilient(
314:     title: str,
315:     description: str,
316:     state: str = "todo",
317:     labels: Optional[list] = None,
318:     override_assignee: Optional[str] = None,
319:     override_project: Optional[str] = None
320: ) -> Dict[str, Any]:
321:     """Create Linear issue with resilience and auth recovery.
322: 
323:     Args:
324:         title: Issue title
325:         description: Issue description
326:         state: Issue state
327:         labels: Optional labels
328:         override_assignee: Optional assignee override
329:         override_project: Optional project override
330: 
331:     Returns:
332:         Result dict from call_linear_mcp_resilient with issue data on success
333:     """
334:     # Import here to avoid circular imports
335:     from ce.linear_utils import create_issue_with_defaults
336: 
337:     # This gets the prepared issue data (not actually calling MCP yet)
338:     issue_data = create_issue_with_defaults(
339:         title=title,
340:         description=description,
341:         state=state,
342:         labels=labels,
343:         override_assignee=override_assignee,
344:         override_project=override_project
345:     )
346: 
347:     # TODO: Replace with actual Linear MCP call
348:     # For now, return prepared data with success flag
349:     logger.warning("Linear MCP create_issue not yet integrated - returning prepared data")
350: 
351:     return {
352:         "success": True,
353:         "result": issue_data,
354:         "method": "prepared_data_only",
355:         "attempts": 1,
356:         "error": None,
357:         "recovery_attempted": False
358:     }
359: 
360: 
361: def update_issue_resilient(
362:     issue_id: str,
363:     description: str,
364:     state: Optional[str] = None
365: ) -> Dict[str, Any]:
366:     """Update Linear issue with resilience and auth recovery.
367: 
368:     Args:
369:         issue_id: Linear issue ID (e.g., "BLA-24")
370:         description: Updated description
371:         state: Optional new state
372: 
373:     Returns:
374:         Result dict from call_linear_mcp_resilient
375:     """
376:     # TODO: Replace with actual Linear MCP call
377:     logger.warning("Linear MCP update_issue not yet integrated")
378: 
379:     return {
380:         "success": False,
381:         "result": None,
382:         "method": "not_implemented",
383:         "attempts": 1,
384:         "error": "Linear MCP update_issue not yet implemented",
385:         "recovery_attempted": False
386:     }
387: 
388: 
389: def get_linear_mcp_status() -> Dict[str, Any]:
390:     """Get Linear MCP health status.
391: 
392:     Returns:
393:         {
394:             "connected": True/False,
395:             "circuit_breaker_state": "closed|open|half_open",
396:             "failure_count": N,
397:             "last_auth_reset": "ISO timestamp or null",
398:             "auth_reset_available": True/False,
399:             "diagnostics": "..."
400:         }
401:     """
402:     return {
403:         "connected": linear_breaker.state == "closed",
404:         "circuit_breaker_state": linear_breaker.state,
405:         "failure_count": linear_breaker.failure_count,
406:         "last_auth_reset": _auth_reset_cache.get("linear_mcp_last_reset"),
407:         "auth_reset_available": _can_reset_auth(),
408:         "diagnostics": f"Circuit state: {linear_breaker.state}, "
409:                       f"Failures: {linear_breaker.failure_count}/{linear_breaker.failure_threshold}"
410:     }
</file>

<file path="tools/ce/pipeline.py">
  1: """CI/CD Pipeline abstraction and validation.
  2: 
  3: Provides platform-agnostic pipeline definition and validation.
  4: """
  5: 
  6: from typing import Dict, Any, List
  7: import yaml
  8: import jsonschema
  9: 
 10: 
 11: PIPELINE_SCHEMA = {
 12:     "type": "object",
 13:     "required": ["name", "stages"],
 14:     "properties": {
 15:         "name": {"type": "string"},
 16:         "description": {"type": "string"},
 17:         "stages": {
 18:             "type": "array",
 19:             "items": {
 20:                 "type": "object",
 21:                 "required": ["name", "nodes"],
 22:                 "properties": {
 23:                     "name": {"type": "string"},
 24:                     "nodes": {
 25:                         "type": "array",
 26:                         "items": {
 27:                             "type": "object",
 28:                             "required": ["name", "command"],
 29:                             "properties": {
 30:                                 "name": {"type": "string"},
 31:                                 "command": {"type": "string"},
 32:                                 "strategy": {"type": "string", "enum": ["real", "mock"]},
 33:                                 "timeout": {"type": "integer"}
 34:                             }
 35:                         }
 36:                     },
 37:                     "parallel": {"type": "boolean"},
 38:                     "depends_on": {"type": "array", "items": {"type": "string"}}
 39:                 }
 40:             }
 41:         }
 42:     }
 43: }
 44: 
 45: 
 46: def load_abstract_pipeline(file_path: str) -> Dict[str, Any]:
 47:     """Load abstract pipeline definition from YAML file.
 48: 
 49:     Args:
 50:         file_path: Path to abstract pipeline YAML file
 51: 
 52:     Returns:
 53:         Dict containing pipeline definition
 54: 
 55:     Raises:
 56:         FileNotFoundError: If file doesn't exist
 57:         yaml.YAMLError: If YAML parse fails
 58: 
 59:     Note: No fishy fallbacks - let exceptions propagate for troubleshooting.
 60:     """
 61:     try:
 62:         with open(file_path, 'r') as f:
 63:             pipeline = yaml.safe_load(f)
 64:     except FileNotFoundError:
 65:         raise FileNotFoundError(
 66:             f"Pipeline file not found: {file_path}\n"
 67:             f"üîß Troubleshooting: Check the file path is correct"
 68:         )
 69:     except yaml.YAMLError as e:
 70:         raise RuntimeError(
 71:             f"Failed to parse pipeline YAML: {e}\n"
 72:             f"üîß Troubleshooting: Validate YAML syntax at the reported line"
 73:         )
 74: 
 75:     return pipeline
 76: 
 77: 
 78: def validate_pipeline(pipeline: Dict[str, Any]) -> Dict[str, Any]:
 79:     """Validate pipeline against schema.
 80: 
 81:     Args:
 82:         pipeline: Pipeline definition dict
 83: 
 84:     Returns:
 85:         Dict with: success (bool), errors (List[str])
 86: 
 87:     Example:
 88:         result = validate_pipeline(pipeline)
 89:         if not result["success"]:
 90:             raise RuntimeError(f"Invalid pipeline: {result['errors']}\nüîß Troubleshooting: Check inputs and system state")
 91:     """
 92:     errors = []
 93: 
 94:     # Schema validation
 95:     try:
 96:         jsonschema.validate(instance=pipeline, schema=PIPELINE_SCHEMA)
 97:     except jsonschema.ValidationError as e:
 98:         errors.append(f"Schema validation failed: {e.message}")
 99:         errors.append(f"üîß Troubleshooting: Check required fields: name, stages")
100:         return {"success": False, "errors": errors}
101: 
102:     # Semantic validation - check depends_on references
103:     stage_names = [s["name"] for s in pipeline["stages"]]
104:     for stage in pipeline["stages"]:
105:         if "depends_on" in stage:
106:             for dep in stage["depends_on"]:
107:                 if dep not in stage_names:
108:                     errors.append(
109:                         f"Stage '{stage['name']}' depends on unknown stage '{dep}'\n"
110:                         f"üîß Troubleshooting: Available stages: {stage_names}"
111:                     )
112: 
113:     return {
114:         "success": len(errors) == 0,
115:         "errors": errors
116:     }
</file>

<file path="tools/ce/toml_merger.py">
  1: #!/usr/bin/env python3
  2: """
  3: TOML Merger - Intelligent pyproject.toml merging with version unification.
  4: 
  5: Supports PEP 621, Poetry, and Setuptools formats.
  6: Uses version intersection strategy (not "higher wins").
  7: """
  8: 
  9: from pathlib import Path
 10: from typing import Dict, Optional
 11: 
 12: try:
 13:     import tomllib  # Python 3.11+
 14: except ImportError:
 15:     import tomli as tomllib  # Fallback for Python 3.10
 16: 
 17: import tomli_w
 18: 
 19: from .toml_formats.pep621_handler import PEP621Handler
 20: from .toml_formats.poetry_handler import PoetryHandler
 21: from .toml_formats.setuptools_handler import SetuptoolsHandler
 22: from .toml_formats.version_resolver import VersionResolver
 23: 
 24: 
 25: class TomlMerger:
 26:     """
 27:     Orchestrates TOML merging using format-specific handlers.
 28: 
 29:     Strategy pattern delegation:
 30:     - PEP621Handler: PEP 621 format
 31:     - PoetryHandler: Poetry format
 32:     - SetuptoolsHandler: Setuptools format
 33:     - VersionResolver: Version intersection logic
 34:     """
 35: 
 36:     def __init__(self, framework_toml: Path, target_toml: Optional[Path] = None):
 37:         self.framework_toml = Path(framework_toml)
 38:         self.target_toml = Path(target_toml) if target_toml else None
 39: 
 40:         # Load framework TOML
 41:         with open(self.framework_toml, "rb") as f:
 42:             self.framework_data = tomllib.load(f)
 43: 
 44:         # Load target TOML if exists
 45:         self.target_data = None
 46:         if self.target_toml and self.target_toml.exists():
 47:             with open(self.target_toml, "rb") as f:
 48:                 self.target_data = tomllib.load(f)
 49: 
 50:     def merge(self) -> Dict:
 51:         """
 52:         Merge framework and target TOMLs with version intersection.
 53: 
 54:         Returns:
 55:             Merged TOML data (dict)
 56: 
 57:         Raises:
 58:             ValueError: If version conflict detected
 59:         """
 60:         # No target TOML ‚Üí use framework directly
 61:         if not self.target_data:
 62:             return self.framework_data.copy()
 63: 
 64:         # Detect target format and convert to PEP 621
 65:         if PoetryHandler.detect(self.target_data):
 66:             target_pep621 = PoetryHandler.convert_to_pep621(self.target_data)
 67:         elif SetuptoolsHandler.detect(self.target_data):
 68:             target_pep621 = SetuptoolsHandler.convert_to_pep621(self.target_data)
 69:         elif PEP621Handler.detect(self.target_data):
 70:             target_pep621 = self.target_data.copy()
 71:         else:
 72:             # Unknown format ‚Üí use framework
 73:             return self.framework_data.copy()
 74: 
 75:         # Start with framework data
 76:         merged = self.framework_data.copy()
 77: 
 78:         # Merge production dependencies
 79:         if "project" in target_pep621 and "dependencies" in target_pep621["project"]:
 80:             framework_deps = self.framework_data.get("project", {}).get("dependencies", [])
 81:             target_deps = target_pep621["project"]["dependencies"]
 82: 
 83:             merged["project"]["dependencies"] = VersionResolver.merge_dependencies(
 84:                 framework_deps, target_deps
 85:             )
 86: 
 87:         # Merge dev dependencies (dependency-groups)
 88:         if "dependency-groups" in target_pep621:
 89:             if "dependency-groups" not in merged:
 90:                 merged["dependency-groups"] = {}
 91: 
 92:             for group, deps in target_pep621["dependency-groups"].items():
 93:                 framework_deps = merged["dependency-groups"].get(group, [])
 94:                 merged["dependency-groups"][group] = VersionResolver.merge_dependencies(
 95:                     framework_deps, deps
 96:                 )
 97: 
 98:         # Preserve target's metadata
 99:         merged = PEP621Handler.preserve_metadata(target_pep621, merged)
100: 
101:         return merged
102: 
103:     def write(self, output_path: Path):
104:         """
105:         Write merged TOML to file.
106: 
107:         Args:
108:             output_path: Path to write merged TOML
109:         """
110:         merged_data = self.merge()
111: 
112:         with open(output_path, "wb") as f:
113:             tomli_w.dump(merged_data, f)
114: 
115: 
116: def merge_toml_files(framework_toml: Path, target_toml: Optional[Path], output_path: Path):
117:     """
118:     Convenience function to merge TOML files.
119: 
120:     Args:
121:         framework_toml: Path to framework pyproject.toml
122:         target_toml: Path to target pyproject.toml (optional)
123:         output_path: Path to write merged TOML
124: 
125:     Raises:
126:         ValueError: If version conflict detected
127:     """
128:     merger = TomlMerger(framework_toml, target_toml)
129:     merger.write(output_path)
</file>

<file path="tools/ce/vacuum.py">
  1: """Vacuum command for project cleanup."""
  2: 
  3: import argparse
  4: import sys
  5: from datetime import datetime
  6: from pathlib import Path
  7: from typing import List
  8: 
  9: from .vacuum_strategies import (
 10:     BackupFileStrategy,
 11:     CleanupCandidate,
 12:     CommentedCodeStrategy,
 13:     ObsoleteDocStrategy,
 14:     OrphanTestStrategy,
 15:     TempFileStrategy,
 16:     UnreferencedCodeStrategy,
 17: )
 18: 
 19: 
 20: class VacuumCommand:
 21:     """Main vacuum command for project cleanup."""
 22: 
 23:     def __init__(self, project_root: Path):
 24:         """Initialize vacuum command.
 25: 
 26:         Args:
 27:             project_root: Path to project root directory
 28:         """
 29:         self.project_root = project_root
 30:         self.strategies = {
 31:             "temp-files": TempFileStrategy,
 32:             "backup-files": BackupFileStrategy,
 33:             "obsolete-docs": ObsoleteDocStrategy,
 34:             "unreferenced-code": UnreferencedCodeStrategy,
 35:             "orphan-tests": OrphanTestStrategy,
 36:             "commented-code": CommentedCodeStrategy,
 37:         }
 38: 
 39:     def run(
 40:         self,
 41:         dry_run: bool = True,
 42:         min_confidence: int = 0,
 43:         exclude_strategies: List[str] = None,
 44:         execute: bool = False,
 45:         force: bool = False,
 46:         auto: bool = False,
 47:         nuclear: bool = False,
 48:         scan_path: Path = None,
 49:     ) -> int:
 50:         """Run vacuum command.
 51: 
 52:         Args:
 53:             dry_run: If True, only generate report without deleting
 54:             min_confidence: Minimum confidence threshold (0-100)
 55:             exclude_strategies: List of strategy names to skip
 56:             execute: Delete HIGH confidence items
 57:             force: Delete HIGH + MEDIUM confidence items
 58:             auto: Alias for force (delete HIGH + MEDIUM automatically)
 59:             nuclear: Delete ALL items (requires confirmation)
 60:             scan_path: Optional directory to scan (defaults to project root)
 61: 
 62:         Returns:
 63:             Exit code: 0 = clean, 1 = candidates found, 2 = error
 64:         """
 65:         exclude_strategies = exclude_strategies or []
 66: 
 67:         # Determine scan scope
 68:         effective_scan_path = scan_path if scan_path else self.project_root
 69: 
 70:         # Show scope
 71:         if scan_path:
 72:             scope_rel = scan_path.relative_to(self.project_root)
 73:             print(f"üéØ Scope: {scope_rel}/ (scoped to directory)")
 74:         else:
 75:             print(f"üéØ Scope: entire project")
 76: 
 77:         # Determine deletion threshold
 78:         if nuclear:
 79:             delete_threshold = 0  # Delete everything
 80:             if not self._confirm_nuclear():
 81:                 print("‚ùå Nuclear mode cancelled by user")
 82:                 return 2
 83:         elif force or auto:
 84:             delete_threshold = 60  # Delete MEDIUM + HIGH
 85:         elif execute:
 86:             delete_threshold = 100  # Delete HIGH only
 87:         else:
 88:             delete_threshold = 101  # Dry-run: delete nothing
 89: 
 90:         # Run all strategies
 91:         all_candidates = []
 92:         for strategy_name, strategy_class in self.strategies.items():
 93:             if strategy_name in exclude_strategies:
 94:                 print(f"‚è≠Ô∏è  Skipping {strategy_name}")
 95:                 continue
 96: 
 97:             print(f"üîç Running {strategy_name}...")
 98:             strategy = strategy_class(self.project_root, effective_scan_path)
 99:             candidates = strategy.find_candidates()
100: 
101:             # Filter by minimum confidence
102:             candidates = [c for c in candidates if c.confidence >= min_confidence]
103: 
104:             all_candidates.extend(candidates)
105:             print(f"   Found {len(candidates)} candidates")
106: 
107:         # Generate report
108:         report_path = self.project_root / ".ce" / "vacuum-report.md"
109:         self._generate_report(all_candidates, report_path)
110:         print(f"\nüìÑ Report generated: {report_path}")
111: 
112:         # Delete files if not dry-run
113:         if delete_threshold <= 100:
114:             deleted_count = self._delete_candidates(all_candidates, delete_threshold)
115:             print(f"\nüóëÔ∏è  Deleted {deleted_count} items")
116: 
117:         # Return exit code
118:         if not all_candidates:
119:             print("\n‚úÖ No cleanup candidates found - project is clean!")
120:             return 0
121:         else:
122:             print(f"\n‚ö†Ô∏è  Found {len(all_candidates)} cleanup candidates")
123:             return 1
124: 
125:     def _generate_report(self, candidates: List[CleanupCandidate], output_path: Path):
126:         """Generate vacuum report.
127: 
128:         Args:
129:             candidates: List of cleanup candidates
130:             output_path: Path to output report file
131:         """
132:         # Ensure output directory exists
133:         output_path.parent.mkdir(parents=True, exist_ok=True)
134: 
135:         # Group by confidence
136:         high = [c for c in candidates if c.confidence >= 100]
137:         medium = [c for c in candidates if 60 <= c.confidence < 100]
138:         low = [c for c in candidates if c.confidence < 60]
139: 
140:         # Calculate total size
141:         total_size = sum(c.size_bytes for c in candidates)
142: 
143:         # Generate report
144:         report = [
145:             f"# Vacuum Report - {datetime.now().isoformat()}",
146:             "",
147:             "## Summary",
148:             f"- Candidates found: {len(candidates)}",
149:             f"- Bytes reclaimable: {self._format_size(total_size)}",
150:             f"- HIGH confidence: {len(high)} items (safe to delete)",
151:             f"- MEDIUM confidence: {len(medium)} items (review recommended)",
152:             f"- LOW confidence: {len(low)} items (manual verification required)",
153:             "",
154:         ]
155: 
156:         # HIGH confidence section
157:         if high:
158:             report.extend([
159:                 "## HIGH Confidence (Safe to Delete)",
160:                 "",
161:                 "| Path | Reason | Size | Last Modified |",
162:                 "|------|--------|------|---------------|",
163:             ])
164:             for c in high:
165:                 rel_path = c.path.relative_to(self.project_root)
166:                 report.append(
167:                     f"| {rel_path} | {c.reason} | {self._format_size(c.size_bytes)} | {c.last_modified[:10]} |"
168:                 )
169:             report.append("")
170: 
171:         # MEDIUM confidence section
172:         if medium:
173:             report.extend([
174:                 "## MEDIUM Confidence (Review Needed)",
175:                 "",
176:                 "| Path | Reason | Confidence | Git History |",
177:                 "|------|--------|------------|-------------|",
178:             ])
179:             for c in medium:
180:                 rel_path = c.path.relative_to(self.project_root)
181:                 report.append(
182:                     f"| {rel_path} | {c.reason} | {c.confidence}% | {c.git_history} |"
183:                 )
184:             report.append("")
185: 
186:         # LOW confidence section
187:         if low:
188:             report.extend([
189:                 "## LOW Confidence (Manual Verification Required)",
190:                 "",
191:                 "| Path | Reason | Confidence | References |",
192:                 "|------|--------|------------|------------|",
193:             ])
194:             for c in low:
195:                 rel_path = c.path.relative_to(self.project_root)
196:                 refs = ", ".join(c.references[:3]) if c.references else "None"
197:                 report.append(f"| {rel_path} | {c.reason} | {c.confidence}% | {refs} |")
198:             report.append("")
199: 
200:         # Write report
201:         output_path.write_text("\n".join(report), encoding="utf-8")
202: 
203:     def _delete_candidates(self, candidates: List[CleanupCandidate], threshold: int) -> int:
204:         """Delete candidates meeting confidence threshold.
205: 
206:         Args:
207:             candidates: List of cleanup candidates
208:             threshold: Minimum confidence to delete
209: 
210:         Returns:
211:             Number of items deleted
212:         """
213:         import shutil
214: 
215:         deleted_count = 0
216: 
217:         for candidate in candidates:
218:             # Skip report-only candidates (never delete these)
219:             if candidate.report_only:
220:                 continue
221: 
222:             if candidate.confidence < threshold:
223:                 continue
224: 
225:             try:
226:                 if candidate.path.is_file():
227:                     candidate.path.unlink()
228:                     deleted_count += 1
229:                 elif candidate.path.is_dir():
230:                     shutil.rmtree(candidate.path)
231:                     deleted_count += 1
232:             except Exception as e:
233:                 print(f"‚ùå Failed to delete {candidate.path}: {e}")
234: 
235:         return deleted_count
236: 
237:     def _confirm_nuclear(self) -> bool:
238:         """Ask user to confirm nuclear mode.
239: 
240:         Returns:
241:             True if user confirms, False otherwise
242:         """
243:         response = input("‚ö†Ô∏è  NUCLEAR MODE: Delete ALL candidates including LOW confidence? (yes/no): ")
244:         return response.lower() == "yes"
245: 
246:     @staticmethod
247:     def _format_size(size_bytes: int) -> str:
248:         """Format file size in human-readable format.
249: 
250:         Args:
251:             size_bytes: Size in bytes
252: 
253:         Returns:
254:             Formatted size string
255:         """
256:         for unit in ["B", "KB", "MB", "GB"]:
257:             if size_bytes < 1024.0:
258:                 return f"{size_bytes:.1f} {unit}"
259:             size_bytes /= 1024.0
260:         return f"{size_bytes:.1f} TB"
261: 
262: 
263: def main():
264:     """CLI entry point for vacuum command."""
265:     parser = argparse.ArgumentParser(description="Clean up project noise")
266:     parser.add_argument(
267:         "--dry-run",
268:         action="store_true",
269:         default=True,
270:         help="Generate report only (default)",
271:     )
272:     parser.add_argument(
273:         "--execute",
274:         action="store_true",
275:         help="Delete HIGH confidence items",
276:     )
277:     parser.add_argument(
278:         "--force",
279:         action="store_true",
280:         help="Delete HIGH + MEDIUM confidence items",
281:     )
282:     parser.add_argument(
283:         "--nuclear",
284:         action="store_true",
285:         help="Delete ALL items (requires confirmation)",
286:     )
287:     parser.add_argument(
288:         "--min-confidence",
289:         type=int,
290:         default=0,
291:         help="Minimum confidence threshold (0-100)",
292:     )
293:     parser.add_argument(
294:         "--exclude-strategy",
295:         action="append",
296:         dest="exclude_strategies",
297:         help="Skip specific strategy",
298:     )
299:     parser.add_argument(
300:         "--path",
301:         type=str,
302:         help="Directory to scan (relative to project root, defaults to entire project)",
303:     )
304:     parser.add_argument(
305:         "--auto",
306:         action="store_true",
307:         help="Automatically delete HIGH + MEDIUM confidence items (same as --force)",
308:     )
309: 
310:     args = parser.parse_args()
311: 
312:     # Find project root (where .ce/ directory exists)
313:     current = Path.cwd()
314:     project_root = None
315: 
316:     for parent in [current] + list(current.parents):
317:         if (parent / ".ce").exists():
318:             project_root = parent
319:             break
320: 
321:     if not project_root:
322:         print("‚ùå Error: Not in a Context Engineering project (.ce/ not found)\nüîß Troubleshooting: Check inputs and system state")
323:         return 2
324: 
325:     # Resolve scan path if provided
326:     scan_path = None
327:     if args.path:
328:         scan_path = project_root / args.path
329:         if not scan_path.exists():
330:             print(f"‚ùå Error: Path does not exist: {args.path}\nüîß Troubleshooting: Verify file path exists")
331:             return 2
332:         if not scan_path.is_dir():
333:             print(f"‚ùå Error: Path is not a directory: {args.path}\nüîß Troubleshooting: Check inputs and system state")
334:             return 2
335: 
336:     # Run vacuum command
337:     vacuum = VacuumCommand(project_root)
338:     return vacuum.run(
339:         dry_run=not (args.execute or args.force or args.auto or args.nuclear),
340:         min_confidence=args.min_confidence,
341:         exclude_strategies=args.exclude_strategies or [],
342:         execute=args.execute,
343:         force=args.force,
344:         auto=args.auto,
345:         nuclear=args.nuclear,
346:         scan_path=scan_path,
347:     )
348: 
349: 
350: if __name__ == "__main__":
351:     sys.exit(main())
</file>

<file path=".ce/blend-config.yml">
  1: # CE Framework Blending Configuration
  2: # Defines blending rules for each domain (settings, CLAUDE.md, memories, etc.)
  3: 
  4: domains:
  5:   settings:
  6:     strategy: rule-based
  7:     source: .claude/settings.local.json
  8:     backup: true
  9:     rules:
 10:       - ce_deny_wins           # CE deny list takes precedence
 11:       - merge_lists            # Merge CE entries to target lists
 12:       - single_membership      # Each tool in ONE list only
 13:     description: "Settings JSON structural merge (3 rules)"
 14: 
 15:   claude_md:
 16:     strategy: nl-blend
 17:     source: CLAUDE.md
 18:     framework_rules: .ce/RULES.md
 19:     backup: true
 20:     llm_model: claude-sonnet-4-5-20250929
 21:     max_tokens: 8192
 22:     description: "CLAUDE.md NL-blend with RULES.md awareness (Sonnet)"
 23: 
 24:   memories:
 25:     strategy: nl-blend
 26:     source: .serena/memories/
 27:     backup: true
 28:     llm_model_similarity: claude-3-5-haiku-20241022    # Fast similarity checks
 29:     llm_model_merge: claude-sonnet-4-5-20250929        # Quality merges
 30:     conflict_resolution: ask-user
 31:     similarity_threshold: 0.9
 32:     description: "Serena memories NL-blend (Haiku similarity ‚Üí Sonnet merge)"
 33: 
 34:   examples:
 35:     strategy: dedupe-copy
 36:     source: examples/
 37:     destination: .ce/examples/
 38:     backup: false
 39:     dedup_method: nl-similarity
 40:     llm_model: claude-3-5-haiku-20241022               # Fast deduplication
 41:     similarity_threshold: 0.9
 42:     description: "Examples NL-dedupe (Haiku semantic comparison)"
 43: 
 44:   prps:
 45:     strategy: move-all
 46:     source: PRPs/
 47:     legacy_sources:
 48:       - PRPs/
 49:       - context-engineering/PRPs/
 50:     destination: .ce/PRPs/
 51:     backup: false
 52:     dedup_method: content-hash                          # Only skip if exact file exists
 53:     add_user_header: true
 54:     note: "Move all user PRPs (no ID-based deduplication - different projects can have same IDs)"
 55:     description: "PRPs move-all (hash dedupe only, no ID dedupe)"
 56: 
 57:   commands:
 58:     strategy: overwrite
 59:     source: .claude/commands/
 60:     backup: true
 61:     backup_location: .claude/commands.backup/
 62:     description: "Commands overwrite (framework canonical)"
 63: 
 64: # LLM Configuration
 65: llm:
 66:   api_key_env: ANTHROPIC_API_KEY
 67:   timeout: 60
 68:   max_retries: 3
 69: 
 70: # Global settings
 71: global:
 72:   verify_before_cleanup: true
 73:   backup_prefix: ".backup"
 74:   legacy_directories:
 75:     - PRPs/
 76:     - examples/
 77:     - context-engineering/
 78:   standard_locations:
 79:     - .claude/
 80:     - .serena/
 81:     - CLAUDE.md
 82:     - .ce/
 83: 
 84: directories:
 85:   output:
 86:     claude_dir: .ce/.claude/
 87:     claude_md: CLAUDE.md
 88:     serena_memories: .serena/memories/
 89:     examples: .ce/examples/
 90:     prps: .ce/PRPs/
 91:   framework:
 92:     serena_memories: .ce/.serena/memories/
 93:     examples: .ce/examples/
 94:     prps: .ce/PRPs/
 95:     commands: .ce/.claude/commands/
 96:     settings: .ce/.claude/settings.local.json
 97:   legacy:
 98:     - PRPs/
 99:     - examples/
100:     - context-engineering/
101:     - .serena.old/
</file>

<file path="examples/INDEX.md">
  1: # Context Engineering Examples Index
  2: 
  3: Comprehensive catalog of all Context Engineering framework examples, organized by type and category for easy discovery.
  4: 
  5: ## Quick Reference
  6: 
  7: **I want to...**
  8: 
  9: - Initialize CE framework ‚Üí [Framework Initialization](#framework-initialization)
 10: - Learn Syntropy MCP tools ‚Üí [Tool Usage Guide](TOOL-USAGE-GUIDE.md)
 11: - Initialize Serena memories ‚Üí [Serena Memory Templates](#serena-memory-templates) (23 framework memories with YAML headers)
 12: - Run batch PRPs ‚Üí Slash commands: `/batch-gen-prp`, `/batch-exe-prp` (see `.claude/commands/`)
 13: - Clean up my project ‚Üí Slash command: `/vacuum` (see `.claude/commands/vacuum.md`)
 14: - Fix context drift ‚Üí `cd tools && uv run ce context health`
 15: - Configure commands/hooks ‚Üí [Slash Commands](#slash-commands)
 16: - Understand patterns ‚Üí [Patterns](#patterns)
 17: - Migrate existing project ‚Üí [Migration Workflows](#migration-workflows)
 18: 
 19: ## All Examples
 20: 
 21: | Name | Type | Category | IsWorkflow | Description | Path |
 22: |------|------|----------|-----------|-------------|------|
 23: | **FRAMEWORK INITIALIZATION** | | | | | |
 24: | Initialization Guide | Guide | Initialization | Yes | Master CE 1.1 framework initialization (5 phases: buckets, user files, repomix, blending, cleanup). Covers 4 scenarios: Greenfield, Mature Project, CE 1.0 Upgrade, Partial Install | [INITIALIZATION.md](INITIALIZATION.md) |
 25: | **TEMPLATES** | | | | | |
 26: | PRP-0 Template | Template | Initialization | Yes | Document framework installation in meta-PRP (PRP-0-CONTEXT-ENGINEERING.md template) | [templates/PRP-0-CONTEXT-ENGINEERING.md](templates/PRP-0-CONTEXT-ENGINEERING.md) |
 27: | **SLASH COMMANDS** | | | | | |
 28: | Batch PRP Execution | Command | Batch | Yes | Execute PRPs in parallel stages with health monitoring (see `.claude/commands/batch-exe-prp.md`) | Command: `/batch-exe-prp` |
 29: | Batch PRP Generation | Command | Batch | Yes | Generate multiple PRPs from plan with dependency analysis (see `.claude/commands/batch-gen-prp.md`) | Command: `/batch-gen-prp` |
 30: | Context Drift Check | Command | Context | Yes | Fast drift score check without full validation (see `.claude/commands/analyze-context.md`) | Command: `/analyze-context` |
 31: | Denoise Documents | Command | Cleanup | Yes | Compress verbose documentation with AI-powered denoising (see `.claude/commands/denoise.md`) | Command: `/denoise` |
 32: | Vacuum Cleanup | Command | Cleanup | Yes | Identify and remove project noise with confidence-based deletion (see `.claude/commands/vacuum.md`) | Command: `/vacuum` |
 33: | **PATTERNS** | | | | | |
 34: | Dedrifting Lessons | Pattern | Context | Yes | Root cause analysis for context drift with prevention strategies | [patterns/dedrifting-lessons.md](patterns/dedrifting-lessons.md) |
 35: | Example Simple Feature | Pattern | PRP | No | Complete PRP example for adding git status summary command (ctx-eng-plus specific) | [patterns/example-simple-feature.md](patterns/example-simple-feature.md) |
 36: | Git Message Rules | Pattern | Git | No | Git commit message formatting and convention rules (ctx-eng-plus specific) | [patterns/git-message-rules.md](patterns/git-message-rules.md) |
 37: | Mock Marking Pattern | Pattern | Testing | Yes | Mark mocks with FIXME comments for tracking temporary test code | [patterns/mocks-marking.md](patterns/mocks-marking.md) |
 38: | **GUIDES** | | | | | |
 39: | Tool Usage Guide | Guide | Tools | Yes | Complete tool selection guide with native-first philosophy, decision trees and examples. Updated 2025-11-06 with CE Framework Commands section | [TOOL-USAGE-GUIDE.md](TOOL-USAGE-GUIDE.md) |
 40: | PRP Decomposition Patterns | Guide | PRP | Yes | Patterns for breaking down large features into manageable PRPs | [prp-decomposition-patterns.md](prp-decomposition-patterns.md) |
 41: | CE Blend Command Usage | Guide | Commands | Yes | Complete guide for ce blend command (PRP-34): 4-phase pipeline, 6 domain strategies, 9 scenarios, troubleshooting | [ce-blend-usage.md](ce-blend-usage.md) |
 42: | CE Init-Project Command Usage | Guide | Commands | Yes | Complete guide for ce init-project command (PRP-36): 4-phase pipeline, 4 scenarios, error handling for 5 known issues, performance metrics | [ce-init-project-usage.md](ce-init-project-usage.md) |
 43: | **REFERENCE** | | | | | |
 44: | L4 Validation Example | Reference | Validation | No | Level 4 pattern conformance validation example (ctx-eng-plus specific) | [l4-validation-example.md](l4-validation-example.md) |
 45: | Linear Integration Example | Reference | MCP | Yes | Linear MCP integration example with configuration defaults | [linear-integration-example.md](linear-integration-example.md) |
 46: | Mermaid Color Palette | Reference | Diagrams | Yes | Standard color palette for mermaid diagrams with light/dark themes | [mermaid-color-palette.md](mermaid-color-palette.md) |
 47: | Syntropy Status Hook | Reference | MCP | No | Syntropy MCP health check system (references ctx-eng-plus scripts) | [syntropy-status-hook-system.md](syntropy-status-hook-system.md) |
 48: | Settings Local Example | Reference | Configuration | Yes | Example .claude/settings.local.json with permissions (framework template) | [example.setting.local.md](example.setting.local.md) |
 49: | tmp/ Directory Convention | Reference | Standards | Yes | Conventions for temporary file storage and cleanup | [tmp-directory-convention.md](tmp-directory-convention.md) |
 50: | **MODEL** | | | | | |
 51: | System Model | Model | Architecture | Yes | Complete Context Engineering framework architecture and design | [model/SystemModel.md](model/SystemModel.md) |
 52: 
 53: ## Statistics
 54: 
 55: ### Examples & Documentation
 56: 
 57: - **Total Examples**: 25 files (+2 new command guides)
 58: - **Framework Initialization**: 6 (Main guide + 4 migration workflows + integration summary)
 59: - **Templates**: 1 (PRP-0-CONTEXT-ENGINEERING.md)
 60: - **Slash Commands**: 5 (Reference - actual commands in `.claude/commands/`)
 61: - **Patterns**: 4 (Git, testing, context, PRP)
 62: - **Guides**: 4 (Tools, PRP decomposition, **ce blend**, **ce init-project**) - NEW
 63: - **Reference**: 6 (Validation, diagrams, standards, Syntropy overview)
 64: - **Model**: 1 (System architecture)
 65: 
 66: **Note**: Workflows previously referenced in INDEX.md now exist as slash commands (`.claude/commands/`) or CLI tools (`ce` command). Migration guides and initialization documentation are new additions for CE 1.1.
 67: 
 68: ### Serena Memories
 69: 
 70: - **Total Memories**: 23 files (~3,621 lines) with YAML type headers (CE 1.1)
 71: - **Type System**: All framework memories default to `type: regular` (users upgrade to `type: critical` during target project initialization)
 72: - **Categories**: documentation (13), pattern (5), architecture (2), configuration (4), troubleshooting (1)
 73: - **Critical Memory Candidates**: 6 memories (code-style-conventions, suggested-commands, task-completion-checklist, testing-standards, tool-usage-syntropy, use-syntropy-tools-not-bash)
 74: - **Memory Type README**: See `.serena/memories/README.md` for complete type system documentation
 75: - **Storage**: `.serena/memories/` (created automatically by Serena MCP)
 76: 
 77: ## Categories
 78: 
 79: ### Framework Initialization
 80: 
 81: Complete CE 1.1 framework initialization and migration workflows:
 82: 
 83: | Example | Type | Duration | Description |
 84: |---------|------|----------|-------------|
 85: | [Initialization Guide](INITIALIZATION.md) | Guide | Variable | Master CE 1.1 framework initialization guide (5 phases: buckets, user files, repomix, blending, cleanup). Covers 4 scenarios: Greenfield (10 min), Mature Project (45 min), CE 1.0 Upgrade (40 min), Partial Install (15 min) |
 86: | [PRP-0 Template](templates/PRP-0-CONTEXT-ENGINEERING.md) | Template | - | Document framework installation in meta-PRP |
 87: 
 88: **Total**: 2 files (1 master guide + 1 template)
 89: 
 90: **Key Features**:
 91: - 5-phase initialization (bucket collection, user files, repomix, blending, cleanup)
 92: - /system/ organization for framework files (separation from user files)
 93: - YAML header system for memories and PRPs (type: regular/critical/user)
 94: - Zero noise guarantee (legacy files cleaned up after migration)
 95: - PRP-0 convention (document installation in meta-PRP)
 96: 
 97: ### Serena Memory Templates
 98: 
 99: #### Recommended Memory Types for New Projects
100: 
101: Recommended initial knowledge base for Serena memory initialization in new projects:
102: 
103: | Memory Type | Purpose | IsWorkflow | When to Use | Example Topics |
104: |-------------|---------|-----------|-------------|-----------------|
105: | `architecture` | Document architectural decisions and design rationale | Yes | Record why certain patterns/structures were chosen | Validation approach, error handling strategy, module organization |
106: | `pattern` | Build reusable solution library | Yes | Store recurring solution patterns discovered during development | Retry strategies, error recovery, async patterns, testing approaches |
107: | `troubleshooting` | Capture issue resolution steps for recurring problems | Yes | Document root causes and fixes for common issues | MCP connection errors, git conflicts, validation failures |
108: | `configuration` | Setup notes and configuration guidelines | Yes | Record framework setup decisions and best practices | Serena project activation, hook configuration, path conventions |
109: | `documentation` | Cache frequently-accessed library documentation | Yes | Store Context7-fetched docs for quick offline access | Next.js routing, React patterns, Python asyncio guides |
110: | `note` | Record session insights, handoffs, and observations | Conditional | Preserve context between sessions; project-specific when filled | Session end state, discovered gotchas, optimization insights |
111: 
112: **Initialization Strategy**: When activating Serena for a new CE project, create template memories for architecture, pattern, troubleshooting, and configuration types with framework-level guidance. Let projects accumulate documentation and note types organically during development.
113: 
114: #### Existing Project Memories
115: 
116: Current knowledge base in `.serena/memories/` (23 files, ~3,719 lines):
117: 
118: **Universal Memories (IsWorkflow = Yes)** - Suitable for copying to new projects:
119: 
120: | Memory | Type | Purpose | Lines |
121: |--------|------|---------|-------|
122: | [code-style-conventions.md](.serena/memories/code-style-conventions.md) | pattern | Coding principles: KISS, no fishy fallbacks, mock marking, function/file size limits | 129 |
123: | [suggested-commands.md](.serena/memories/suggested-commands.md) | documentation | Common commands reference (UV, pytest, CE tools, Darwin) | 98 |
124: | [task-completion-checklist.md](.serena/memories/task-completion-checklist.md) | documentation | Pre-commit verification checklist with all quality gates | 80 |
125: | [testing-standards.md](.serena/memories/testing-standards.md) | pattern | Testing philosophy: real functionality, no mocks, TDD approach | 87 |
126: | [tool-usage-syntropy.md](.serena/memories/tool-usage-syntropy.md) | documentation | Comprehensive Syntropy tool selection guide with decision trees | 425 |
127: | [use-syntropy-tools-not-bash.md](.serena/memories/use-syntropy-tools-not-bash.md) | pattern | Core principle & migration patterns: prefer Syntropy over bash | 200 |
128: 
129: **Project-Specific Memories (IsWorkflow = No)** - Ctx-eng-plus custom knowledge:
130: 
131: | Memory | Type | Purpose | Lines |
132: |--------|------|---------|-------|
133: | [codebase-structure.md](.serena/memories/codebase-structure.md) | architecture | Complete directory layout and module organization | 196 |
134: | [cwe78-prp22-newline-escape-issue.md](.serena/memories/cwe78-prp22-newline-escape-issue.md) | troubleshooting | Security issue with Serena regex replacement and workaround | 100 |
135: | [l4-validation-usage.md](.serena/memories/l4-validation-usage.md) | pattern | L4 validation system usage, modules, and drift thresholds | 150 |
136: | [linear-issue-creation-pattern.md](.serena/memories/linear-issue-creation-pattern.md) | pattern | Working example for Linear issue creation with PRP metadata | 69 |
137: | [linear-issue-tracking-integration.md](.serena/memories/linear-issue-tracking-integration.md) | pattern | Bi-directional Linear/PRP integration workflow | 213 |
138: | [linear-mcp-integration-example.md](.serena/memories/linear-mcp-integration-example.md) | pattern | Linear MCP integration with configuration defaults | 101 |
139: | [linear-mcp-integration.md](.serena/memories/linear-mcp-integration.md) | documentation | Complete Linear MCP tool reference (20+ tools) | 114 |
140: | [project-overview.md](.serena/memories/project-overview.md) | documentation | Master project documentation with tech stack & features | 188 |
141: | [PRP-15-remediation-workflow-implementation.md](.serena/memories/PRP-15-remediation-workflow-implementation.md) | documentation | Implementation record for PRP-15 remediation workflow | 206 |
142: | [prp-2-implementation-patterns.md](.serena/memories/prp-2-implementation-patterns.md) | pattern | State management patterns and atomic write practices | 330 |
143: | [prp-backlog-system.md](.serena/memories/prp-backlog-system.md) | configuration | PRP backlog directory system and workflow | 106 |
144: | [prp-structure-initialized.md](.serena/memories/prp-structure-initialized.md) | documentation | PRP structure initialization completion record | 80 |
145: | [serena-implementation-verification-pattern.md](.serena/memories/serena-implementation-verification-pattern.md) | pattern | Pattern for verifying PRP implementations with Serena symbol lookup | 139 |
146: | [serena-mcp-tool-restrictions.md](.serena/memories/serena-mcp-tool-restrictions.md) | configuration | Current tool restrictions, allowed tools, and workarounds | 236 |
147: | [syntropy-status-hook-pattern.md](.serena/memories/syntropy-status-hook-pattern.md) | pattern | Cache-based architecture for SessionStart hook MCP access | 177 |
148: | [system-model-specification.md](.serena/memories/system-model-specification.md) | documentation | Formal specification of Context Engineering target architecture | 157 |
149: | [tool-config-optimization-completed.md](.serena/memories/tool-config-optimization-completed.md) | documentation | Completion record for tool config optimization (7 violations resolved) | 63 |
150: 
151: **Summary**: 23 framework memories with YAML type headers (CE 1.1)
152: - 6 critical memory candidates (type: regular by default, upgrade to type: critical during initialization)
153: - 17 project-specific memories (ctx-eng-plus custom knowledge)
154: - See `.serena/memories/README.md` for complete memory type system documentation
155: 
156: **Storage**: `.serena/memories/` (created automatically by Serena MCP)
157: 
158: **Related Documentation**:
159: - [Tool Usage Guide](TOOL-USAGE-GUIDE.md) - Native-first tool selection philosophy
160: - [Initialization Guide](INITIALIZATION.md) - Framework initialization and memory setup
161: 
162: ### Slash Commands & CLI Tools
163: 
164: Workflow automation via slash commands and CLI tools:
165: 
166: | Command | Type | Description |
167: |---------|------|-------------|
168: | `/batch-gen-prp` | Slash Command | Generate multiple PRPs from plan with dependency analysis (see `.claude/commands/batch-gen-prp.md`) |
169: | `/batch-exe-prp` | Slash Command | Execute PRPs in parallel stages with health monitoring (see `.claude/commands/batch-exe-prp.md`) |
170: | `/vacuum` | Slash Command | Identify and remove project noise with confidence-based deletion (see `.claude/commands/vacuum.md`) |
171: | `/denoise` | Slash Command | Compress verbose documentation with AI-powered denoising (see `.claude/commands/denoise.md`) |
172: | `/analyze-context` | Slash Command | Fast drift score check without full validation (see `.claude/commands/analyze-context.md`) |
173: | `ce context health` | CLI Tool | Context health check with drift analysis (see `tools/ce/context.py`) |
174: | `ce validate --level 4` | CLI Tool | Full validation suite with L1-L4 checks (see `tools/ce/validate.py`) |
175: | `ce vacuum` | CLI Tool | Vacuum cleanup with execute/auto modes (see `tools/ce/` CLI) |
176: 
177: **Total**: 5 slash commands + 3 CLI tools
178: 
179: **Documentation**: All slash commands documented in `.claude/commands/`, CLI tools documented in `tools/README.md`
180: 
181: ### Configuration
182: 
183: Commands and hooks:
184: 
185: | Example | Lines | Focus |
186: |---------|-------|-------|
187: | [Hook Configuration](config/hook-configuration.md) | 649 | Lifecycle hooks (pre-commit, session-start) |
188: | [Slash Command Template](config/slash-command-template.md) | 622 | Custom command creation |
189: 
190: **Total**: 2 examples, 1,271 lines
191: 
192: ### Patterns
193: 
194: Reusable patterns and practices:
195: 
196: | Example | Lines | Focus |
197: |---------|-------|-------|
198: | [Dedrifting Lessons](patterns/dedrifting-lessons.md) | 241 | Context drift prevention |
199: | [Git Message Rules](patterns/git-message-rules.md) | 205 | Commit message conventions |
200: | [Example Simple Feature](patterns/example-simple-feature.md) | 182 | Complete PRP example |
201: | [Mock Marking](patterns/mocks-marking.md) | 96 | Test mock tracking |
202: 
203: **Total**: 4 examples, 724 lines
204: 
205: ### Guides
206: 
207: Comprehensive guides:
208: 
209: | Example | Lines | Focus |
210: |---------|-------|-------|
211: | [Tool Usage Guide](TOOL-USAGE-GUIDE.md) | 606 | Native-first tool selection philosophy |
212: | [PRP Decomposition Patterns](prp-decomposition-patterns.md) | 357 | Breaking down large features |
213: 
214: **Total**: 2 examples, 963 lines
215: 
216: ### Reference
217: 
218: Quick reference materials:
219: 
220: | Example | Lines | Focus |
221: |---------|-------|-------|
222: | [Mermaid Color Palette](mermaid-color-palette.md) | 313 | Diagram color standards |
223: | [L4 Validation Example](l4-validation-example.md) | 290 | Pattern conformance validation |
224: | [Linear Integration Example](linear-integration-example.md) | 204 | Legacy Linear example |
225: | [Syntropy Status Hook](syntropy-status-hook-system.md) | 149 | MCP health check |
226: | [tmp/ Convention](tmp-directory-convention.md) | 130 | Temp file standards |
227: | [Settings Local Example](example.setting.local.md) | 17 | Configuration example |
228: 
229: **Total**: 6 examples, 1,103 lines
230: 
231: ### Model
232: 
233: System architecture:
234: 
235: | Example | Lines | Focus |
236: |---------|-------|-------|
237: | [System Model](model/SystemModel.md) | 2,981 | Complete framework architecture |
238: 
239: **Total**: 1 example, 2,981 lines
240: 
241: ## IsWorkflow Distribution
242: 
243: ### Examples
244: 
245: - **Yes** (Universal/Framework): 21 examples (84%)
246: - **No** (Project-Specific): 4 examples (16%)
247: 
248: ### Serena Memories
249: 
250: - **Yes** (Universal/Framework): 6 memories (1,013 lines, 28%) - Suitable for all CE projects
251: - **No** (Project-Specific): 17 memories (2,608 lines, 72%) - Ctx-eng-plus custom knowledge
252: 
253: ### Classification Legend
254: 
255: **IsWorkflow = Yes**: Universal CE framework documentation that should be copied to any target project during initialization. Includes MCP patterns, generic workflows, framework config templates, reusable practices, and essential coding/testing standards.
256: 
257: **IsWorkflow = No**: Project-specific documentation tied to ctx-eng-plus codebase, conventions, or implementation details. Not suitable for general distribution to other projects.
258: 
259: ### Project-Specific Examples (No)
260: 
261: 1. **Example Simple Feature** (patterns/) - Demonstrates adding git status summary command specific to ctx-eng-plus
262: 2. **Git Message Rules** (patterns/) - Commit message conventions specific to this project
263: 3. **L4 Validation Example** (reference/) - Validation patterns specific to ctx-eng-plus infrastructure
264: 4. **Syntropy Status Hook** (reference/) - References ctx-eng-plus-specific scripts (scripts/session-startup.sh)
265: 
266: ### Universal Serena Memories to Copy to New Projects
267: 
268: 1. **code-style-conventions** (pattern) - Coding principles and standards
269: 2. **suggested-commands** (documentation) - Common command reference
270: 3. **task-completion-checklist** (documentation) - Quality gates verification
271: 4. **testing-standards** (pattern) - Testing philosophy and practices
272: 5. **tool-usage-syntropy** (documentation) - Syntropy tool selection guide
273: 6. **use-syntropy-tools-not-bash** (pattern) - Migration patterns and principles
274: 
275: ## Syntropy Integration
276: 
277: **Examples using Syntropy MCP**: 9/25 (36%)
278: 
279: - **Heavy usage** (20+ references): Serena Symbol Search (58), Linear Integration (30), Context7 Docs (29), Tool Usage Guide (34), Memory Management (34), Syntropy README (navigation hub)
280: - **Moderate usage** (5-20 references): Thinking Sequential (17), System Model (2)
281: - **Light usage** (1-5 references): Syntropy Status Hook (1), Slash Command Template (1)
282: 
283: ## Usage Patterns
284: 
285: ### By Use Case
286: 
287: **Starting with Context Engineering**:
288: 
289: 1. [System Model](model/SystemModel.md) - Understand framework architecture
290: 2. [Tool Usage Guide](TOOL-USAGE-GUIDE.md) - Learn tool selection
291: 3. [Example Simple Feature](patterns/example-simple-feature.md) - See complete PRP
292: 4. [Execute PRP workflow](workflows/batch-prp-execution.md) - Implement PRPs
293: 
294: **Learning Syntropy MCP**:
295: 
296: 1. [Syntropy README](syntropy/README.md) - Master overview, decision matrix, tool naming
297: 2. [Serena Symbol Search](syntropy/serena-symbol-search.md) - Code navigation and refactoring
298: 3. [Context7 Docs Fetch](syntropy/context7-docs-fetch.md) - Library documentation fetching
299: 4. [Linear Integration](syntropy/linear-integration.md) - Issue tracking and project management
300: 
301: **Maintaining Project Health**:
302: 
303: 1. [Context Drift Remediation](workflows/context-drift-remediation.md) - Sync PRPs
304: 2. [Vacuum Cleanup](workflows/vacuum-cleanup.md) - Remove noise
305: 3. [Denoise Documents](workflows/denoise-documents.md) - Compress docs
306: 4. [Dedrifting Lessons](patterns/dedrifting-lessons.md) - Prevention strategies
307: 
308: **Batch Operations**:
309: 
310: 1. [Batch PRP Generation](workflows/batch-prp-generation.md) - Generate from plan
311: 2. [Batch PRP Execution](workflows/batch-prp-execution.md) - Execute in parallel
312: 3. [PRP Decomposition Patterns](prp-decomposition-patterns.md) - Break down features
313: 
314: **Configuration**:
315: 
316: 1. [Slash Command Template](config/slash-command-template.md) - Create commands
317: 2. [Hook Configuration](config/hook-configuration.md) - Lifecycle hooks
318: 3. [Settings Local Example](example.setting.local.md) - Configuration format
319: 
320: ## Maintenance
321: 
322: **Updating this index**:
323: 
324: When adding new examples:
325: 
326: 1. Create example file following content template (150-300 lines)
327: 2. Add entry to appropriate category section above
328: 3. Update statistics
329: 4. Commit: `git add examples/ && git commit -m "Examples: Added [name]"`
330: 
331: **Content template**:
332: 
333: - **Purpose**: What this example demonstrates, when to use
334: - **Prerequisites**: Required setup
335: - **Examples**: 3-4 concrete examples with input/output
336: - **Common Patterns**: 3-5 patterns
337: - **Anti-Patterns**: 2-3 things not to do
338: - **Related**: Links to related examples
339: 
340: ## Related Documentation
341: 
342: - [CLAUDE.md](../CLAUDE.md) - Project guide and quick commands
343: - [PRPs/](../PRPs/) - Executed and feature request PRPs
344: - [.claude/commands/](../.claude/commands/) - Slash commands (11 framework commands including peer-review)
345: 
346: ## Contributing
347: 
348: To add new examples:
349: 
350: 1. Follow [content template](syntropy/README.md#content-template) structure
351: 2. Add to appropriate directory (syntropy/, workflows/, config/, patterns/)
352: 3. Update INDEX.md with new entry
353: 4. Cross-link with related examples
354: 5. Run validation: `cd tools && uv run ce validate --level 4`
</file>

<file path="tools/ce/blending/strategies/claude_md.py">
  1: """CLAUDE.md blending strategy with Sonnet-based section merging.
  2: 
  3: Philosophy: Copy framework sections (authoritative) + import target [PROJECT]
  4: sections where non-contradictory. RULES.md enforced as invariants.
  5: """
  6: 
  7: import logging
  8: from typing import Dict, List, Any, Optional
  9: 
 10: from .base import BlendStrategy
 11: 
 12: logger = logging.getLogger(__name__)
 13: 
 14: # Section categories
 15: FRAMEWORK_SECTIONS = {
 16:     "Core Principles", "Framework Initialization", "Tool Naming Convention",
 17:     "Testing Standards", "Code Quality", "Context Commands",
 18:     "Batch PRP Generation", "PRP Sizing", "Testing Patterns",
 19:     "Documentation Standards", "Efficient Doc Review", "Git Worktree"
 20: }
 21: 
 22: HYBRID_SECTIONS = {
 23:     "Quick Commands", "Allowed Tools Summary", "Command Permissions",
 24:     "Resources", "Troubleshooting"
 25: }
 26: 
 27: 
 28: class ClaudeMdBlendStrategy(BlendStrategy):
 29:     """
 30:     Blend framework and target CLAUDE.md files with Sonnet-based merging.
 31: 
 32:     Uses H2 section parsing to categorize content:
 33:     - Framework sections: Copied as-is (authoritative)
 34:     - Hybrid sections: Sonnet merge (framework + target)
 35:     - Target-only sections: Imported if non-contradictory
 36: 
 37:     Usage:
 38:         >>> strategy = ClaudeMdBlendStrategy()
 39:         >>> blended = strategy.blend(framework_md, target_md, context)
 40:     """
 41: 
 42:     def can_handle(self, domain: str) -> bool:
 43:         """
 44:         Return True for 'claude_md' domain.
 45: 
 46:         Args:
 47:             domain: Domain identifier (e.g., 'claude_md', 'settings', 'commands')
 48: 
 49:         Returns:
 50:             True if domain == 'claude_md', False otherwise
 51:         """
 52:         return domain == "claude_md"
 53: 
 54:     def parse_sections(self, content: str) -> Dict[str, str]:
 55:         """
 56:         Split CLAUDE.md into H2 sections.
 57: 
 58:         Args:
 59:             content: CLAUDE.md file content
 60: 
 61:         Returns:
 62:             Dict mapping section names to section content (including H2 header)
 63:         """
 64:         if not content.strip():
 65:             return {}
 66: 
 67:         sections = {}
 68:         current_section = None
 69:         current_lines = []
 70: 
 71:         for line in content.split('\n'):
 72:             if line.startswith('## '):
 73:                 # Save previous section
 74:                 if current_section:
 75:                     sections[current_section] = '\n'.join(current_lines).strip()
 76: 
 77:                 # Start new section
 78:                 current_section = line[3:].strip()
 79:                 current_lines = [line]
 80:             else:
 81:                 current_lines.append(line)
 82: 
 83:         # Save last section
 84:         if current_section:
 85:             sections[current_section] = '\n'.join(current_lines).strip()
 86: 
 87:         return sections
 88: 
 89:     def categorize_section(self, section_name: str) -> str:
 90:         """
 91:         Categorize section as framework, hybrid, or project.
 92: 
 93:         Args:
 94:             section_name: Section name (from H2 header)
 95: 
 96:         Returns:
 97:             "framework", "hybrid", or "project"
 98:         """
 99:         if section_name in FRAMEWORK_SECTIONS:
100:             return "framework"
101:         elif section_name in HYBRID_SECTIONS:
102:             return "hybrid"
103:         else:
104:             return "project"
105: 
106:     def blend(
107:         self,
108:         framework_content: str,
109:         target_content: Optional[str],
110:         context: Dict[str, Any]
111:     ) -> str:
112:         """
113:         Blend framework and target CLAUDE.md files.
114: 
115:         Algorithm:
116:         1. Parse both into H2 sections
117:         2. Framework sections ‚Üí Copy as-is (authoritative)
118:         3. Hybrid sections ‚Üí Sonnet merge (framework + target)
119:         4. Target-only sections ‚Üí Import if non-contradictory
120:         5. Reassemble in logical order
121: 
122:         Args:
123:             framework_content: Framework CLAUDE.md content
124:             target_content: Target project CLAUDE.md content (may be empty)
125:             context: Dict with llm_client and optional rules_content
126: 
127:         Returns:
128:             Blended CLAUDE.md content
129: 
130:         Raises:
131:             ValueError: If LLM client not provided or blending fails
132:         """
133:         if not context or "llm_client" not in context:
134:             raise ValueError(
135:                 "LLM client required in context\n"
136:                 "üîß Troubleshooting:\n"
137:                 "  1. Pass context={'llm_client': BlendingLLM()} to blend()\n"
138:                 "  2. See PRP-34.2.6 for BlendingLLM setup"
139:             )
140: 
141:         llm = context["llm_client"]
142:         rules = context.get("rules_content", "")
143: 
144:         logger.info("Blending CLAUDE.md with Sonnet...")
145: 
146:         # Parse sections
147:         framework_sections = self.parse_sections(framework_content)
148:         target_sections = self.parse_sections(target_content) if target_content and target_content.strip() else {}
149: 
150:         blended_sections = {}
151: 
152:         # 1. Framework sections - copy as-is (authoritative)
153:         for section_name in FRAMEWORK_SECTIONS:
154:             if section_name in framework_sections:
155:                 blended_sections[section_name] = framework_sections[section_name]
156:                 logger.debug(f"  Framework section: {section_name} (copied)")
157: 
158:         # 2. Hybrid sections - Sonnet merge
159:         for section_name in HYBRID_SECTIONS:
160:             if section_name in framework_sections:
161:                 target_section = target_sections.get(section_name, "")
162: 
163:                 # Build merge prompt
164:                 logger.debug(f"  Hybrid section: {section_name} (merging)")
165:                 result = llm.blend_content(
166:                     framework_content=framework_sections[section_name],
167:                     target_content=target_section,
168:                     rules_content=rules,
169:                     domain=f"claude_md_{section_name.replace(' ', '_').lower()}"
170:                 )
171: 
172:                 blended_sections[section_name] = result["blended"]
173: 
174:         # 3. Target-only sections - import if project-specific
175:         for section_name in target_sections:
176:             if section_name not in framework_sections:
177:                 category = self.categorize_section(section_name)
178: 
179:                 if category == "project":
180:                     # Project-specific section - import as-is
181:                     blended_sections[section_name] = target_sections[section_name]
182:                     logger.debug(f"  Target section: {section_name} (imported)")
183:                 else:
184:                     # Unknown section - validate with Sonnet
185:                     logger.debug(f"  Target section: {section_name} (validating)")
186:                     result = llm.blend_content(
187:                         framework_content=rules if rules else "# Framework Rules\n\nNo rules provided.",
188:                         target_content=target_sections[section_name],
189:                         rules_content=rules,
190:                         domain=f"validate_{section_name.replace(' ', '_').lower()}"
191:                     )
192: 
193:                     # Import if high confidence (non-contradictory)
194:                     if result["confidence"] > 0.7:
195:                         blended_sections[section_name] = target_sections[section_name]
196:                         logger.debug(f"    ‚Üí Imported (confidence: {result['confidence']:.2f})")
197:                     else:
198:                         logger.warning(f"    ‚Üí Skipped (confidence: {result['confidence']:.2f})")
199: 
200:         # 4. Reassemble in logical order
201:         return self._reassemble_sections(blended_sections, framework_sections)
202: 
203:     def _reassemble_sections(
204:         self,
205:         blended_sections: Dict[str, str],
206:         framework_sections: Dict[str, str]
207:     ) -> str:
208:         """
209:         Reassemble sections in logical order.
210: 
211:         Order:
212:         1. Framework sections (in original order)
213:         2. Hybrid sections (in original order)
214:         3. Project sections (alphabetical)
215: 
216:         Args:
217:             blended_sections: Dict of section_name ‚Üí blended_content
218:             framework_sections: Original framework sections (for ordering)
219: 
220:         Returns:
221:             Reassembled CLAUDE.md content
222:         """
223:         output_lines = []
224: 
225:         # Get framework section order
226:         framework_order = list(framework_sections.keys())
227: 
228:         # Add sections in order
229:         for section_name in framework_order:
230:             if section_name in blended_sections:
231:                 output_lines.append(blended_sections[section_name])
232:                 output_lines.append("")  # Blank line between sections
233: 
234:         # Add remaining sections (alphabetically)
235:         remaining = sorted(set(blended_sections.keys()) - set(framework_order))
236:         for section_name in remaining:
237:             output_lines.append(blended_sections[section_name])
238:             output_lines.append("")
239: 
240:         # Remove trailing blank lines
241:         while output_lines and not output_lines[-1].strip():
242:             output_lines.pop()
243: 
244:         return '\n'.join(output_lines)
245: 
246:     def validate(self, blended_content: str, context: Dict[str, Any]) -> bool:
247:         """
248:         Validate blended CLAUDE.md.
249: 
250:         Checks:
251:         1. Valid markdown (has H2 sections)
252:         2. Contains required framework sections
253:         3. No duplicate H2 section names
254: 
255:         Args:
256:             blended_content: Blended CLAUDE.md content
257:             context: Additional context (not used currently)
258: 
259:         Returns:
260:             True if valid
261: 
262:         Raises:
263:             ValueError: If validation fails
264:         """
265:         if not blended_content.strip():
266:             raise ValueError("Blended content is empty\nüîß Troubleshooting: Check inputs and system state")
267: 
268:         # Parse sections
269:         sections = self.parse_sections(blended_content)
270: 
271:         if not sections:
272:             raise ValueError("No H2 sections found in blended content\nüîß Troubleshooting: Check inputs and system state")
273: 
274:         # Check required framework sections present
275:         required = {"Core Principles", "Framework Initialization", "Testing Standards"}
276:         missing = required - set(sections.keys())
277:         if missing:
278:             raise ValueError(
279:                 f"Missing required framework sections: {missing}\n"
280:                 f"üîß Troubleshooting:\n"
281:                 f"  1. Check framework CLAUDE.md has these sections\n"
282:                 f"  2. Verify blend() preserves framework sections"
283:             )
284: 
285:         # Check no duplicate section names
286:         section_names = [
287:             line[3:].strip()
288:             for line in blended_content.split('\n')
289:             if line.startswith('## ')
290:         ]
291: 
292:         duplicates = [s for s in section_names if section_names.count(s) > 1]
293:         if duplicates:
294:             raise ValueError(
295:                 f"Duplicate H2 sections found: {set(duplicates)}\n"
296:                 f"üîß Troubleshooting:\n"
297:                 f"  1. Check target CLAUDE.md for duplicate sections\n"
298:                 f"  2. Verify blend() deduplicates correctly"
299:             )
300: 
301:         logger.info(f"‚úì Validated CLAUDE.md ({len(sections)} sections)")
302:         return True
</file>

<file path="tools/ce/blending/strategies/settings.py">
  1: """Settings blending strategy for .claude/settings.local.json files.
  2: 
  3: Implements 3-rule blending logic from PRP-33:
  4: 1. CE deny removes from target allow
  5: 2. Merge CE entries to target lists (dedupe)
  6: 3. Ensure tool appears in ONE list only
  7: """
  8: 
  9: import json
 10: from typing import Dict, List, Set, Any, Optional
 11: 
 12: from .base import BlendStrategy
 13: 
 14: 
 15: class SettingsBlendStrategy(BlendStrategy):
 16:     """Blend CE and target settings.local.json files.
 17: 
 18:     Philosophy: Copy ours (CE settings) + import target permissions where not contradictory.
 19: 
 20:     Blending Rules:
 21:     1. CE Deny Precedence: Target allow entries in CE deny ‚Üí Remove from target allow
 22:     2. List Merging: CE entries ‚Üí Add to target's respective lists (deduplicate)
 23:     3. Single Membership: CE entries ‚Üí Ensure not in other lists (CE list takes precedence)
 24:     """
 25: 
 26:     def can_handle(self, domain: str) -> bool:
 27:         """Return True for 'settings' domain.
 28: 
 29:         Args:
 30:             domain: Domain identifier (e.g., 'settings', 'claude-md', 'commands')
 31: 
 32:         Returns:
 33:             True if domain == 'settings', False otherwise
 34:         """
 35:         return domain == "settings"
 36: 
 37:     def blend(
 38:         self,
 39:         framework_content: Any,
 40:         target_content: Optional[Any],
 41:         context: Dict[str, Any]
 42:     ) -> Any:
 43:         """Blend CE and target settings with 3-rule logic.
 44: 
 45:         Args:
 46:             framework_content: CE settings (dict or JSON string)
 47:             target_content: Target project settings (dict, JSON string, or None)
 48:             context: Additional context (unused for settings)
 49: 
 50:         Returns:
 51:             Blended settings as dict
 52: 
 53:         Raises:
 54:             ValueError: If JSON parsing fails or settings invalid
 55:             RuntimeError: If blending logic fails
 56:         """
 57:         # Parse CE settings
 58:         if isinstance(framework_content, str):
 59:             try:
 60:                 ce_settings = json.loads(framework_content)
 61:             except json.JSONDecodeError as e:
 62:                 raise ValueError(f"CE settings JSON invalid: {e}\nüîß Troubleshooting: Check inputs and system state")
 63:         elif isinstance(framework_content, dict):
 64:             ce_settings = framework_content
 65:         else:
 66:             raise ValueError(f"CE settings must be dict or JSON string, got {type(framework_content)}\nüîß Troubleshooting: Check inputs and system state")
 67: 
 68:         # Parse target settings
 69:         if target_content is None or target_content == "":
 70:             target_settings = {"allow": [], "deny": [], "ask": []}
 71:         elif isinstance(target_content, str):
 72:             if not target_content.strip():
 73:                 target_settings = {"allow": [], "deny": [], "ask": []}
 74:             else:
 75:                 try:
 76:                     target_settings = json.loads(target_content)
 77:                 except json.JSONDecodeError as e:
 78:                     raise ValueError(f"Target settings JSON invalid: {e}\nüîß Troubleshooting: Check inputs and system state")
 79:         elif isinstance(target_content, dict):
 80:             target_settings = target_content
 81:         else:
 82:             raise ValueError(f"Target settings must be dict or JSON string, got {type(target_content)}\nüîß Troubleshooting: Check inputs and system state")
 83: 
 84:         # Initialize default structure if missing
 85:         for list_name in ["allow", "deny", "ask"]:
 86:             if list_name not in target_settings:
 87:                 target_settings[list_name] = []
 88:             if list_name not in ce_settings:
 89:                 ce_settings[list_name] = []
 90: 
 91:         # Rule 1: Remove from target's allow list entries in CE's deny list
 92:         target_settings["allow"] = [
 93:             entry for entry in target_settings["allow"]
 94:             if entry not in ce_settings["deny"]
 95:         ]
 96: 
 97:         # Rule 2: Add CE entries to target's respective lists (deduplicate)
 98:         for list_name in ["allow", "deny", "ask"]:
 99:             merged = set(target_settings[list_name]) | set(ce_settings[list_name])
100:             target_settings[list_name] = sorted(merged)
101: 
102:         # Rule 3: Ensure CE entries only appear in one list
103:         for list_name in ["allow", "deny", "ask"]:
104:             other_lists = [l for l in ["allow", "deny", "ask"] if l != list_name]
105:             for entry in ce_settings[list_name]:
106:                 for other_list in other_lists:
107:                     if entry in target_settings[other_list]:
108:                         target_settings[other_list].remove(entry)
109: 
110:         return target_settings
111: 
112:     def validate(self, blended_content: Any, context: Dict[str, Any]) -> bool:
113:         """Validate blended settings.
114: 
115:         Checks:
116:         1. Contains allow, deny, ask lists
117:         2. All lists are actually lists
118:         3. No duplicates across lists
119: 
120:         Args:
121:             blended_content: Blended settings (dict or JSON string)
122:             context: Additional context (unused for settings)
123: 
124:         Returns:
125:             True if valid
126: 
127:         Raises:
128:             ValueError: If validation fails
129:         """
130:         # Parse if string
131:         if isinstance(blended_content, str):
132:             try:
133:                 settings = json.loads(blended_content)
134:             except json.JSONDecodeError as e:
135:                 raise ValueError(f"Blended settings JSON invalid: {e}\nüîß Troubleshooting: Check inputs and system state")
136:         elif isinstance(blended_content, dict):
137:             settings = blended_content
138:         else:
139:             raise ValueError(f"Blended content must be dict or JSON string, got {type(blended_content)}\nüîß Troubleshooting: Check inputs and system state")
140: 
141:         # Check structure
142:         for list_name in ["allow", "deny", "ask"]:
143:             if list_name not in settings:
144:                 raise ValueError(f"Missing '{list_name}' list in blended settings\nüîß Troubleshooting: Check inputs and system state")
145:             if not isinstance(settings[list_name], list):
146:                 raise ValueError(f"'{list_name}' must be a list, got {type(settings[list_name])}\nüîß Troubleshooting: Check inputs and system state")
147: 
148:         # Check no duplicates across lists
149:         all_entries = (
150:             settings["allow"] + settings["deny"] + settings["ask"]
151:         )
152:         duplicates = [entry for entry in all_entries if all_entries.count(entry) > 1]
153:         if duplicates:
154:             raise ValueError(f"Duplicate entries across lists: {set(duplicates)}\nüîß Troubleshooting: Check inputs and system state")
155: 
156:         return True
</file>

<file path="tools/ce/blending/detection.py">
  1: """Legacy file detection for CE initialization.
  2: 
  3: Scans multiple legacy locations for CE framework files, handles symlinks,
  4: and filters garbage files. Uses config-driven path resolution.
  5: """
  6: 
  7: from pathlib import Path
  8: from typing import Dict, List, Set, Optional, Any
  9: import logging
 10: 
 11: logger = logging.getLogger(__name__)
 12: 
 13: # Garbage filter patterns
 14: GARBAGE_PATTERNS = [
 15:     "REPORT", "INITIAL", "summary", "analysis",
 16:     "PLAN", ".backup", "~", ".tmp", ".log"
 17: ]
 18: 
 19: 
 20: class LegacyFileDetector:
 21:     """Detect legacy CE files across multiple locations.
 22: 
 23:     Scans PRPs/, examples/, context-engineering/, .serena/, and .claude/
 24:     directories for CE framework files. Includes smart filtering to exclude
 25:     garbage files and proper symlink handling. Uses config-driven search patterns.
 26: 
 27:     Example:
 28:         >>> from ce.config_loader import BlendConfig
 29:         >>> config = BlendConfig(Path(".ce/blend-config.yml"))
 30:         >>> detector = LegacyFileDetector(Path("/project/root"), config)
 31:         >>> inventory = detector.scan_all()
 32:         >>> print(f"Found {len(inventory['prps'])} PRPs")
 33:     """
 34: 
 35:     def __init__(self, project_root: Path, config: Optional[Any] = None):
 36:         """Initialize detector with project root and config.
 37: 
 38:         Args:
 39:             project_root: Path to project root directory
 40:             config: BlendConfig instance with directory paths (optional for backward compatibility)
 41:         """
 42:         self.project_root = Path(project_root).resolve()
 43:         self.config = config
 44:         self.visited_symlinks: Set[Path] = set()
 45: 
 46:     def scan_all(self) -> Dict[str, List[Path]]:
 47:         """Scan all domains and return inventory.
 48: 
 49:         Uses config-driven search patterns if config available, otherwise uses
 50:         sensible defaults for backward compatibility.
 51: 
 52:         Returns:
 53:             Dict with keys: prps, examples, claude_md, settings, commands, memories
 54:             Each value is List[Path] of detected files
 55: 
 56:         Example:
 57:             >>> from ce.config_loader import BlendConfig
 58:             >>> config = BlendConfig(Path(".ce/blend-config.yml"))
 59:             >>> detector = LegacyFileDetector(Path("/project/root"), config)
 60:             >>> inventory = detector.scan_all()
 61:             >>> inventory.keys()
 62:             dict_keys(['prps', 'examples', 'claude_md', 'settings', 'commands', 'memories'])
 63:         """
 64:         # Domains to scan
 65:         domains = ["prps", "examples", "claude_md", "settings", "commands", "memories"]
 66:         inventory = {domain: [] for domain in domains}
 67: 
 68:         for domain in domains:
 69:             patterns = self._get_domain_search_paths(domain)
 70: 
 71:             for pattern in patterns:
 72:                 search_path = self.project_root / pattern
 73: 
 74:                 if not search_path.exists():
 75:                     continue
 76: 
 77:                 if search_path.is_file():
 78:                     # Single file (e.g., CLAUDE.md)
 79:                     resolved = self._resolve_symlink(search_path)
 80:                     if resolved and not self._is_garbage(resolved):
 81:                         inventory[domain].append(resolved)
 82:                 else:
 83:                     # Directory - collect .md files
 84:                     files = self._collect_files(search_path)
 85:                     inventory[domain].extend(files)
 86: 
 87:         return inventory
 88: 
 89:     def _get_domain_search_paths(self, domain: str) -> List[Path]:
 90:         """Get search paths for domain from config or defaults.
 91: 
 92:         Args:
 93:             domain: Domain name (prps, examples, claude_md, settings, commands, memories)
 94: 
 95:         Returns:
 96:             List of Path objects to search for the domain
 97:         """
 98:         # If config available, use domain-specific sources
 99:         if self.config:
100:             try:
101:                 sources = self.config.get_domain_legacy_sources(domain)
102:                 if sources:
103:                     return sources
104:             except (KeyError, ValueError):
105:                 pass
106: 
107:         # Fallback to defaults for backward compatibility
108:         defaults = {
109:             "prps": [Path("PRPs/"), Path("context-engineering/PRPs/")],
110:             "examples": [Path("examples/"), Path("context-engineering/examples/")],
111:             "claude_md": [Path("CLAUDE.md")],
112:             "settings": [Path(".claude/settings.local.json")],
113:             "commands": [Path(".claude/commands/")],
114:             "memories": [Path(".serena/memories/")]
115:         }
116:         return defaults.get(domain, [])
117: 
118:     def _resolve_symlink(self, path: Path) -> Path | None:
119:         """Resolve symlink, detect circular references.
120: 
121:         Args:
122:             path: Path to resolve (may be symlink or regular file)
123: 
124:         Returns:
125:             Resolved path or None if circular/broken
126: 
127:         Example:
128:             >>> resolved = detector._resolve_symlink(Path("CLAUDE.md"))
129:             >>> resolved
130:             Path("/real/path/to/CLAUDE.md")
131:         """
132:         if not path.is_symlink():
133:             return path
134: 
135:         if path in self.visited_symlinks:
136:             logger.warning(f"Circular symlink detected: {path}")
137:             return None
138: 
139:         self.visited_symlinks.add(path)
140: 
141:         try:
142:             resolved = path.resolve(strict=True)
143:             return resolved
144:         except (OSError, RuntimeError) as e:
145:             logger.warning(f"Broken symlink: {path} - {e}")
146:             return None
147: 
148:     def _collect_files(self, directory: Path) -> List[Path]:
149:         """Recursively collect .md files from directory.
150: 
151:         Args:
152:             directory: Path to directory to scan
153: 
154:         Returns:
155:             List of .md file paths (garbage filtered)
156: 
157:         Example:
158:             >>> files = detector._collect_files(Path("PRPs/"))
159:             >>> len(files)
160:             23
161:         """
162:         files = []
163: 
164:         try:
165:             for item in directory.rglob("*.md"):
166:                 if item.is_file() and not self._is_garbage(item):
167:                     resolved = self._resolve_symlink(item)
168:                     if resolved:
169:                         files.append(resolved)
170:         except PermissionError as e:
171:             logger.warning(f"Permission denied: {directory} - {e}")
172: 
173:         return files
174: 
175:     def _is_garbage(self, path: Path) -> bool:
176:         """Check if file matches garbage patterns.
177: 
178:         Args:
179:             path: Path to check
180: 
181:         Returns:
182:             True if file should be filtered out
183: 
184:         Example:
185:             >>> detector._is_garbage(Path("PRP-1-REPORT.md"))
186:             True
187:             >>> detector._is_garbage(Path("PRP-1-feature.md"))
188:             False
189:         """
190:         name = path.name
191:         return any(pattern in name for pattern in GARBAGE_PATTERNS)
</file>

<file path="tools/ce/toml_formats/version_resolver.py">
  1: """Version resolution using packaging.specifiers for version intersection."""
  2: 
  3: from typing import Dict, List
  4: from packaging.specifiers import SpecifierSet, InvalidSpecifier
  5: from packaging.version import Version
  6: 
  7: 
  8: class VersionResolver:
  9:     """
 10:     Resolves version conflicts using intersection strategy.
 11: 
 12:     Key principle: Version intersection, NOT "higher wins".
 13:     Example: Framework >=6.0 + Target ~=5.4 ‚Üí Error (no intersection)
 14:     Example: Framework >=6.0,<7.0 + Target >=6.2 ‚Üí >=6.2,<7.0
 15:     """
 16: 
 17:     @staticmethod
 18:     def parse_dependencies(deps: List[str]) -> Dict[str, SpecifierSet]:
 19:         """
 20:         Parse dependency list into {package: SpecifierSet} dict.
 21: 
 22:         Args:
 23:             deps: List of dependency strings (e.g., ["pyyaml>=6.0", "click"])
 24: 
 25:         Returns:
 26:             Dict mapping package name to SpecifierSet
 27:         """
 28:         result = {}
 29:         for dep in deps:
 30:             # Handle extras: requests[security]>=2.0
 31:             if "[" in dep:
 32:                 package = dep.split("[")[0]
 33:                 version_part = dep.split("]", 1)[1] if "]" in dep else ""
 34:             else:
 35:                 # Split at first operator
 36:                 for op in [">=", "<=", "==", "!=", "~=", ">", "<"]:
 37:                     if op in dep:
 38:                         package, version_part = dep.split(op, 1)
 39:                         version_part = op + version_part
 40:                         break
 41:                 else:
 42:                     # No version specifier
 43:                     package = dep.strip()
 44:                     version_part = ""
 45: 
 46:             package = package.strip()
 47: 
 48:             try:
 49:                 spec = SpecifierSet(version_part) if version_part else SpecifierSet()
 50:                 result[package] = spec
 51:             except InvalidSpecifier:
 52:                 # Invalid specifier ‚Üí store as empty (no constraints)
 53:                 result[package] = SpecifierSet()
 54: 
 55:         return result
 56: 
 57:     @staticmethod
 58:     def _is_intersection_satisfiable(intersection: SpecifierSet) -> bool:
 59:         """
 60:         Check if SpecifierSet intersection is satisfiable by any version.
 61: 
 62:         Args:
 63:             intersection: SpecifierSet resulting from intersection
 64: 
 65:         Returns:
 66:             True if any version can satisfy the specifier, False otherwise
 67:         """
 68:         # Test a comprehensive range of versions to see if any satisfy the intersection
 69:         # This handles cases where SpecifierSet creates invalid combinations like ">=6.0,~=5.4"
 70: 
 71:         # Generate test versions: 0.1 to 10.0 with finer granularity
 72:         test_versions = []
 73: 
 74:         # Major versions 0-10
 75:         for major in range(11):
 76:             test_versions.append(f"{major}.0.0")
 77:             # Minor versions 0-9 for each major
 78:             for minor in range(10):
 79:                 test_versions.append(f"{major}.{minor}.0")
 80:                 # Patch versions for common minors (2, 5, 8)
 81:                 if minor in [2, 5, 8]:
 82:                     for patch in range(5):
 83:                         test_versions.append(f"{major}.{minor}.{patch}")
 84: 
 85:         for version_str in test_versions:
 86:             try:
 87:                 if Version(version_str) in intersection:
 88:                     return True
 89:             except Exception:
 90:                 continue
 91: 
 92:         return False
 93: 
 94:     @staticmethod
 95:     def merge_dependencies(framework_deps: List[str], target_deps: List[str]) -> List[str]:
 96:         """
 97:         Merge two dependency lists with version intersection.
 98: 
 99:         Args:
100:             framework_deps: Framework dependencies
101:             target_deps: Target project dependencies
102: 
103:         Returns:
104:             Merged dependency list with unified versions
105: 
106:         Raises:
107:             ValueError: If version conflict detected (no intersection)
108:         """
109:         framework_map = VersionResolver.parse_dependencies(framework_deps)
110:         target_map = VersionResolver.parse_dependencies(target_deps)
111: 
112:         merged_map = {}
113:         all_packages = set(framework_map.keys()) | set(target_map.keys())
114: 
115:         for package in all_packages:
116:             framework_spec = framework_map.get(package)
117:             target_spec = target_map.get(package)
118: 
119:             if framework_spec and target_spec:
120:                 # Both have version constraints ‚Üí compute intersection
121:                 try:
122:                     intersection = framework_spec & target_spec
123: 
124:                     # Check if intersection is satisfiable
125:                     if not VersionResolver._is_intersection_satisfiable(intersection):
126:                         raise ValueError(
127:                             f"‚ùå Dependency conflict: {package}\n"
128:                             f"   Framework requires: {framework_spec}\n"
129:                             f"   Target requires: {target_spec}\n"
130:                             f"   No compatible version exists.\n"
131:                             f"üîß Resolution:\n"
132:                             f"   1. Update target project to use compatible version\n"
133:                             f"   2. Or update framework dependencies in tools/pyproject.toml"
134:                         )
135:                     merged_map[package] = intersection
136:                 except InvalidSpecifier as e:
137:                     raise ValueError(
138:                         f"‚ùå Invalid version specifier for {package}: {e}\n"
139:                         f"üîß Check dependency syntax in pyproject.toml"
140:                     )
141:             elif framework_spec:
142:                 merged_map[package] = framework_spec
143:             else:
144:                 merged_map[package] = target_spec
145: 
146:         # Convert back to list format
147:         return [
148:             f"{pkg}{spec}" if str(spec) else pkg
149:             for pkg, spec in sorted(merged_map.items())
150:         ]
</file>

<file path="tools/ce/config_loader.py">
  1: """Centralized configuration loader for init-project.
  2: 
  3: Loads blend-config.yml and provides typed access to directory paths.
  4: """
  5: 
  6: from pathlib import Path
  7: from typing import Dict, List, Optional, Any
  8: import yaml
  9: import logging
 10: 
 11: logger = logging.getLogger(__name__)
 12: 
 13: 
 14: class BlendConfig:
 15:     """Configuration container with path resolution.
 16: 
 17:     Provides centralized access to directory locations and blending strategies
 18:     defined in blend-config.yml. Validates config structure and ensures all
 19:     required sections are present.
 20:     """
 21: 
 22:     def __init__(self, config_path: Path):
 23:         """Load and validate configuration.
 24: 
 25:         Args:
 26:             config_path: Path to blend-config.yml
 27: 
 28:         Raises:
 29:             ValueError: If config invalid or missing required fields
 30:             FileNotFoundError: If config file doesn't exist
 31:         """
 32:         config_path = Path(config_path).resolve()
 33: 
 34:         if not config_path.exists():
 35:             raise ValueError(
 36:                 f"Config file not found: {config_path}\n"
 37:                 f"üîß Troubleshooting: Ensure blend-config.yml exists"
 38:             )
 39: 
 40:         try:
 41:             with open(config_path) as f:
 42:                 self._config = yaml.safe_load(f)
 43:         except yaml.YAMLError as e:
 44:             raise ValueError(
 45:                 f"Invalid YAML in config: {e}\n"
 46:                 f"üîß Troubleshooting: Check blend-config.yml syntax"
 47:             )
 48: 
 49:         if self._config is None:
 50:             self._config = {}
 51: 
 52:         # Load directories from separate file if needed (repomix workaround)
 53:         self._load_directories_fallback(config_path)
 54: 
 55:         self._validate()
 56: 
 57:     def _load_directories_fallback(self, config_path: Path) -> None:
 58:         """Load directories from separate file if needed (repomix workaround).
 59: 
 60:         If directories section is None/missing in blend-config.yml, try loading
 61:         from directories.yml in the same directory.
 62:         """
 63:         try:
 64:             # Check if directories section is None
 65:             if self._config.get("directories") is None:
 66:                 directories_path = config_path.parent / "directories.yml"
 67:                 if directories_path.exists():
 68:                     with open(directories_path) as f:
 69:                         directories_data = yaml.safe_load(f)
 70:                     if directories_data:
 71:                         self._config["directories"] = directories_data
 72:         except Exception:
 73:             pass  # If fallback fails, proceed with validation error
 74: 
 75:     def _validate(self) -> None:
 76:         """Validate config structure.
 77: 
 78:         Raises:
 79:             ValueError: If required sections missing
 80:         """
 81:         # If domains is null (repomix issue), it's OK as long as we have directories from fallback
 82:         if (self._config.get("domains") is None and
 83:             (self._config.get("directories") is None or self._config.get("directories") == {})):
 84:             raise ValueError(
 85:                 f"Missing required configuration: both 'domains' and 'directories' are null/empty\n"
 86:                 f"üîß Troubleshooting: Check blend-config.yml - repomix may have corrupted it\n"
 87:                 f"üîß Ensure directories.yml exists and was loaded"
 88:             )
 89: 
 90:         # Validate directories section if present (optional for backward compatibility)
 91:         if "directories" in self._config and self._config["directories"] is not None:
 92:             dir_config = self._config["directories"]
 93:             required_subsections = ["output", "framework", "legacy"]
 94:             for subsection in required_subsections:
 95:                 if subsection not in dir_config:
 96:                     raise ValueError(
 97:                         f"Missing directories.{subsection} in config\n"
 98:                         f"üîß Troubleshooting: Add to blend-config.yml"
 99:                     )
100: 
101:     def get_output_path(self, domain: str) -> Path:
102:         """Get output path for domain.
103: 
104:         Args:
105:             domain: Domain name (settings, memories, examples, prps, claude_dir, claude_md, serena_memories)
106: 
107:         Returns:
108:             Path object for output location (relative to project root)
109: 
110:         Raises:
111:             ValueError: If domain not found in config
112:             KeyError: If directories section missing
113: 
114:         Example:
115:             >>> config.get_output_path("claude_dir")
116:             Path(".claude")
117:         """
118:         if "directories" not in self._config:
119:             raise KeyError(
120:                 "directories section not found in config\n"
121:                 f"üîß Troubleshooting: Add directories section to blend-config.yml"
122:             )
123: 
124:         output_config = self._config["directories"]["output"]
125:         if domain not in output_config:
126:             raise ValueError(
127:                 f"Unknown output domain: {domain}\n"
128:                 f"üîß Troubleshooting: Valid domains: {list(output_config.keys())}"
129:             )
130:         return Path(output_config[domain])
131: 
132:     def get_framework_path(self, domain: str) -> Path:
133:         """Get framework source path for domain.
134: 
135:         Args:
136:             domain: Domain name (serena_memories, examples, prps, commands, settings)
137: 
138:         Returns:
139:             Path object for framework source location
140: 
141:         Raises:
142:             ValueError: If domain not found
143:             KeyError: If directories section missing
144:         """
145:         if "directories" not in self._config:
146:             raise KeyError(
147:                 "directories section not found in config\n"
148:                 f"üîß Troubleshooting: Add directories section to blend-config.yml"
149:             )
150: 
151:         fw_config = self._config["directories"]["framework"]
152:         if domain not in fw_config:
153:             raise ValueError(f"Unknown framework domain: {domain}")
154:         return Path(fw_config[domain])
155: 
156:     def get_legacy_paths(self) -> List[Path]:
157:         """Get all legacy search paths.
158: 
159:         Returns:
160:             List of Path objects for legacy directories to search
161: 
162:         Raises:
163:             KeyError: If directories section missing
164:         """
165:         if "directories" not in self._config:
166:             raise KeyError(
167:                 "directories section not found in config\n"
168:                 f"üîß Troubleshooting: Add directories section to blend-config.yml"
169:             )
170: 
171:         legacy_list = self._config["directories"]["legacy"]
172:         return [Path(p) for p in legacy_list]
173: 
174:     def get_domain_config(self, domain: str) -> Dict[str, Any]:
175:         """Get full domain configuration.
176: 
177:         Args:
178:             domain: Domain name (settings, memories, examples, prps, commands, claude_md)
179: 
180:         Returns:
181:             Dictionary with domain-specific config (strategy, source, etc.)
182:         """
183:         if self._config.get("domains") is None:
184:             return {}  # Return empty dict if domains is null (repomix issue)
185:         return self._config["domains"].get(domain, {})
186: 
187:     def get_domain_legacy_sources(self, domain: str) -> List[Path]:
188:         """Get legacy sources specific to a domain.
189: 
190:         Args:
191:             domain: Domain name
192: 
193:         Returns:
194:             List of Path objects for domain-specific legacy locations
195:         """
196:         domain_config = self.get_domain_config(domain)
197: 
198:         # Get sources from domain config
199:         sources = []
200:         if "legacy_source" in domain_config:
201:             sources.append(Path(domain_config["legacy_source"]))
202:         if "legacy_sources" in domain_config:
203:             sources.extend([Path(s) for s in domain_config["legacy_sources"]])
204: 
205:         return sources
</file>

<file path="tools/ce/repomix_unpack.py">
  1: #!/usr/bin/env python3
  2: """
  3: Repomix XML unpacker - extracts files from repomix-generated XML packages.
  4: 
  5: Since repomix CLI doesn't yet support --unpack, this utility extracts files
  6: from XML packages generated by repomix.
  7: """
  8: 
  9: import argparse
 10: import re
 11: import sys
 12: from pathlib import Path
 13: from typing import List, Tuple
 14: 
 15: 
 16: def parse_repomix_xml(xml_content: str) -> List[Tuple[str, str]]:
 17:     """
 18:     Parse repomix XML and extract (file_path, content) tuples.
 19: 
 20:     Args:
 21:         xml_content: Raw XML content from repomix package
 22: 
 23:     Returns:
 24:         List of (file_path, file_content) tuples
 25:     """
 26:     files = []
 27: 
 28:     # Pattern to match <file path="...">content</file>
 29:     # Using non-greedy match and DOTALL to handle multiline content
 30:     pattern = r'<file path="([^"]+)">\n(.*?)\n</file>'
 31: 
 32:     matches = re.finditer(pattern, xml_content, re.DOTALL)
 33: 
 34:     for match in matches:
 35:         file_path = match.group(1)
 36:         content = match.group(2)
 37: 
 38:         # Remove line number prefix (format: " 1: " or "   123‚Üí")
 39:         # Each line in repomix XML has line numbers
 40:         lines = content.split('\n')
 41:         cleaned_lines = []
 42:         for line in lines:
 43:             # Match line number format: spaces + number + (‚Üí or : + space)
 44:             cleaned = re.sub(r'^\s*\d+[‚Üí:]\s*', '', line)
 45:             cleaned_lines.append(cleaned)
 46: 
 47:         cleaned_content = '\n'.join(cleaned_lines)
 48:         files.append((file_path, cleaned_content))
 49: 
 50:     return files
 51: 
 52: 
 53: def extract_files(xml_path: Path, target_dir: Path, verbose: bool = False) -> int:
 54:     """
 55:     Extract all files from repomix XML to target directory.
 56: 
 57:     Args:
 58:         xml_path: Path to repomix XML file
 59:         target_dir: Directory to extract files to
 60:         verbose: Print extraction progress
 61: 
 62:     Returns:
 63:         Number of files extracted
 64:     """
 65:     if not xml_path.exists():
 66:         print(f"‚ùå Error: XML file not found: {xml_path}", file=sys.stderr)
 67:         return 0
 68: 
 69:     # Read XML content
 70:     if verbose:
 71:         print(f"üì¶ Reading package: {xml_path}")
 72: 
 73:     xml_content = xml_path.read_text()
 74: 
 75:     # Parse files
 76:     files = parse_repomix_xml(xml_content)
 77: 
 78:     if not files:
 79:         print(f"‚ö†Ô∏è  Warning: No files found in package", file=sys.stderr)
 80:         return 0
 81: 
 82:     if verbose:
 83:         print(f"üìã Found {len(files)} files to extract")
 84: 
 85:     # Extract each file
 86:     extracted = 0
 87:     for file_path, content in files:
 88:         # Construct full path
 89:         full_path = target_dir / file_path
 90: 
 91:         # Create parent directories
 92:         full_path.parent.mkdir(parents=True, exist_ok=True)
 93: 
 94:         # Write file
 95:         full_path.write_text(content)
 96: 
 97:         if verbose:
 98:             print(f"   ‚úì {file_path}")
 99: 
100:         extracted += 1
101: 
102:     return extracted
103: 
104: 
105: def main():
106:     """Main entry point for CLI."""
107:     parser = argparse.ArgumentParser(
108:         description="Extract files from repomix XML package",
109:         formatter_class=argparse.RawDescriptionHelpFormatter,
110:         epilog="""
111: Examples:
112:   # Extract to current directory
113:   python repomix_unpack.py ce-infrastructure.xml
114: 
115:   # Extract to specific directory
116:   python repomix_unpack.py ce-infrastructure.xml --target /path/to/project
117: 
118:   # Verbose output
119:   python repomix_unpack.py ce-infrastructure.xml -v
120:         """
121:     )
122: 
123:     parser.add_argument(
124:         "xml_file",
125:         type=Path,
126:         help="Path to repomix XML file"
127:     )
128: 
129:     parser.add_argument(
130:         "--target",
131:         type=Path,
132:         default=Path.cwd(),
133:         help="Target directory for extraction (default: current directory)"
134:     )
135: 
136:     parser.add_argument(
137:         "-v", "--verbose",
138:         action="store_true",
139:         help="Verbose output"
140:     )
141: 
142:     args = parser.parse_args()
143: 
144:     # Extract files
145:     count = extract_files(args.xml_file, args.target, args.verbose)
146: 
147:     if count > 0:
148:         print(f"\n‚úÖ Extracted {count} files to {args.target}")
149:         return 0
150:     else:
151:         print(f"\n‚ùå Extraction failed", file=sys.stderr)
152:         return 1
153: 
154: 
155: if __name__ == "__main__":
156:     sys.exit(main())
</file>

<file path="tools/pyproject.toml">
 1: [project]
 2: name = "ce-tools"
 3: version = "0.1.0"
 4: description = "Context Engineering CLI Tools"
 5: readme = "README.md"
 6: requires-python = ">=3.10"
 7: dependencies = [
 8:     "anthropic>=0.40.0",
 9:     "deepdiff>=6.0",
10:     "diagrams>=0.24.4",
11:     "jsonschema>=4.25.1",
12:     "packaging>=25.0",
13:     "python-frontmatter>=1.1.0",
14:     "pyyaml>=6.0",
15:     "tomli>=2.3.0",
16:     "tomli-w>=1.2.0",
17: ]
18: 
19: [project.scripts]
20: ce = "ce.__main__:main"
21: 
22: [build-system]
23: requires = ["hatchling"]
24: build-backend = "hatchling.build"
25: 
26: [tool.pytest.ini_options]
27: testpaths = ["tests"]
28: python_files = "test_*.py"
29: python_functions = "test_*"
30: addopts = "-v --tb=short"
31: 
32: [tool.hatch.build.targets.wheel]
33: packages = ["ce"]
34: 
35: [dependency-groups]
36: dev = [
37:     "pytest>=8.4.2",
38:     "pytest-cov>=7.0.0",
39: ]
</file>

<file path="examples/TOOL-USAGE-GUIDE.md">
  1: # Tool Usage Guide - Claude Code Native-First Philosophy
  2: 
  3: **Last Updated**: 2025-11-08
  4: **Status**: Authoritative reference for tool selection
  5: **Replaces**: Obsolete MCP tool documentation
  6: 
  7: **See Also**:
  8: - [Syntropy MCP Naming Convention](syntropy-mcp-naming-convention.md) - Complete naming spec, testing, prevention
  9: - [Syntropy Naming Quick Guide](syntropy-naming-convention.md) - Quick reference for tool names
 10: 
 11: ---
 12: 
 13: ## Philosophy
 14: 
 15: ### Native-First Principle
 16: 
 17: Use Claude Code native tools (Read, Write, Edit, Glob, Grep, Bash) over MCP wrappers whenever possible.
 18: 
 19: **Why?**
 20: 
 21: - **Token Efficiency**: 96% reduction in MCP tools context (~46k ‚Üí ~2k tokens)
 22: - **Performance**: Direct tool access, no MCP routing overhead
 23: - **Reliability**: Fewer abstraction layers, clearer error messages
 24: - **Universality**: Native tools work across all codebases without configuration
 25: 
 26: ### When to Use MCP Tools
 27: 
 28: **Only use MCP tools for capabilities not available natively**:
 29: 
 30: - **Serena**: Code symbol navigation, memory management
 31: - **Linear**: Issue tracking integration
 32: - **Context7**: Library documentation fetching
 33: - **Sequential Thinking**: Complex reasoning workflows
 34: - **Syntropy**: System health checks, knowledge search
 35: 
 36: ---
 37: 
 38: ## Decision Tree
 39: 
 40: ```
 41: Need to [action]?
 42: ‚îÇ
 43: ‚îú‚îÄ File Operations?
 44: ‚îÇ  ‚îú‚îÄ Read file ‚Üí Read (native)
 45: ‚îÇ  ‚îú‚îÄ Write new file ‚Üí Write (native)
 46: ‚îÇ  ‚îú‚îÄ Edit existing ‚Üí Edit (native)
 47: ‚îÇ  ‚îî‚îÄ Find files ‚Üí Glob (native)
 48: ‚îÇ
 49: ‚îú‚îÄ Code Search?
 50: ‚îÇ  ‚îú‚îÄ Search content ‚Üí Grep (native)
 51: ‚îÇ  ‚îú‚îÄ Find symbol ‚Üí mcp__syntropy__serena_find_symbol
 52: ‚îÇ  ‚îî‚îÄ Symbol usage ‚Üí mcp__syntropy__serena_find_referencing_symbols
 53: ‚îÇ
 54: ‚îú‚îÄ Version Control?
 55: ‚îÇ  ‚îú‚îÄ Git operations ‚Üí Bash(git:*)
 56: ‚îÇ  ‚îî‚îÄ GitHub API ‚Üí Bash(gh:*)
 57: ‚îÇ
 58: ‚îú‚îÄ External Knowledge?
 59: ‚îÇ  ‚îú‚îÄ Web search ‚Üí WebSearch (native)
 60: ‚îÇ  ‚îú‚îÄ Library docs ‚Üí mcp__syntropy__context7_get_library_docs
 61: ‚îÇ  ‚îî‚îÄ Web content ‚Üí WebFetch (native)
 62: ‚îÇ
 63: ‚îú‚îÄ Project Management?
 64: ‚îÇ  ‚îî‚îÄ Linear issues ‚Üí mcp__syntropy__linear_*
 65: ‚îÇ
 66: ‚îî‚îÄ Complex Reasoning?
 67:    ‚îî‚îÄ Multi-step analysis ‚Üí mcp__syntropy__thinking_sequentialthinking
 68: ```
 69: 
 70: ---
 71: 
 72: ## Common Tasks
 73: 
 74: ### Task 1: Read and Modify Files
 75: 
 76: **‚ùå WRONG (MCP)**:
 77: 
 78: ```python
 79: mcp__syntropy__filesystem_read_file(path="foo.py")
 80: mcp__syntropy__filesystem_edit_file(path="foo.py", edits=[...])
 81: ```
 82: 
 83: **‚úÖ CORRECT (Native)**:
 84: 
 85: ```python
 86: Read(file_path="/absolute/path/foo.py")
 87: Edit(file_path="/absolute/path/foo.py", old_string="...", new_string="...")
 88: ```
 89: 
 90: **Why**: Native tools are direct, support more features (Edit preserves formatting), consume fewer tokens.
 91: 
 92: ---
 93: 
 94: ### Task 2: Search Codebase
 95: 
 96: **‚ùå WRONG (MCP)**:
 97: 
 98: ```python
 99: mcp__syntropy__filesystem_search_files(pattern="*.py")
100: mcp__syntropy__repomix_pack_codebase(directory=".")
101: ```
102: 
103: **‚úÖ CORRECT (Native + Serena)**:
104: 
105: ```python
106: # Find files by pattern
107: Glob(pattern="**/*.py")
108: 
109: # Search content
110: Grep(pattern="def calculate", type="py", output_mode="content")
111: 
112: # Find specific symbol (when you know the name)
113: mcp__syntropy__serena_find_symbol(name_path="MyClass.calculate")
114: ```
115: 
116: **Why**: Incremental exploration (Glob ‚Üí Grep ‚Üí Read) is more efficient than packing entire codebase. Serena is for symbol-level navigation.
117: 
118: ---
119: 
120: ### Task 3: Git Operations
121: 
122: **‚ùå WRONG (MCP)**:
123: 
124: ```python
125: mcp__syntropy__git_git_status(repo_path="/path")
126: mcp__syntropy__git_git_commit(repo_path="/path", message="Fix bug")
127: ```
128: 
129: **‚úÖ CORRECT (Native Bash)**:
130: 
131: ```bash
132: # Pre-approved: Bash(git:*)
133: git status
134: git diff --staged
135: git add file.py
136: git commit -m "Fix bug"
137: ```
138: 
139: **Why**: Native `git` supports all flags, universally familiar, no MCP routing delay.
140: 
141: ---
142: 
143: ### Task 4: GitHub Operations
144: 
145: **‚ùå WRONG (MCP)**:
146: 
147: ```python
148: mcp__syntropy__github_create_pull_request(owner="...", repo="...", ...)
149: mcp__syntropy__github_list_issues(owner="...", repo="...")
150: ```
151: 
152: **‚úÖ CORRECT (Native gh CLI)**:
153: 
154: ```bash
155: # Pre-approved: Bash(gh:*)
156: gh pr create --title "Fix bug" --body "Description"
157: gh issue list --label bug
158: gh pr view 123
159: ```
160: 
161: **Why**: Official GitHub CLI, more features, better docs, no permission complexity.
162: 
163: ---
164: 
165: ### Task 5: Find Symbol Usages
166: 
167: **‚úÖ CORRECT (Serena - unique capability)**:
168: 
169: ```python
170: # Find where MyClass.calculate is used
171: mcp__syntropy__serena_find_referencing_symbols(name_path="MyClass.calculate")
172: 
173: # Get overview of all symbols in file
174: mcp__syntropy__serena_get_symbols_overview(relative_path="src/utils.py")
175: ```
176: 
177: **Why**: Serena provides AST-level symbol analysis not available via native tools.
178: 
179: ---
180: 
181: ### Task 6: Library Documentation
182: 
183: **‚úÖ CORRECT (Context7 - unique capability)**:
184: 
185: ```python
186: # Resolve library ID
187: mcp__syntropy__context7_resolve_library_id(libraryName="numpy")
188: 
189: # Fetch docs
190: mcp__syntropy__context7_get_library_docs(
191:   context7CompatibleLibraryID="/numpy/doc",
192:   topic="array indexing"
193: )
194: ```
195: 
196: **Why**: Context7 provides curated, AI-optimized library docs not available via WebSearch.
197: 
198: ---
199: 
200: ### Task 7: Project Management
201: 
202: **‚úÖ CORRECT (Linear - unique capability)**:
203: 
204: ```python
205: # List issues
206: mcp__syntropy__linear_list_issues(team_id="TEAM-123")
207: 
208: # Create issue
209: mcp__syntropy__linear_create_issue(
210:   title="Bug: Login fails",
211:   team_id="TEAM-123",
212:   description="..."
213: )
214: ```
215: 
216: **Why**: Direct Linear API integration not available via native tools.
217: 
218: ---
219: 
220: ### Task 8: Complex Reasoning
221: 
222: **‚úÖ CORRECT (Sequential Thinking - unique capability)**:
223: 
224: ```python
225: mcp__syntropy__thinking_sequentialthinking(
226:   thought="Analyzing trade-offs between approach A and B...",
227:   thoughtNumber=1,
228:   totalThoughts=5,
229:   nextThoughtNeeded=True
230: )
231: ```
232: 
233: **Why**: Structured multi-step reasoning process not available natively.
234: 
235: ---
236: 
237: ### Task 9: System Health Check
238: 
239: **‚úÖ CORRECT (Syntropy - unique capability)**:
240: 
241: ```python
242: # Quick health check
243: mcp__syntropy__healthcheck()
244: 
245: # Detailed diagnostics
246: mcp__syntropy__healthcheck(detailed=True, timeout_ms=5000)
247: ```
248: 
249: **Why**: Aggregates health across all MCP servers, not available via native tools.
250: 
251: ---
252: 
253: ## Anti-Patterns
254: 
255: ### Anti-Pattern 1: Using MCP for Simple File Ops
256: 
257: **‚ùå WRONG**:
258: 
259: ```python
260: mcp__syntropy__filesystem_read_file(path="config.json")
261: mcp__syntropy__filesystem_write_file(path="config.json", content="...")
262: ```
263: 
264: **Problem**: Unnecessary MCP overhead, consumes more tokens, slower execution.
265: 
266: **‚úÖ FIX**:
267: 
268: ```python
269: Read(file_path="/absolute/path/config.json")
270: Write(file_path="/absolute/path/config.json", content="...")
271: ```
272: 
273: ---
274: 
275: ### Anti-Pattern 2: Packing Entire Codebase
276: 
277: **‚ùå WRONG**:
278: 
279: ```python
280: mcp__syntropy__repomix_pack_codebase(directory=".")
281: # Then search packed output
282: ```
283: 
284: **Problem**: Monolithic approach, inefficient for incremental work, high token cost.
285: 
286: **‚úÖ FIX**:
287: 
288: ```python
289: # Incremental exploration
290: Glob(pattern="**/auth*.py")  # Find relevant files
291: Grep(pattern="def authenticate", type="py")  # Search specific pattern
292: Read(file_path="/path/to/auth.py")  # Read only what you need
293: ```
294: 
295: ---
296: 
297: ### Anti-Pattern 3: MCP for Git Commands
298: 
299: **‚ùå WRONG**:
300: 
301: ```python
302: mcp__syntropy__git_git_status(repo_path="/path")
303: mcp__syntropy__git_git_diff(repo_path="/path")
304: ```
305: 
306: **Problem**: Limited flag support, unnecessary abstraction.
307: 
308: **‚úÖ FIX**:
309: 
310: ```bash
311: git status
312: git diff --staged
313: git log --oneline -10
314: ```
315: 
316: ---
317: 
318: ### Anti-Pattern 4: Using Playwright for Simple Web Content
319: 
320: **‚ùå WRONG**:
321: 
322: ```python
323: mcp__syntropy__playwright_navigate(url="https://example.com")
324: mcp__syntropy__playwright_get_visible_text()
325: ```
326: 
327: **Problem**: Overkill for static content, slow browser startup.
328: 
329: **‚úÖ FIX**:
330: 
331: ```python
332: # For static content
333: WebFetch(url="https://example.com", prompt="Extract main content")
334: 
335: # For search queries
336: WebSearch(query="Python asyncio best practices")
337: ```
338: 
339: ---
340: 
341: ### Anti-Pattern 5: GitHub MCP Instead of gh CLI
342: 
343: **‚ùå WRONG**:
344: 
345: ```python
346: mcp__syntropy__github_create_pull_request(
347:   owner="user",
348:   repo="project",
349:   title="Fix",
350:   head="fix-branch",
351:   base="main",
352:   body="Description"
353: )
354: ```
355: 
356: **Problem**: Verbose, requires explicit owner/repo, limited features.
357: 
358: **‚úÖ FIX**:
359: 
360: ```bash
361: # Infers owner/repo from current directory
362: gh pr create --title "Fix" --body "Description"
363: ```
364: 
365: ---
366: 
367: ## Tool Quick Reference
368: 
369: ### Native Tools (Always Prefer)
370: 
371: | Tool | Purpose | Example |
372: |------|---------|---------|
373: | **Read** | Read file contents | `Read(file_path="/abs/path/file.py")` |
374: | **Write** | Create new file | `Write(file_path="/abs/path/new.py", content="...")` |
375: | **Edit** | Modify existing file | `Edit(file_path="...", old_string="...", new_string="...")` |
376: | **Glob** | Find files by pattern | `Glob(pattern="**/*.py")` |
377: | **Grep** | Search file contents | `Grep(pattern="def foo", type="py", output_mode="content")` |
378: | **Bash** | Run shell commands | `Bash(command="git status")` |
379: | **WebSearch** | AI-powered search | `WebSearch(query="Python asyncio")` |
380: | **WebFetch** | Fetch web content | `WebFetch(url="...", prompt="Extract...")` |
381: 
382: ### MCP Tools (Use Only When Native Unavailable)
383: 
384: #### Serena (Code Navigation) - 13 tools
385: 
386: | Tool | Purpose | Example |
387: |------|---------|---------|
388: | `mcp__syntropy__serena_activate_project` | Activate project for symbol indexing | `project="/abs/path/to/project"` |
389: | `mcp__syntropy__serena_find_symbol` | Find symbol definition | `name_path="MyClass.method"` |
390: | `mcp__syntropy__serena_get_symbols_overview` | List all symbols in file | `relative_path="src/utils.py"` |
391: | `mcp__syntropy__serena_find_referencing_symbols` | Find symbol usages | `name_path="MyClass.method"` |
392: | `mcp__syntropy__serena_replace_symbol_body` | Replace function/class body | `name_path="MyClass.method", new_body="..."` |
393: | `mcp__syntropy__serena_search_for_pattern` | Regex search | `pattern="def.*async"` |
394: | `mcp__syntropy__serena_list_dir` | List directory contents | `directory_path="src/"` |
395: | `mcp__syntropy__serena_read_file` | Read file contents | `relative_path="src/main.py"` |
396: | `mcp__syntropy__serena_create_text_file` | Create new text file | `path="new.py", content="..."` |
397: | `mcp__syntropy__serena_write_memory` | Store project context | `memory_type="architecture", content="..."` |
398: | `mcp__syntropy__serena_read_memory` | Retrieve context | `memory_name="architecture"` |
399: | `mcp__syntropy__serena_list_memories` | List all memories | No parameters |
400: | `mcp__syntropy__serena_delete_memory` | Delete memory | `memory_name="temporary"` |
401: 
402: #### Linear (Project Management) - 9 tools
403: 
404: | Tool | Purpose | Example |
405: |------|---------|---------|
406: | `mcp__syntropy__linear_create_issue` | Create issue | `title="Bug", team_id="..."` |
407: | `mcp__syntropy__linear_get_issue` | Get issue details | `issue_id="ISSUE-123"` |
408: | `mcp__syntropy__linear_list_issues` | List issues | `team_id="TEAM-123"` |
409: | `mcp__syntropy__linear_update_issue` | Update issue | `issue_id="...", updates={...}` |
410: | `mcp__syntropy__linear_list_projects` | List all projects | No parameters |
411: | `mcp__syntropy__linear_list_teams` | List all teams | No parameters |
412: | `mcp__syntropy__linear_get_team` | Get team details | `team_id="TEAM-123"` |
413: | `mcp__syntropy__linear_list_users` | List all users | No parameters |
414: | `mcp__syntropy__linear_create_project` | Create new project | `name="Project", team_id="..."` |
415: 
416: #### Context7 (Library Docs) - 2 tools
417: 
418: | Tool | Purpose | Example |
419: |------|---------|---------|
420: | `mcp__syntropy__context7_resolve_library_id` | Find library ID | `libraryName="numpy"` |
421: | `mcp__syntropy__context7_get_library_docs` | Fetch docs | `context7CompatibleLibraryID="/numpy/doc"` |
422: 
423: #### Thinking (Reasoning) - 1 tool
424: 
425: | Tool | Purpose | Example |
426: |------|---------|---------|
427: | `mcp__syntropy__thinking_sequentialthinking` | Structured reasoning | `thought="...", thoughtNumber=1` |
428: 
429: #### Syntropy (System) - 3 tools
430: 
431: | Tool | Purpose | Example |
432: |------|---------|---------|
433: | `mcp__syntropy__healthcheck` | Check MCP servers | `detailed=True` |
434: | `mcp__syntropy__enable_tools` | Enable/disable tools dynamically | `enable=["tool1"], disable=["tool2"]` |
435: | `mcp__syntropy__list_all_tools` | List all tools with enabled/disabled status | No parameters |
436: 
437: **Total: 28 allowed tools** (13 Serena + 9 Linear + 2 Context7 + 1 Thinking + 3 Syntropy)
438: 
439: ---
440: 
441: ## Migration Table: Denied Tools ‚Üí Alternatives
442: 
443: ### Filesystem (8 tools denied)
444: 
445: | Denied Tool | Alternative | Example |
446: |-------------|-------------|---------|
447: | `filesystem_read_file` | **Read** (native) | `Read(file_path="/abs/path/file.py")` |
448: | `filesystem_read_text_file` | **Read** (native) | Same as above |
449: | `filesystem_write_file` | **Write** (native) | `Write(file_path="...", content="...")` |
450: | `filesystem_edit_file` | **Edit** (native) | `Edit(file_path="...", old_string="...", new_string="...")` |
451: | `filesystem_list_directory` | **Bash** (ls) | `Bash(command="ls -la /path")` |
452: | `filesystem_search_files` | **Glob** (native) | `Glob(pattern="**/*.py")` |
453: | `filesystem_directory_tree` | **Bash** (tree) | `Bash(command="tree -L 2")` |
454: | `filesystem_get_file_info` | **Bash** (stat) | `Bash(command="stat file.py")` |
455: 
456: ### Git (5 tools denied)
457: 
458: | Denied Tool | Alternative | Example |
459: |-------------|-------------|---------|
460: | `git_git_status` | **Bash(git)** | `Bash(command="git status")` |
461: | `git_git_diff` | **Bash(git)** | `Bash(command="git diff --staged")` |
462: | `git_git_log` | **Bash(git)** | `Bash(command="git log --oneline -10")` |
463: | `git_git_add` | **Bash(git)** | `Bash(command="git add file.py")` |
464: | `git_git_commit` | **Bash(git)** | `Bash(command='git commit -m "msg"')` |
465: 
466: ### GitHub (26 tools denied)
467: 
468: | Denied Tool | Alternative | Example |
469: |-------------|-------------|---------|
470: | `github_create_or_update_file` | **Bash(gh)** | `gh api repos/owner/repo/contents/path -f content=...` |
471: | `github_search_repositories` | **Bash(gh)** | `gh search repos "keyword"` |
472: | `github_create_repository` | **Bash(gh)** | `gh repo create name --public` |
473: | `github_get_file_contents` | **Bash(gh)** | `gh api repos/owner/repo/contents/path` |
474: | `github_push_files` | **Bash(git)** | `git add . && git commit -m "msg" && git push` |
475: | `github_create_issue` | **Bash(gh)** | `gh issue create --title "Bug" --body "..."` |
476: | `github_create_pull_request` | **Bash(gh)** | `gh pr create --title "Fix" --body "..."` |
477: | `github_fork_repository` | **Bash(gh)** | `gh repo fork owner/repo` |
478: | `github_create_branch` | **Bash(git)** | `git checkout -b branch-name` |
479: | `github_list_commits` | **Bash(gh)** | `gh api repos/owner/repo/commits` |
480: | `github_list_issues` | **Bash(gh)** | `gh issue list --label bug` |
481: | `github_update_issue` | **Bash(gh)** | `gh issue edit 123 --title "New"` |
482: | `github_add_issue_comment` | **Bash(gh)** | `gh issue comment 123 --body "..."` |
483: | `github_search_code` | **Bash(gh)** | `gh search code "query"` |
484: | `github_search_issues` | **Bash(gh)** | `gh search issues "bug"` |
485: | `github_search_users` | **Bash(gh)** | `gh search users "name"` |
486: | `github_get_issue` | **Bash(gh)** | `gh issue view 123` |
487: | `github_get_pull_request` | **Bash(gh)** | `gh pr view 123` |
488: | `github_list_pull_requests` | **Bash(gh)** | `gh pr list --state open` |
489: | `github_create_pull_request_review` | **Bash(gh)** | `gh pr review 123 --approve` |
490: | `github_merge_pull_request` | **Bash(gh)** | `gh pr merge 123 --squash` |
491: | `github_get_pull_request_files` | **Bash(gh)** | `gh pr diff 123` |
492: | `github_get_pull_request_status` | **Bash(gh)** | `gh pr checks 123` |
493: | `github_update_pull_request_branch` | **Bash(git)** | `git checkout branch && git pull origin main` |
494: | `github_get_pull_request_comments` | **Bash(gh)** | `gh pr view 123 --comments` |
495: | `github_get_pull_request_reviews` | **Bash(gh)** | `gh api repos/owner/repo/pulls/123/reviews` |
496: 
497: ### Repomix (4 tools denied)
498: 
499: | Denied Tool | Alternative | Example |
500: |-------------|-------------|---------|
501: | `repomix_pack_codebase` | **Glob + Grep + Read** | Incremental exploration |
502: | `repomix_grep_repomix_output` | **Grep** (native) | `Grep(pattern="...", output_mode="content")` |
503: | `repomix_read_repomix_output` | **Read** (native) | `Read(file_path="...")` |
504: | `repomix_pack_remote_repository` | **Bash(git clone)** + native tools | `git clone <url> && Glob/Grep/Read` |
505: 
506: ### Playwright (6 tools denied)
507: 
508: | Denied Tool | Alternative | Example |
509: |-------------|-------------|---------|
510: | `playwright_navigate` | **WebFetch** (static) or **Bash(playwright CLI)** (dynamic) | `WebFetch(url="...", prompt="...")` |
511: | `playwright_screenshot` | **Bash(playwright CLI)** | `playwright screenshot <url> screenshot.png` |
512: | `playwright_click` | **Bash(playwright CLI)** | Rarely needed for CLI tooling |
513: | `playwright_fill` | **Bash(playwright CLI)** | Rarely needed for CLI tooling |
514: | `playwright_evaluate` | **Bash(playwright CLI)** | Rarely needed for CLI tooling |
515: | `playwright_get_visible_text` | **WebFetch** | `WebFetch(url="...", prompt="Extract text")` |
516: 
517: ### Perplexity (1 tool denied)
518: 
519: | Denied Tool | Alternative | Example |
520: |-------------|-------------|---------|
521: | `perplexity_perplexity_ask` | **WebSearch** (native) | `WebSearch(query="Python asyncio patterns")` |
522: 
523: ### Syntropy System (5 tools denied)
524: 
525: | Denied Tool | Alternative | Example |
526: |-------------|-------------|---------|
527: | `init_project` | **Manual setup** | One-time operation, rarely needed |
528: | `get_system_doc` | **Read** (native) | `Read(file_path=".ce/RULES.md")` |
529: | `get_user_doc` | **Read** (native) | `Read(file_path="PRPs/executed/PRP-1.md")` |
530: | `get_summary` | **Read** + manual analysis | Read REPLKAN, analyze structure |
531: | `denoise` | **Edit** (native) | Manually edit verbose docs |
532: 
533: ---
534: 
535: ## Best Practices
536: 
537: ### 1. Start with Native Tools
538: 
539: Always check if Read/Write/Edit/Glob/Grep/Bash can solve the task before reaching for MCP tools.
540: 
541: ### 2. Use Serena for Symbol Navigation
542: 
543: When you need to find a specific function/class definition or its usages across the codebase.
544: 
545: ### 3. Incremental > Monolithic
546: 
547: Prefer Glob ‚Üí Grep ‚Üí Read over packing entire codebase with Repomix.
548: 
549: ### 4. Bash for System Commands
550: 
551: Git, gh, tree, ls, find, etc. are pre-approved and more flexible than MCP wrappers.
552: 
553: ### 5. MCP for Integration
554: 
555: Use MCP tools when you need external service integration (Linear, Context7) not available via native tools.
556: 
557: ### 6. Validate with Healthcheck
558: 
559: Periodically run `mcp__syntropy__healthcheck(detailed=True)` to ensure all servers are connected.
560: 
561: ---
562: 
563: ## CE Framework Commands
564: 
565: The Context Engineering framework provides specialized commands for framework management. These are project-specific tools, not generic utilities.
566: 
567: ### When to Use CE Commands
568: 
569: Use CE commands for:
570: - **Framework installation**: `ce init-project`
571: - **Content merging**: `ce blend`
572: - **Context updates**: `ce update-context`
573: - **Validation**: `ce validate`
574: - **Cleanup**: `ce vacuum`
575: 
576: **Do NOT** use CE commands for:
577: - Generic file operations (use native tools)
578: - General git operations (use Bash(git:*))
579: - External web requests (use WebFetch/WebSearch)
580: 
581: ---
582: 
583: ### ce init-project
584: 
585: **Purpose**: Initialize CE Framework in target project
586: 
587: **Usage**:
588: ```bash
589: cd tools
590: uv run ce init-project ~/projects/target-app
591: 
592: # With flags
593: uv run ce init-project ~/projects/app --dry-run
594: uv run ce init-project ~/projects/app --phase extract
595: uv run ce init-project ~/projects/app --blend-only
596: ```
597: 
598: **When to use**:
599: - Setting up CE framework in new project
600: - Upgrading existing CE installation
601: - Re-initializing after major changes
602: 
603: **Output**: 4-phase pipeline (extract ‚Üí blend ‚Üí initialize ‚Üí verify)
604: 
605: **See**: [ce-init-project-usage.md](ce-init-project-usage.md) for comprehensive guide
606: 
607: ---
608: 
609: ### ce blend
610: 
611: **Purpose**: Merge CE framework files with target project customizations
612: 
613: **Usage**:
614: ```bash
615: cd tools
616: uv run ce blend --all
617: uv run ce blend --claude-md --memories
618: uv run ce blend --all --target-dir ~/projects/app
619: ```
620: 
621: **When to use**:
622: - After updating framework files (memories, examples)
623: - Re-merging user customizations
624: - Upgrading framework content
625: 
626: **Domains**: CLAUDE.md, memories, examples, settings, commands, PRPs
627: 
628: **See**: [ce-blend-usage.md](ce-blend-usage.md) for comprehensive guide
629: 
630: ---
631: 
632: ### ce update-context
633: 
634: **Purpose**: Sync PRPs with codebase implementation state
635: 
636: **Usage**:
637: ```bash
638: cd tools
639: uv run ce update-context
640: uv run ce update-context --prp PRPs/executed/PRP-34.1.1-core-blending-framework.md
641: ```
642: 
643: **When to use**:
644: - After completing PRP implementation
645: - Weekly system hygiene
646: - Detecting pattern drift
647: 
648: **Auto-features**:
649: - Rebuilds repomix packages if framework files changed
650: - Detects drift violations
651: - Updates YAML headers
652: 
653: **See**: `.claude/commands/update-context.md`
654: 
655: ---
656: 
657: ### ce validate
658: 
659: **Purpose**: Validate project structure and framework files
660: 
661: **Usage**:
662: ```bash
663: cd tools
664: uv run ce validate --level all
665: uv run ce validate --level 4  # Pattern conformance
666: ```
667: 
668: **Levels**:
669: - L1: File structure
670: - L2: YAML headers
671: - L3: Content validation
672: - L4: Pattern conformance
673: 
674: **See**: `.serena/memories/l4-validation-usage.md`
675: 
676: ---
677: 
678: ### ce vacuum
679: 
680: **Purpose**: Clean up temporary files and obsolete artifacts
681: 
682: **Usage**:
683: ```bash
684: cd tools
685: uv run ce vacuum                  # Dry-run (report only)
686: uv run ce vacuum --execute        # Delete temp files
687: uv run ce vacuum --auto           # Delete temp + obsolete
688: ```
689: 
690: **See**: `.claude/commands/vacuum.md`
691: 
692: ---
693: 
694: ### CE Command Patterns
695: 
696: **‚úÖ CORRECT**:
697: ```python
698: # Use ce commands for framework operations
699: Bash(command="cd tools && uv run ce blend --all")
700: Bash(command="cd tools && uv run ce init-project ~/projects/app")
701: ```
702: 
703: **‚ùå WRONG**:
704: ```python
705: # Don't use ce commands for generic operations
706: Bash(command="ce blend somefile.txt")  # Blend is not for arbitrary files
707: Bash(command="ce init-project .")      # Must run from tools/ directory
708: ```
709: 
710: ---
711: 
712: ## Troubleshooting
713: 
714: ### Issue: "Tool not found" error
715: 
716: **Cause**: Tool is in deny list or MCP server disconnected.
717: 
718: **Fix**:
719: 
720: 1. Check `.claude/settings.local.json` permissions.deny
721: 2. Run `mcp__syntropy__healthcheck(detailed=True)`
722: 3. Reconnect MCP: `/mcp` (in main repo, not worktrees)
723: 
724: ### Issue: "Permission denied" for Bash command
725: 
726: **Cause**: Command not in allow list.
727: 
728: **Fix**:
729: 
730: 1. Check if command matches allow pattern: `Bash(git:*)`, `Bash(uv run:*)`, etc.
731: 2. If needed frequently, add to allow list in settings
732: 3. Temporary: User can approve via prompt
733: 
734: ### Issue: MCP tool slow or timing out
735: 
736: **Cause**: Server connectivity issue or large operation.
737: 
738: **Fix**:
739: 
740: 1. Check server health: `mcp__syntropy__healthcheck()`
741: 2. Increase timeout if supported
742: 3. Consider native alternative (e.g., Grep instead of repomix)
743: 
744: ### Issue: Serena "symbol not found"
745: 
746: **Cause**: Incorrect name_path format or file not indexed.
747: 
748: **Fix**:
749: 
750: 1. Use `serena_get_symbols_overview` to list all symbols in file
751: 2. Ensure format: `ClassName.method_name` or `function_name`
752: 3. Check relative_path is correct from project root
753: 
754: ---
755: 
756: ## See Also
757: 
758: **Code Examples**:
759: 
760: - `tools/ce/examples/syntropy/` - MCP tool usage patterns in Python
761: - `.serena/memories/` - Serena memory management examples and patterns
762: 
763: **Related Documentation**:
764: 
765: - `CLAUDE.md` - Project guide and quick commands
766: - `TOOL-PERMISSION-LOCKDOWN-PLAN.md` - Detailed rationale for tool deny list
767: - `PRPs/executed/PRP-B-tool-usage-guide.md` - PRP that created this guide
768: 
769: ---
770: 
771: **End of Guide**
772: 
773: For questions or suggestions, update this guide via PR following Context Engineering framework.
</file>

<file path="tools/ce/blending/strategies/examples.py">
  1: """Examples blending strategy with semantic deduplication."""
  2: 
  3: import hashlib
  4: import logging
  5: from pathlib import Path
  6: from typing import Dict, Any, List, Set, Optional
  7: 
  8: from ce.blending.llm_client import BlendingLLM
  9: 
 10: logger = logging.getLogger(__name__)
 11: 
 12: 
 13: class ExamplesBlendStrategy:
 14:     """
 15:     Blend framework and target examples with semantic deduplication.
 16: 
 17:     Philosophy: "Copy ours + skip if target has equivalent"
 18: 
 19:     Process:
 20:     1. Hash deduplication: Skip framework examples with identical hash
 21:     2. Semantic deduplication: Skip framework examples >90% similar to target
 22:     3. Copy remaining framework examples to target
 23: 
 24:     Usage:
 25:         >>> strategy = ExamplesBlendStrategy(llm_client)
 26:         >>> result = strategy.blend(framework_examples_dir, target_examples_dir, context)
 27:     """
 28: 
 29:     def __init__(self, llm_client: BlendingLLM, similarity_threshold: float = 0.9):
 30:         """
 31:         Initialize examples blending strategy.
 32: 
 33:         Args:
 34:             llm_client: BlendingLLM instance for semantic comparison
 35:             similarity_threshold: Similarity threshold 0.0-1.0 (default: 0.9)
 36:         """
 37:         self.llm = llm_client
 38:         self.threshold = similarity_threshold
 39:         logger.debug(f"ExamplesBlendStrategy initialized (threshold={self.threshold})")
 40: 
 41:     def blend(
 42:         self,
 43:         framework_dir: Path,
 44:         target_dir: Path,
 45:         context: Optional[Dict[str, Any]] = None
 46:     ) -> Dict[str, Any]:
 47:         """
 48:         Blend or migrate examples based on framework availability.
 49: 
 50:         Dual-Mode Operation:
 51:         - Blend Mode: Framework exists ‚Üí semantic dedup + copy framework‚Üítarget
 52:         - Migration Mode: Framework missing ‚Üí migrate target‚Üí.ce/examples/user/
 53: 
 54:         Args:
 55:             framework_dir: Framework examples directory (.ce/examples/)
 56:             target_dir: Target examples directory (examples/)
 57:             context: Optional context dict (backup_dir, dry_run, target_dir, etc.)
 58: 
 59:         Returns:
 60:             Dict with blend mode keys (copied, skipped_hash, skipped_similar) OR
 61:             migration mode keys (migrated, skipped, errors, success)
 62:         """
 63:         # Check if framework examples exist
 64:         if not framework_dir.exists() or not framework_dir.is_dir():
 65:             logger.info(f"Framework examples not found - switching to migration mode")
 66:             return self._migrate_user_examples(target_dir, context)
 67: 
 68:         # Create target dir if not exists
 69:         target_dir.mkdir(parents=True, exist_ok=True)
 70: 
 71:         # Get all framework examples
 72:         framework_examples = self._get_examples(framework_dir)
 73:         target_examples = self._get_examples(target_dir)
 74: 
 75:         logger.info(
 76:             f"Blending examples: {len(framework_examples)} framework, "
 77:             f"{len(target_examples)} target"
 78:         )
 79: 
 80:         # Track results
 81:         copied = []
 82:         skipped_hash = []
 83:         skipped_similar = []
 84:         errors = []
 85: 
 86:         # Build target hash set for O(1) lookup
 87:         target_hashes = self._build_hash_set(target_examples)
 88: 
 89:         # Process each framework example
 90:         for fw_example in framework_examples:
 91:             try:
 92:                 # Phase 1: Hash deduplication
 93:                 fw_hash = self._hash_file(fw_example)
 94:                 if fw_hash in target_hashes:
 95:                     logger.debug(f"Skip {fw_example.name} (exact duplicate)")
 96:                     skipped_hash.append(fw_example.name)
 97:                     continue
 98: 
 99:                 # Phase 2: Semantic deduplication
100:                 is_duplicate, similar_file = self._check_semantic_similarity(
101:                     fw_example, target_examples
102:                 )
103:                 if is_duplicate:
104:                     logger.debug(
105:                         f"Skip {fw_example.name} (similar to {similar_file})"
106:                     )
107:                     skipped_similar.append(fw_example.name)
108:                     continue
109: 
110:                 # Phase 3: Copy framework example
111:                 target_path = target_dir / fw_example.name
112:                 if context and context.get("dry_run"):
113:                     logger.info(f"[DRY RUN] Would copy {fw_example.name}")
114:                 else:
115:                     target_path.write_text(fw_example.read_text())
116:                     logger.info(f"‚úì Copied {fw_example.name}")
117:                 copied.append(fw_example.name)
118: 
119:             except Exception as e:
120:                 error_msg = f"Failed to process {fw_example.name}: {e}"
121:                 logger.warning(error_msg)
122:                 errors.append(error_msg)
123:                 # Continue processing remaining examples
124: 
125:         # Get token usage
126:         token_usage = self.llm.get_token_usage()
127: 
128:         logger.info(
129:             f"Examples blending complete: {len(copied)} copied, "
130:             f"{len(skipped_hash)} hash-skipped, {len(skipped_similar)} similarity-skipped"
131:         )
132: 
133:         return {
134:             "copied": copied,
135:             "skipped_hash": skipped_hash,
136:             "skipped_similar": skipped_similar,
137:             "errors": errors,
138:             "token_usage": token_usage
139:         }
140: 
141:     def _migrate_user_examples(
142:         self,
143:         source_dir: Path,
144:         context: Optional[Dict[str, Any]] = None
145:     ) -> Dict[str, Any]:
146:         """
147:         Migrate user examples from source to .ce/examples/user/.
148: 
149:         Similar to PRPMoveStrategy but for examples.
150:         Supports recursive file discovery and all file types.
151: 
152:         Args:
153:             source_dir: Source examples directory (examples/)
154:             context: Context dict with target_dir, dry_run, etc.
155: 
156:         Returns:
157:             Dict with:
158:                 - migrated: Count of migrated files
159:                 - skipped: Count of skipped files (hash duplicate)
160:                 - errors: List of error messages
161:                 - success: True if no errors
162:                 - files_processed: Count for compatibility
163:         """
164:         if not source_dir.exists():
165:             logger.info(f"Source examples directory not found: {source_dir}")
166:             return {
167:                 "migrated": 0,
168:                 "skipped": 0,
169:                 "errors": [],
170:                 "success": True,
171:                 "files_processed": 0
172:             }
173: 
174:         # Get target base directory from context
175:         if not context or "target_dir" not in context:
176:             raise ValueError("context['target_dir'] required for migration mode\nüîß Troubleshooting: Check inputs and system state")
177: 
178:         target_base = context["target_dir"] / ".ce" / "examples" / "user"
179:         target_base.mkdir(parents=True, exist_ok=True)
180: 
181:         migrated = 0
182:         skipped = 0
183:         errors = []
184: 
185:         # Find all example files recursively (all file types)
186:         example_files = list(source_dir.rglob("*"))
187:         example_files = [f for f in example_files if f.is_file()]
188: 
189:         logger.info(f"Migrating {len(example_files)} example files from {source_dir}")
190: 
191:         for source_file in example_files:
192:             try:
193:                 # Preserve subdirectory structure
194:                 relative_path = source_file.relative_to(source_dir)
195:                 target_file = target_base / relative_path
196: 
197:                 # Create parent directories
198:                 target_file.parent.mkdir(parents=True, exist_ok=True)
199: 
200:                 # Hash-based deduplication
201:                 if target_file.exists():
202:                     source_hash = self._hash_file(source_file)
203:                     target_hash = self._hash_file(target_file)
204:                     if source_hash == target_hash:
205:                         logger.debug(f"Skip {source_file.name} (hash duplicate)")
206:                         skipped += 1
207:                         continue
208: 
209:                 # Copy to target
210:                 if context and context.get("dry_run"):
211:                     logger.info(f"[DRY RUN] Would migrate {relative_path}")
212:                 else:
213:                     target_file.write_bytes(source_file.read_bytes())
214:                     logger.debug(f"‚úì Migrated {relative_path}")
215:                 migrated += 1
216: 
217:             except Exception as e:
218:                 error_msg = f"Error processing {source_file.name}: {e}"
219:                 logger.warning(error_msg)
220:                 errors.append(error_msg)
221: 
222:         logger.info(
223:             f"Examples migration complete: {migrated} migrated, {skipped} skipped"
224:         )
225: 
226:         return {
227:             "migrated": migrated,
228:             "skipped": skipped,
229:             "errors": errors,
230:             "success": len(errors) == 0,
231:             "files_processed": migrated
232:         }
233: 
234:     def _get_examples(self, examples_dir: Path) -> List[Path]:
235:         """Get all .md files in examples directory (non-recursive)."""
236:         if not examples_dir.exists():
237:             return []
238:         return [f for f in examples_dir.iterdir() if f.suffix == ".md" and f.is_file()]
239: 
240:     def _hash_file(self, file_path: Path) -> str:
241:         """Compute SHA256 hash of file content."""
242:         content = file_path.read_bytes()
243:         return hashlib.sha256(content).hexdigest()
244: 
245:     def _build_hash_set(self, examples: List[Path]) -> Set[str]:
246:         """Build set of file hashes for O(1) lookup."""
247:         return {self._hash_file(ex) for ex in examples}
248: 
249:     def _check_semantic_similarity(
250:         self,
251:         framework_example: Path,
252:         target_examples: List[Path]
253:     ) -> tuple:
254:         """
255:         Check if framework example is semantically similar to any target example.
256: 
257:         Uses Haiku for fast, cheap comparison.
258: 
259:         Args:
260:             framework_example: Framework example file
261:             target_examples: List of target example files
262: 
263:         Returns:
264:             Tuple of (is_duplicate, similar_file_name)
265:             - is_duplicate: True if any target example >threshold similar
266:             - similar_file_name: Name of similar file (or None)
267:         """
268:         if not target_examples:
269:             return (False, None)
270: 
271:         # Extract comparison content from framework example
272:         fw_content = self._extract_comparison_content(framework_example)
273: 
274:         # Compare against each target example
275:         for target_example in target_examples:
276:             target_content = self._extract_comparison_content(target_example)
277: 
278:             # Call Haiku via BlendingLLM
279:             try:
280:                 result = self.llm.check_similarity(
281:                     fw_content, target_content, threshold=self.threshold
282:                 )
283: 
284:                 if result["similar"]:
285:                     logger.debug(
286:                         f"{framework_example.name} similar to {target_example.name} "
287:                         f"(score: {result['score']:.2f})"
288:                     )
289:                     return (True, target_example.name)
290: 
291:             except Exception as e:
292:                 # Log warning but continue (fail open, copy framework example)
293:                 logger.warning(
294:                     f"Similarity check failed for {framework_example.name} vs "
295:                     f"{target_example.name}: {e}"
296:                 )
297:                 # Continue to next target example
298: 
299:         # No similar examples found
300:         return (False, None)
301: 
302:     def _extract_comparison_content(self, example_file: Path) -> str:
303:         """
304:         Extract comparison content from example file.
305: 
306:         Returns first 1500 chars (title + description + code snippets).
307:         This optimizes token usage while preserving semantic meaning.
308: 
309:         Args:
310:             example_file: Example markdown file
311: 
312:         Returns:
313:             Comparison content (truncated to 1500 chars)
314:         """
315:         content = example_file.read_text()
316: 
317:         # Truncate to 1500 chars for token optimization
318:         # This typically includes: title, description, first code snippet
319:         comparison_content = content[:1500]
320: 
321:         # If truncated mid-code-block, add closing fence
322:         if comparison_content.count("```") % 2 != 0:
323:             comparison_content += "\n```"
324: 
325:         return comparison_content
</file>

<file path="tools/ce/blending/strategies/memories.py">
  1: """Memories blending strategy with Haiku similarity + Sonnet merging."""
  2: 
  3: import yaml
  4: import logging
  5: from pathlib import Path
  6: from typing import Dict, Any, List, Optional
  7: from dataclasses import dataclass
  8: 
  9: from ce.blending.llm_client import BlendingLLM
 10: from ce.blending.strategies.base import BlendStrategy
 11: 
 12: logger = logging.getLogger(__name__)
 13: 
 14: 
 15: # Critical memories - always use framework version (no blending)
 16: CRITICAL_MEMORIES = {
 17:     "code-style-conventions.md",
 18:     "suggested-commands.md",
 19:     "task-completion-checklist.md",
 20:     "testing-standards.md",
 21:     "tool-usage-syntropy.md",
 22:     "use-syntropy-tools-not-bash.md",
 23: }
 24: 
 25: 
 26: @dataclass
 27: class BlendResult:
 28:     """Result of blending operation."""
 29:     success: bool
 30:     files_processed: int
 31:     skipped: List[str]
 32:     merged: List[str]
 33:     copied: List[str]
 34:     errors: List[str]
 35: 
 36: 
 37: class MemoriesBlendStrategy(BlendStrategy):
 38:     """
 39:     Blend Serena memories with Haiku similarity + Sonnet merge.
 40: 
 41:     Philosophy: Copy ours + import complementary target memories.
 42: 
 43:     Strategy:
 44:     1. Haiku similarity check (fast, cheap)
 45:     2. If >90% similar: Use framework version (skip target)
 46:     3. If contradicting: Use framework version (or ask user)
 47:     4. If complementary: Sonnet merge
 48:     5. YAML header blending: framework type wins, merge tags, earlier created, later updated
 49:     6. Critical memories: always framework (no blending)
 50:     7. Target-only memories: preserve with type: user header
 51:     """
 52: 
 53:     def __init__(self, llm_client: Optional[BlendingLLM] = None):
 54:         """
 55:         Initialize strategy with LLM client.
 56: 
 57:         Args:
 58:             llm_client: LLM client for similarity/merge (creates default if None)
 59:         """
 60:         self.llm_client = llm_client or BlendingLLM()
 61: 
 62:     def can_handle(self, domain: str) -> bool:
 63:         """Check if strategy can handle this domain."""
 64:         return domain == "memories"
 65: 
 66:     def blend(
 67:         self,
 68:         framework_content: Any,
 69:         target_content: Optional[Any],
 70:         context: Dict[str, Any]
 71:     ) -> Any:
 72:         """
 73:         Blend framework and target memories directories.
 74: 
 75:         Args:
 76:             framework_content: Path to framework .serena/memories/
 77:             target_content: Path to target .serena/memories/ (may be None)
 78:             context: Dict with output_path (Path to output .serena/memories/)
 79: 
 80:         Returns:
 81:             BlendResult with operation summary
 82: 
 83:         Raises:
 84:             RuntimeError: If LLM call fails or critical error occurs
 85:             ValueError: If framework_content is not a Path or doesn't exist
 86:         """
 87:         if not isinstance(framework_content, Path):
 88:             raise ValueError(f"framework_content must be Path, got {type(framework_content)}\nüîß Troubleshooting: Check inputs and system state")
 89: 
 90:         if not framework_content.exists():
 91:             raise ValueError(f"Framework memories path does not exist: {framework_content}\nüîß Troubleshooting: Check inputs and system state")
 92: 
 93:         framework_path = framework_content
 94:         target_path = target_content if target_content else None
 95:         output_path = context.get("output_path")
 96: 
 97:         if not output_path:
 98:             raise ValueError("context must contain 'output_path'\nüîß Troubleshooting: Check inputs and system state")
 99: 
100:         output_path.mkdir(parents=True, exist_ok=True)
101: 
102:         skipped = []
103:         merged = []
104:         copied = []
105:         errors = []
106: 
107:         # Get all framework memories
108:         framework_files = self._list_memory_files(framework_path)
109:         target_files = self._list_memory_files(target_path) if target_path else []
110: 
111:         logger.info(f"Framework memories: {len(framework_files)}")
112:         logger.info(f"Target memories: {len(target_files)}")
113: 
114:         # Process framework memories
115:         for fw_file in framework_files:
116:             try:
117:                 result = self._process_memory(
118:                     fw_file=framework_path / fw_file,
119:                     target_file=target_path / fw_file if target_path and fw_file in target_files else None,
120:                     output_file=output_path / fw_file
121:                 )
122: 
123:                 if result["action"] == "skip":
124:                     skipped.append(fw_file)
125:                 elif result["action"] == "merge":
126:                     merged.append(fw_file)
127:                 elif result["action"] == "copy":
128:                     copied.append(fw_file)
129: 
130:             except Exception as e:
131:                 error_msg = f"{fw_file}: {str(e)}"
132:                 logger.error(f"Failed to process {fw_file}: {e}")
133:                 errors.append(error_msg)
134: 
135:         # Process target-only memories (preserve with type: user)
136:         if target_path:
137:             target_only = set(target_files) - set(framework_files)
138:             for target_file in target_only:
139:                 try:
140:                     self._preserve_target_memory(
141:                         target_file=target_path / target_file,
142:                         output_file=output_path / target_file
143:                     )
144:                     copied.append(f"{target_file} (target-only)")
145: 
146:                 except Exception as e:
147:                     error_msg = f"{target_file}: {str(e)}"
148:                     logger.error(f"Failed to preserve {target_file}: {e}")
149:                     errors.append(error_msg)
150: 
151:         total_processed = len(skipped) + len(merged) + len(copied)
152:         success = len(errors) == 0
153: 
154:         return BlendResult(
155:             success=success,
156:             files_processed=total_processed,
157:             skipped=skipped,
158:             merged=merged,
159:             copied=copied,
160:             errors=errors
161:         )
162: 
163:     def validate(self, blended_content: Any, context: Dict[str, Any]) -> bool:
164:         """
165:         Validate blended memories result.
166: 
167:         Args:
168:             blended_content: BlendResult from blend()
169:             context: Additional context
170: 
171:         Returns:
172:             True if valid (success=True, no errors), False otherwise
173:         """
174:         if not isinstance(blended_content, BlendResult):
175:             return False
176: 
177:         return blended_content.success and len(blended_content.errors) == 0
178: 
179:     def _process_memory(
180:         self,
181:         fw_file: Path,
182:         target_file: Optional[Path],
183:         output_file: Path
184:     ) -> Dict[str, str]:
185:         """
186:         Process single memory file.
187: 
188:         Returns:
189:             Dict with "action": "skip"|"merge"|"copy"
190:         """
191:         # Critical memory: always use framework
192:         if fw_file.name in CRITICAL_MEMORIES:
193:             logger.info(f"Critical memory: {fw_file.name} (using framework)")
194:             self._copy_file(fw_file, output_file)
195:             return {"action": "copy"}
196: 
197:         # No target file: copy framework
198:         if target_file is None or not target_file.exists():
199:             logger.info(f"No target version: {fw_file.name} (using framework)")
200:             self._copy_file(fw_file, output_file)
201:             return {"action": "copy"}
202: 
203:         # Both exist: check similarity with Haiku
204:         fw_content = fw_file.read_text()
205:         target_content = target_file.read_text()
206: 
207:         similarity_result = self.llm_client.check_similarity(
208:             text1=fw_content,
209:             text2=target_content,
210:             threshold=0.9
211:         )
212: 
213:         similarity_score = similarity_result["score"]
214:         logger.info(f"Similarity for {fw_file.name}: {similarity_score:.0%}")
215: 
216:         # >90% similar: skip target, use framework
217:         if similarity_result["similar"]:
218:             logger.info(f"High similarity ({similarity_score:.0%}): using framework")
219:             self._copy_file(fw_file, output_file)
220:             return {"action": "skip"}
221: 
222:         # Check if contradicting (using Haiku)
223:         is_contradicting = self._check_contradiction(fw_content, target_content)
224: 
225:         if is_contradicting:
226:             logger.warning(f"Contradicting content: {fw_file.name} (using framework)")
227:             self._copy_file(fw_file, output_file)
228:             return {"action": "skip"}
229: 
230:         # Complementary: merge with Sonnet
231:         logger.info(f"Complementary content: merging with Sonnet")
232:         merged_content = self._merge_with_sonnet(fw_content, target_content, fw_file.name)
233: 
234:         # Blend YAML headers
235:         fw_header, fw_body = self._parse_memory(fw_content)
236:         target_header, target_body = self._parse_memory(target_content)
237:         blended_header = self._blend_headers(fw_header, target_header)
238: 
239:         # Write merged file
240:         final_content = self._format_memory(blended_header, merged_content)
241:         output_file.write_text(final_content)
242: 
243:         return {"action": "merge"}
244: 
245:     def _check_contradiction(self, fw_content: str, target_content: str) -> bool:
246:         """
247:         Check if contents contradict using simple heuristic.
248: 
249:         Since BlendingLLM doesn't have check_contradiction method,
250:         we use a simple heuristic: if similarity is very low (<0.3),
251:         assume potential contradiction.
252: 
253:         Args:
254:             fw_content: Framework content
255:             target_content: Target content
256: 
257:         Returns:
258:             True if contradicting, False otherwise
259:         """
260:         # Simple heuristic: very low similarity suggests contradiction
261:         # In production, this could call Haiku with specific contradiction prompt
262:         similarity = self.llm_client.check_similarity(
263:             text1=fw_content,
264:             text2=target_content,
265:             threshold=0.3
266:         )
267: 
268:         # If similarity < 0.3, consider it potentially contradicting
269:         return similarity["score"] < 0.3
270: 
271:     def _merge_with_sonnet(
272:         self,
273:         fw_content: str,
274:         target_content: str,
275:         filename: str
276:     ) -> str:
277:         """
278:         Merge complementary content using Sonnet.
279: 
280:         Args:
281:             fw_content: Framework memory content (with YAML)
282:             target_content: Target memory content (with YAML)
283:             filename: Memory filename for context
284: 
285:         Returns:
286:             Merged content (body only, no YAML)
287:         """
288:         # Extract bodies (remove YAML)
289:         _, fw_body = self._parse_memory(fw_content)
290:         _, target_body = self._parse_memory(target_content)
291: 
292:         # Use BlendingLLM.blend_content() for merging
293:         result = self.llm_client.blend_content(
294:             framework_content=fw_body,
295:             target_content=target_body,
296:             rules_content=None,
297:             domain=f"memory:{filename}"
298:         )
299: 
300:         return result["blended"]
301: 
302:     def _blend_headers(
303:         self,
304:         fw_header: Dict[str, Any],
305:         target_header: Dict[str, Any]
306:     ) -> Dict[str, Any]:
307:         """
308:         Blend YAML headers.
309: 
310:         Rules:
311:         - type: framework wins
312:         - category: framework wins
313:         - tags: merge both lists (deduplicate)
314:         - created: use earlier date
315:         - updated: use later date
316: 
317:         Args:
318:             fw_header: Framework YAML header
319:             target_header: Target YAML header
320: 
321:         Returns:
322:             Blended YAML header
323:         """
324:         blended = fw_header.copy()
325: 
326:         # Merge tags
327:         fw_tags = set(fw_header.get("tags", []))
328:         target_tags = set(target_header.get("tags", []))
329:         blended["tags"] = sorted(fw_tags | target_tags)
330: 
331:         # Use earlier created date
332:         fw_created = fw_header.get("created", "")
333:         target_created = target_header.get("created", "")
334:         if target_created and target_created < fw_created:
335:             blended["created"] = target_created
336: 
337:         # Use later updated date
338:         fw_updated = fw_header.get("updated", "")
339:         target_updated = target_header.get("updated", "")
340:         if target_updated and target_updated > fw_updated:
341:             blended["updated"] = target_updated
342: 
343:         return blended
344: 
345:     def _preserve_target_memory(
346:         self,
347:         target_file: Path,
348:         output_file: Path
349:     ) -> None:
350:         """
351:         Preserve target-only memory with type: user header.
352: 
353:         Args:
354:             target_file: Path to target memory file
355:             output_file: Path to output file
356:         """
357:         content = target_file.read_text()
358:         header, body = self._parse_memory(content)
359: 
360:         # Set type: user
361:         header["type"] = "user"
362:         header["source"] = "target-project"
363: 
364:         # Write with updated header
365:         final_content = self._format_memory(header, body)
366:         output_file.write_text(final_content)
367: 
368:         logger.info(f"Preserved target-only: {target_file.name} (type: user)")
369: 
370:     def _parse_memory(self, content: str) -> tuple[Dict[str, Any], str]:
371:         """
372:         Parse memory file into YAML header and body.
373: 
374:         Args:
375:             content: Full memory file content
376: 
377:         Returns:
378:             Tuple of (header dict, body string)
379: 
380:         Raises:
381:             ValueError: If YAML parse fails or invalid structure
382:         """
383:         if not content.startswith("---\n"):
384:             raise ValueError("Memory file must start with YAML frontmatter (---)\nüîß Troubleshooting: Check inputs and system state")
385: 
386:         parts = content.split("---", 2)
387:         if len(parts) < 3:
388:             raise ValueError("Missing closing --- delimiter for YAML header\nüîß Troubleshooting: Check inputs and system state")
389: 
390:         yaml_content = parts[1].strip()
391:         body = parts[2].strip()
392: 
393:         try:
394:             header = yaml.safe_load(yaml_content)
395:         except yaml.YAMLError as e:
396:             raise ValueError(f"YAML parse error: {e}\nüîß Troubleshooting: Check inputs and system state")
397: 
398:         return header, body
399: 
400:     def _format_memory(self, header: Dict[str, Any], body: str) -> str:
401:         """
402:         Format memory with YAML header and body.
403: 
404:         Args:
405:             header: YAML header dict
406:             body: Memory body content
407: 
408:         Returns:
409:             Formatted memory file content
410:         """
411:         yaml_str = yaml.dump(header, default_flow_style=False, sort_keys=False)
412:         return f"---\n{yaml_str}---\n\n{body}\n"
413: 
414:     def _copy_file(self, src: Path, dst: Path) -> None:
415:         """
416:         Copy file from src to dst.
417: 
418:         Args:
419:             src: Source file path
420:             dst: Destination file path
421:         """
422:         dst.write_text(src.read_text())
423: 
424:     def _list_memory_files(self, path: Path) -> List[str]:
425:         """
426:         List all .md files in memory directory.
427: 
428:         Args:
429:             path: Path to memory directory
430: 
431:         Returns:
432:             List of filenames (not paths)
433:         """
434:         if not path or not path.exists():
435:             return []
436: 
437:         return [f.name for f in path.glob("*.md") if f.is_file() and not f.is_symlink()]
</file>

<file path="tools/ce/blending/__init__.py">
 1: """Blending framework for CE initialization."""
 2: 
 3: from .core import backup_context, BlendingOrchestrator
 4: from .detection import LegacyFileDetector
 5: from .llm_client import BlendingLLM
 6: from .validation import validate_all_domains
 7: from .strategies import (
 8:     SettingsBlendStrategy,
 9:     ClaudeMdBlendStrategy,
10:     MemoriesBlendStrategy,
11:     ExamplesBlendStrategy,
12:     PRPMoveStrategy,
13:     CommandOverwriteStrategy
14: )
15: 
16: __all__ = [
17:     'backup_context',
18:     'BlendingOrchestrator',
19:     'LegacyFileDetector',
20:     'BlendingLLM',
21:     'validate_all_domains',
22:     'SettingsBlendStrategy',
23:     'ClaudeMdBlendStrategy',
24:     'MemoriesBlendStrategy',
25:     'ExamplesBlendStrategy',
26:     'PRPMoveStrategy',
27:     'CommandOverwriteStrategy'
28: ]
</file>

<file path="examples/INITIALIZATION.md">
   1: # CE 1.1 Framework Initialization Guide
   2: 
   3: **Purpose**: Complete guide for installing and configuring the Context Engineering (CE) 1.1 framework across all project scenarios
   4: 
   5: **Version**: 1.1
   6: 
   7: **Last Updated**: 2025-11-04
   8: 
   9: **NOTE (2025-11-04)**: Historical references to `.ce/examples/system/` directory in this guide refer to a planned structure that was consolidated. All framework examples now reside in `examples/` directory. See PRP-32.2.1 for details on the consolidation.
  10: 
  11: ---
  12: 
  13: ## Overview
  14: 
  15: This guide covers the complete process of installing the Context Engineering framework into target projects using a unified 5-phase workflow. The process adapts to your specific scenario while maintaining consistent structure and validation.
  16: 
  17: ### What is CE 1.1?
  18: 
  19: Context Engineering is a framework for managing project context, documentation, and workflows through:
  20: 
  21: - **Structured knowledge base** (Serena memories)
  22: - **Automated workflows** (slash commands)
  23: - **PRP-based development** (Plan-Review-Produce pattern)
  24: - **Tool integration** (Syntropy MCP, Linear, UV)
  25: - **System/user separation** (framework vs project docs)
  26: 
  27: ### Key Benefits
  28: 
  29: - **Zero noise**: Clean separation of framework and project files
  30: - **Consistent validation**: Multi-level validation gates
  31: - **Automated workflows**: 11 framework commands for common tasks
  32: - **Knowledge persistence**: 23 framework memories + your project memories
  33: - **Easy upgrades**: System docs updated independently
  34: 
  35: ---
  36: 
  37: ## Quick Start: Choose Your Scenario
  38: 
  39: Use this decision tree to identify your installation scenario:
  40: 
  41: ```
  42: START: What's your project's current state?
  43: ‚îÇ
  44: ‚îú‚îÄ NO CE components exist
  45: ‚îÇ  ‚îÇ
  46: ‚îÇ  ‚îú‚îÄ New project (empty or minimal code)
  47: ‚îÇ  ‚îÇ  ‚îî‚îÄ‚Üí SCENARIO 1: Greenfield (~10 min)
  48: ‚îÇ  ‚îÇ
  49: ‚îÇ  ‚îî‚îÄ Existing codebase with working code
  50: ‚îÇ     ‚îî‚îÄ‚Üí SCENARIO 2: Mature Project (~45 min)
  51: ‚îÇ
  52: ‚îî‚îÄ Has CE components
  53:    ‚îÇ
  54:    ‚îú‚îÄ Has .ce/ directory (CE 1.1)
  55:    ‚îÇ  ‚îî‚îÄ‚Üí SCENARIO 4: Partial Install (~15 min)
  56:    ‚îÇ
  57:    ‚îî‚îÄ No .ce/ directory (CE 1.0 or legacy)
  58:       ‚îî‚îÄ‚Üí SCENARIO 3: CE 1.0 Upgrade (~40 min)
  59: ```
  60: 
  61: ### Scenario Descriptions
  62: 
  63: | Scenario | When to Use | Phases | Time |
  64: |----------|-------------|--------|------|
  65: | **Greenfield** | New project, no CE components | 1, 3, 4 (skip 2, 5) | 10 min |
  66: | **Mature Project** | Existing code, no CE | 1, 2, 3, 4 (skip 5) | 45 min |
  67: | **CE 1.0 Upgrade** | Legacy CE installation | 1, 2, 3, 4, 5 (all) | 40 min |
  68: | **Partial Install** | Missing CE components | 1, 3, 4 (selective) | 15 min |
  69: 
  70: ---
  71: 
  72: ## Quick Start: Automated Installation (RECOMMENDED)
  73: 
  74: **Time**: <5 minutes | **Recommended for**: All scenarios
  75: 
  76: Use Syntropy MCP for one-command CE framework installation:
  77: 
  78: ```bash
  79: npx syntropy-mcp init ce-framework
  80: ```
  81: 
  82: This command automatically:
  83: 1. **Extracts unpacker tool** (`tools/ce/repomix_unpack.py` + `tools/pyproject.toml`)
  84: 2. **Installs UV package manager** (if not already installed)
  85: 3. **Extracts infrastructure package** (50 files):
  86:    - 23 framework memories (`.serena/memories/*.md`)
  87:    - 11 framework commands (`.claude/commands/*.md`)
  88:    - 27 CE tools (`tools/ce/*.py`)
  89:    - PRP-0 documentation (`.ce/PRPs/executed/system/PRP-0-CONTEXT-ENGINEERING.md`)
  90:    - Framework settings (`.claude/settings.local.json`)
  91: 4. **Reorganizes tools** to `.ce/tools/` directory
  92: 5. **Copies workflow docs** to `.ce/examples/ce-workflow-docs.xml`
  93: 6. **Blends settings.local.json** with existing project settings (if any)
  94: 7. **Verifies installation** (checks critical files exist)
  95: 
  96: ### What You Get
  97: 
  98: After automated installation:
  99: - ‚úÖ Complete CE 1.1 framework installed
 100: - ‚úÖ All tools available at `.ce/tools/`
 101: - ‚úÖ Framework commands ready to use
 102: - ‚úÖ Settings blended with project defaults
 103: - ‚úÖ No manual file copying needed
 104: 
 105: ### Next Steps
 106: 
 107: 1. **Verify installation**:
 108:    ```bash
 109:    ls .ce/tools/ce/repomix_unpack.py
 110:    ls .claude/commands/generate-prp.md
 111:    ls .ce/PRPs/executed/system/PRP-0-CONTEXT-ENGINEERING.md
 112:    ```
 113: 
 114: 2. **Install CE CLI tools** (optional but recommended):
 115:    ```bash
 116:    cd .ce/tools
 117:    uv sync
 118:    ```
 119: 
 120: 3. **Test a command**:
 121:    ```bash
 122:    /generate-prp   # Create your first PRP
 123:    ```
 124: 
 125: 4. **Review PRP-0** for complete framework documentation:
 126:    ```bash
 127:    cat .ce/PRPs/executed/system/PRP-0-CONTEXT-ENGINEERING.md
 128:    ```
 129: 
 130: ### Settings Blending
 131: 
 132: The automated init intelligently blends CE framework settings with your project's existing `.claude/settings.local.json`:
 133: 
 134: **Blending Rules**:
 135: 1. **Deny list takes precedence**: If CE denies a tool, it's removed from your project's allow list
 136: 2. **Merge lists**: CE entries added to corresponding allow/deny/ask lists (deduplicated)
 137: 3. **CE list wins**: CE entries only appear in one list (their designated CE list)
 138: 
 139: **Example**:
 140: ```json
 141: // Your project allows filesystem_read_file
 142: // CE denies filesystem_read_file (prefer native Read tool)
 143: // Result: Removed from allow list, added to deny list
 144: ```
 145: 
 146: ---
 147: 
 148: ## Manual Installation (Fallback)
 149: 
 150: **Time**: 10-45 minutes depending on scenario | **Use when**: Syntropy MCP not available
 151: 
 152: If you cannot use the automated installation, follow the manual 5-phase workflow below.
 153: 
 154: ---
 155: 
 156: ## Prerequisites
 157: 
 158: ### Required for All Scenarios
 159: 
 160: - **Repomix CLI**: `npm install -g repomix`
 161: - **CE framework packages**:
 162:   - `ce-infrastructure.xml` (complete framework)
 163:   - `ce-workflow-docs.xml` (reference documentation)
 164: - **Git repository**: Project initialized with git
 165: - **Project directory**: Write access to target directory
 166: 
 167: ### Optional (Recommended)
 168: 
 169: - **Backup branch**: `git checkout -b pre-ce-backup`
 170: - **Linear account**: For issue tracking integration
 171: - **Syntropy MCP**: For tool integration
 172: - **UV package manager**: For CE CLI tools
 173: 
 174: ### Before You Begin
 175: 
 176: ```bash
 177: # Verify prerequisites
 178: which repomix && echo "‚úì Repomix installed"
 179: which git && echo "‚úì Git installed"
 180: test -f ce-infrastructure.xml && echo "‚úì Framework package available"
 181: 
 182: # Create backup (recommended)
 183: git checkout -b pre-ce-backup
 184: git push origin pre-ce-backup
 185: git checkout main
 186: ```
 187: 
 188: ---
 189: 
 190: ## The 5-Phase Workflow
 191: 
 192: ### Phase Overview
 193: 
 194: 1. **Bucket Collection** (5-10 min) - Stage files for validation
 195: 2. **User Files Copy** (0-15 min) - Migrate project-specific files
 196: 3. **Repomix Package Handling** (5 min) - Extract framework packages
 197: 4. **CLAUDE.md Blending** (10 min) - Merge framework + project guide
 198: 5. **Legacy Cleanup** (0-5 min) - Remove duplicate legacy files
 199: 
 200: **Note**: Not all scenarios execute all phases. See scenario-specific instructions below.
 201: 
 202: ---
 203: 
 204: ## Phase 1: Bucket Collection (Universal)
 205: 
 206: **Duration**: 5-10 minutes
 207: **Applies to**: All scenarios
 208: 
 209: ### Purpose
 210: 
 211: Create a staging area to organize and validate files before copying to CE 1.1 destinations.
 212: 
 213: ### Step 1.1: Create Staging Area
 214: 
 215: ```bash
 216: # Create bucket directories
 217: mkdir -p tmp/syntropy-initialization/{serena,examples,prps,claude-md,claude-dir}
 218: 
 219: # Verify structure
 220: ls -d tmp/syntropy-initialization/*/
 221: # Expected:
 222: # tmp/syntropy-initialization/serena/
 223: # tmp/syntropy-initialization/examples/
 224: # tmp/syntropy-initialization/prps/
 225: # tmp/syntropy-initialization/claude-md/
 226: # tmp/syntropy-initialization/claude-dir/
 227: ```
 228: 
 229: ### Step 1.2: Copy Files to Buckets
 230: 
 231: **Bucket 1: Serena Memories**
 232: 
 233: ```bash
 234: # Copy existing Serena memories (if any)
 235: if [ -d .serena/memories ]; then
 236:   cp -R .serena/memories/*.md tmp/syntropy-initialization/serena/ 2>/dev/null || true
 237:   echo "Serena files: $(ls tmp/syntropy-initialization/serena/*.md 2>/dev/null | wc -l)"
 238: fi
 239: ```
 240: 
 241: **Bucket 2: Examples**
 242: 
 243: ```bash
 244: # Copy existing examples (if any)
 245: if [ -d examples ]; then
 246:   find examples -name "*.md" -exec cp {} tmp/syntropy-initialization/examples/ \; 2>/dev/null || true
 247:   echo "Example files: $(ls tmp/syntropy-initialization/examples/*.md 2>/dev/null | wc -l)"
 248: fi
 249: ```
 250: 
 251: **Bucket 3: PRPs**
 252: 
 253: ```bash
 254: # Copy existing PRPs (if any)
 255: if [ -d PRPs ]; then
 256:   find PRPs -name "*.md" -exec cp {} tmp/syntropy-initialization/prps/ \; 2>/dev/null || true
 257:   echo "PRP files: $(ls tmp/syntropy-initialization/prps/*.md 2>/dev/null | wc -l)"
 258: fi
 259: ```
 260: 
 261: **Bucket 4: CLAUDE.md**
 262: 
 263: ```bash
 264: # Copy existing CLAUDE.md (if any)
 265: if [ -f CLAUDE.md ]; then
 266:   cp CLAUDE.md tmp/syntropy-initialization/claude-md/
 267:   echo "‚úì CLAUDE.md copied"
 268: fi
 269: ```
 270: 
 271: **Bucket 5: Claude Directory**
 272: 
 273: ```bash
 274: # Copy existing .claude directory (if any)
 275: if [ -d .claude ]; then
 276:   cp -R .claude/* tmp/syntropy-initialization/claude-dir/ 2>/dev/null || true
 277:   echo "Claude files: $(ls tmp/syntropy-initialization/claude-dir/ 2>/dev/null | wc -l)"
 278: fi
 279: ```
 280: 
 281: ### Step 1.3: Validate Bucket Contents
 282: 
 283: Review files in each bucket to verify they match bucket characteristics:
 284: 
 285: **Serena Bucket Validation**
 286: 
 287: ```bash
 288: cd tmp/syntropy-initialization/serena/
 289: 
 290: for file in *.md 2>/dev/null; do
 291:   # Check for memory-like content
 292:   if ! grep -qi "memory\|pattern\|guide" "$file" 2>/dev/null; then
 293:     mv "$file" "$file.fake"
 294:     echo "‚ö† Marked $file as fake (not a memory)"
 295:   fi
 296: done
 297: ```
 298: 
 299: **Examples Bucket Validation**
 300: 
 301: ```bash
 302: cd tmp/syntropy-initialization/examples/
 303: 
 304: for file in *.md 2>/dev/null; do
 305:   # Check for example/pattern structure
 306:   if ! grep -qi "example\|pattern\|workflow\|guide" "$file" 2>/dev/null; then
 307:     mv "$file" "$file.fake"
 308:     echo "‚ö† Marked $file as fake (not an example)"
 309:   fi
 310: done
 311: ```
 312: 
 313: **PRPs Bucket Validation**
 314: 
 315: ```bash
 316: cd tmp/syntropy-initialization/prps/
 317: 
 318: for file in *.md 2>/dev/null; do
 319:   # Check for PRP structure (YAML header or PRP-ID in filename)
 320:   if ! grep -q "^---" "$file" 2>/dev/null && ! echo "$file" | grep -qi "prp-"; then
 321:     mv "$file" "$file.fake"
 322:     echo "‚ö† Marked $file as fake (not a PRP)"
 323:   fi
 324: done
 325: ```
 326: 
 327: ### Step 1.4: Bucket Summary
 328: 
 329: ```bash
 330: # Return to project root
 331: cd /path/to/your/project
 332: 
 333: # Generate bucket report
 334: cat > tmp/syntropy-initialization/bucket-report.txt << 'EOF'
 335: # Bucket Collection Report
 336: 
 337: ## Serena Bucket
 338: Valid: $(ls tmp/syntropy-initialization/serena/*.md 2>/dev/null | grep -v ".fake" | wc -l)
 339: Fake: $(ls tmp/syntropy-initialization/serena/*.fake 2>/dev/null | wc -l)
 340: 
 341: ## Examples Bucket
 342: Valid: $(ls tmp/syntropy-initialization/examples/*.md 2>/dev/null | grep -v ".fake" | wc -l)
 343: Fake: $(ls tmp/syntropy-initialization/examples/*.fake 2>/dev/null | wc -l)
 344: 
 345: ## PRPs Bucket
 346: Valid: $(ls tmp/syntropy-initialization/prps/*.md 2>/dev/null | grep -v ".fake" | wc -l)
 347: Fake: $(ls tmp/syntropy-initialization/prps/*.fake 2>/dev/null | wc -l)
 348: 
 349: ## CLAUDE.md Bucket
 350: Valid: $(ls tmp/syntropy-initialization/claude-md/CLAUDE.md 2>/dev/null | wc -l)
 351: 
 352: ## Claude Dir Bucket
 353: Files: $(ls tmp/syntropy-initialization/claude-dir/ 2>/dev/null | wc -l)
 354: EOF
 355: 
 356: # Display report
 357: cat tmp/syntropy-initialization/bucket-report.txt
 358: ```
 359: 
 360: **Phase 1 Complete**: Files staged and validated. Proceed to Phase 2 (or skip if Greenfield).
 361: 
 362: ---
 363: 
 364: ## Phase 2: User Files Copy
 365: 
 366: **Duration**: 0-15 minutes
 367: **Applies to**: Mature Project, CE 1.0 Upgrade, Partial Install
 368: **Skip for**: Greenfield
 369: 
 370: ### Scenario Variations
 371: 
 372: **Skip this phase if**:
 373: - **Greenfield**: No user files exist yet
 374: 
 375: **Full migration if**:
 376: - **Mature Project**: Copy all validated files from buckets
 377: - **CE 1.0 Upgrade**: Copy all validated files + classify existing files
 378: 
 379: **Selective migration if**:
 380: - **Partial Install**: Copy only missing components
 381: 
 382: ### Step 2.1: User Memory Migration
 383: 
 384: **For Mature Project and CE 1.0 Upgrade**:
 385: 
 386: ```bash
 387: # Copy validated user memories (non-.fake files)
 388: find tmp/syntropy-initialization/serena -name "*.md" ! -name "*.fake" -exec cp {} .serena/memories/ \; 2>/dev/null || true
 389: 
 390: # Add YAML headers to memories without them
 391: cd .serena/memories/
 392: 
 393: for memory in *.md 2>/dev/null; do
 394:   # Skip if already has YAML header
 395:   if head -n 1 "$memory" | grep -q "^---"; then
 396:     continue
 397:   fi
 398: 
 399:   # Determine type (heuristic based on content)
 400:   if grep -qi "architecture\|security\|core principle\|critical" "$memory"; then
 401:     TYPE="critical"
 402:   else
 403:     TYPE="regular"
 404:   fi
 405: 
 406:   # Add YAML header for user memory
 407:   cat > "${memory}.tmp" << EOF
 408: ---
 409: type: user
 410: source: target-project
 411: created: "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
 412: updated: "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
 413: ---
 414: 
 415: $(cat "$memory")
 416: EOF
 417:   mv "${memory}.tmp" "$memory"
 418:   echo "‚úì Added YAML header to $memory (type: $TYPE)"
 419: done
 420: 
 421: cd ../..
 422: ```
 423: 
 424: ### Step 2.2: User PRP Migration
 425: 
 426: **For Mature Project and CE 1.0 Upgrade**:
 427: 
 428: ```bash
 429: # Create PRP directories if needed
 430: mkdir -p .ce/PRPs/{executed,feature-requests,archived}
 431: 
 432: # Copy validated user PRPs (non-.fake files)
 433: find tmp/syntropy-initialization/prps -name "*.md" ! -name "*.fake" -exec sh -c '
 434:   for prp; do
 435:     # Determine destination based on filename or content
 436:     if echo "$prp" | grep -qi "executed"; then
 437:       cp "$prp" .ce/PRPs/executed/
 438:     elif echo "$prp" | grep -qi "feature\|request"; then
 439:       cp "$prp" .ce/PRPs/feature-requests/
 440:     else
 441:       cp "$prp" .ce/PRPs/executed/
 442:     fi
 443:   done
 444: ' sh {} +
 445: 
 446: # Add YAML headers to user PRPs (if missing)
 447: cd .ce/PRPs/executed/
 448: 
 449: for prp in *.md 2>/dev/null; do
 450:   # Skip if already has YAML header
 451:   if head -n 1 "$prp" | grep -q "^---"; then
 452:     continue
 453:   fi
 454: 
 455:   # Extract PRP ID from filename (e.g., USER-001 from USER-001-feature.md)
 456:   PRP_ID=$(echo "$prp" | sed 's/\.md$//' | cut -d'-' -f1-2)
 457:   TITLE=$(echo "$prp" | sed 's/\.md$//' | sed 's/^[^-]*-[^-]*-//' | tr '-' ' ')
 458: 
 459:   # Add YAML header for user PRP
 460:   cat > "${prp}.tmp" << EOF
 461: ---
 462: prp_id: $PRP_ID
 463: title: $TITLE
 464: status: completed
 465: created: "$(date -u +%Y-%m-%d)"
 466: source: target-project
 467: type: user
 468: ---
 469: 
 470: $(cat "$prp")
 471: EOF
 472:   mv "${prp}.tmp" "$prp"
 473:   echo "‚úì Added YAML header to $prp"
 474: done
 475: 
 476: cd ../../..
 477: echo "‚úì User PRPs migrated to .ce/PRPs/"
 478: ```
 479: 
 480: **User PRP YAML Header Format**:
 481: 
 482: ```yaml
 483: ---
 484: prp_id: USER-001
 485: title: User Feature Implementation
 486: status: completed
 487: created: "2025-11-04"
 488: source: target-project
 489: type: user
 490: ---
 491: ```
 492: 
 493: **Notes**:
 494: - `type: user` distinguishes user PRPs from framework PRPs
 495: - `source: target-project` indicates origin (vs. framework source: ctx-eng-plus)
 496: - Framework PRPs use `type: regular` or `type: critical`
 497: 
 498: ### Step 2.3: User Examples Migration
 499: 
 500: **For Mature Project and CE 1.0 Upgrade**:
 501: 
 502: ```bash
 503: # Create examples directory if needed
 504: mkdir -p .ce/examples
 505: 
 506: # Copy validated user examples (non-.fake files)
 507: find tmp/syntropy-initialization/examples -name "*.md" ! -name "*.fake" -exec cp {} .ce/examples/ \; 2>/dev/null || true
 508: 
 509: echo "‚úì User examples migrated to .ce/examples/"
 510: ```
 511: 
 512: ### Step 2.4: User Commands/Settings Migration
 513: 
 514: **For all scenarios with existing .claude/ content**:
 515: 
 516: ```bash
 517: # Copy custom commands (preserve existing)
 518: if [ -d tmp/syntropy-initialization/claude-dir/commands ]; then
 519:   mkdir -p .claude/commands
 520:   cp -n tmp/syntropy-initialization/claude-dir/commands/*.md .claude/commands/ 2>/dev/null || true
 521:   echo "‚úì User commands preserved"
 522: fi
 523: 
 524: # Backup existing settings (will be merged in Phase 3)
 525: if [ -f tmp/syntropy-initialization/claude-dir/settings.local.json ]; then
 526:   mkdir -p .claude
 527:   cp tmp/syntropy-initialization/claude-dir/settings.local.json .claude/settings.pre-ce.json
 528:   echo "‚úì Existing settings backed up"
 529: fi
 530: ```
 531: 
 532: ### Step 2.5: Migration Summary
 533: 
 534: ```bash
 535: # Generate migration report
 536: cat > tmp/syntropy-initialization/phase2-report.txt << EOF
 537: # Phase 2: User Files Migration Report
 538: 
 539: ## Memories Migrated
 540: User memories: $(find .serena/memories -maxdepth 1 -name "*.md" 2>/dev/null | wc -l)
 541: Headers added (type: user): $(grep -l "^type: user" .serena/memories/*.md 2>/dev/null | wc -l)
 542: 
 543: ## PRPs Migrated
 544: Executed: $(ls .ce/PRPs/executed/*.md 2>/dev/null | wc -l)
 545: Feature requests: $(ls .ce/PRPs/feature-requests/*.md 2>/dev/null | wc -l)
 546: Headers added (type: user): $(grep -l "^type: user" .ce/PRPs/executed/*.md 2>/dev/null | wc -l)
 547: 
 548: ## Examples Migrated
 549: User examples: $(ls .ce/examples/*.md 2>/dev/null | wc -l)
 550: 
 551: ## Commands/Settings
 552: Custom commands: $(ls .claude/commands/*.md 2>/dev/null | wc -l)
 553: Settings backed up: $(test -f .claude/settings.pre-ce.json && echo "yes" || echo "no")
 554: 
 555: ## YAML Header Summary
 556: - User memories: type: user, source: target-project
 557: - User PRPs: type: user, source: target-project, prp_id, title, status
 558: - Framework memories: type: regular (default) or type: critical (upgraded manually)
 559: - Framework PRPs: Standard PRP YAML with batch_id, stage, order
 560: EOF
 561: 
 562: cat tmp/syntropy-initialization/phase2-report.txt
 563: ```
 564: 
 565: **Phase 2 Complete**: User files migrated with YAML headers (`type: user`). Framework files use `type: regular` or `type: critical`. Proceed to Phase 3.
 566: 
 567: ---
 568: 
 569: ## Phase 3: Repomix Package Handling (Universal)
 570: 
 571: **Duration**: 5 minutes
 572: **Applies to**: All scenarios
 573: 
 574: **Note**: If you used automated installation (`npx syntropy-mcp init ce-framework`), this phase is already complete. Skip to Phase 4.
 575: 
 576: ### Purpose
 577: 
 578: Extract framework packages to install system documentation, memories, examples, commands, and tools.
 579: 
 580: ### Step 3.1: Copy Workflow Package
 581: 
 582: ```bash
 583: # Create system directory
 584: mkdir -p .ce/examples/system/
 585: 
 586: # Copy workflow documentation package (reference only, not extracted)
 587: cp ce-workflow-docs.xml .ce/examples/system/
 588: 
 589: echo "‚úì Workflow package copied to .ce/examples/system/ce-workflow-docs.xml"
 590: ```
 591: 
 592: **Note**: `ce-workflow-docs.xml` is stored as-is for reference and redistribution. The actual framework files come from `ce-infrastructure.xml` extraction below.
 593: 
 594: ### Step 3.2: Extract Infrastructure Package
 595: 
 596: ```bash
 597: # Extract complete framework infrastructure
 598: repomix --unpack ce-infrastructure.xml --target ./
 599: 
 600: # Reorganize tools to .ce/tools/ (framework convention)
 601: mkdir -p .ce/tools
 602: mv tools/* .ce/tools/
 603: rmdir tools
 604: 
 605: # Verify extraction
 606: echo "Framework files extracted:"
 607: echo "  PRP-0 template: $(test -f .ce/PRPs/executed/system/PRP-0-CONTEXT-ENGINEERING.md && echo '‚úì' || echo '‚úó')"
 608: echo "  System memories: $(ls .serena/memories/system/*.md 2>/dev/null | wc -l)"
 609: echo "  Framework commands: $(ls .claude/commands/*.md 2>/dev/null | wc -l)"
 610: echo "  Tool files: $(find .ce/tools -name "*.py" 2>/dev/null | wc -l)"
 611: ```
 612: 
 613: **What gets extracted**:
 614: 
 615: ```
 616: .ce/PRPs/executed/system/     # PRP-0 template (framework initialization)
 617: .serena/memories/system/      # 23 framework memories (6 critical + 17 regular)
 618: .claude/commands/             # 11 framework commands
 619: .claude/settings.local.json   # Framework settings (merged with existing)
 620: .ce/tools/                    # 33 tool source files
 621: CLAUDE.md                     # Framework sections (merged with existing)
 622: ```
 623: 
 624: **Note**: Examples are NOT extracted from infrastructure. They are in `ce-workflow-docs.xml` (copied above for reference).
 625: 
 626: ### Step 3.3: Validate Framework Installation
 627: 
 628: ```bash
 629: # Check system memories (expected: 23)
 630: SYSTEM_MEMORIES=$(ls .serena/memories/system/*.md 2>/dev/null | wc -l)
 631: test $SYSTEM_MEMORIES -eq 23 && echo "‚úì All 23 system memories installed" || echo "‚ö† Only $SYSTEM_MEMORIES system memories found"
 632: 
 633: # Check system examples (expected: 21)
 634: SYSTEM_EXAMPLES=$(ls .ce/examples/system/*.md 2>/dev/null | wc -l)
 635: test $SYSTEM_EXAMPLES -eq 21 && echo "‚úì All 21 system examples installed" || echo "‚ö† Only $SYSTEM_EXAMPLES system examples found"
 636: 
 637: # Check framework commands (expected: 11)
 638: FRAMEWORK_COMMANDS=$(ls .claude/commands/*.md 2>/dev/null | wc -l)
 639: test $FRAMEWORK_COMMANDS -ge 11 && echo "‚úì All 11 framework commands installed" || echo "‚ö† Only $FRAMEWORK_COMMANDS commands found"
 640: 
 641: # Check tool files (expected: 33)
 642: TOOL_FILES=$(find .ce/tools -name "*.py" 2>/dev/null | wc -l)
 643: test $TOOL_FILES -ge 33 && echo "‚úì Tool source files installed" || echo "‚ö† Only $TOOL_FILES tool files found"
 644: 
 645: # Check settings merged
 646: test -f .claude/settings.local.json && jq empty .claude/settings.local.json && echo "‚úì Settings valid JSON" || echo "‚ö† Settings invalid"
 647: ```
 648: 
 649: ### Step 3.4: Initialize CE Tools
 650: 
 651: ```bash
 652: # Install CE CLI tools
 653: cd .ce/tools
 654: ./bootstrap.sh
 655: 
 656: # Verify installation
 657: uv run ce --version
 658: # Expected: ce version 1.1.0
 659: 
 660: # Run basic validation
 661: uv run ce validate --level 1
 662: # Expected: Structure validation passes
 663: 
 664: cd ..
 665: ```
 666: 
 667: ### Step 3.5: Automated Blending (Optional)
 668: 
 669: **Note**: This step is optional. Use the blend tool for automated file merging, or proceed to Phase 4 for manual CLAUDE.md blending.
 670: 
 671: ```bash
 672: # Return to project root
 673: cd /path/to/your/project
 674: 
 675: # Run automated blend (all domains)
 676: uv run -C .ce/tools ce blend --all
 677: 
 678: # Or run in dry-run mode first (recommended)
 679: uv run -C .ce/tools ce blend --all --dry-run
 680: 
 681: # Or run specific phases
 682: uv run -C .ce/tools ce blend --phase detect    # Scan for files
 683: uv run -C .ce/tools ce blend --phase classify  # Validate patterns
 684: uv run -C .ce/tools ce blend --phase blend     # Merge content
 685: uv run -C .ce/tools ce blend --phase cleanup   # Remove legacy dirs
 686: ```
 687: 
 688: **Blend Tool Flags**:
 689: 
 690: - `--all`: Run all 4 phases (detect, classify, blend, cleanup)
 691: - `--dry-run`: Show what would be done without executing
 692: - `--interactive`: Prompt before each phase
 693: - `--fast`: Fast mode (skip expensive validations)
 694: - `--quality`: Quality mode (use Sonnet for all LLM calls)
 695: - `--skip-cleanup`: Skip Phase D (keep legacy directories)
 696: - `--scan`: Scan mode (detect + classify only, no blending)
 697: 
 698: **What it blends**:
 699: 
 700: 1. **Settings** (`.claude/settings.local.json`): Merge CE permissions with target settings
 701: 2. **CLAUDE.md**: Merge framework sections with target project sections
 702: 3. **Memories** (`.serena/memories/`): Move user memories with deduplication
 703: 4. **Examples** (`.ce/examples/`): Copy user examples with type headers
 704: 5. **PRPs**: Move user PRPs to `.ce/PRPs/` with type headers
 705: 6. **Commands** (`.claude/commands/`): Framework commands overwrite user versions (backups created)
 706: 
 707: **Example Output**:
 708: 
 709: ```
 710: üîÄ Running Phase: DETECT
 711: ‚úì Detected 42 files across 6 domains
 712:   prps: 12 files
 713:   memories: 8 files
 714:   examples: 15 files
 715:   claude_md: 1 file
 716:   settings: 1 file
 717:   commands: 5 files
 718: 
 719: üîÄ Running Phase: CLASSIFY
 720: ‚úì Classified 38 valid files
 721:   Filtered garbage: 4 files
 722: 
 723: üîÄ Running Phase: BLEND
 724:   Blending settings (1 files)...
 725:   Blending claude_md (1 files)...
 726:   Blending memories (8 files)...
 727:   Blending examples (15 files)...
 728:   Blending prps (12 files)...
 729:   Blending commands (5 files)...
 730: ‚úì Blending complete (6 domains processed)
 731: 
 732: üîÄ Running Phase: CLEANUP
 733: ‚úì Cleanup complete (3/3 directories removed)
 734: 
 735: ‚úÖ Blending complete!
 736: ```
 737: 
 738: **Troubleshooting**:
 739: 
 740: - **Error: "Config file not found"**: Create `.ce/blend-config.yml` (see PRP-34.1.1)
 741: - **Error: "No detected files"**: Run `--phase detect` first to scan project
 742: - **Error: "Target directory not found"**: Provide valid `--target-dir` path
 743: - **Settings conflict**: Use `--interactive` to review conflicts before merge
 744: 
 745: **Phase 3 Complete**: Framework installed with /system/ organization. Proceed to Phase 4.
 746: 
 747: ---
 748: 
 749: ## Phase 4: CLAUDE.md Blending (Universal)
 750: 
 751: **Duration**: 10 minutes
 752: **Applies to**: All scenarios
 753: 
 754: **Note**: If you used automated installation (`npx syntropy-mcp init ce-framework`), settings.local.json blending is already complete. This phase covers CLAUDE.md blending only.
 755: 
 756: ### Purpose
 757: 
 758: Merge framework CLAUDE.md sections with project-specific sections, creating a single unified project guide.
 759: 
 760: ### Step 4.1: Backup Existing CLAUDE.md
 761: 
 762: ```bash
 763: # Backup user's CLAUDE.md (if exists)
 764: if [ -f CLAUDE.md ]; then
 765:   cp CLAUDE.md "CLAUDE.md.backup-$(date +%Y%m%d-%H%M%S)"
 766:   echo "‚úì CLAUDE.md backed up"
 767: else
 768:   echo "‚Ñπ No existing CLAUDE.md (fresh installation)"
 769: fi
 770: ```
 771: 
 772: ### Step 4.2: Identify Framework vs Project Sections
 773: 
 774: Framework sections (from ce-infrastructure.xml):
 775: - Communication
 776: - Core Principles
 777: - UV Package Management
 778: - Ad-Hoc Code Policy
 779: - Quick Commands
 780: - Tool Naming Convention
 781: - Allowed Tools Summary
 782: - Command Permissions
 783: - Quick Tool Selection
 784: - Project Structure
 785: - Testing Standards
 786: - Code Quality
 787: - Context Commands
 788: - Syntropy MCP Tool Sync
 789: - Linear Integration
 790: - Batch PRP Generation
 791: - PRP Sizing
 792: - Testing Patterns
 793: - Documentation Standards
 794: - Efficient Doc Review
 795: - Resources
 796: - Keyboard Shortcuts
 797: - Git Worktree
 798: - Troubleshooting
 799: 
 800: Project sections (user-defined) are any sections not in framework list.
 801: 
 802: ### Step 4.3: Blend Sections
 803: 
 804: **Manual Blending** (recommended for first installation):
 805: 
 806: ```bash
 807: # Edit CLAUDE.md to mark sections
 808: vim CLAUDE.md
 809: 
 810: # Add [FRAMEWORK] or [PROJECT] markers to section headers:
 811: # ## [FRAMEWORK] Communication
 812: # ## [PROJECT] Project-Specific Communication
 813: # ## [FRAMEWORK] Core Principles
 814: # ## [PROJECT] Team Conventions
 815: ```
 816: 
 817: **Automated Blending** (if framework provides blending tool):
 818: 
 819: ```bash
 820: # Use denoise command to merge sections
 821: /denoise CLAUDE.md
 822: 
 823: # Review merged output
 824: less CLAUDE.md
 825: ```
 826: 
 827: ### Step 4.4: Validate Blended CLAUDE.md
 828: 
 829: ```bash
 830: # Check for framework markers
 831: grep -c "\[FRAMEWORK\]" CLAUDE.md
 832: # Expected: Multiple framework sections marked
 833: 
 834: # Check for project markers (if any)
 835: grep -c "\[PROJECT\]" CLAUDE.md
 836: # Expected: 0+ (depending on scenario)
 837: 
 838: # Verify key framework sections present
 839: for section in "Communication" "Core Principles" "Quick Commands" "Tool Naming Convention"; do
 840:   grep -q "## .*$section" CLAUDE.md && echo "‚úì $section section present" || echo "‚ö† $section section missing"
 841: done
 842: ```
 843: 
 844: ### Step 4.5: Add Project-Specific Sections
 845: 
 846: **For Mature Project and CE 1.0 Upgrade**:
 847: 
 848: Add project-specific sections to CLAUDE.md:
 849: 
 850: ```bash
 851: cat >> CLAUDE.md << 'EOF'
 852: 
 853: ---
 854: 
 855: ## [PROJECT] Project-Specific Information
 856: 
 857: ### Project Structure
 858: 
 859: **Architecture**: [Your architecture description]
 860: 
 861: **Key Components**:
 862: - Component 1: [Description]
 863: - Component 2: [Description]
 864: 
 865: ### Development Workflow
 866: 
 867: **Branch Strategy**: [Your branching model]
 868: 
 869: **Code Review Process**: [Your review process]
 870: 
 871: ### Testing Standards
 872: 
 873: **Test Coverage**: [Your coverage requirements]
 874: 
 875: **Test Frameworks**: [Your test tools]
 876: 
 877: ### Deployment Process
 878: 
 879: **CI/CD Pipeline**: [Your pipeline description]
 880: 
 881: **Deployment Stages**: [Your stages]
 882: 
 883: ---
 884: 
 885: EOF
 886: ```
 887: 
 888: **Phase 4 Complete**: CLAUDE.md blended with framework + project sections. Proceed to Phase 5 (if applicable).
 889: 
 890: ---
 891: 
 892: ## Phase 5: Legacy Cleanup
 893: 
 894: **Duration**: 0-5 minutes
 895: **Applies to**: CE 1.0 Upgrade only
 896: **Skip for**: Greenfield, Mature Project, Partial Install
 897: 
 898: ### Scenario Variations
 899: 
 900: **Skip this phase if**:
 901: - **Greenfield**: No legacy files (new project)
 902: - **Mature Project**: No legacy CE files (first CE installation)
 903: - **Partial Install**: Selective cleanup only
 904: 
 905: **Full cleanup if**:
 906: - **CE 1.0 Upgrade**: Aggressive cleanup of legacy CE 1.0 structure
 907: 
 908: ### Step 5.1: Verify Migration Completed
 909: 
 910: ```bash
 911: # Verify all files migrated to CE 1.1 structure
 912: test -d .ce/PRPs/system && echo "‚úì System PRPs migrated"
 913: test -d .ce/examples/system && echo "‚úì System examples migrated"
 914: test -d .serena/memories/system && echo "‚úì System memories migrated"
 915: 
 916: # Check for errors in migration
 917: if [ -f tmp/syntropy-initialization/phase2-report.txt ]; then
 918:   grep -i error tmp/syntropy-initialization/phase2-report.txt
 919:   # Expected: No errors
 920: fi
 921: ```
 922: 
 923: ### Step 5.2: Archive Legacy Organization
 924: 
 925: ```bash
 926: # Create archive of legacy files before deletion (safety backup)
 927: mkdir -p tmp/syntropy-initialization/legacy-backup/
 928: 
 929: # Archive legacy directories
 930: tar -czf "tmp/syntropy-initialization/legacy-backup/pre-ce-1.1-$(date +%Y%m%d-%H%M%S).tar.gz" \
 931:   PRPs/ \
 932:   examples/ \
 933:   .serena/memories/*.md \
 934:   2>/dev/null || true
 935: 
 936: echo "‚úì Legacy files archived"
 937: 
 938: # Verify archive created
 939: ARCHIVE_FILE=$(ls -t tmp/syntropy-initialization/legacy-backup/*.tar.gz | head -n 1)
 940: test -f "$ARCHIVE_FILE" && echo "‚úì Archive: $ARCHIVE_FILE" || echo "‚ö† Archive not created"
 941: ```
 942: 
 943: ### Step 5.3: Delete Legacy Organization Files
 944: 
 945: **Delete legacy PRPs directory** (now in .ce/PRPs/):
 946: 
 947: ```bash
 948: if [ -d PRPs/ ]; then
 949:   # Count files before deletion
 950:   LEGACY_PRPS=$(find PRPs -name "*.md" | wc -l)
 951: 
 952:   # Delete directory
 953:   rm -rf PRPs/
 954: 
 955:   echo "‚úì Deleted legacy PRPs/ ($LEGACY_PRPS files migrated to .ce/PRPs/)"
 956: fi
 957: ```
 958: 
 959: **Delete legacy examples directory** (now in .ce/examples/):
 960: 
 961: ```bash
 962: if [ -d examples/ ]; then
 963:   # Count files before deletion
 964:   LEGACY_EXAMPLES=$(find examples -name "*.md" | wc -l)
 965: 
 966:   # Delete directory
 967:   rm -rf examples/
 968: 
 969:   echo "‚úì Deleted legacy examples/ ($LEGACY_EXAMPLES files migrated to .ce/examples/)"
 970: fi
 971: ```
 972: 
 973: **Delete legacy memories** (now in .serena/memories/system/):
 974: 
 975: ```bash
 976: # Delete only memories that were migrated to /system/
 977: # Preserve user memories (not in system/)
 978: if [ -d .serena/memories/ ]; then
 979:   find .serena/memories/ -maxdepth 1 -name "*.md" -type f | while read file; do
 980:     basename=$(basename "$file")
 981:     if [ -f .serena/memories/system/"$basename" ]; then
 982:       rm -f "$file"
 983:       echo "‚úì Deleted legacy .serena/memories/$basename (migrated to system/)"
 984:     fi
 985:   done
 986: fi
 987: ```
 988: 
 989: ### Step 5.4: Log Cleanup Summary
 990: 
 991: ```bash
 992: # Log cleanup actions to initialization report
 993: cat > tmp/syntropy-initialization/phase5-report.txt << EOF
 994: # Phase 5: Legacy Organization Cleanup Report
 995: 
 996: ## Deleted Legacy Files
 997: 
 998: **PRPs Directory**: $(test ! -d PRPs && echo "Deleted" || echo "Still exists")
 999: **Examples Directory**: $(test ! -d examples && echo "Deleted" || echo "Still exists")
1000: **Legacy Memories**: $(find .serena/memories -maxdepth 1 -name "*.md" 2>/dev/null | wc -l) remaining at root level
1001: 
1002: ## Backup
1003: 
1004: **Archive Created**: $(ls tmp/syntropy-initialization/legacy-backup/*.tar.gz | head -n 1)
1005: **Archive Size**: $(du -h tmp/syntropy-initialization/legacy-backup/*.tar.gz | head -n 1 | cut -f1)
1006: 
1007: ## Verification
1008: 
1009: **CE 1.1 Structure Active**: $(test -d .ce/PRPs/system && test -d .ce/examples/system && test -d .serena/memories/system && echo "Yes" || echo "No")
1010: **Zero Noise**: $(test ! -d PRPs && test ! -d examples && echo "Yes - Clean project" || echo "No - Legacy files remain")
1011: 
1012: EOF
1013: 
1014: cat tmp/syntropy-initialization/phase5-report.txt
1015: ```
1016: 
1017: ### Step 5.5: Zero Noise Verification
1018: 
1019: ```bash
1020: # Final verification: No legacy noise
1021: echo ""
1022: echo "=== Zero Noise Verification ==="
1023: 
1024: # Check legacy directories deleted
1025: ! test -d PRPs && echo "‚úÖ PRPs/ removed" || echo "‚ùå PRPs/ still exists"
1026: ! test -d examples && echo "‚úÖ examples/ removed" || echo "‚ùå examples/ still exists"
1027: 
1028: # Check no duplicate system memories
1029: DUPLICATE_SYSTEM_MEMORIES=$(find .serena/memories/ -maxdepth 1 -name "*.md" -type f | while read f; do
1030:   basename=$(basename "$f")
1031:   test -f .serena/memories/system/"$basename" && echo "$f"
1032: done | wc -l)
1033: 
1034: test "$DUPLICATE_SYSTEM_MEMORIES" -eq 0 && echo "‚úÖ No duplicate system memories" || echo "‚ùå $DUPLICATE_SYSTEM_MEMORIES duplicate system memories found"
1035: 
1036: # Final status
1037: if [ ! -d PRPs ] && [ ! -d examples ] && [ "$DUPLICATE_SYSTEM_MEMORIES" -eq 0 ]; then
1038:   echo ""
1039:   echo "‚úÖ PROJECT IS CLEAN - Zero noise achieved"
1040: else
1041:   echo ""
1042:   echo "‚ö† PROJECT HAS LEGACY NOISE - Manual cleanup required"
1043: fi
1044: ```
1045: 
1046: **Phase 5 Complete**: Legacy files removed, CE 1.1 structure clean. Installation complete.
1047: 
1048: ---
1049: 
1050: ## Scenario-Specific Workflows
1051: 
1052: ### Scenario 1: Greenfield Project
1053: 
1054: **Use when**: New project with no existing CE components
1055: 
1056: **Phases**: 1, 3, 4 (skip 2, 5)
1057: 
1058: **Duration**: ~10 minutes
1059: 
1060: #### Quick Steps
1061: 
1062: ```bash
1063: # Phase 1: Bucket Collection (will be empty)
1064: mkdir -p tmp/syntropy-initialization/{serena,examples,prps,claude-md,claude-dir}
1065: echo "‚úì Buckets created (empty for greenfield)"
1066: 
1067: # Phase 2: SKIP (no user files)
1068: echo "‚è≠ Skipping Phase 2 (no user files)"
1069: 
1070: # Phase 3: Repomix Package Handling
1071: repomix --unpack ce-infrastructure.xml --target ./
1072: cd .ce/tools && ./bootstrap.sh && cd ../..
1073: echo "‚úì Framework installed"
1074: 
1075: # Phase 4: CLAUDE.md Blending (framework only)
1076: grep -q "## Communication" CLAUDE.md && echo "‚úì Framework CLAUDE.md installed"
1077: 
1078: # Phase 5: SKIP (no legacy files)
1079: echo "‚è≠ Skipping Phase 5 (no legacy files)"
1080: 
1081: # Validate
1082: cd .ce/tools && uv run ce validate --level 4 && cd ../..
1083: echo "‚úÖ Greenfield installation complete"
1084: ```
1085: 
1086: ---
1087: 
1088: ### Scenario 2: Mature Project (No CE)
1089: 
1090: **Use when**: Existing codebase with no CE components
1091: 
1092: **Phases**: 1, 2, 3, 4 (skip 5)
1093: 
1094: **Duration**: ~45 minutes
1095: 
1096: ---
1097: 
1098: ### Scenario 3: CE 1.0 Upgrade
1099: 
1100: **Use when**: Existing CE installation, no .ce/ directory (legacy structure)
1101: 
1102: **Phases**: 1, 2, 3, 4, 5 (all phases)
1103: 
1104: **Duration**: ~40 minutes
1105: 
1106: ---
1107: 
1108: ### Scenario 4: Partial Install (Completion)
1109: 
1110: **Use when**: Project has some CE components but missing others
1111: 
1112: **Phases**: 1, 3, 4 (selective)
1113: 
1114: **Duration**: ~15 minutes
1115: 
1116: ---
1117: 
1118: ## Validation Checklist
1119: 
1120: ### Structure Validation
1121: 
1122: ```bash
1123: # Check CE 1.1 directory structure
1124: test -d .ce/examples/system && echo "‚úÖ .ce/examples/system/"
1125: test -d .ce/PRPs/system && echo "‚úÖ .ce/PRPs/system/"
1126: test -d .serena/memories/system && echo "‚úÖ .serena/memories/system/"
1127: test -d .claude/commands && echo "‚úÖ .claude/commands/"
1128: test -f CLAUDE.md && echo "‚úÖ CLAUDE.md"
1129: ```
1130: 
1131: ### Component Counts
1132: 
1133: ```bash
1134: # Expected file counts
1135: echo "System memories: $(ls .serena/memories/system/*.md 2>/dev/null | wc -l) (expected: 23)"
1136: echo "System examples: $(ls .ce/examples/system/*.md 2>/dev/null | wc -l) (expected: 21)"
1137: echo "Framework commands: $(ls .claude/commands/*.md 2>/dev/null | wc -l) (expected: 11+)"
1138: ```
1139: 
1140: ---
1141: 
1142: ## Troubleshooting
1143: 
1144: ### Automated Installation Issues
1145: 
1146: #### Issue: `syntropy-mcp command not found`
1147: 
1148: **Solution**:
1149: 
1150: ```bash
1151: # Install syntropy-mcp globally
1152: npm install -g syntropy-mcp
1153: 
1154: # Verify installation
1155: npx syntropy-mcp --version
1156: ```
1157: 
1158: #### Issue: `repomix_unpack.py not found` after init
1159: 
1160: **Cause**: Infrastructure package may not include unpacker tool
1161: 
1162: **Solution**:
1163: 
1164: ```bash
1165: # Verify infrastructure package includes unpacker
1166: grep "repomix_unpack.py" ce-infrastructure.xml
1167: 
1168: # If missing, rebuild infrastructure package (ctx-eng-plus repo):
1169: cd ctx-eng-plus
1170: .ce/build-and-distribute.sh
1171: ```
1172: 
1173: #### Issue: Settings not blended correctly
1174: 
1175: **Cause**: Settings blending logic may have failed
1176: 
1177: **Solution**:
1178: 
1179: ```bash
1180: # Check if backup exists
1181: ls .claude/settings.local.json.backup
1182: 
1183: # If backup exists, manually verify blending:
1184: cat .claude/settings.local.json | jq '.permissions.allow' | grep "mcp__syntropy__"
1185: cat .claude/settings.local.json | jq '.permissions.deny' | grep "mcp__syntropy__filesystem"
1186: 
1187: # If incorrect, restore backup and re-run init
1188: cp .claude/settings.local.json.backup .claude/settings.local.json
1189: npx syntropy-mcp init ce-framework
1190: ```
1191: 
1192: ---
1193: 
1194: ### Manual Installation Issues
1195: 
1196: #### Issue: Repomix command not found
1197: 
1198: **Solution**:
1199: 
1200: ```bash
1201: # Install repomix globally
1202: npm install -g repomix
1203: 
1204: # Or use npx
1205: npx repomix --unpack ce-infrastructure.xml --target ./
1206: ```
1207: 
1208: ### Issue: Bootstrap script fails
1209: 
1210: **Solution**:
1211: 
1212: ```bash
1213: cd .ce/tools
1214: 
1215: # Install UV manually
1216: curl -LsSf https://astral.sh/uv/install.sh | sh
1217: 
1218: # Retry bootstrap
1219: ./bootstrap.sh
1220: 
1221: # Or install dependencies directly
1222: uv sync
1223: ```
1224: 
1225: ---
1226: 
1227: ## Success Criteria
1228: 
1229: Your installation is complete when:
1230: 
1231: - ‚úÖ All CE 1.1 directories created (`.ce/`, `.serena/`, `.claude/`)
1232: - ‚úÖ System files installed (23 memories, 21 examples, 11 commands)
1233: - ‚úÖ CLAUDE.md present with framework sections
1234: - ‚úÖ Settings JSON valid and merged
1235: - ‚úÖ CE tools installed (`ce --version` works)
1236: - ‚úÖ Validation level 4 passes
1237: - ‚úÖ Context drift <5%
1238: - ‚úÖ Zero noise (no legacy directories for CE 1.0 upgrades)
1239: - ‚úÖ Serena memories loaded
1240: - ‚úÖ Linear configured (if using)
1241: - ‚úÖ First PRP created successfully
1242: 
1243: ---
1244: 
1245: ## Related Documentation
1246: 
1247: - **Framework Rules**: `.ce/RULES.md`
1248: - **Tool Usage Guide**: `.ce/examples/system/TOOL-USAGE-GUIDE.md`
1249: - **PRP-0 Template**: `.ce/examples/system/templates/PRP-0-CONTEXT-ENGINEERING.md`
1250: - **Validation Levels**: `.serena/memories/system/validation-levels.md`
1251: - **Testing Standards**: `.serena/memories/system/testing-standards.md`
1252: 
1253: ---
1254: 
1255: **Installation Guide Version**: 1.1
1256: **Last Updated**: 2025-11-04
1257: **Framework Version**: CE 1.1
</file>

<file path="tools/ce/blending/strategies/__init__.py">
 1: """Blending strategies for different domains."""
 2: 
 3: from ce.blending.strategies.simple import PRPMoveStrategy, CommandOverwriteStrategy
 4: from ce.blending.strategies.memories import MemoriesBlendStrategy
 5: from ce.blending.strategies.examples import ExamplesBlendStrategy
 6: from ce.blending.strategies.settings import SettingsBlendStrategy
 7: from ce.blending.strategies.claude_md import ClaudeMdBlendStrategy
 8: 
 9: __all__ = [
10:     "PRPMoveStrategy",
11:     "CommandOverwriteStrategy",
12:     "MemoriesBlendStrategy",
13:     "ExamplesBlendStrategy",
14:     "SettingsBlendStrategy",
15:     "ClaudeMdBlendStrategy"
16: ]
</file>

<file path="tools/ce/__main__.py">
  1: """Context Engineering CLI - Main entry point.
  2: 
  3: This module provides the CLI interface with argparse configuration.
  4: All command handlers are delegated to cli_handlers module for better organization.
  5: """
  6: 
  7: import argparse
  8: import sys
  9: 
 10: from . import __version__
 11: from .cli_handlers import (
 12:     cmd_validate,
 13:     cmd_git,
 14:     cmd_context,
 15:     cmd_drift,
 16:     cmd_run_py,
 17:     cmd_prp_validate,
 18:     cmd_prp_generate,
 19:     cmd_prp_execute,
 20:     cmd_prp_analyze,
 21:     cmd_pipeline_validate,
 22:     cmd_pipeline_render,
 23:     cmd_metrics,
 24:     cmd_analyze_context,
 25:     cmd_update_context,
 26:     cmd_vacuum,
 27:     cmd_blend,
 28:     cmd_cleanup,
 29:     cmd_init_project,
 30: )
 31: 
 32: 
 33: def main():
 34:     """Main CLI entry point."""
 35:     parser = argparse.ArgumentParser(
 36:         description="Context Engineering CLI Tools",
 37:         formatter_class=argparse.RawDescriptionHelpFormatter,
 38:         epilog="""
 39: Examples:
 40:   ce validate --level all
 41:   ce git status
 42:   ce git checkpoint "Phase 1 complete"
 43:   ce context sync
 44:   ce context health --json
 45:   ce run_py "print('hello')"
 46:   ce run_py "x = [1,2,3]; print(sum(x))"
 47:   ce run_py tmp/script.py
 48:   ce run_py --code "import sys; print(sys.version)"
 49:   ce run_py --file tmp/script.py --args "--input data.csv"
 50:         """
 51:     )
 52: 
 53:     parser.add_argument("--version", action="version", version=f"ce {__version__}")
 54: 
 55:     subparsers = parser.add_subparsers(dest="command", help="Command to execute")
 56: 
 57:     # === VALIDATE COMMAND ===
 58:     validate_parser = subparsers.add_parser(
 59:         "validate",
 60:         help="Run validation gates"
 61:     )
 62:     validate_parser.add_argument(
 63:         "--level",
 64:         choices=["1", "2", "3", "4", "all"],
 65:         default="all",
 66:         help="Validation level (1=lint/type, 2=unit tests, 3=integration, 4=pattern conformance, all=all levels)"
 67:     )
 68:     validate_parser.add_argument(
 69:         "--prp",
 70:         help="Path to PRP file (required for level 4)"
 71:     )
 72:     validate_parser.add_argument(
 73:         "--files",
 74:         help="Comma-separated list of implementation files (for level 4, optional - auto-detected if not provided)"
 75:     )
 76:     validate_parser.add_argument(
 77:         "--json",
 78:         action="store_true",
 79:         help="Output as JSON"
 80:     )
 81: 
 82:     # === GIT COMMAND ===
 83:     git_parser = subparsers.add_parser(
 84:         "git",
 85:         help="Git operations"
 86:     )
 87:     git_parser.add_argument(
 88:         "action",
 89:         choices=["status", "checkpoint", "diff"],
 90:         help="Git action to perform"
 91:     )
 92:     git_parser.add_argument(
 93:         "--message",
 94:         help="Checkpoint message (for checkpoint action)"
 95:     )
 96:     git_parser.add_argument(
 97:         "--since",
 98:         help="Git ref for diff (default: HEAD~5)"
 99:     )
100:     git_parser.add_argument(
101:         "--json",
102:         action="store_true",
103:         help="Output as JSON"
104:     )
105: 
106:     # === CONTEXT COMMAND ===
107:     context_parser = subparsers.add_parser(
108:         "context",
109:         help="Context management"
110:     )
111:     context_parser.add_argument(
112:         "action",
113:         choices=["sync", "health", "prune", "pre-sync", "post-sync", "auto-sync"],
114:         help="Context action to perform"
115:     )
116:     # Common flags
117:     context_parser.add_argument(
118:         "--json",
119:         action="store_true",
120:         help="Output as JSON"
121:     )
122:     # For health action
123:     context_parser.add_argument(
124:         "--verbose",
125:         action="store_true",
126:         help="Verbose health report with component breakdown (for health)"
127:     )
128:     # For prune action
129:     context_parser.add_argument(
130:         "--age",
131:         type=int,
132:         help="Age in days for pruning (default: 7, for prune)"
133:     )
134:     context_parser.add_argument(
135:         "--dry-run",
136:         action="store_true",
137:         help="Dry run mode (for prune)"
138:     )
139:     # For pre-sync action
140:     context_parser.add_argument(
141:         "--force",
142:         action="store_true",
143:         help="Skip drift abort check (for pre-sync, dangerous)"
144:     )
145:     # For post-sync action
146:     context_parser.add_argument(
147:         "--prp-id",
148:         help="PRP identifier (for post-sync)"
149:     )
150:     context_parser.add_argument(
151:         "--skip-cleanup",
152:         action="store_true",
153:         help="Skip cleanup protocol (for post-sync)"
154:     )
155:     # For auto-sync action
156:     auto_sync_group = context_parser.add_mutually_exclusive_group()
157:     auto_sync_group.add_argument(
158:         "--enable",
159:         action="store_true",
160:         help="Enable auto-sync mode (for auto-sync)"
161:     )
162:     auto_sync_group.add_argument(
163:         "--disable",
164:         action="store_true",
165:         help="Disable auto-sync mode (for auto-sync)"
166:     )
167:     auto_sync_group.add_argument(
168:         "--status",
169:         action="store_true",
170:         help="Check auto-sync status (for auto-sync)"
171:     )
172: 
173:     # === DRIFT COMMAND ===
174:     drift_parser = subparsers.add_parser(
175:         "drift",
176:         help="Drift history tracking and analysis"
177:     )
178:     drift_parser.add_argument(
179:         "action",
180:         choices=["history", "show", "summary", "compare"],
181:         help="Drift action to perform"
182:     )
183:     drift_parser.add_argument(
184:         "--last",
185:         type=int,
186:         help="Show last N decisions (for history)"
187:     )
188:     drift_parser.add_argument(
189:         "--prp-id",
190:         help="Filter by PRP ID (for history/show)"
191:     )
192:     drift_parser.add_argument(
193:         "--prp-id2",
194:         help="Second PRP ID (for compare)"
195:     )
196:     drift_parser.add_argument(
197:         "--action-filter",
198:         choices=["accepted", "rejected", "examples_updated"],
199:         help="Filter by action type (for history)"
200:     )
201:     drift_parser.add_argument(
202:         "--json",
203:         action="store_true",
204:         help="Output as JSON"
205:     )
206: 
207:     # === RUN_PY COMMAND ===
208:     runpy_parser = subparsers.add_parser(
209:         "run_py",
210:         help="Execute Python code (auto-detect or explicit mode)"
211:     )
212:     runpy_group = runpy_parser.add_mutually_exclusive_group(required=False)
213:     runpy_group.add_argument(
214:         "input",
215:         nargs="?",
216:         help="Auto-detect: code (‚â§3 LOC) or file path (tmp/*.py)"
217:     )
218:     runpy_parser.add_argument(
219:         "--code",
220:         help="Explicit: Ad-hoc Python code (max 3 LOC)"
221:     )
222:     runpy_parser.add_argument(
223:         "--file",
224:         help="Explicit: Path to Python file in tmp/ folder"
225:     )
226:     runpy_parser.add_argument(
227:         "--args",
228:         dest="script_args",
229:         help="Arguments to pass to Python script"
230:     )
231:     runpy_parser.add_argument(
232:         "--json",
233:         action="store_true",
234:         help="Output execution summary as JSON"
235:     )
236: 
237:     # === PRP COMMAND ===
238:     prp_parser = subparsers.add_parser(
239:         "prp", help="PRP management commands"
240:     )
241:     prp_subparsers = prp_parser.add_subparsers(dest="prp_command", required=True)
242: 
243:     # prp validate subcommand
244:     prp_validate_parser = prp_subparsers.add_parser(
245:         "validate", help="Validate PRP YAML header"
246:     )
247:     prp_validate_parser.add_argument(
248:         "file", help="Path to PRP markdown file"
249:     )
250:     prp_validate_parser.add_argument(
251:         "--json", action="store_true", help="Output as JSON"
252:     )
253: 
254:     # prp generate subcommand
255:     prp_generate_parser = prp_subparsers.add_parser(
256:         "generate", help="Generate PRP from INITIAL.md"
257:     )
258:     prp_generate_parser.add_argument(
259:         "initial_md", help="Path to INITIAL.md file"
260:     )
261:     prp_generate_parser.add_argument(
262:         "-o", "--output",
263:         help="Output directory for PRP (default: PRPs/feature-requests)"
264:     )
265:     prp_generate_parser.add_argument(
266:         "--json", action="store_true", help="Output as JSON"
267:     )
268:     prp_generate_parser.add_argument(
269:         "--join-prp",
270:         help="Update existing PRP's Linear issue (PRP number, ID like 'PRP-12', or file path)"
271:     )
272:     prp_generate_parser.add_argument(
273:         "--use-thinking",
274:         action="store_true",
275:         default=True,
276:         help="Use sequential thinking for analysis (default: True)"
277:     )
278:     prp_generate_parser.add_argument(
279:         "--no-thinking",
280:         dest="use_thinking",
281:         action="store_false",
282:         help="Disable sequential thinking (use heuristics)"
283:     )
284: 
285:     # prp execute subcommand
286:     prp_execute_parser = prp_subparsers.add_parser(
287:         "execute", help="Execute PRP implementation"
288:     )
289:     prp_execute_parser.add_argument(
290:         "prp_id", help="PRP identifier (e.g., PRP-4)"
291:     )
292:     prp_execute_parser.add_argument(
293:         "--start-phase", type=int, help="Start from specific phase"
294:     )
295:     prp_execute_parser.add_argument(
296:         "--end-phase", type=int, help="End at specific phase"
297:     )
298:     prp_execute_parser.add_argument(
299:         "--skip-validation", action="store_true", help="Skip validation loops"
300:     )
301:     prp_execute_parser.add_argument(
302:         "--dry-run", action="store_true", help="Parse blueprint only, don't execute"
303:     )
304:     prp_execute_parser.add_argument(
305:         "--json", action="store_true", help="Output as JSON"
306:     )
307: 
308:     # prp analyze subcommand
309:     prp_analyze_parser = prp_subparsers.add_parser(
310:         "analyze", help="Analyze PRP size and complexity"
311:     )
312:     prp_analyze_parser.add_argument(
313:         "file", help="Path to PRP markdown file"
314:     )
315:     prp_analyze_parser.add_argument(
316:         "--json", action="store_true", help="Output as JSON"
317:     )
318: 
319:     # === PIPELINE COMMAND ===
320:     pipeline_parser = subparsers.add_parser(
321:         "pipeline", help="CI/CD pipeline management commands"
322:     )
323:     pipeline_subparsers = pipeline_parser.add_subparsers(dest="pipeline_command", required=True)
324: 
325:     # pipeline validate subcommand
326:     pipeline_validate_parser = pipeline_subparsers.add_parser(
327:         "validate", help="Validate abstract pipeline definition"
328:     )
329:     pipeline_validate_parser.add_argument(
330:         "pipeline_file", help="Path to abstract pipeline YAML file"
331:     )
332: 
333:     # pipeline render subcommand
334:     pipeline_render_parser = pipeline_subparsers.add_parser(
335:         "render", help="Render abstract pipeline to platform-specific format"
336:     )
337:     pipeline_render_parser.add_argument(
338:         "pipeline_file", help="Path to abstract pipeline YAML file"
339:     )
340:     pipeline_render_parser.add_argument(
341:         "--executor", type=str, choices=["github-actions", "mock"],
342:         default="github-actions", help="Platform executor to use"
343:     )
344:     pipeline_render_parser.add_argument(
345:         "-o", "--output", help="Output file path"
346:     )
347: 
348:     # === METRICS COMMAND ===
349:     metrics_parser = subparsers.add_parser(
350:         "metrics",
351:         help="Display system metrics and success rates"
352:     )
353:     metrics_parser.add_argument(
354:         "--format",
355:         choices=["text", "json"],
356:         default="text",
357:         help="Output format (default: text)"
358:     )
359:     metrics_parser.add_argument(
360:         "--file",
361:         default="metrics.json",
362:         help="Path to metrics file (default: metrics.json)"
363:     )
364: 
365:     # === ANALYZE-CONTEXT COMMAND ===
366:     analyze_context_parser = subparsers.add_parser(
367:         "analyze-context",
368:         aliases=["analyse-context"],
369:         help="Analyze context drift without updating metadata (fast check for CI/CD)"
370:     )
371:     analyze_context_parser.add_argument(
372:         "--json",
373:         action="store_true",
374:         help="Output JSON for scripting"
375:     )
376:     analyze_context_parser.add_argument(
377:         "--force",
378:         action="store_true",
379:         help="Force re-analysis, bypass cache"
380:     )
381:     analyze_context_parser.add_argument(
382:         "--cache-ttl",
383:         type=int,
384:         help="Cache TTL in minutes (default: from config or 5)"
385:     )
386: 
387:     # === UPDATE-CONTEXT COMMAND ===
388:     update_context_parser = subparsers.add_parser(
389:         "update-context",
390:         help="Sync CE/Serena with codebase changes"
391:     )
392:     update_context_parser.add_argument(
393:         "--prp",
394:         help="Target specific PRP file (path relative to project root)"
395:     )
396:     update_context_parser.add_argument(
397:         "--remediate",
398:         action="store_true",
399:         help="Auto-remediate drift violations (YOLO mode - skips approval)"
400:     )
401:     update_context_parser.add_argument(
402:         "--json",
403:         action="store_true",
404:         help="Output as JSON"
405:     )
406: 
407:     # === VACUUM COMMAND ===
408:     vacuum_parser = subparsers.add_parser(
409:         "vacuum",
410:         help="Clean up project noise (temp files, obsolete docs, unreferenced code)"
411:     )
412:     vacuum_parser.add_argument(
413:         "--dry-run",
414:         action="store_true",
415:         default=True,
416:         help="Generate report only (default)"
417:     )
418:     vacuum_parser.add_argument(
419:         "--execute",
420:         action="store_true",
421:         help="Delete HIGH confidence items (temp files, backups)"
422:     )
423:     vacuum_parser.add_argument(
424:         "--force",
425:         action="store_true",
426:         help="Delete HIGH + MEDIUM confidence items"
427:     )
428:     vacuum_parser.add_argument(
429:         "--auto",
430:         action="store_true",
431:         help="Automatically delete HIGH + MEDIUM confidence items (same as --force)"
432:     )
433:     vacuum_parser.add_argument(
434:         "--nuclear",
435:         action="store_true",
436:         help="Delete ALL items including LOW confidence (requires confirmation)"
437:     )
438:     vacuum_parser.add_argument(
439:         "--min-confidence",
440:         type=int,
441:         default=0,
442:         help="Minimum confidence threshold 0-100 (default: 0)"
443:     )
444:     vacuum_parser.add_argument(
445:         "--exclude-strategy",
446:         action="append",
447:         dest="exclude_strategies",
448:         help="Skip specific strategy (can be used multiple times)"
449:     )
450:     vacuum_parser.add_argument(
451:         "--path",
452:         type=str,
453:         help="Directory to scan (relative to project root, defaults to entire project)"
454:     )
455: 
456:     # === CLEANUP COMMAND ===
457:     cleanup_parser = subparsers.add_parser(
458:         "cleanup",
459:         help="Remove legacy directories after CE 1.1 migration"
460:     )
461:     cleanup_parser.add_argument(
462:         "--dry-run",
463:         action="store_true",
464:         default=True,
465:         help="Show what would be removed without deleting (default: True)"
466:     )
467:     cleanup_parser.add_argument(
468:         "--execute",
469:         action="store_true",
470:         default=False,
471:         help="Execute cleanup (remove legacy directories)"
472:     )
473: 
474:     # === BLEND COMMAND ===
475:     blend_parser = subparsers.add_parser(
476:         "blend",
477:         help="CE Framework Blending Tool - Migrate and blend framework files"
478:     )
479:     # Operation modes
480:     mode_group = blend_parser.add_mutually_exclusive_group(required=True)
481:     mode_group.add_argument('--all', action='store_true', help='Run all 4 phases')
482:     mode_group.add_argument('--phase', choices=['detect', 'classify', 'blend', 'cleanup'], help='Run specific phase')
483:     mode_group.add_argument('--cleanup-only', action='store_true', help='Run cleanup only')
484:     mode_group.add_argument('--rollback', action='store_true', help='Restore backups')
485:     # Domain selection (optional)
486:     blend_parser.add_argument('--domain', help='Blend specific domain only (settings, claude_md, memories, examples, prps, commands)')
487:     # Behavior flags
488:     blend_parser.add_argument('--dry-run', action='store_true', help='Show what would be done without executing')
489:     blend_parser.add_argument('--interactive', action='store_true', help='Ask before each phase')
490:     blend_parser.add_argument('--skip-cleanup', action='store_true', help='Skip Phase D (keep legacy dirs)')
491:     blend_parser.add_argument('--fast', action='store_true', help='Fast mode (Haiku only, skip expensive ops)')
492:     blend_parser.add_argument('--quality', action='store_true', help='Quality mode (Sonnet for all LLM calls)')
493:     blend_parser.add_argument('--scan', action='store_true', help='Scan mode (detect + classify only, no blending)')
494:     # Configuration
495:     blend_parser.add_argument('--config', default='.ce/blend-config.yml', help='Path to blend config (default: .ce/blend-config.yml)')
496:     blend_parser.add_argument('--target-dir', default='.', help='Target project directory (default: current)')
497:     # Debugging
498:     blend_parser.add_argument('-v', '--verbose', action='store_true', help='Verbose output')
499: 
500:     # === INIT-PROJECT COMMAND ===
501:     init_project_parser = subparsers.add_parser(
502:         "init-project",
503:         help="Initialize CE Framework in a target project"
504:     )
505:     init_project_parser.add_argument(
506:         "target_dir",
507:         help="Target directory for CE Framework initialization"
508:     )
509:     init_project_parser.add_argument(
510:         "--dry-run",
511:         action="store_true",
512:         help="Show what would be done without executing"
513:     )
514:     init_project_parser.add_argument(
515:         "--phase",
516:         choices=["extract", "blend", "initialize", "verify", "all"],
517:         default="all",
518:         help="Run specific initialization phase (default: all)"
519:     )
520:     init_project_parser.add_argument(
521:         "--blend-only",
522:         action="store_true",
523:         help="Skip extraction phase (for re-initialization)"
524:     )
525: 
526:     # Parse arguments
527:     args = parser.parse_args()
528: 
529:     if not args.command:
530:         parser.print_help()
531:         return 0
532: 
533:     # Execute command
534:     if args.command == "validate":
535:         return cmd_validate(args)
536:     elif args.command == "git":
537:         return cmd_git(args)
538:     elif args.command == "context":
539:         return cmd_context(args)
540:     elif args.command == "drift":
541:         return cmd_drift(args)
542:     elif args.command == "run_py":
543:         return cmd_run_py(args)
544:     elif args.command == "prp":
545:         if args.prp_command == "validate":
546:             return cmd_prp_validate(args)
547:         elif args.prp_command == "generate":
548:             return cmd_prp_generate(args)
549:         elif args.prp_command == "execute":
550:             return cmd_prp_execute(args)
551:         elif args.prp_command == "analyze":
552:             return cmd_prp_analyze(args)
553:     elif args.command == "pipeline":
554:         if args.pipeline_command == "validate":
555:             return cmd_pipeline_validate(args)
556:         elif args.pipeline_command == "render":
557:             return cmd_pipeline_render(args)
558:     elif args.command == "metrics":
559:         return cmd_metrics(args)
560:     elif args.command in ["analyze-context", "analyse-context"]:
561:         return cmd_analyze_context(args)
562:     elif args.command == "update-context":
563:         return cmd_update_context(args)
564:     elif args.command == "vacuum":
565:         return cmd_vacuum(args)
566:     elif args.command == "cleanup":
567:         return cmd_cleanup(args)
568:     elif args.command == "blend":
569:         return cmd_blend(args)
570:     elif args.command == "init-project":
571:         return cmd_init_project(args)
572:     else:
573:         print(f"Unknown command: {args.command}", file=sys.stderr)
574:         return 1
575: 
576: 
577: if __name__ == "__main__":
578:     sys.exit(main())
</file>

<file path="tools/ce/blend.py">
  1: """Blending operations module."""
  2: 
  3: import sys
  4: import logging
  5: from pathlib import Path
  6: from typing import Dict, Any
  7: 
  8: from .blending.core import BlendingOrchestrator
  9: from .config_loader import BlendConfig
 10: 
 11: logger = logging.getLogger(__name__)
 12: 
 13: 
 14: def setup_logging(verbose: bool = False) -> None:
 15:     """Configure logging."""
 16:     level = logging.DEBUG if verbose else logging.INFO
 17:     logging.basicConfig(
 18:         level=level,
 19:         format='%(message)s'
 20:     )
 21: 
 22: 
 23: def load_config(config_path: Path) -> Dict[str, Any]:
 24:     """
 25:     Load blend configuration from YAML.
 26: 
 27:     Args:
 28:         config_path: Path to blend-config.yml
 29: 
 30:     Returns:
 31:         Configuration dict
 32: 
 33:     Raises:
 34:         FileNotFoundError: If config file not found
 35:         ValueError: If config invalid
 36:     """
 37:     if not config_path.exists():
 38:         raise FileNotFoundError(
 39:             f"Config file not found: {config_path}\n"
 40:             f"üîß Create .ce/blend-config.yml (see PRP-34.1.1)"
 41:         )
 42: 
 43:     import yaml
 44: 
 45:     try:
 46:         with open(config_path) as f:
 47:             config = yaml.safe_load(f)
 48: 
 49:         if not config or 'domains' not in config:
 50:             raise ValueError("Config missing 'domains' section\nüîß Troubleshooting: Check input parameters and documentation")
 51: 
 52:         return config
 53: 
 54:     except yaml.YAMLError as e:
 55:         raise ValueError(
 56:             f"Invalid YAML config: {e}\n"
 57:             f"üîß Check syntax: {config_path}"
 58:         ) from e
 59: 
 60: 
 61: def run_blend(args) -> int:
 62:     """
 63:     Execute blending operation.
 64: 
 65:     Args:
 66:         args: Parsed CLI arguments (from argparse.Namespace)
 67: 
 68:     Returns:
 69:         Exit code (0 = success, 1 = failure)
 70:     """
 71:     # Setup logging first
 72:     setup_logging(getattr(args, 'verbose', False))
 73: 
 74:     try:
 75:         # Load configuration using BlendConfig
 76:         config_path = Path(args.config)
 77: 
 78:         # Create BlendConfig instance for config-driven operations
 79:         blend_config = BlendConfig(config_path)
 80: 
 81:         # Also load raw config for backward compatibility with existing orchestrator
 82:         config = blend_config._config
 83: 
 84:         # Initialize orchestrator with BlendConfig instance
 85:         orchestrator = BlendingOrchestrator(
 86:             config=blend_config,  # Pass BlendConfig instance instead of dict
 87:             dry_run=args.dry_run
 88:         )
 89: 
 90:         # Determine target directory
 91:         target_dir = Path(args.target_dir).resolve()
 92:         if not target_dir.exists():
 93:             raise ValueError(
 94:                 f"Target directory not found: {target_dir}\n"
 95:                 f"üîß Provide valid project directory"
 96:             )
 97: 
 98:         # Run phases
 99:         blend_result = None  # Track blend phase result
100: 
101:         if args.all:
102:             # Run all 4 phases
103:             phases = ['detect', 'classify', 'blend']
104:             if not args.skip_cleanup:
105:                 phases.append('cleanup')
106: 
107:             for phase in phases:
108:                 result = orchestrator.run_phase(phase, target_dir)
109: 
110:                 # Track blend phase result for exit code
111:                 if phase == 'blend':
112:                     blend_result = result
113: 
114:                 # Check for failures in critical phases
115:                 if phase == 'blend' and not result.get("success", True):
116:                     failed = result.get("failed_domains", [])
117:                     logger.error(f"‚ùå Blend failed for domains: {', '.join(failed)}")
118:                     logger.info(result.get("message", "See error details above"))
119:                     return 1
120: 
121:                 logger.info(f"‚úì Phase {phase} complete")
122: 
123:                 # Interactive mode - ask before next phase
124:                 if args.interactive and phase != phases[-1]:
125:                     response = input(f"Continue to {phases[phases.index(phase) + 1]}? [Y/n] ")
126:                     if response.lower() == 'n':
127:                         logger.info("Stopped by user")
128:                         return 0
129: 
130:         elif args.phase:
131:             # Run specific phase
132:             result = orchestrator.run_phase(args.phase, target_dir)
133: 
134:             # Check blend phase result
135:             if args.phase == 'blend':
136:                 blend_result = result
137:                 if not result.get("success", True):
138:                     failed = result.get("failed_domains", [])
139:                     logger.error(f"‚ùå Blend failed for domains: {', '.join(failed)}")
140:                     logger.info(result.get("message", "See error details above"))
141:                     return 1
142: 
143:             logger.info(f"‚úì Phase {args.phase} complete")
144: 
145:         elif args.cleanup_only:
146:             # Run cleanup only (requires prior blend)
147:             result = orchestrator.run_phase('cleanup', target_dir)
148:             logger.info("‚úì Cleanup complete")
149: 
150:         elif args.rollback:
151:             # Restore backups
152:             logger.info("üîÑ Rolling back blending operations...")
153:             # Stub - implement in validation.py (PRP-34.1.1)
154:             logger.warning("Rollback not yet fully implemented")
155:             return 1
156: 
157:         else:
158:             logger.error("No operation specified (use --all, --phase, --cleanup-only, or --rollback)")
159:             return 1
160: 
161:         logger.info("‚úÖ Blending complete!")
162:         return 0
163: 
164:     except Exception as e:
165:         logger.error(f"‚ùå Blending failed: {e}")
166:         if getattr(args, 'verbose', False):
167:             logger.exception("Full traceback:")
168:         return 1
</file>

<file path="tools/ce/blending/strategies/simple.py">
  1: """Simple blending strategies for PRPs and Commands.
  2: 
  3: This module implements Python-only blending strategies that don't require LLM:
  4: - PRPMoveStrategy: Move user PRPs with hash deduplication, add type headers
  5: - CommandOverwriteStrategy: Overwrite user commands with framework versions
  6: 
  7: Philosophy:
  8: - No ID deduplication for PRPs (preserve all user PRPs)
  9: - Framework authority for commands (backup user versions)
 10: - Hash-based deduplication (skip identical files)
 11: - Atomic operations with proper error handling
 12: """
 13: 
 14: import hashlib
 15: import re
 16: import shutil
 17: from datetime import datetime
 18: from pathlib import Path
 19: from typing import Dict, Any, List
 20: 
 21: 
 22: class PRPMoveStrategy:
 23:     """
 24:     Move user PRPs to CE structure with hash deduplication.
 25: 
 26:     Behavior:
 27:     - Moves all PRPs from source to target/.ce/PRPs/
 28:     - Determines status (executed vs feature-requests) from content
 29:     - Adds 'type: user' YAML header if missing
 30:     - Skips if identical file exists (hash-based dedupe)
 31:     - No ID-based deduplication (all PRPs preserved)
 32: 
 33:     Usage:
 34:         >>> strategy = PRPMoveStrategy()
 35:         >>> result = strategy.execute({
 36:         ...     "source_dir": Path("PRPs"),
 37:         ...     "target_dir": Path(".ce/PRPs")
 38:         ... })
 39:         >>> print(result["prps_moved"])
 40:     """
 41: 
 42:     def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
 43:         """
 44:         Execute PRP move strategy.
 45: 
 46:         Args:
 47:             input_data: {
 48:                 "source_dir": Path to source PRPs directory,
 49:                 "target_dir": Path to target .ce/PRPs directory
 50:             }
 51: 
 52:         Returns:
 53:             {
 54:                 "prps_moved": int,
 55:                 "prps_skipped": int,
 56:                 "errors": List[str]
 57:             }
 58:         """
 59:         source_dir = input_data.get("source_dir")
 60:         target_dir = input_data.get("target_dir")
 61: 
 62:         if not source_dir or not target_dir:
 63:             raise ValueError("source_dir and target_dir are required\nüîß Troubleshooting: Check inputs and system state")
 64: 
 65:         source_dir = Path(source_dir)
 66:         target_dir = Path(target_dir)
 67: 
 68:         if not source_dir.exists():
 69:             return {
 70:                 "prps_moved": 0,
 71:                 "prps_skipped": 0,
 72:                 "errors": [f"Source directory does not exist: {source_dir}"]
 73:             }
 74: 
 75:         # Ensure target subdirectories exist
 76:         (target_dir / "executed").mkdir(parents=True, exist_ok=True)
 77:         (target_dir / "feature-requests").mkdir(parents=True, exist_ok=True)
 78: 
 79:         moved = 0
 80:         skipped = 0
 81:         errors = []
 82: 
 83:         # Find all markdown files in source (recursively)
 84:         for prp_file in source_dir.glob("**/*.md"):
 85:             try:
 86:                 # Read content
 87:                 content = prp_file.read_text(encoding="utf-8")
 88: 
 89:                 # Add user header if missing
 90:                 if not self._has_yaml_header(content):
 91:                     content = self._add_user_header(content)
 92: 
 93:                 # Determine status (executed vs feature-requests)
 94:                 status = self._parse_prp_status(content)
 95: 
 96:                 # Preserve subdirectory structure
 97:                 # Source: PRPs/executed/PRP-1.md ‚Üí Target: .ce/PRPs/executed/PRP-1.md
 98:                 relative_path = prp_file.relative_to(source_dir)
 99: 
100:                 # Determine target subdirectory
101:                 # Check if any parent in the path hierarchy is a target subdirectory
102:                 target_subdirs = ["executed", "feature-requests", "system"]
103:                 found_subdir = None
104:                 for parent in relative_path.parents:
105:                     if parent.name in target_subdirs:
106:                         found_subdir = parent.name
107:                         break
108: 
109:                 if found_subdir:
110:                     # Flatten to target_subdir/filename (preserves subdirectory, flattens deep nesting)
111:                     dest = target_dir / found_subdir / prp_file.name
112:                 else:
113:                     # Root-level or other subdirectories: classify by content
114:                     dest = target_dir / status / prp_file.name
115: 
116:                 # Ensure parent directory exists
117:                 dest.parent.mkdir(parents=True, exist_ok=True)
118: 
119:                 # Hash-based deduplication
120:                 if dest.exists():
121:                     if self._calculate_hash(content) == self._calculate_hash(dest.read_text(encoding="utf-8")):
122:                         skipped += 1
123:                         continue
124: 
125:                 # Write to destination
126:                 dest.write_text(content, encoding="utf-8")
127:                 moved += 1
128: 
129:             except Exception as e:
130:                 errors.append(f"Error processing {prp_file.name}: {str(e)}")
131: 
132:         return {
133:             "success": len(errors) == 0,
134:             "prps_moved": moved,
135:             "prps_skipped": skipped,
136:             "errors": errors,
137:             "files_processed": moved
138:         }
139: 
140:     def _has_yaml_header(self, content: str) -> bool:
141:         """
142:         Check if content has YAML frontmatter header.
143: 
144:         Args:
145:             content: File content
146: 
147:         Returns:
148:             True if starts with "---"
149:         """
150:         return content.strip().startswith("---")
151: 
152:     def _add_user_header(self, content: str) -> str:
153:         """
154:         Add user YAML header to content.
155: 
156:         Args:
157:             content: Original file content
158: 
159:         Returns:
160:             Content with added YAML header
161:         """
162:         now = datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
163: 
164:         header = f"""---
165: type: user
166: source: target-project
167: created: "{now}"
168: updated: "{now}"
169: ---
170: 
171: """
172:         return header + content
173: 
174:     def _parse_prp_status(self, content: str) -> str:
175:         """
176:         Determine PRP status from content.
177: 
178:         Looks for keywords indicating completion:
179:         - "completed", "merged", "deployed" ‚Üí executed
180:         - Default ‚Üí feature-requests
181: 
182:         Args:
183:             content: PRP file content
184: 
185:         Returns:
186:             "executed" or "feature-requests"
187:         """
188:         content_lower = content.lower()
189: 
190:         # Check for completion keywords
191:         completed_keywords = ["status: completed", "status: merged", "status: deployed"]
192: 
193:         for keyword in completed_keywords:
194:             if keyword in content_lower:
195:                 return "executed"
196: 
197:         # Default to feature-requests
198:         return "feature-requests"
199: 
200:     def _calculate_hash(self, content: str) -> str:
201:         """
202:         Calculate SHA256 hash of content.
203: 
204:         Args:
205:             content: String content
206: 
207:         Returns:
208:             Hex digest hash string
209:         """
210:         return hashlib.sha256(content.encode("utf-8")).hexdigest()
211: 
212: 
213: class CommandOverwriteStrategy:
214:     """
215:     Overwrite user commands with framework commands.
216: 
217:     Behavior:
218:     - Backs up existing commands to .claude/commands.backup/
219:     - Overwrites with framework commands from source
220:     - Skips if identical file exists (hash-based dedupe)
221:     - Preserves user custom commands (not in framework)
222: 
223:     Usage:
224:         >>> strategy = CommandOverwriteStrategy()
225:         >>> result = strategy.execute({
226:         ...     "source_dir": Path(".ce/commands"),
227:         ...     "target_dir": Path(".claude/commands"),
228:         ...     "backup_dir": Path(".claude/commands.backup")
229:         ... })
230:         >>> print(result["commands_overwritten"])
231:     """
232: 
233:     def execute(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
234:         """
235:         Execute command overwrite strategy.
236: 
237:         Args:
238:             input_data: {
239:                 "source_dir": Path to framework commands,
240:                 "target_dir": Path to target .claude/commands,
241:                 "backup_dir": Path to backup directory
242:             }
243: 
244:         Returns:
245:             {
246:                 "commands_overwritten": int,
247:                 "commands_backed_up": int,
248:                 "commands_skipped": int,
249:                 "errors": List[str]
250:             }
251:         """
252:         source_dir = input_data.get("source_dir")
253:         target_dir = input_data.get("target_dir")
254:         backup_dir = input_data.get("backup_dir")
255: 
256:         if not source_dir or not target_dir or not backup_dir:
257:             raise ValueError("source_dir, target_dir, and backup_dir are required\nüîß Troubleshooting: Check inputs and system state")
258: 
259:         source_dir = Path(source_dir)
260:         target_dir = Path(target_dir)
261:         backup_dir = Path(backup_dir)
262: 
263:         if not source_dir.exists():
264:             return {
265:                 "commands_overwritten": 0,
266:                 "commands_backed_up": 0,
267:                 "commands_skipped": 0,
268:                 "errors": [f"Source directory does not exist: {source_dir}"]
269:             }
270: 
271:         # Ensure directories exist
272:         target_dir.mkdir(parents=True, exist_ok=True)
273:         backup_dir.mkdir(parents=True, exist_ok=True)
274: 
275:         overwritten = 0
276:         backed_up = 0
277:         skipped = 0
278:         errors = []
279: 
280:         # Process all command files in source
281:         for cmd_file in source_dir.glob("*.md"):
282:             try:
283:                 target_file = target_dir / cmd_file.name
284: 
285:                 # Read source content
286:                 source_content = cmd_file.read_text(encoding="utf-8")
287:                 source_hash = self._calculate_hash(source_content)
288: 
289:                 # Check if target exists and needs backup
290:                 if target_file.exists():
291:                     target_content = target_file.read_text(encoding="utf-8")
292:                     target_hash = self._calculate_hash(target_content)
293: 
294:                     # Hash-based deduplication
295:                     if source_hash == target_hash:
296:                         skipped += 1
297:                         continue
298: 
299:                     # Backup existing command
300:                     backup_file = backup_dir / cmd_file.name
301:                     shutil.copy2(target_file, backup_file)
302:                     backed_up += 1
303: 
304:                 # Overwrite with framework command
305:                 target_file.write_text(source_content, encoding="utf-8")
306:                 overwritten += 1
307: 
308:             except Exception as e:
309:                 errors.append(f"Error processing {cmd_file.name}: {str(e)}")
310: 
311:         return {
312:             "success": len(errors) == 0,
313:             "commands_overwritten": overwritten,
314:             "commands_backed_up": backed_up,
315:             "commands_skipped": skipped,
316:             "errors": errors,
317:             "files_processed": overwritten
318:         }
319: 
320:     def _calculate_hash(self, content: str) -> str:
321:         """
322:         Calculate SHA256 hash of content.
323: 
324:         Args:
325:             content: String content
326: 
327:         Returns:
328:             Hex digest hash string
329:         """
330:         return hashlib.sha256(content.encode("utf-8")).hexdigest()
</file>

<file path="tools/ce/cli_handlers.py">
   1: """CLI command handlers with delegation pattern.
   2: 
   3: Extracted from __main__.py to reduce nesting depth and improve maintainability.
   4: Each handler follows KISS principle with max 4 nesting levels.
   5: """
   6: 
   7: import sys
   8: import json
   9: from typing import Any, Dict
  10: 
  11: from .core import git_status, git_checkpoint, git_diff, run_py
  12: from .validate import validate_level_1, validate_level_2, validate_level_3, validate_level_4, validate_all
  13: from .context import (
  14:     sync, health, prune,
  15:     pre_generation_sync, post_execution_sync,
  16:     context_health_verbose, drift_report_markdown,
  17:     enable_auto_sync, disable_auto_sync, get_auto_sync_status
  18: )
  19: from .generate import generate_prp
  20: from .drift import (
  21:     get_drift_history,
  22:     drift_summary,
  23:     show_drift_decision,
  24:     compare_drift_decisions
  25: )
  26: from .pipeline import load_abstract_pipeline, validate_pipeline
  27: from .executors.github_actions import GitHubActionsExecutor
  28: from .executors.mock import MockExecutor
  29: from .metrics import MetricsCollector
  30: from .update_context import sync_context
  31: from .blend import run_blend as blend_run_blend
  32: from .blending.cleanup import cleanup_legacy_dirs
  33: 
  34: # Conditional import for init_project (implemented in PRP-36.2.2)
  35: try:
  36:     from .init_project import ProjectInitializer
  37:     _HAS_INIT_PROJECT = True
  38: except ImportError:
  39:     _HAS_INIT_PROJECT = False
  40: 
  41: 
  42: def format_output(data: Dict[str, Any], as_json: bool = False) -> str:
  43:     """Format output for display.
  44: 
  45:     Args:
  46:         data: Data to format
  47:         as_json: If True, return JSON string
  48: 
  49:     Returns:
  50:         Formatted string
  51:     """
  52:     if as_json:
  53:         return json.dumps(data, indent=2)
  54: 
  55:     # Human-readable format
  56:     lines = []
  57:     for key, value in data.items():
  58:         if isinstance(value, list):
  59:             lines.append(f"{key}:")
  60:             for item in value:
  61:                 lines.append(f"  - {item}")
  62:         elif isinstance(value, dict):
  63:             lines.append(f"{key}:")
  64:             for k, v in value.items():
  65:                 lines.append(f"  {k}: {v}")
  66:         else:
  67:             lines.append(f"{key}: {value}")
  68: 
  69:     return "\n".join(lines)
  70: 
  71: 
  72: # === VALIDATE COMMAND ===
  73: 
  74: def cmd_validate(args) -> int:
  75:     """Execute validate command."""
  76:     try:
  77:         if args.level == "1":
  78:             result = validate_level_1()
  79:         elif args.level == "2":
  80:             result = validate_level_2()
  81:         elif args.level == "3":
  82:             result = validate_level_3()
  83:         elif args.level == "4":
  84:             if not args.prp:
  85:                 print("‚ùå Level 4 validation requires --prp argument", file=sys.stderr)
  86:                 return 1
  87: 
  88:             files = None
  89:             if args.files:
  90:                 files = [f.strip() for f in args.files.split(",")]
  91: 
  92:             result = validate_level_4(prp_path=args.prp, implementation_paths=files)
  93:         else:  # "all"
  94:             result = validate_all()
  95: 
  96:         print(format_output(result, args.json))
  97:         return 0 if result["success"] else 1
  98: 
  99:     except Exception as e:
 100:         print(f"‚ùå Validation failed: {str(e)}", file=sys.stderr)
 101:         return 1
 102: 
 103: 
 104: # === GIT COMMAND ===
 105: 
 106: def cmd_git(args) -> int:
 107:     """Execute git command."""
 108:     try:
 109:         if args.action == "status":
 110:             result = git_status()
 111:             print(format_output(result, args.json))
 112:             return 0
 113: 
 114:         if args.action == "checkpoint":
 115:             message = args.message or "Context Engineering checkpoint"
 116:             checkpoint_id = git_checkpoint(message)
 117:             result = {"checkpoint_id": checkpoint_id, "message": message}
 118:             print(format_output(result, args.json))
 119:             return 0
 120: 
 121:         if args.action == "diff":
 122:             since = args.since or "HEAD~5"
 123:             files = git_diff(since=since, name_only=True)
 124:             result = {"changed_files": files, "count": len(files), "since": since}
 125:             print(format_output(result, args.json))
 126:             return 0
 127: 
 128:         print(f"Unknown git action: {args.action}", file=sys.stderr)
 129:         return 1
 130: 
 131:     except Exception as e:
 132:         print(f"‚ùå Git operation failed: {str(e)}", file=sys.stderr)
 133:         return 1
 134: 
 135: 
 136: # === CONTEXT COMMAND (delegated) ===
 137: 
 138: def _handle_context_sync(args) -> int:
 139:     """Handle context sync action."""
 140:     result = sync()
 141:     print(format_output(result, args.json))
 142:     return 0
 143: 
 144: 
 145: def _handle_context_health(args) -> int:
 146:     """Handle context health action."""
 147:     verbose = getattr(args, 'verbose', False)
 148: 
 149:     if verbose:
 150:         result = context_health_verbose()
 151:         if args.json:
 152:             print(format_output(result, True))
 153:         else:
 154:             print(drift_report_markdown())
 155:         return 0 if result["threshold"] != "critical" else 1
 156: 
 157:     result = health()
 158:     print(format_output(result, args.json))
 159: 
 160:     if not args.json:
 161:         print()
 162:         if result["healthy"]:
 163:             print("‚úÖ Context is healthy")
 164:         else:
 165:             print("‚ö†Ô∏è  Context needs attention:")
 166:             for rec in result["recommendations"]:
 167:                 print(f"  ‚Ä¢ {rec}")
 168: 
 169:     return 0 if result["healthy"] else 1
 170: 
 171: 
 172: def _handle_context_prune(args) -> int:
 173:     """Handle context prune action."""
 174:     age = args.age or 7
 175:     dry_run = args.dry_run or False
 176:     result = prune(age_days=age, dry_run=dry_run)
 177:     print(format_output(result, args.json))
 178:     return 0
 179: 
 180: 
 181: def _handle_context_pre_sync(args) -> int:
 182:     """Handle context pre-sync action."""
 183:     force = getattr(args, 'force', False)
 184:     result = pre_generation_sync(force=force)
 185:     if args.json:
 186:         print(format_output(result, True))
 187:     else:
 188:         print(f"‚úÖ Pre-generation sync complete")
 189:         print(f"   Drift score: {result['drift_score']:.1f}%")
 190:         print(f"   Git clean: {result['git_clean']}")
 191:     return 0
 192: 
 193: 
 194: def _handle_context_post_sync(args) -> int:
 195:     """Handle context post-sync action."""
 196:     prp_id = getattr(args, 'prp_id', None)
 197:     if not prp_id:
 198:         print("‚ùå post-sync requires --prp-id argument", file=sys.stderr)
 199:         return 1
 200: 
 201:     skip_cleanup = getattr(args, 'skip_cleanup', False)
 202:     result = post_execution_sync(prp_id, skip_cleanup=skip_cleanup)
 203:     if args.json:
 204:         print(format_output(result, True))
 205:     else:
 206:         print(f"‚úÖ Post-execution sync complete (PRP-{prp_id})")
 207:         print(f"   Cleanup: {result['cleanup_completed']}")
 208:         print(f"   Drift score: {result['drift_score']:.1f}%")
 209:         if result['final_checkpoint']:
 210:             print(f"   Checkpoint: {result['final_checkpoint']}")
 211:     return 0
 212: 
 213: 
 214: def _handle_context_auto_sync(args) -> int:
 215:     """Handle context auto-sync action."""
 216:     subaction = getattr(args, 'subaction', None)
 217: 
 218:     if subaction == "enable" or getattr(args, 'enable', False):
 219:         result = enable_auto_sync()
 220:         if args.json:
 221:             print(format_output(result, True))
 222:         else:
 223:             print(f"‚úÖ {result['mode'].title()}: Auto-sync enabled")
 224:             print(f"   Steps 2.5 and 6.5 will run automatically")
 225:         return 0
 226: 
 227:     if subaction == "disable" or getattr(args, 'disable', False):
 228:         result = disable_auto_sync()
 229:         if args.json:
 230:             print(format_output(result, True))
 231:         else:
 232:             print(f"‚úÖ {result['mode'].title()}: Auto-sync disabled")
 233:             print(f"   Manual sync required")
 234:         return 0
 235: 
 236:     if subaction == "status" or getattr(args, 'status', False):
 237:         result = get_auto_sync_status()
 238:         if args.json:
 239:             print(format_output(result, True))
 240:         else:
 241:             status_emoji = "‚úÖ" if result["enabled"] else "‚ùå"
 242:             print(f"{status_emoji} {result['message']}")
 243:         return 0
 244: 
 245:     print("‚ùå auto-sync requires --enable, --disable, or --status", file=sys.stderr)
 246:     return 1
 247: 
 248: 
 249: def cmd_context(args) -> int:
 250:     """Execute context command with delegation."""
 251:     handlers = {
 252:         "sync": _handle_context_sync,
 253:         "health": _handle_context_health,
 254:         "prune": _handle_context_prune,
 255:         "pre-sync": _handle_context_pre_sync,
 256:         "post-sync": _handle_context_post_sync,
 257:         "auto-sync": _handle_context_auto_sync,
 258:     }
 259: 
 260:     handler = handlers.get(args.action)
 261:     if not handler:
 262:         print(f"Unknown context action: {args.action}", file=sys.stderr)
 263:         return 1
 264: 
 265:     try:
 266:         return handler(args)
 267:     except Exception as e:
 268:         print(f"‚ùå Context operation failed: {str(e)}", file=sys.stderr)
 269:         import traceback
 270:         traceback.print_exc()
 271:         return 1
 272: 
 273: 
 274: # === DRIFT COMMAND (delegated) ===
 275: 
 276: def _handle_drift_history(args) -> int:
 277:     """Handle drift history action."""
 278:     history = get_drift_history(
 279:         last_n=args.last,
 280:         prp_id=args.prp_id,
 281:         action_filter=args.action_filter
 282:     )
 283: 
 284:     if args.json:
 285:         print(format_output({"history": history}, True))
 286:         return 0
 287: 
 288:     if not history:
 289:         print("No drift decisions found")
 290:         return 0
 291: 
 292:     print("\nüìä DRIFT DECISION HISTORY\n")
 293:     print("‚îÅ" * 80)
 294:     print(f"{'PRP ID':<12} {'Score':<8} {'Action':<18} {'Reviewer':<12} {'Date':<20}")
 295:     print("‚îÅ" * 80)
 296: 
 297:     for h in history:
 298:         decision = h["drift_decision"]
 299:         prp_id = h["prp_id"]
 300:         score = decision["score"]
 301:         action = decision["action"]
 302:         reviewer = decision.get("reviewer", "unknown")
 303:         timestamp = decision.get("timestamp", "N/A")[:10]
 304: 
 305:         print(f"{prp_id:<12} {score:<8.2f} {action:<18} {reviewer:<12} {timestamp:<20}")
 306: 
 307:     print("‚îÅ" * 80)
 308:     print(f"\nTotal: {len(history)} decisions\n")
 309:     return 0
 310: 
 311: 
 312: def _handle_drift_show(args) -> int:
 313:     """Handle drift show action."""
 314:     if not args.prp_id:
 315:         print("‚ùå show requires PRP ID argument", file=sys.stderr)
 316:         return 1
 317: 
 318:     decision = show_drift_decision(args.prp_id)
 319: 
 320:     if args.json:
 321:         print(format_output(decision, True))
 322:         return 0
 323: 
 324:     dd = decision["drift_decision"]
 325:     print(f"\nüìã DRIFT DECISION: {decision['prp_id']}")
 326:     print(f"PRP: {decision['prp_name']}\n")
 327:     print(f"Score: {dd['score']:.2f}%")
 328:     print(f"Action: {dd['action']}")
 329:     print(f"Reviewer: {dd.get('reviewer', 'unknown')}")
 330:     print(f"Timestamp: {dd.get('timestamp', 'N/A')}\n")
 331: 
 332:     print("Justification:")
 333:     print(f"  {dd['justification']}\n")
 334: 
 335:     if "category_breakdown" in dd:
 336:         print("Category Breakdown:")
 337:         for cat, score in dd["category_breakdown"].items():
 338:             print(f"  ‚Ä¢ {cat}: {score:.2f}%")
 339:         print()
 340: 
 341:     return 0
 342: 
 343: 
 344: def _handle_drift_summary(args) -> int:
 345:     """Handle drift summary action."""
 346:     summary = drift_summary()
 347: 
 348:     if args.json:
 349:         print(format_output(summary, True))
 350:         return 0
 351: 
 352:     print("\nüìä DRIFT SUMMARY\n")
 353:     print("‚îÅ" * 60)
 354:     print(f"Total PRPs: {summary['total_prps']}")
 355:     print(f"PRPs with Drift: {summary['prps_with_drift']}")
 356:     print(f"Average Drift Score: {summary['avg_drift_score']:.2f}%\n")
 357: 
 358:     print("Decisions:")
 359:     for action, count in summary.get("decisions", {}).items():
 360:         print(f"  ‚Ä¢ {action}: {count}")
 361:     print()
 362: 
 363:     print("Score Distribution:")
 364:     dist = summary.get("score_distribution", {})
 365:     print(f"  ‚Ä¢ Low (0-10%): {dist.get('low', 0)}")
 366:     print(f"  ‚Ä¢ Medium (10-30%): {dist.get('medium', 0)}")
 367:     print(f"  ‚Ä¢ High (30%+): {dist.get('high', 0)}")
 368:     print()
 369: 
 370:     if summary.get("category_breakdown"):
 371:         print("Category Breakdown:")
 372:         for cat, data in summary["category_breakdown"].items():
 373:             print(f"  ‚Ä¢ {cat}: {data['avg']:.2f}% avg ({data['count']} PRPs)")
 374:         print()
 375: 
 376:     if summary.get("reviewer_breakdown"):
 377:         print("Reviewer Breakdown:")
 378:         for reviewer, count in summary["reviewer_breakdown"].items():
 379:             print(f"  ‚Ä¢ {reviewer}: {count}")
 380:         print()
 381: 
 382:     return 0
 383: 
 384: 
 385: def _handle_drift_compare(args) -> int:
 386:     """Handle drift compare action."""
 387:     if not args.prp_id or not args.prp_id2:
 388:         print("‚ùå compare requires two PRP IDs", file=sys.stderr)
 389:         return 1
 390: 
 391:     comparison = compare_drift_decisions(args.prp_id, args.prp_id2)
 392: 
 393:     if args.json:
 394:         print(format_output(comparison, True))
 395:         return 0
 396: 
 397:     comp = comparison["comparison"]
 398:     prp1 = comparison["prp_1"]
 399:     prp2 = comparison["prp_2"]
 400: 
 401:     print(f"\nüîç DRIFT COMPARISON: {args.prp_id} vs {args.prp_id2}\n")
 402:     print("‚îÅ" * 60)
 403: 
 404:     print(f"\n{args.prp_id}:")
 405:     print(f"  Score: {prp1['drift_decision']['score']:.2f}%")
 406:     print(f"  Action: {prp1['drift_decision']['action']}")
 407: 
 408:     print(f"\n{args.prp_id2}:")
 409:     print(f"  Score: {prp2['drift_decision']['score']:.2f}%")
 410:     print(f"  Action: {prp2['drift_decision']['action']}")
 411: 
 412:     print(f"\nDifferences:")
 413:     print(f"  Score Difference: {comp['score_diff']:.2f}%")
 414:     print(f"  Same Action: {'Yes' if comp['same_action'] else 'No'}")
 415: 
 416:     if comp.get("common_categories"):
 417:         print(f"\nCommon Categories:")
 418:         for cat in comp["common_categories"]:
 419:             print(f"  ‚Ä¢ {cat}")
 420: 
 421:     if comp.get("divergent_categories"):
 422:         print(f"\nDivergent Categories:")
 423:         for cat in comp["divergent_categories"]:
 424:             print(f"  ‚Ä¢ {cat}")
 425: 
 426:     print()
 427:     return 0
 428: 
 429: 
 430: def cmd_drift(args) -> int:
 431:     """Execute drift history command with delegation."""
 432:     handlers = {
 433:         "history": _handle_drift_history,
 434:         "show": _handle_drift_show,
 435:         "summary": _handle_drift_summary,
 436:         "compare": _handle_drift_compare,
 437:     }
 438: 
 439:     handler = handlers.get(args.action)
 440:     if not handler:
 441:         print(f"Unknown drift action: {args.action}", file=sys.stderr)
 442:         return 1
 443: 
 444:     try:
 445:         return handler(args)
 446:     except ValueError as e:
 447:         print(f"‚ùå {str(e)}", file=sys.stderr)
 448:         return 1
 449:     except Exception as e:
 450:         print(f"‚ùå Drift operation failed: {str(e)}", file=sys.stderr)
 451:         return 1
 452: 
 453: 
 454: # === RUN_PY COMMAND ===
 455: 
 456: def cmd_run_py(args) -> int:
 457:     """Execute run_py command."""
 458:     try:
 459:         auto_input = getattr(args, 'input', None)
 460: 
 461:         result = run_py(
 462:             code=args.code if hasattr(args, 'code') else None,
 463:             file=args.file if hasattr(args, 'file') else None,
 464:             auto=auto_input,
 465:             args=args.script_args or ""
 466:         )
 467: 
 468:         if result["stdout"]:
 469:             print(result["stdout"], end="")
 470: 
 471:         if result["stderr"]:
 472:             print(result["stderr"], end="", file=sys.stderr)
 473: 
 474:         if args.json:
 475:             summary = {
 476:                 "exit_code": result["exit_code"],
 477:                 "success": result["success"],
 478:                 "duration": result["duration"]
 479:             }
 480:             print(json.dumps(summary, indent=2))
 481: 
 482:         return result["exit_code"]
 483: 
 484:     except Exception as e:
 485:         print(f"‚ùå Python execution failed: {str(e)}", file=sys.stderr)
 486:         return 1
 487: 
 488: 
 489: # === PRP COMMANDS ===
 490: 
 491: def cmd_prp_validate(args) -> int:
 492:     """Execute prp validate command."""
 493:     from ce.prp import validate_prp_yaml, format_validation_result
 494: 
 495:     try:
 496:         result = validate_prp_yaml(args.file)
 497: 
 498:         if args.json:
 499:             print(format_output(result, True))
 500:         else:
 501:             print(format_validation_result(result))
 502: 
 503:         return 0 if result["success"] else 1
 504:     except FileNotFoundError as e:
 505:         print(f"‚ùå {str(e)}", file=sys.stderr)
 506:         return 1
 507:     except Exception as e:
 508:         print(f"‚ùå PRP validation failed: {str(e)}", file=sys.stderr)
 509:         return 1
 510: 
 511: 
 512: def cmd_prp_generate(args) -> int:
 513:     """Execute prp generate command."""
 514:     try:
 515:         # Set environment variable for sequential thinking
 516:         import os
 517:         if hasattr(args, 'use_thinking'):
 518:             os.environ['CE_USE_SEQUENTIAL_THINKING'] = 'true' if args.use_thinking else 'false'
 519: 
 520:         output_dir = args.output or "PRPs/feature-requests"
 521:         join_prp = getattr(args, 'join_prp', None)
 522:         prp_path = generate_prp(args.initial_md, output_dir, join_prp=join_prp)
 523: 
 524:         result = {
 525:             "success": True,
 526:             "prp_path": prp_path,
 527:             "message": f"PRP generated: {prp_path}"
 528:         }
 529: 
 530:         if args.json:
 531:             print(format_output(result, True))
 532:         else:
 533:             print(f"‚úÖ PRP generated: {prp_path}")
 534: 
 535:         return 0
 536: 
 537:     except FileNotFoundError as e:
 538:         print(f"‚ùå {str(e)}", file=sys.stderr)
 539:         return 1
 540:     except ValueError as e:
 541:         print(f"‚ùå Invalid INITIAL.md: {str(e)}", file=sys.stderr)
 542:         return 1
 543:     except Exception as e:
 544:         print(f"‚ùå PRP generation failed: {str(e)}", file=sys.stderr)
 545:         return 1
 546: 
 547: 
 548: def cmd_prp_execute(args) -> int:
 549:     """Execute prp execute command."""
 550:     from .execute import execute_prp
 551:     from .exceptions import EscalationRequired
 552: 
 553:     try:
 554:         result = execute_prp(
 555:             prp_id=args.prp_id,
 556:             start_phase=args.start_phase,
 557:             end_phase=args.end_phase,
 558:             skip_validation=args.skip_validation,
 559:             dry_run=args.dry_run
 560:         )
 561: 
 562:         if args.json:
 563:             print(format_output(result, True))
 564:             return 0 if result["success"] else 1
 565: 
 566:         if result.get("dry_run"):
 567:             print(f"\n‚úÖ Dry run: {len(result['phases'])} phases parsed")
 568:             for phase in result['phases']:
 569:                 print(f"  Phase {phase['phase_number']}: {phase['phase_name']} ({phase['hours']}h)")
 570:         else:
 571:             print(f"\n{'='*80}")
 572:             print(f"‚úÖ PRP-{args.prp_id} execution complete")
 573:             print(f"{'='*80}")
 574:             print(f"Phases completed: {result['phases_completed']}")
 575:             print(f"Confidence score: {result['confidence_score']}")
 576:             print(f"Execution time: {result['execution_time']}")
 577:             print(f"Checkpoints created: {len(result['checkpoints_created'])}")
 578: 
 579:         return 0 if result["success"] else 1
 580: 
 581:     except EscalationRequired as e:
 582:         print(f"\n{'='*80}", file=sys.stderr)
 583:         print(f"üö® ESCALATION REQUIRED", file=sys.stderr)
 584:         print(f"{'='*80}", file=sys.stderr)
 585:         print(f"Reason: {e.reason}", file=sys.stderr)
 586:         print(f"\nError Details:", file=sys.stderr)
 587:         print(f"  Type: {e.error.get('type', 'unknown')}", file=sys.stderr)
 588:         print(f"  Location: {e.error.get('file', 'unknown')}:{e.error.get('line', '?')}", file=sys.stderr)
 589:         print(f"  Message: {e.error.get('message', 'No message')}", file=sys.stderr)
 590:         print(f"\nüîß Troubleshooting:", file=sys.stderr)
 591:         print(e.troubleshooting, file=sys.stderr)
 592:         return 2
 593: 
 594:     except FileNotFoundError as e:
 595:         print(f"‚ùå {str(e)}", file=sys.stderr)
 596:         return 1
 597:     except RuntimeError as e:
 598:         print(f"‚ùå Execution failed: {str(e)}", file=sys.stderr)
 599:         return 1
 600:     except Exception as e:
 601:         print(f"‚ùå Unexpected error: {str(e)}", file=sys.stderr)
 602:         import traceback
 603:         traceback.print_exc()
 604:         return 1
 605: 
 606: 
 607: def cmd_prp_analyze(args) -> int:
 608:     """Execute prp analyze command."""
 609:     from pathlib import Path
 610:     from .prp_analyzer import analyze_prp, format_analysis_report
 611: 
 612:     try:
 613:         prp_path = Path(args.file)
 614:         analysis = analyze_prp(prp_path)
 615:         print(format_analysis_report(analysis, json_output=args.json))
 616: 
 617:         # Return exit code based on size category
 618:         if analysis.size_category.value == "RED":
 619:             return 2
 620:         elif analysis.size_category.value == "YELLOW":
 621:             return 1
 622:         else:
 623:             return 0
 624: 
 625:     except FileNotFoundError as e:
 626:         print(f"‚ùå {str(e)}", file=sys.stderr)
 627:         return 1
 628:     except Exception as e:
 629:         print(f"‚ùå PRP analysis failed: {str(e)}", file=sys.stderr)
 630:         import traceback
 631:         traceback.print_exc()
 632:         return 1
 633: 
 634: 
 635: # === PIPELINE COMMANDS ===
 636: 
 637: def cmd_pipeline_validate(args) -> int:
 638:     """Execute pipeline validate command."""
 639:     try:
 640:         pipeline = load_abstract_pipeline(args.pipeline_file)
 641:         result = validate_pipeline(pipeline)
 642: 
 643:         if result["success"]:
 644:             print("‚úÖ Pipeline validation passed")
 645:             return 0
 646:         else:
 647:             print("‚ùå Pipeline validation failed:")
 648:             for error in result["errors"]:
 649:                 print(f"  - {error}")
 650:             return 1
 651: 
 652:     except Exception as e:
 653:         print(f"‚ùå Validation error: {str(e)}", file=sys.stderr)
 654:         return 1
 655: 
 656: 
 657: def cmd_pipeline_render(args) -> int:
 658:     """Execute pipeline render command."""
 659:     from pathlib import Path
 660: 
 661:     try:
 662:         pipeline = load_abstract_pipeline(args.pipeline_file)
 663: 
 664:         if args.executor == "github-actions":
 665:             executor = GitHubActionsExecutor()
 666:         else:
 667:             executor = MockExecutor()
 668: 
 669:         rendered = executor.render(pipeline)
 670: 
 671:         if args.output:
 672:             Path(args.output).write_text(rendered)
 673:             print(f"‚úÖ Rendered to {args.output}")
 674:         else:
 675:             print(rendered)
 676: 
 677:         return 0
 678: 
 679:     except Exception as e:
 680:         print(f"‚ùå Render error: {str(e)}", file=sys.stderr)
 681:         return 1
 682: 
 683: 
 684: # === METRICS COMMAND ===
 685: 
 686: def cmd_metrics(args) -> int:
 687:     """Display system metrics and success rates."""
 688:     try:
 689:         collector = MetricsCollector(metrics_file=args.file)
 690:         summary = collector.get_summary()
 691: 
 692:         if args.format == "json":
 693:             print(json.dumps(summary, indent=2))
 694:             return 0
 695: 
 696:         # Human-readable format
 697:         print("\nüìä Context Engineering Metrics")
 698:         print("=" * 60)
 699: 
 700:         # Success rates
 701:         rates = summary["success_rates"]
 702:         print("\nüéØ Success Rates:")
 703:         print(f"  First-pass:  {rates['first_pass_rate']:.1f}%")
 704:         print(f"  Second-pass: {rates['second_pass_rate']:.1f}%")
 705:         print(f"  Overall:     {rates['overall_rate']:.1f}%")
 706:         print(f"  Total PRPs:  {rates['total_executions']}")
 707: 
 708:         # Validation stats
 709:         val_stats = summary["validation_stats"]
 710:         if val_stats:
 711:             print("\n‚úÖ Validation Pass Rates:")
 712:             for key, value in sorted(val_stats.items()):
 713:                 if key.endswith("_pass_rate"):
 714:                     level = key.replace("_pass_rate", "")
 715:                     total_key = f"{level}_total"
 716:                     total = val_stats.get(total_key, 0)
 717:                     print(f"  {level.upper()}: {value:.1f}% ({total} executions)")
 718: 
 719:         # Performance
 720:         perf = summary["performance"]
 721:         print("\n‚ö° Performance:")
 722:         print(f"  Avg duration: {perf['avg_duration']:.1f}s")
 723:         print(f"  Total PRPs:   {perf['total_prps']}")
 724:         print(f"  Total validations: {perf['total_validations']}")
 725: 
 726:         print("=" * 60)
 727:         return 0
 728: 
 729:     except FileNotFoundError:
 730:         print(f"‚ùå Metrics file not found: {args.file}", file=sys.stderr)
 731:         print(f"üîß Troubleshooting: Run PRP executions to collect metrics", file=sys.stderr)
 732:         return 1
 733:     except Exception as e:
 734:         print(f"‚ùå Metrics error: {str(e)}", file=sys.stderr)
 735:         return 1
 736: 
 737: 
 738: # === ANALYZE-CONTEXT COMMAND ===
 739: 
 740: def _get_analysis_result(args, cache_ttl: int):
 741:     """Get analysis result from cache or fresh analysis.
 742:     
 743:     Args:
 744:         args: Command arguments
 745:         cache_ttl: Cache TTL in minutes
 746:     
 747:     Returns:
 748:         Analysis result dict
 749:     """
 750:     from .update_context import (
 751:         analyze_context_drift,
 752:         get_cached_analysis,
 753:         is_cache_valid,
 754:     )
 755:     
 756:     # Skip cache if forced
 757:     if getattr(args, 'force', False):
 758:         return analyze_context_drift()
 759:     
 760:     # Try to use cache
 761:     cached = get_cached_analysis()
 762:     if cached and is_cache_valid(cached, ttl_minutes=cache_ttl):
 763:         return cached
 764:     
 765:     # Cache miss or invalid - run fresh analysis
 766:     return analyze_context_drift()
 767: 
 768: 
 769: def _print_analysis_output(result, args, cache_ttl: int) -> None:
 770:     """Print analysis output in human or JSON format.
 771:     
 772:     Args:
 773:         result: Analysis result
 774:         args: Command arguments
 775:         cache_ttl: Cache TTL in minutes
 776:     """
 777:     if args.json:
 778:         print(format_output(result, True))
 779:         return
 780:     
 781:     # Calculate cache age for display
 782:     from datetime import datetime, timezone
 783:     cache_age_str = ""
 784:     if not getattr(args, 'force', False):
 785:         try:
 786:             generated_at = datetime.fromisoformat(
 787:                 result["generated_at"].replace("+00:00", "+00:00")
 788:             )
 789:             if generated_at.tzinfo is None:
 790:                 generated_at = generated_at.replace(tzinfo=timezone.utc)
 791:             now = datetime.now(timezone.utc)
 792:             age_minutes = int((now - generated_at).total_seconds() / 60)
 793:             cache_age_str = f" ({age_minutes}m old, TTL: {cache_ttl}m)"
 794:         except Exception:
 795:             cache_age_str = f" (TTL: {cache_ttl}m)"
 796:     
 797:     # Human-readable output
 798:     drift_score = result["drift_score"]
 799:     drift_level = result["drift_level"]
 800:     violations = result.get("violation_count", 0)
 801:     missing = len(result.get("missing_examples", []))
 802:     duration = result.get("duration_seconds", 0)
 803:     
 804:     # Emoji indicators
 805:     if drift_level == "ok":
 806:         indicator = "‚úÖ"
 807:     elif drift_level == "warning":
 808:         indicator = "‚ö†Ô∏è "
 809:     else:  # critical
 810:         indicator = "üö®"
 811:     
 812:     print("üîç Analyzing context drift...")
 813:     if duration > 0:
 814:         print("   üìä Pattern conformance: scan complete")
 815:         print("   üìö Documentation gaps: check complete")
 816:         print()
 817:     
 818:     if cache_age_str and not getattr(args, 'force', False):
 819:         print(f"‚úÖ Using cached analysis{cache_age_str}")
 820:         print(f"   Use --force to re-analyze")
 821:     
 822:     print(f"{indicator} Analysis complete ({duration}s)")
 823:     print(f"   Drift Score: {drift_score:.1f}% ({drift_level.upper()})")
 824:     print(f"   Violations: {violations}")
 825:     if missing > 0:
 826:         print(f"   Missing Examples: {missing}")
 827:     print(f"   Report: {result['report_path']}")
 828: 
 829: 
 830: def cmd_analyze_context(args) -> int:
 831:     """Execute analyze-context command.
 832:     
 833:     Fast drift check without metadata updates - optimized for CI/CD.
 834:     
 835:     Returns:
 836:         Exit code: 0 (ok), 1 (warning), 2 (critical)
 837:     """
 838:     from .update_context import get_cache_ttl
 839:     
 840:     try:
 841:         # Get cache TTL (CLI flag > config > default)
 842:         cache_ttl = get_cache_ttl(getattr(args, 'cache_ttl', None))
 843:         
 844:         # Get analysis result (cached or fresh)
 845:         result = _get_analysis_result(args, cache_ttl)
 846:         
 847:         # Print output
 848:         _print_analysis_output(result, args, cache_ttl)
 849:         
 850:         # Return exit code based on drift level
 851:         exit_codes = {"ok": 0, "warning": 1, "critical": 2}
 852:         return exit_codes[result["drift_level"]]
 853:     
 854:     except Exception as e:
 855:         print(f"‚ùå Analysis failed: {str(e)}", file=sys.stderr)
 856:         import traceback
 857:         traceback.print_exc()
 858:         return 1
 859: 
 860: 
 861: # === UPDATE-CONTEXT COMMAND ===
 862: 
 863: def _should_rebuild_packages() -> bool:
 864:     """
 865:     Check if repomix packages should be rebuilt.
 866: 
 867:     Checks:
 868:     - .serena/memories/ modified (git status)
 869:     - examples/ modified (git status)
 870: 
 871:     Returns:
 872:         True if rebuild needed, False otherwise
 873:     """
 874:     import subprocess
 875:     from pathlib import Path
 876: 
 877:     try:
 878:         # Check if in git repo
 879:         result = subprocess.run(
 880:             ["git", "rev-parse", "--is-inside-work-tree"],
 881:             capture_output=True,
 882:             text=True,
 883:             timeout=5
 884:         )
 885:         if result.returncode != 0:
 886:             return False
 887: 
 888:         # Check for modifications in framework directories
 889:         result = subprocess.run(
 890:             ["git", "status", "--porcelain", ".serena/memories/", "examples/"],
 891:             capture_output=True,
 892:             text=True,
 893:             timeout=5
 894:         )
 895: 
 896:         # If any modifications detected, rebuild
 897:         return bool(result.stdout.strip())
 898: 
 899:     except Exception:
 900:         # If check fails, don't rebuild (non-fatal)
 901:         return False
 902: 
 903: 
 904: def _rebuild_repomix_packages() -> bool:
 905:     """
 906:     Rebuild repomix packages by running build-and-distribute.sh.
 907: 
 908:     Returns:
 909:         True if rebuild successful, False otherwise
 910:     """
 911:     import subprocess
 912:     from pathlib import Path
 913:     import shutil
 914: 
 915:     try:
 916:         project_root = Path.cwd()
 917:         build_script = project_root / ".ce" / "build-and-distribute.sh"
 918: 
 919:         if not build_script.exists():
 920:             return False
 921: 
 922:         # Run build script
 923:         result = subprocess.run(
 924:             [str(build_script)],
 925:             cwd=project_root,
 926:             capture_output=True,
 927:             text=True,
 928:             timeout=60
 929:         )
 930: 
 931:         if result.returncode != 0:
 932:             return False
 933: 
 934:         # Copy packages from ce-32/builds/ to .ce/
 935:         builds_dir = project_root / "ce-32" / "builds"
 936:         ce_dir = project_root / ".ce"
 937: 
 938:         if builds_dir.exists():
 939:             for xml_file in builds_dir.glob("*.xml"):
 940:                 shutil.copy2(xml_file, ce_dir / xml_file.name)
 941: 
 942:         return True
 943: 
 944:     except Exception as e:
 945:         print(f"   Rebuild error: {str(e)}")
 946:         return False
 947: 
 948: 
 949: def cmd_update_context(args) -> int:
 950:     """Execute update-context command.
 951: 
 952:     Workflow:
 953:         1. Standard context sync (always runs)
 954:         2. Drift remediation workflow (always runs)
 955:            - Vanilla mode (no --remediate): asks approval before PRP generation
 956:            - YOLO mode (--remediate): skips approval, auto-generates PRP
 957:     """
 958:     try:
 959:         # Step 1: ALWAYS run standard context sync first
 960:         target_prp = args.prp if hasattr(args, 'prp') and args.prp else None
 961:         result = sync_context(target_prp=target_prp)
 962: 
 963:         if args.json:
 964:             print(format_output(result, True))
 965:         else:
 966:             print("‚úÖ Context sync completed")
 967:             print(f"   PRPs scanned: {result['prps_scanned']}")
 968:             print(f"   PRPs updated: {result['prps_updated']}")
 969:             print(f"   PRPs moved: {result['prps_moved']}")
 970:             print(f"   CE updated: {result['ce_updated_count']}")
 971:             print(f"   Serena updated: {result['serena_updated_count']}")
 972: 
 973:             if result['errors']:
 974:                 print(f"\n‚ö†Ô∏è  Errors encountered:")
 975:                 for error in result['errors']:
 976:                     print(f"   - {error}")
 977: 
 978:         # Step 1.5: Auto-rebuild repomix packages if framework files updated
 979:         if not args.json and _should_rebuild_packages():
 980:             print("\nüì¶ Framework files updated - rebuilding packages...")
 981:             rebuild_success = _rebuild_repomix_packages()
 982:             if rebuild_success:
 983:                 print("‚úÖ Packages rebuilt successfully")
 984:             else:
 985:                 print("‚ö†Ô∏è  Package rebuild failed (non-fatal)")
 986: 
 987:         # Step 2: ALWAYS run drift remediation workflow after sync
 988:         from .update_context import remediate_drift_workflow
 989: 
 990:         yolo_mode = hasattr(args, 'remediate') and args.remediate
 991:         remediate_result = remediate_drift_workflow(yolo_mode=yolo_mode)
 992: 
 993:         if args.json:
 994:             # Combine both results in JSON output
 995:             combined = {
 996:                 "sync": result,
 997:                 "remediation": remediate_result
 998:             }
 999:             print(format_output(combined, True))
1000: 
1001:         # Combine success status from both workflows
1002:         success = result['success'] and remediate_result['success']
1003:         return 0 if success else 1
1004: 
1005:     except Exception as e:
1006:         print(f"‚ùå Update context failed: {str(e)}", file=sys.stderr)
1007:         import traceback
1008:         traceback.print_exc()
1009:         return 1
1010: 
1011: 
1012: def cmd_vacuum(args):
1013:     """Handle vacuum command.
1014: 
1015:     Args:
1016:         args: Parsed command-line arguments
1017: 
1018:     Returns:
1019:         Exit code (0 = success, 1 = candidates found, 2 = error)
1020:     """
1021:     from pathlib import Path
1022:     from .vacuum import VacuumCommand
1023: 
1024:     try:
1025:         # Find project root (where .ce/ directory exists)
1026:         current = Path.cwd()
1027:         project_root = None
1028: 
1029:         for parent in [current] + list(current.parents):
1030:             if (parent / ".ce").exists():
1031:                 project_root = parent
1032:                 break
1033: 
1034:         if not project_root:
1035:             print("‚ùå Error: Not in a Context Engineering project (.ce/ not found)\nüîß Troubleshooting: Check inputs and system state", file=sys.stderr)
1036:             return 2
1037: 
1038:         # Resolve scan path if provided
1039:         scan_path = None
1040:         if hasattr(args, 'path') and args.path:
1041:             scan_path = project_root / args.path
1042:             if not scan_path.exists():
1043:                 print(f"‚ùå Error: Path does not exist: {args.path}\nüîß Troubleshooting: Verify file path exists", file=sys.stderr)
1044:                 return 2
1045:             if not scan_path.is_dir():
1046:                 print(f"‚ùå Error: Path is not a directory: {args.path}\nüîß Troubleshooting: Check inputs and system state", file=sys.stderr)
1047:                 return 2
1048: 
1049:         # Run vacuum command
1050:         vacuum = VacuumCommand(project_root)
1051:         return vacuum.run(
1052:             dry_run=not (args.execute or args.force or args.auto or args.nuclear),
1053:             min_confidence=args.min_confidence,
1054:             exclude_strategies=args.exclude_strategies or [],
1055:             execute=args.execute,
1056:             force=args.force,
1057:             auto=args.auto,
1058:             nuclear=args.nuclear,
1059:             scan_path=scan_path,
1060:         )
1061: 
1062:     except KeyboardInterrupt:
1063:         print("\n‚ùå Vacuum cancelled by user", file=sys.stderr)
1064:         return 2
1065:     except Exception as e:
1066:         print(f"‚ùå Vacuum failed: {str(e)}", file=sys.stderr)
1067:         import traceback
1068:         traceback.print_exc()
1069:         return 2
1070: 
1071: 
1072: # === BLEND COMMAND ===
1073: 
1074: def cmd_blend(args) -> int:
1075:     """Execute blend command."""
1076:     return blend_run_blend(args)
1077: 
1078: 
1079: # === CLEANUP COMMAND ===
1080: 
1081: def cmd_cleanup(args) -> int:
1082:     """Execute cleanup command."""
1083:     from pathlib import Path
1084:     from .blending.cleanup import cleanup_legacy_dirs
1085: 
1086:     try:
1087:         # Determine dry_run mode
1088:         dry_run = not args.execute
1089: 
1090:         target_project = Path.cwd()
1091: 
1092:         status = cleanup_legacy_dirs(
1093:             target_project=target_project,
1094:             dry_run=dry_run
1095:         )
1096: 
1097:         # Exit with success if all cleanups succeeded
1098:         if all(status.values()):
1099:             return 0
1100:         else:
1101:             return 1
1102: 
1103:     except ValueError as e:
1104:         # Migration incomplete
1105:         print(f"‚ùå {e}", file=sys.stderr)
1106:         return 2
1107:     except Exception as e:
1108:         print(f"‚ùå Cleanup failed: {e}", file=sys.stderr)
1109:         import traceback
1110:         traceback.print_exc()
1111:         return 1
1112: 
1113: 
1114: # === INIT-PROJECT COMMAND ===
1115: 
1116: def cmd_init_project(args) -> int:
1117:     """Execute init-project command.
1118: 
1119:     Orchestrates CE framework installation on target projects using ProjectInitializer.
1120: 
1121:     Args:
1122:         args: Parsed command-line arguments with:
1123:             - target_dir: Path to target project
1124:             - dry_run: If True, show actions without executing
1125:             - blend_only: If True, run only blend phase
1126:             - phase: Which phase to run (extract, blend, initialize, verify, all)
1127: 
1128:     Returns:
1129:         Exit code: 0 (success), 1 (user error), 2 (initialization error)
1130:     """
1131:     from pathlib import Path
1132: 
1133:     try:
1134:         # Parse and resolve target directory to absolute path
1135:         target_dir = Path(args.target_dir).resolve()
1136: 
1137:         # Validate target directory exists
1138:         if not target_dir.exists():
1139:             print(f"‚ùå Target directory not found: {target_dir}", file=sys.stderr)
1140:             print(f"üîß Troubleshooting: Verify path and ensure directory exists", file=sys.stderr)
1141:             return 1
1142: 
1143:         # Create ProjectInitializer instance
1144:         dry_run = getattr(args, 'dry_run', False)
1145:         initializer = ProjectInitializer(target_dir, dry_run=dry_run)
1146: 
1147:         # Handle --blend-only flag (takes precedence over --phase)
1148:         if getattr(args, 'blend_only', False):
1149:             result = initializer.blend()
1150: 
1151:             if args.json:
1152:                 print(format_output({"blend": result}, True))
1153:             else:
1154:                 print(result.get("message", "Blend phase completed"))
1155:                 if result.get("stdout"):
1156:                     print(result["stdout"])
1157:                 if result.get("stderr"):
1158:                     print(result["stderr"], file=sys.stderr)
1159: 
1160:             return 0 if result.get("success", False) else 2
1161: 
1162:         # Handle --phase flag or run all phases
1163:         phase = getattr(args, 'phase', 'all')
1164:         results = initializer.run(phase=phase)
1165: 
1166:         # Output results
1167:         if getattr(args, 'json', False):
1168:             print(format_output(results, True))
1169:         else:
1170:             for phase_name, result in results.items():
1171:                 print(f"\n{'='*60}")
1172:                 print(f"Phase: {phase_name}")
1173:                 print(f"{'='*60}")
1174:                 print(result.get("message", "No message"))
1175: 
1176:                 # Print stdout/stderr if present
1177:                 if result.get("stdout"):
1178:                     print(result["stdout"])
1179:                 if result.get("stderr"):
1180:                     print(result["stderr"], file=sys.stderr)
1181: 
1182:         # Determine exit code based on all phases
1183:         all_success = all(r.get("success", True) for r in results.values())
1184:         return 0 if all_success else 2
1185: 
1186:     except ValueError as e:
1187:         # Invalid phase or other user errors
1188:         print(f"‚ùå {str(e)}", file=sys.stderr)
1189:         print(f"üîß Troubleshooting: Check command arguments and try again", file=sys.stderr)
1190:         return 1
1191:     except Exception as e:
1192:         # Initialization errors from ProjectInitializer
1193:         print(f"‚ùå Initialization failed: {str(e)}", file=sys.stderr)
1194:         print(f"üîß Troubleshooting: Check error details above and verify framework files exist", file=sys.stderr)
1195:         import traceback
1196:         traceback.print_exc()
1197:         return 2
</file>

<file path="CLAUDE.md">
  1: # Context Engineering Tools - Project Guide
  2: 
  3: **Project**: CLI tooling for Context Engineering framework operations
  4: 
  5: ## Communication
  6: 
  7: Direct, token-efficient. No fluff. Call out problems directly.
  8: 
  9: ## Core Principles
 10: 
 11: ### Syntropy MCP First
 12: - Use `mcp__syntropy__<server>_<tool>` format (standard MCP protocol - client-side prefixing)
 13: - Prefer Syntropy tools over bash/cmdline
 14: - See [Naming Convention](examples/syntropy-mcp-naming-convention.md) for complete spec
 15: 
 16: ### No Fishy Fallbacks
 17: - Fast failure: Let exceptions bubble up
 18: - Actionable errors: Include üîß troubleshooting
 19: - No silent corruption
 20: 
 21: ### KISS
 22: - Simple solutions first
 23: - Clear code over clever code
 24: - Minimal dependencies (stdlib only)
 25: - Single responsibility per function
 26: 
 27: ### UV Package Management - STRICT
 28: ```bash
 29: uv add package-name              # Production
 30: uv add --dev package-name        # Development
 31: uv sync                          # Install
 32: 
 33: # ‚ùå FORBIDDEN: Manual pyproject.toml editing
 34: ```
 35: 
 36: ### Ad-Hoc Code Policy
 37: - Max 3 LOC inline
 38: - Longer code ‚Üí tmp/ file and execute
 39: - Must execute via run_py
 40: 
 41: ## Quick Commands
 42: 
 43: ```bash
 44: cd tools
 45: 
 46: # Validation & health
 47: uv run ce validate --level all
 48: uv run ce context health
 49: uv run ce git status
 50: 
 51: # Cleanup
 52: uv run ce vacuum                  # Dry-run (report only)
 53: uv run ce vacuum --execute        # Delete temp files only
 54: uv run ce vacuum --auto           # Delete temp files + obsolete docs/dead links
 55: 
 56: # Testing
 57: uv run pytest tests/ -v
 58: 
 59: # Run Python (3 LOC max ad-hoc)
 60: uv run ce run_py "print('hello')"
 61: uv run ce run_py ../tmp/script.py
 62: ```
 63: 
 64: ## Framework Initialization
 65: 
 66: **Quick Start** (Automated - RECOMMENDED):
 67: ```bash
 68: npx syntropy-mcp init ce-framework
 69: ```
 70: Time: <5 minutes | Automatically extracts 50 files, reorganizes tools, blends settings
 71: 
 72: **Manual Setup**: See [examples/INITIALIZATION.md](examples/INITIALIZATION.md) for complete CE 1.1 initialization guide.
 73: 
 74: **Manual Key Steps** (5-phase workflow):
 75: 1. **Bucket Collection**: Extract existing Serena memories, examples, PRPs, CLAUDE.md, .claude directory
 76: 2. **User Files Migration**: Copy validated user files with `type: user` YAML headers
 77: 3. **Repomix Package Handling**: Extract ce-infrastructure.xml to /system/ subfolders
 78: 4. **Blending**: Merge framework + user files (CLAUDE.md sections, settings.local.json, commands)
 79: 5. **Cleanup**: Remove initialization artifacts, verify structure
 80: 
 81: **Repomix Usage** (manual context loading):
 82: 
 83: ```bash
 84: # Load workflow docs (commands, validation, PRP patterns)
 85: # Reference package - stored in .ce/, not extracted during initialization
 86: cat .ce/ce-workflow-docs.xml
 87: 
 88: # Load infrastructure docs (memories, rules, system architecture)
 89: # Extracted to /system/ subfolders during Phase 3 of initialization
 90: npx repomix --unpack .ce/ce-infrastructure.xml --target tmp/extraction/
 91: ```
 92: 
 93: **Repomix Package Structure** (CE 1.1):
 94: - **ce-workflow-docs.xml**: 85KB (reference package, not extracted)
 95: - **ce-infrastructure.xml**: 206KB (all framework files with /system/ organization)
 96: - **Combined**: 291KB total
 97: 
 98: **Build and Distribution**:
 99: ```bash
100: # Regenerate packages and distribute to syntropy-mcp
101: .ce/build-and-distribute.sh
102: ```
103: This script regenerates both packages and copies them to `syntropy-mcp/boilerplate/ce-framework/` for automated init.
104: 
105: **Migration Scenarios**:
106: 
107: All scenarios documented in [INITIALIZATION.md](examples/INITIALIZATION.md) with scenario-specific variations within each phase:
108: - **Greenfield**: New project setup (10 min)
109: - **Mature Project**: Add CE to existing codebase (45 min)
110: - **CE 1.0 Upgrade**: Upgrade CE 1.0 ‚Üí CE 1.1 (40 min)
111: - **Partial Install**: Complete partial CE installation (15 min)
112: 
113: **Memory Type System** (CE 1.1):
114: 
115: Framework memories (23 files) use `type: regular` by default:
116: ```yaml
117: ---
118: type: regular
119: category: documentation
120: tags: [tag1, tag2, tag3]
121: created: "2025-11-04T17:30:00Z"
122: updated: "2025-11-04T17:30:00Z"
123: ---
124: ```
125: 
126: **Critical Memory Candidates** (upgrade during initialization):
127: - code-style-conventions.md
128: - suggested-commands.md
129: - task-completion-checklist.md
130: - testing-standards.md
131: - tool-usage-syntropy.md
132: - use-syntropy-tools-not-bash.md
133: 
134: **User File Headers** (added during Phase 2 of initialization):
135: 
136: User memories:
137: ```yaml
138: ---
139: type: user
140: source: target-project
141: created: "2025-11-04T00:00:00Z"
142: updated: "2025-11-04T00:00:00Z"
143: ---
144: ```
145: 
146: User PRPs:
147: ```yaml
148: ---
149: prp_id: USER-001
150: title: User Feature Implementation
151: status: completed
152: created: "2025-11-04"
153: source: target-project
154: type: user
155: ---
156: ```
157: 
158: **See Also**:
159: - [examples/INITIALIZATION.md](examples/INITIALIZATION.md) - Complete initialization guide
160: - [.serena/memories/README.md](.serena/memories/README.md) - Memory type system documentation
161: - [examples/templates/PRP-0-CONTEXT-ENGINEERING.md](examples/templates/PRP-0-CONTEXT-ENGINEERING.md) - Document framework installation
162: 
163: ## Working Directory
164: 
165: **Default**: `/Users/bprzybysz/nc-src/ctx-eng-plus`
166: 
167: **For tools/ commands**: Use `cd tools &&` or `uv run -C tools`
168: 
169: **Note**: In THIS repo (ctx-eng-plus development), tools are at `tools/`. In TARGET projects that extract the infrastructure package, tools are installed to `.ce/tools/`.
170: 
171: ## Hooks
172: 
173: **Pre-Commit**: Runs `ce validate --level 4` before commit (skip: `--no-verify`)
174: 
175: **Session Start**: Auto drift score check
176: 
177: **Shell Functions** (optional): Source `.ce/shell-functions.sh` for `cet` alias
178: 
179: ## Tool Naming Convention
180: 
181: Format: `mcp__syntropy__<server>_<tool>`
182: - `mcp__` - MCP prefix (double underscore)
183: - `syntropy__` - Syntropy server (double underscore)
184: - `<server>_` - Server name + single underscore
185: - `<tool>` - Tool name
186: 
187: Example: `mcp__syntropy__serena_find_symbol`
188: 
189: ## Allowed Tools Summary
190: 
191: **Post-Lockdown State** (after PRP-A & PRP-D):
192: - **Before**: 87 MCP tools (via Syntropy aggregator)
193: - **After**: 28 MCP tools (59 denied for native tool preference)
194: - **Token reduction**: ~44k tokens (96% reduction from 46k‚Üí2k)
195: 
196: ### Kept Tools by Category
197: 
198: **Serena** (13 tools): Code symbol navigation
199: - activate_project, find_symbol, get_symbols_overview, search_for_pattern
200: - find_referencing_symbols, replace_symbol_body, write_memory, read_memory
201: - list_memories, create_text_file, read_file, list_dir, delete_memory
202: 
203: **Linear** (9 tools): Project management integration
204: - create_issue, get_issue, list_issues, update_issue
205: - list_projects, list_teams, list_users, get_team, create_project
206: 
207: **Context7** (2 tools): Library documentation
208: - resolve_library_id, get_library_docs
209: 
210: **Thinking** (1 tool): Complex reasoning
211: - sequentialthinking
212: 
213: **Syntropy System** (3 tools): System utilities
214: - healthcheck (MCP diagnostics)
215: - enable_tools (dynamic tool management)
216: - list_all_tools (list all available tools with states)
217: 
218: **Bash Commands** (~50 patterns): See "Command Permissions" section below
219: **Native Tools**: Read, Write, Edit, Glob, Grep, WebSearch, WebFetch
220: 
221: ### Denied Tools (55 total)
222: 
223: **Rationale**: Native Claude Code tools provide equivalent or better functionality
224: 
225: **Categories**:
226: - Filesystem (8): Use Read, Write, Edit, Glob instead
227: - Git (5): Use Bash(git:*) instead
228: - GitHub (26): Use Bash(gh:*) instead
229: - Repomix (4): Use incremental Glob/Grep/Read instead
230: - Playwright (6): Use WebFetch or Bash(playwright CLI) instead
231: - Perplexity (1): Use WebSearch instead
232: - Syntropy (5): Use Read for docs, rare-use tools
233: 
234: **Full details**: See [TOOL-USAGE-GUIDE.md](TOOL-USAGE-GUIDE.md)
235: 
236: ## Command Permissions
237: 
238: **Permission Model**: Auto-allow safe commands, ask-first for potentially destructive operations.
239: 
240: ### Auto-Allow Patterns (~35 bash patterns)
241: 
242: Commands that never prompt:
243: 
244: **File Inspection**:
245: - `ls`, `cat`, `head`, `tail`, `less`, `more`, `file`, `stat`
246: 
247: **Navigation**:
248: - `cd`, `pwd`, `which`, `whereis`
249: 
250: **Search**:
251: - `find`, `grep`, `rg`, `tree`
252: 
253: **Text Processing**:
254: - `sed`, `awk`, `sort`, `uniq`, `cut`, `diff`, `comm`, `wc`
255: 
256: **Environment**:
257: - `env`, `ps`, `echo`
258: 
259: **Development**:
260: - `git` (all operations), `gh` (GitHub CLI)
261: - `uv`, `uvx`, `pytest`
262: - `python`, `python3`
263: 
264: **Special Cases**:
265: - `rm -rf ~/.mcp-auth` (MCP troubleshooting)
266: 
267: **Full list**: See `.claude/settings.local.json` "allow" array
268: 
269: ### Ask-First Patterns (15 patterns)
270: 
271: Commands that require confirmation:
272: 
273: **File Operations** (potentially destructive):
274: - `rm`, `mv`, `cp`
275: 
276: **Network Operations**:
277: - `curl`, `wget`, `nc`, `telnet`, `ssh`, `scp`, `rsync`
278: 
279: **Package Management**:
280: - `brew install`, `npm install`, `pip install`, `gem install`
281: 
282: **System Operations**:
283: - `sudo` (any sudo command)
284: 
285: **Rationale**: Safety gate for operations that modify files, access network, or require elevated privileges.
286: 
287: **Full list**: See `.claude/settings.local.json` "ask" array
288: 
289: ### Permission Behavior
290: 
291: **Unlisted commands**: Prompt by default (ask before execution)
292: **Workaround**: Add to allow list in `.claude/settings.local.json` if frequently used
293: 
294: ## Quick Tool Selection
295: 
296: **üîó Comprehensive Guide**: See [examples/TOOL-USAGE-GUIDE.md](examples/TOOL-USAGE-GUIDE.md) for:
297: - Decision tree (flowchart for tool selection)
298: - Common tasks with right/wrong examples
299: - Anti-patterns to avoid
300: - Migration table (55 denied tools ‚Üí alternatives)
301: 
302: **Quick Reference**:
303: 
304: **Analyze code**:
305: - Know symbol ‚Üí `serena_find_symbol`
306: - Explore file ‚Üí `serena_get_symbols_overview`
307: - Search patterns ‚Üí `Grep` (native, not serena_search_for_pattern)
308: - Find usages ‚Üí `serena_find_referencing_symbols`
309: 
310: **Modify files**:
311: - New ‚Üí `Write` (native)
312: - Existing (surgical) ‚Üí `Edit` (native)
313: - Config/text ‚Üí `Read` (native)
314: 
315: **Version control**:
316: - Use `Bash(git:*)` (native git commands)
317: - NOT `mcp__syntropy__git_git_status` (denied)
318: 
319: **GitHub operations**:
320: - Use `Bash(gh:*)` (native gh CLI)
321: - NOT `mcp__syntropy__github_*` (denied)
322: 
323: **External knowledge**:
324: - Documentation ‚Üí `context7_get_library_docs`
325: - Web search ‚Üí `WebSearch` (native)
326: - Web content ‚Üí `WebFetch` (native)
327: 
328: **Complex reasoning**: `sequentialthinking`
329: 
330: **Project management**: Linear tools (all 9 kept)
331: 
332: **System health**: `healthcheck` (detailed diagnostics with `detailed=true`)
333: 
334: ## Project Structure
335: 
336: ```
337: tools/
338: ‚îú‚îÄ‚îÄ ce/                 # Source code
339: ‚îÇ   ‚îú‚îÄ‚îÄ core.py         # File, git, shell ops
340: ‚îÇ   ‚îú‚îÄ‚îÄ validate.py     # 3-level validation
341: ‚îÇ   ‚îî‚îÄ‚îÄ context.py      # Context management
342: ‚îú‚îÄ‚îÄ tests/              # Test suite
343: ‚îú‚îÄ‚îÄ pyproject.toml      # UV config (don't edit!)
344: ‚îî‚îÄ‚îÄ bootstrap.sh        # Setup script
345: ```
346: 
347: ## Testing Standards
348: 
349: **TDD**: Test first ‚Üí fail ‚Üí implement ‚Üí refactor
350: 
351: **Real functionality**: No fake results, no mocks in tests
352: 
353: **Test before critical changes** (tool naming, API changes, refactoring)
354: 
355: ## Code Quality
356: 
357: - Functions: 50 lines (single responsibility)
358: - Files: 300-500 lines (logical modules)
359: - Classes: 100 lines (single concept)
360: - Mark mocks with FIXME in production code
361: 
362: ## Context Commands
363: 
364: ```bash
365: # Sync all PRPs with codebase
366: cd tools && uv run ce update-context
367: 
368: # Sync specific PRP
369: cd tools && uv run ce update-context --prp PRPs/executed/PRP-6.md
370: 
371: # Fast drift check (2-3s vs 10-15s)
372: cd tools && uv run ce analyze-context
373: 
374: # Force re-analysis
375: cd tools && uv run ce analyze-context --force
376: ```
377: 
378: **Drift Exit Codes**:
379: - 0: <5% (healthy)
380: - 1: 5-15% (warning)
381: - 2: ‚â•15% (critical)
382: 
383: ## Syntropy MCP Tool Sync
384: 
385: **Dynamic tool management** - Enable/disable tools at runtime without restart
386: 
387: ```bash
388: # Sync settings with Syntropy MCP tool state
389: /sync-with-syntropy
390: 
391: # Workflow example:
392: # 1. Enable/disable tools via Syntropy
393: mcp__syntropy__enable_tools(
394:   enable=["serena_find_symbol", "context7_get_library_docs"],
395:   disable=["filesystem_read_file", "git_git_status"]
396: )
397: 
398: # 2. Sync settings to .claude/settings.local.json
399: /sync-with-syntropy
400: 
401: # 3. Verify changes
402: cat .claude/settings.local.json
403: ```
404: 
405: **How it works**:
406: 1. Call `mcp__syntropy__list_all_tools` to get current tool states
407: 2. Update `.claude/settings.local.json` to match
408: 3. Backup original settings to `.claude/settings.local.json.backup`
409: 4. Output clear summary of changes made
410: 
411: **Benefits**:
412: - Real-time tool control (no MCP restart needed)
413: - Persistent state across sessions (`~/.syntropy/tool-state.json`)
414: - Context-aware tool sets (enable 10 tools for quick tasks, all 87 for deep analysis)
415: 
416: ## Linear Integration
417: 
418: **Config**: `.ce/linear-defaults.yml`
419: - Project: "Context Engineering"
420: - Assignee: "blazej.przybyszewski@gmail.com"
421: - Team: "Blaise78"
422: 
423: **Auto-create issues**: `/generate-prp` uses defaults
424: 
425: **Join existing issue**: `/generate-prp --join-prp 12`
426: 
427: **Troubleshooting**: `rm -rf ~/.mcp-auth` (pre-approved)
428: 
429: ## Batch PRP Generation
430: 
431: **Decompose large plans into staged, parallelizable PRPs with automatic dependency analysis**
432: 
433: ```bash
434: # Create plan document
435: vim FEATURE-PLAN.md
436: 
437: # Generate all PRPs with parallel subagents
438: /batch-gen-prp FEATURE-PLAN.md
439: 
440: # Output: Multiple PRPs with format PRP-X.Y.Z
441: #   X = Batch ID (next free number)
442: #   Y = Stage number
443: #   Z = Order within stage
444: ```
445: 
446: **Plan Format**:
447: ```markdown
448: # Plan Title
449: 
450: ## Phases
451: 
452: ### Phase 1: Name
453: 
454: **Goal**: One-sentence objective
455: **Estimated Hours**: 0.5
456: **Complexity**: low
457: **Files Modified**: path/to/file
458: **Dependencies**: None
459: **Implementation Steps**: [steps]
460: **Validation Gates**: [gates]
461: ```
462: 
463: **What It Does**:
464: 1. Parses plan document ‚Üí Extracts phases
465: 2. Builds dependency graph ‚Üí Analyzes deps + file conflicts
466: 3. Assigns stages ‚Üí Groups independent PRPs for parallel execution
467: 4. Spawns Sonnet subagents ‚Üí Parallel generation per stage
468: 5. Monitors via heartbeat files ‚Üí 30s polling, kills after 2 failed polls
469: 6. Creates Linear issues ‚Üí One per PRP
470: 7. Outputs summary ‚Üí All generated PRPs grouped by stage
471: 
472: **Example Output**:
473: ```
474: Batch 43:
475:   Stage 1: PRP-43.1.1
476:   Stage 2: PRP-43.2.1, PRP-43.2.2, PRP-43.2.3 (parallel)
477:   Stage 3: PRP-43.3.1
478: ```
479: 
480: **Integration with Execution**:
481: ```bash
482: # Generate PRPs from plan
483: /batch-gen-prp BIG-FEATURE-PLAN.md
484: 
485: # Execute entire batch
486: /batch-exe-prp --batch 43
487: 
488: # Or stage-by-stage
489: /batch-exe-prp --batch 43 --stage 1
490: /batch-exe-prp --batch 43 --stage 2
491: ```
492: 
493: **Time Savings**: 8 PRPs sequential (30 min) ‚Üí parallel (10-12 min) = **60% faster**
494: 
495: **See**: `.claude/commands/batch-gen-prp.md` for complete documentation
496: 
497: ## PRP Sizing
498: 
499: ```bash
500: cd tools && uv run ce prp analyze <path-to-prp.md>
501: ```
502: 
503: **Size Categories**:
504: - GREEN: ‚â§700 lines, ‚â§8h, LOW-MEDIUM risk
505: - YELLOW: 700-1000 lines, 8-12h, MEDIUM risk
506: - RED: >1000 lines, >12h, HIGH risk
507: 
508: **Exit Codes**: 0 (GREEN), 1 (YELLOW), 2 (RED)
509: 
510: ## Testing Patterns
511: 
512: **Strategy pattern** for composable testing:
513: - **Unit**: Test single strategy in isolation
514: - **Integration**: Test subgraph with real + mock
515: - **E2E**: Full pipeline, all external deps mocked
516: 
517: **Mock Strategies**: MockSerenaStrategy, MockContext7Strategy, MockLLMStrategy
518: 
519: **Real Strategies**: RealParserStrategy, RealCommandStrategy
520: 
521: ## Documentation Standards
522: 
523: **Mermaid Diagrams**: Always specify text color
524: - Light backgrounds ‚Üí `color:#000`
525: - Dark backgrounds ‚Üí `color:#fff`
526: - Format: `style X fill:#bgcolor,color:#textcolor`
527: 
528: ## Efficient Doc Review
529: 
530: **Grep-first validation** (90% token reduction):
531: 1. Structural validation (Grep patterns, 1-2k tokens)
532: 2. Code quality checks (Grep anti-patterns, 500 tokens)
533: 3. Targeted reads (2-3 files only, 3-5k tokens)
534: 
535: **Total**: ~5-7k tokens vs 200k+ for read-all
536: 
537: ## Resources
538: 
539: - `.ce/` - System boilerplate (don't modify)
540: - `.ce/RULES.md` - Framework rules
541: - `PRPs/[executed,feature-requests]` - Feature requests
542: - `examples/` - Framework patterns and user code
543: 
544: ## Keyboard Shortcuts
545: 
546: ### Image Pasting (macOS)
547: 
548: **cmd+v**: Paste screenshot images into Claude Code
549: - Requires Karabiner-Elements (configured via PRP-30)
550: - Remaps cmd+v ‚Üí ctrl+v in terminals only
551: - Config: `~/.config/karabiner/assets/complex_modifications/claude-code-cmd-v.json`
552: - Toggle: Karabiner-Elements ‚Üí Complex Modifications
553: 
554: **Setup** (one-time):
555: ```bash
556: brew install --cask karabiner-elements
557: # Enable rule in Karabiner-Elements UI ‚Üí Complex Modifications
558: ```
559: 
560: ## Git Worktree - Parallel PRP Development
561: 
562: **Native git solution for working on multiple PRPs simultaneously**
563: 
564: ### Quick Start
565: 
566: ```bash
567: # Create worktree for PRP-A (creates ../ctx-eng-plus-prp-a)
568: git worktree add ../ctx-eng-plus-prp-a -b prp-a-feature
569: 
570: # Work in worktree
571: cd ../ctx-eng-plus-prp-a
572: # Make changes...
573: git add .
574: git commit -m "Implement feature"
575: 
576: # List all worktrees
577: git worktree list
578: 
579: # Remove worktree after merging
580: git worktree remove ../ctx-eng-plus-prp-a
581: ```
582: 
583: ### Commands
584: 
585: **Create**:
586: ```bash
587: git worktree add <path> -b <branch-name>
588: # Example: git worktree add ../ctx-eng-plus-prp-12 -b prp-12-validation
589: ```
590: 
591: **List**:
592: ```bash
593: git worktree list
594: # Shows: path, commit hash, branch name
595: ```
596: 
597: **Remove**:
598: ```bash
599: git worktree remove <path>
600: # or: git worktree remove --force <path>  # if uncommitted changes
601: ```
602: 
603: **Prune** (clean stale references):
604: ```bash
605: git worktree prune
606: ```
607: 
608: ### Workflow for Parallel PRPs
609: 
610: **Stage 1: Create Worktrees**
611: ```bash
612: # From main repo: /Users/bprzybysz/nc-src/ctx-eng-plus
613: git worktree add ../ctx-eng-plus-prp-a -b prp-a-tool-deny
614: git worktree add ../ctx-eng-plus-prp-b -b prp-b-usage-guide
615: git worktree add ../ctx-eng-plus-prp-c -b prp-c-worktree-docs
616: ```
617: 
618: **Stage 2: Execute in Parallel**
619: ```bash
620: # Terminal 1
621: cd ../ctx-eng-plus-prp-a
622: # Edit .claude/settings.local.json
623: git add .
624: git commit -m "PRP-A: Add tools to deny list"
625: 
626: # Terminal 2
627: cd ../ctx-eng-plus-prp-b
628: # Create TOOL-USAGE-GUIDE.md
629: git add .
630: git commit -m "PRP-B: Create tool usage guide"
631: 
632: # Terminal 3
633: cd ../ctx-eng-plus-prp-c
634: # Update CLAUDE.md
635: git add .
636: git commit -m "PRP-C: Migrate to worktree docs"
637: ```
638: 
639: **Stage 3: Merge in Order**
640: ```bash
641: cd /Users/bprzybysz/nc-src/ctx-eng-plus
642: git checkout main
643: 
644: # Merge PRP-A first
645: git merge prp-a-tool-deny --no-ff
646: git push origin main
647: 
648: # Merge PRP-B
649: git merge prp-b-usage-guide --no-ff
650: git push origin main
651: 
652: # Merge PRP-C (may conflict with PRP-A on settings.local.json)
653: git merge prp-c-worktree-docs --no-ff
654: # If conflicts, resolve manually (see Conflict Resolution below)
655: git push origin main
656: ```
657: 
658: **Stage 4: Cleanup**
659: ```bash
660: git worktree remove ../ctx-eng-plus-prp-a
661: git worktree remove ../ctx-eng-plus-prp-b
662: git worktree remove ../ctx-eng-plus-prp-c
663: git worktree prune
664: ```
665: 
666: ### Critical Constraints
667: 
668: **‚ö†Ô∏è Same Branch Limitation**
669: 
670: **CANNOT** check out the same branch in multiple worktrees simultaneously.
671: 
672: **Example of ERROR**:
673: ```bash
674: # Main repo on `main` branch
675: cd /Users/bprzybysz/nc-src/ctx-eng-plus
676: git branch
677: # * main
678: 
679: # Try to create worktree on `main`
680: git worktree add ../ctx-eng-plus-test -b main
681: # ERROR: fatal: 'main' is already checked out at '/Users/bprzybysz/nc-src/ctx-eng-plus'
682: ```
683: 
684: **Solution**: Each worktree must use a **unique branch**.
685: 
686: ```bash
687: # Main repo stays on gitbutler/workspace or main
688: # Each PRP worktree uses dedicated branch
689: git worktree add ../ctx-eng-plus-prp-a -b prp-a-unique  # ‚úì
690: git worktree add ../ctx-eng-plus-prp-b -b prp-b-unique  # ‚úì
691: ```
692: 
693: ### Conflict Resolution
694: 
695: When merging parallel PRPs, conflicts may occur if they modify the same file sections.
696: 
697: **Scenario 1: No Conflicts** (PRP-A + PRP-B)
698: ```bash
699: git merge prp-a-tool-deny --no-ff  # ‚úì Success
700: git merge prp-b-usage-guide --no-ff  # ‚úì Success (different files)
701: ```
702: 
703: **Scenario 2: Merge Conflict** (PRP-A + PRP-D both edit settings.local.json)
704: 
705: **Step 1: Attempt Merge**
706: ```bash
707: git merge prp-d-command-perms --no-ff
708: # Auto-merging .claude/settings.local.json
709: # CONFLICT (content): Merge conflict in .claude/settings.local.json
710: # Automatic merge failed; fix conflicts and then commit the result.
711: ```
712: 
713: **Step 2: Check Conflict Markers**
714: ```bash
715: git status
716: # Unmerged paths:
717: #   both modified:   .claude/settings.local.json
718: ```
719: 
720: **Step 3: Read File to See Conflicts**
721: ```python
722: Read(file_path="/Users/bprzybysz/nc-src/ctx-eng-plus/.claude/settings.local.json")
723: # Look for:
724: # <<<<<<< HEAD
725: # ... current branch content ...
726: # =======
727: # ... incoming branch content ...
728: # >>>>>>> prp-d-command-perms
729: ```
730: 
731: **Step 4: Resolve with Edit Tool**
732: ```python
733: # Remove conflict markers, keep desired changes from both branches
734: Edit(
735:   file_path="/Users/bprzybysz/nc-src/ctx-eng-plus/.claude/settings.local.json",
736:   old_string="""<<<<<<< HEAD
737:   "deny": [existing tools...]
738: =======
739:   "deny": [incoming tools...]
740: >>>>>>> prp-d-command-perms""",
741:   new_string="""  "deny": [merged tools from both branches...]"""
742: )
743: ```
744: 
745: **Step 5: Stage and Commit**
746: ```bash
747: git add .claude/settings.local.json
748: git commit -m "Merge prp-d-command-perms: Resolve settings conflict"
749: ```
750: 
751: **Scenario 3: Conflicting Logic** (PRP-A denies tool, PRP-D allows same tool)
752: 
753: **Resolution**: Apply **last-merged wins** or **manual decision**.
754: 
755: ```json
756: // PRP-A (merged first): Denies "mcp__syntropy__git_git_status"
757: "deny": ["mcp__syntropy__git_git_status"]
758: 
759: // PRP-D (merging now): Allows "git" commands implicitly
760: "allow": ["Bash(git:*)"]
761: 
762: // Decision: Keep Bash(git:*) in allow, keep git_git_status in deny
763: // Rationale: Native bash git preferred over MCP wrapper
764: ```
765: 
766: ### Comparison: GitButler vs Worktree
767: 
768: | Feature | GitButler | Git Worktree |
769: |---------|-----------|--------------|
770: | **Parallel Development** | ‚úì Virtual branches | ‚úì Physical worktrees |
771: | **Branch Switching** | ‚úó Not needed | ‚úó Not needed |
772: | **Conflict Detection** | ‚úì Real-time üîí icon | ‚ö†Ô∏è At merge time |
773: | **Native Git** | ‚úó Proprietary layer | ‚úì Built-in since Git 2.5 |
774: | **Learning Curve** | Medium (new concepts) | Low (standard git) |
775: | **Merge Strategy** | UI-based | CLI-based (standard) |
776: | **Same Branch Limit** | ‚úì Can work on same "virtual" branch | ‚úó Must use unique branches |
777: | **Tool Requirement** | Requires GitButler app + CLI | ‚úì Native git (no install) |
778: | **Workspace Branch** | Auto-merges to `gitbutler/workspace` | Manual merge to `main` |
779: 
780: ### Benefits of Worktree Approach
781: 
782: 1. **Native Git**: No external dependencies, works everywhere
783: 2. **Explicit Branches**: Clear separation, standard git workflow
784: 3. **Merge Control**: Full control over merge order and conflict resolution
785: 4. **Universal**: Works on any git version ‚â•2.5 (2015)
786: 5. **Simple Cleanup**: `git worktree remove` + `git worktree prune`
787: 
788: ### Example: 3-PRP Parallel Execution
789: 
790: ```bash
791: # Stage 1: Create worktrees (30 seconds)
792: git worktree add ../ctx-eng-plus-prp-a -b prp-a-tool-deny
793: git worktree add ../ctx-eng-plus-prp-b -b prp-b-usage-guide
794: git worktree add ../ctx-eng-plus-prp-c -b prp-c-worktree-docs
795: 
796: # Stage 2: Execute in parallel (15 minutes total, vs 45 sequential)
797: # Each PRP executes independently in its worktree
798: 
799: # Stage 3: Merge in dependency order (5 minutes)
800: git merge prp-a-tool-deny --no-ff     # Merge order: 1
801: git merge prp-b-usage-guide --no-ff   # Merge order: 2
802: git merge prp-c-worktree-docs --no-ff # Merge order: 3
803: 
804: # Stage 4: Cleanup (30 seconds)
805: git worktree remove ../ctx-eng-plus-prp-a
806: git worktree remove ../ctx-eng-plus-prp-b
807: git worktree remove ../ctx-eng-plus-prp-c
808: git worktree prune
809: ```
810: 
811: **Time Savings**: 45 min sequential ‚Üí 20 min parallel (55% reduction)
812: 
813: ---
814: 
815: ## Troubleshooting
816: 
817: ```bash
818: # Tool not found
819: cd tools && uv pip install -e .
820: 
821: # Tests failing
822: uv sync
823: uv run pytest tests/ -v
824: 
825: # Linear "Not connected"
826: rm -rf ~/.mcp-auth
827: 
828: # Check PRP's Linear issue ID
829: grep "^issue:" PRPs/executed/PRP-12-feature.md
830: ```
831: 
832: **New Issues** (added after lockdown):
833: 
834: ### Issue: "Permission prompt for safe command"
835: 
836: **Symptom**: Commands like `ls` or `cat` prompt for permission
837: 
838: **Cause**: Command not in auto-allow list
839: 
840: **Solution**:
841: 1. Check if command matches pattern: `grep 'Bash(ls' .claude/settings.local.json`
842: 2. If missing, add pattern to allow list
843: 3. Or approve once (permission remembered for session)
844: 
845: ### Issue: "Command denied" or "tool not found"
846: 
847: **Symptom**: MCP tool like `mcp__syntropy__filesystem_read_file` fails
848: 
849: **Cause**: Tool in deny list (post-lockdown)
850: 
851: **Solution**:
852: 1. Check TOOL-USAGE-GUIDE.md for alternative
853: 2. Example: `filesystem_read_file` ‚Üí Use `Read` (native) instead
854: 3. If tool should be allowed, remove from deny list (rare)
855: 
856: ### Issue: "MCP tools context too large"
857: 
858: **Symptom**: Token usage warning for MCP tools
859: 
860: **Cause**: Deny list not applied (MCP not reconnected)
861: 
862: **Solution**:
863: ```bash
864: # Reconnect MCP servers
865: /mcp
866: 
867: # Verify token reduction
868: # Expected: ~2k tokens for MCP tools (was ~46k)
869: ```
870: 
871: ## Permissions
872: 
873: **‚ùå NEVER** replace all permissions with one entry in `.claude/settings.local.json`
874: 
875: **‚úÖ ALLOWED**: Surgical edits to individual permissions
876: 
877: ## Special Notes
878: 
879: - Linear MCP context: "linear( mcp)" = linear-server mcp
880: - Compact conversation: Use claude-3-haiku-20240307
881: - Activate Serena: Use project's root full path
882: - Ad-hoc code strict: 3 LOC max, no exceptions
883: - reset target branch = means reset --hard /Users/bprzybyszi/nc-src/ctx-eng-plus-test-target to main HEAD; create 'prp36test' branch
</file>

<file path="tools/ce/blending/cleanup.py">
  1: """Phase D: Cleanup module for safe legacy directory removal."""
  2: 
  3: import shutil
  4: import logging
  5: from pathlib import Path
  6: from typing import Dict, List, Tuple
  7: 
  8: logger = logging.getLogger(__name__)
  9: 
 10: # System files that should be ignored during cleanup validation
 11: # All legacy domain locations (PRPs/, examples/, context-engineering/, .serena/)
 12: # and their .old variants are removed after migration validation
 13: SYSTEM_FILES = [
 14:     ".DS_Store",    # Mac system files
 15:     ".gitignore",   # Git ignore files
 16:     "Thumbs.db"     # Windows system files
 17: ]
 18: 
 19: 
 20: def cleanup_legacy_dirs(
 21:     target_project: Path,
 22:     dry_run: bool = True
 23: ) -> Dict[str, bool]:
 24:     """
 25:     Remove legacy directories after CE 1.1 migration.
 26: 
 27:     Args:
 28:         target_project: Target project root path
 29:         dry_run: If True, show actions without deleting (default: True)
 30: 
 31:     Returns:
 32:         Dict[dir_path, cleanup_success]: Status for each directory
 33: 
 34:     Raises:
 35:         ValueError: If migration not complete (unmigrated files detected)
 36:     """
 37:     legacy_dirs = [
 38:         "PRPs",
 39:         "examples",
 40:         "context-engineering",
 41:         ".serena.old"  # NEW: Cleanup after memories blending
 42:     ]
 43: 
 44:     status: Dict[str, bool] = {}
 45: 
 46:     print("\n" + "=" * 60)
 47:     print("üßπ Legacy Directory Cleanup")
 48:     print("=" * 60)
 49: 
 50:     if dry_run:
 51:         print("‚ö†Ô∏è  DRY-RUN MODE: No files will be deleted")
 52:         print()
 53: 
 54:     for legacy_dir in legacy_dirs:
 55:         legacy_path = target_project / legacy_dir
 56: 
 57:         # Skip if directory doesn't exist
 58:         if not legacy_path.exists():
 59:             print(f"‚è≠Ô∏è  {legacy_dir}/ - Not found (skipping)")
 60:             status[legacy_dir] = True
 61:             continue
 62: 
 63:         # Skip root examples/ directory (user code, not CE framework files)
 64:         # Examples domain only migrates framework examples from .ce.old/examples/
 65:         # Root examples/ are considered user code outside CE structure
 66:         if legacy_dir == "examples":
 67:             print(f"‚è≠Ô∏è  {legacy_dir}/ - Skipping (user code, not CE framework)")
 68:             status[legacy_dir] = True
 69:             continue
 70: 
 71:         # Verify migration complete
 72:         print(f"üîç Verifying {legacy_dir}/ migration...")
 73:         is_migrated, unmigrated = verify_migration_complete(
 74:             legacy_path,
 75:             target_project
 76:         )
 77: 
 78:         if not is_migrated:
 79:             print(f"‚ùå {legacy_dir}/ - Migration incomplete!")
 80:             print(f"   Unmigrated files: {len(unmigrated)}")
 81:             for file in unmigrated[:5]:  # Show first 5
 82:                 print(f"     - {file}")
 83:             if len(unmigrated) > 5:
 84:                 print(f"     ... and {len(unmigrated) - 5} more")
 85: 
 86:             raise ValueError(
 87:                 f"Cannot cleanup {legacy_dir}/: {len(unmigrated)} unmigrated files detected. "
 88:                 f"Run migration again or manually verify."
 89:             )
 90: 
 91:         # Safe to remove
 92:         if dry_run:
 93:             print(f"‚úì {legacy_dir}/ - Would remove (verified complete)")
 94:             status[legacy_dir] = True
 95:         else:
 96:             try:
 97:                 shutil.rmtree(legacy_path)
 98:                 print(f"‚úÖ {legacy_dir}/ - Removed successfully")
 99:                 status[legacy_dir] = True
100:             except Exception as e:
101:                 print(f"‚ùå {legacy_dir}/ - Removal failed: {e}")
102:                 status[legacy_dir] = False
103: 
104:     print()
105:     print("=" * 60)
106: 
107:     if dry_run:
108:         print("‚ÑπÔ∏è  Dry-run complete. Run with --execute to perform cleanup.")
109:     else:
110:         success_count = sum(1 for v in status.values() if v)
111:         print(f"‚úÖ Cleanup complete: {success_count}/{len(status)} directories removed")
112: 
113:     return status
114: 
115: 
116: def _should_skip_file(file_path: Path) -> bool:
117:     """
118:     Check if file should be skipped (system files only).
119: 
120:     Args:
121:         file_path: File path to check
122: 
123:     Returns:
124:         True if file is a system file that should be ignored
125:     """
126:     filename = file_path.name
127:     return filename in SYSTEM_FILES
128: 
129: 
130: def verify_migration_complete(
131:     legacy_dir: Path,
132:     target_project: Path
133: ) -> Tuple[bool, List[str]]:
134:     """
135:     Verify all files in legacy_dir have been migrated.
136: 
137:     Skips files that should NOT be migrated (templates, garbage patterns).
138: 
139:     Args:
140:         legacy_dir: Legacy directory path (e.g., PRPs/)
141:         target_project: Target project root
142: 
143:     Returns:
144:         (is_complete, unmigrated_files): Migration status + list of unmigrated files
145:     """
146:     ce_dir = target_project / ".ce"
147: 
148:     # Find all files in legacy dir
149:     legacy_files = list(legacy_dir.rglob("*"))
150:     legacy_files = [f for f in legacy_files if f.is_file()]
151: 
152:     # Map to expected .ce/ locations
153:     unmigrated: List[str] = []
154: 
155:     for legacy_file in legacy_files:
156:         relative_path = legacy_file.relative_to(target_project)
157: 
158:         # Skip files that should NOT be migrated
159:         if _should_skip_file(relative_path):
160:             logger.debug(f"  Skipping expected unmigrated file: {relative_path}")
161:             continue
162: 
163:         # Check if migrated to .ce/
164:         # PRPs get reorganized during migration (classified into executed/ or feature-requests/)
165:         # So we search by filename, not path
166:         # examples/pattern.py ‚Üí .ce/examples/pattern.py (direct mapping)
167: 
168:         # For PRPs: search by filename (files get reorganized during migration)
169:         if relative_path.parts[0] == "PRPs":
170:             # Search in all PRP subdirectories for this filename
171:             filename = legacy_file.name
172:             ce_path = None
173:             for subdir in ["executed", "feature-requests", "system"]:
174:                 candidate = ce_dir / "PRPs" / subdir / filename
175:                 if candidate.exists():
176:                     ce_path = candidate
177:                     break
178:             # If not found in subdirs, check if it exists with direct mapping
179:             if not ce_path:
180:                 ce_path = ce_dir / relative_path
181:         # For examples: direct mapping (framework examples extracted directly to .ce/examples/)
182:         elif relative_path.parts[0] == "examples":
183:             ce_path = ce_dir / relative_path
184:         # For context-engineering: .ce/ itself
185:         elif relative_path.parts[0] == "context-engineering":
186:             ce_path = ce_dir / "/".join(relative_path.parts[1:])
187:         # For .serena.old/: Check if migrated to .serena/
188:         elif relative_path.parts[0] == ".serena.old":
189:             # Only check files in memories/ subdirectory
190:             # Skip root-level files (.gitignore, project.yml, etc.)
191:             if len(relative_path.parts) >= 3 and relative_path.parts[1] == "memories":
192:                 # .serena.old/memories/file.md ‚Üí .serena/memories/file.md
193:                 ce_path = target_project / ".serena" / "/".join(relative_path.parts[1:])
194:             else:
195:                 # Skip non-memory files in .serena.old/ (e.g., .gitignore, project.yml)
196:                 logger.debug(f"  Skipping .serena.old/ non-memory file: {relative_path}")
197:                 continue
198:         else:
199:             # Unknown legacy structure
200:             ce_path = ce_dir / relative_path
201: 
202:         # Check if migrated file exists
203:         if not ce_path.exists():
204:             unmigrated.append(str(relative_path))
205: 
206:     is_complete = len(unmigrated) == 0
207: 
208:     return is_complete, unmigrated
209: 
210: 
211: def find_unmigrated_files(
212:     legacy_dir: Path,
213:     ce_dir: Path
214: ) -> List[str]:
215:     """
216:     Find files in legacy_dir not present in ce_dir.
217: 
218:     Args:
219:         legacy_dir: Legacy directory path
220:         ce_dir: .ce/ directory path
221: 
222:     Returns:
223:         List of unmigrated file paths (relative to legacy_dir)
224:     """
225:     unmigrated: List[str] = []
226: 
227:     if not legacy_dir.exists():
228:         return unmigrated
229: 
230:     for legacy_file in legacy_dir.rglob("*"):
231:         if not legacy_file.is_file():
232:             continue
233: 
234:         # Calculate relative path
235:         relative_path = legacy_file.relative_to(legacy_dir)
236: 
237:         # Check if exists in .ce/
238:         ce_file = ce_dir / legacy_dir.name / relative_path
239: 
240:         if not ce_file.exists():
241:             unmigrated.append(str(relative_path))
242: 
243:     return unmigrated
</file>

<file path="tools/ce/init_project.py">
  1: #!/usr/bin/env python3
  2: """
  3: CE Framework Project Initializer - Core Module
  4: 
  5: Implements the 4-phase pipeline for installing CE framework on target projects:
  6: 1. Extract: Unpack ce-infrastructure.xml to target project
  7: 2. Blend: Merge framework + user files (CLAUDE.md, settings, commands)
  8: 3. Initialize: Run uv sync to install dependencies
  9: 4. Verify: Validate installation and report status
 10: """
 11: 
 12: import json
 13: import shutil
 14: import subprocess
 15: import sys
 16: from pathlib import Path
 17: from typing import Dict, List, Optional
 18: 
 19: 
 20: class ProjectInitializer:
 21:     """
 22:     Core initializer for CE Framework installation on target projects.
 23: 
 24:     Handles 4-phase pipeline:
 25:     - extract: Unpack repomix package to .ce/
 26:     - blend: Merge framework + user files
 27:     - initialize: Install Python dependencies
 28:     - verify: Validate installation
 29:     """
 30: 
 31:     def __init__(self, target_project: Path, dry_run: bool = False):
 32:         """
 33:         Initialize ProjectInitializer.
 34: 
 35:         Args:
 36:             target_project: Path to target project root
 37:             dry_run: If True, show actions without executing
 38:         """
 39:         self.target_project = Path(target_project).resolve()
 40:         self.dry_run = dry_run
 41:         self.ce_dir = self.target_project / ".ce"
 42:         self.tools_dir = self.ce_dir / "tools"
 43: 
 44:         # Paths to framework packages (in ctx-eng-plus repo)
 45:         self.ctx_eng_root = Path(__file__).parent.parent.parent.resolve()
 46:         self.infrastructure_xml = self.ctx_eng_root / ".ce" / "ce-infrastructure.xml"
 47:         self.workflow_xml = self.ctx_eng_root / ".ce" / "ce-workflow-docs.xml"
 48: 
 49:     def run(self, phase: str = "all") -> Dict:
 50:         """
 51:         Run initialization pipeline.
 52: 
 53:         Args:
 54:             phase: Which phase(s) to run - "all", "extract", "blend", "initialize", "verify"
 55: 
 56:         Returns:
 57:             Dict with status info for each phase executed
 58: 
 59:         Raises:
 60:             ValueError: If invalid phase specified
 61:         """
 62:         valid_phases = ["all", "extract", "blend", "initialize", "verify"]
 63:         if phase not in valid_phases:
 64:             raise ValueError(f"Invalid phase '{phase}'. Must be one of: {valid_phases}\nüîß Troubleshooting: Check input parameters and documentation")
 65: 
 66:         results = {}
 67: 
 68:         if phase == "all":
 69:             results["extract"] = self.extract()
 70:             results["blend"] = self.blend()
 71:             results["initialize"] = self.initialize()
 72:             results["verify"] = self.verify()
 73:         else:
 74:             # Run single phase
 75:             method = getattr(self, phase)
 76:             results[phase] = method()
 77: 
 78:         return results
 79: 
 80:     def extract(self) -> Dict:
 81:         """
 82:         Extract ce-infrastructure.xml to target project.
 83: 
 84:         Steps:
 85:         1. Check if ce-infrastructure.xml exists
 86:         2. Extract to .ce/ directory
 87:         3. Reorganize tools/ to .ce/tools/
 88:         4. Copy ce-workflow-docs.xml to .ce/
 89: 
 90:         Returns:
 91:             Dict with extraction status and file counts
 92:         """
 93:         status = {"success": False, "files_extracted": 0, "message": ""}
 94: 
 95:         # Check for infrastructure package
 96:         if not self.infrastructure_xml.exists():
 97:             status["message"] = (
 98:                 f"‚ùå ce-infrastructure.xml not found at {self.infrastructure_xml}\n"
 99:                 f"üîß Ensure you're running from ctx-eng-plus repo root"
100:             )
101:             return status
102: 
103:         if self.dry_run:
104:             status["success"] = True
105:             status["message"] = f"[DRY-RUN] Would extract to {self.ce_dir}"
106:             return status
107: 
108:         # Check for existing .ce/ directory - rename to .ce.old
109:         ce_old_dir = self.target_project / ".ce.old"
110:         renamed_existing = False
111:         if self.ce_dir.exists():
112:             # Remove old .ce.old if it exists
113:             if ce_old_dir.exists():
114:                 shutil.rmtree(ce_old_dir)
115: 
116:             # Rename .ce to .ce.old
117:             shutil.move(str(self.ce_dir), str(ce_old_dir))
118:             renamed_existing = True
119: 
120:         try:
121:             # Import repomix_unpack module
122:             from ce.repomix_unpack import extract_files
123: 
124:             # Extract to temporary location first
125:             temp_extract = self.target_project / "tmp" / "ce-extraction"
126:             temp_extract.mkdir(parents=True, exist_ok=True)
127: 
128:             # Extract files
129:             files_extracted = extract_files(
130:                 xml_path=self.infrastructure_xml,
131:                 target_dir=temp_extract,
132:                 verbose=False
133:             )
134: 
135:             if files_extracted == 0:
136:                 status["message"] = "‚ùå No files extracted from package"
137:                 return status
138: 
139:             # Reorganize extracted files to .ce/ structure
140:             self.ce_dir.mkdir(parents=True, exist_ok=True)
141: 
142:             # Reorganize extracted files:
143:             # - .ce/* contents ‚Üí target/.ce/ (blend-config.yml, PRPs/, etc.)
144:             # - .serena/ ‚Üí target/.serena/ (project root - configured as output/framework location)
145:             # - .claude/, tools/, CLAUDE.md, examples/ ‚Üí target/.ce/ (framework files for blending)
146: 
147:             # First, move .ce/ contents to target/.ce/
148:             ce_extracted = temp_extract / ".ce"
149:             if ce_extracted.exists():
150:                 for item in ce_extracted.iterdir():
151:                     dest = self.ce_dir / item.name
152:                     if dest.exists():
153:                         if dest.is_dir():
154:                             shutil.rmtree(dest)
155:                         else:
156:                             dest.unlink()
157:                     shutil.move(str(item), str(dest))
158: 
159:             # Then, move other extracted directories
160:             for item in temp_extract.iterdir():
161:                 if item.name == ".ce":
162:                     continue  # Already processed
163: 
164:                 # Special case: .serena goes to project root, not .ce/
165:                 if item.name == ".serena":
166:                     dest = self.target_project / item.name
167:                 else:
168:                     dest = self.ce_dir / item.name
169: 
170:                 if dest.exists():
171:                     if dest.is_dir():
172:                         shutil.rmtree(dest)
173:                     else:
174:                         dest.unlink()
175:                 shutil.move(str(item), str(dest))
176: 
177:             # Copy ce-workflow-docs.xml (reference package)
178:             if self.workflow_xml.exists():
179:                 shutil.copy2(self.workflow_xml, self.ce_dir / "ce-workflow-docs.xml")
180: 
181:             # Copy directories.yml (workaround for repomix YAML indentation issue)
182:             directories_src = self.ctx_eng_root / ".ce" / "directories.yml"
183:             directories_dst = self.ce_dir / "directories.yml"
184:             if directories_src.exists():
185:                 try:
186:                     shutil.copy2(directories_src, directories_dst)
187:                 except Exception:
188:                     pass  # If copy fails, config loader will use fallback
189: 
190:             # Cleanup temp directory
191:             shutil.rmtree(temp_extract.parent)
192: 
193:             status["success"] = True
194:             status["files_extracted"] = files_extracted
195: 
196:             # Include rename message if applicable
197:             if renamed_existing:
198:                 status["message"] = (
199:                     f"‚ÑπÔ∏è  Renamed existing .ce/ to .ce.old/\n"
200:                     f"üí° .ce.old/ will be included as additional context source during blend\n"
201:                     f"‚úÖ Extracted {files_extracted} files to {self.ce_dir}"
202:                 )
203:             else:
204:                 status["message"] = f"‚úÖ Extracted {files_extracted} files to {self.ce_dir}"
205: 
206:         except Exception as e:
207:             status["message"] = f"‚ùå Extraction failed: {str(e)}\nüîß Check error details above"
208: 
209:         return status
210: 
211:     def _fix_yaml_indentation(self, yaml_path: Path) -> None:
212:         """
213:         Fix YAML indentation in extracted config files.
214: 
215:         Repomix sometimes strips indentation when packing. This method loads
216:         the YAML and re-dumps it with correct indentation.
217: 
218:         Args:
219:             yaml_path: Path to YAML file to fix
220:         """
221:         try:
222:             import yaml
223: 
224:             # Load the YAML
225:             with open(yaml_path) as f:
226:                 data = yaml.safe_load(f)
227: 
228:             if data is None:
229:                 return  # Empty or unparseable file
230: 
231:             # Re-dump with proper indentation
232:             with open(yaml_path, 'w') as f:
233:                 yaml.dump(data, f, default_flow_style=False, sort_keys=False, indent=2)
234:         except Exception:
235:             # If fixing fails, continue anyway (shouldn't block initialization)
236:             pass
237: 
238:     def blend(self) -> Dict:
239:         """
240:         Blend framework + user files.
241: 
242:         Delegates to: uv run ce blend --all --target-dir <target>
243: 
244:         Returns:
245:             Dict with blend status and stdout/stderr
246:         """
247:         status = {"success": False, "stdout": "", "stderr": ""}
248: 
249:         if self.dry_run:
250:             status["success"] = True
251:             status["stdout"] = f"[DRY-RUN] Would run: uv run ce blend --all --target-dir {self.target_project}"
252:             return status
253: 
254:         try:
255:             # Fix YAML indentation after extraction (repomix sometimes breaks it)
256:             blend_config = self.ce_dir / "blend-config.yml"
257:             self._fix_yaml_indentation(blend_config)
258: 
259:             # Run blend command with explicit config path
260:             result = subprocess.run(
261:                 ["uv", "run", "ce", "blend", "--all",
262:                  "--config", str(blend_config),
263:                  "--target-dir", str(self.target_project)],
264:                 cwd=self.ctx_eng_root / "tools",
265:                 capture_output=True,
266:                 text=True,
267:                 timeout=120
268:             )
269: 
270:             status["stdout"] = result.stdout
271:             status["stderr"] = result.stderr
272:             status["success"] = result.returncode == 0
273: 
274:             if not status["success"]:
275:                 status["message"] = (
276:                     f"‚ùå Blend phase failed (exit code {result.returncode})\n"
277:                     f"üîß Check blend tool output:\n{result.stderr}"
278:                 )
279:             else:
280:                 # Cleanup: Remove framework .claude/ and CLAUDE.md from .ce/ after blending
281:                 # These should only exist at root, not in .ce/
282:                 ce_claude = self.ce_dir / ".claude"
283:                 ce_claude_md = self.ce_dir / "CLAUDE.md"
284: 
285:                 if ce_claude.exists():
286:                     shutil.rmtree(ce_claude)
287:                 if ce_claude_md.exists():
288:                     ce_claude_md.unlink()
289: 
290:                 # Check if .ce.old exists to mention it
291:                 ce_old_dir = self.target_project / ".ce.old"
292:                 if ce_old_dir.exists():
293:                     status["message"] = (
294:                         "‚úÖ Blend phase completed\n"
295:                         "üí° Note: .ce.old/ detected - blend tool will include it as additional source"
296:                     )
297:                 else:
298:                     status["message"] = "‚úÖ Blend phase completed"
299: 
300:         except subprocess.TimeoutExpired:
301:             status["message"] = "‚ùå Blend phase timed out (120s limit)\nüîß Check for hanging processes"
302:         except FileNotFoundError:
303:             status["message"] = (
304:                 "‚ùå uv not found in PATH\n"
305:                 "üîß Install UV: curl -LsSf https://astral.sh/uv/install.sh | sh"
306:             )
307:         except Exception as e:
308:             status["message"] = f"‚ùå Blend phase failed: {str(e)}"
309: 
310:         return status
311: 
312:     def initialize(self) -> Dict:
313:         """
314:         Initialize Python environment.
315: 
316:         Runs: uv sync in .ce/tools/ directory
317: 
318:         Returns:
319:             Dict with initialization status and command output
320:         """
321:         status = {"success": False, "stdout": "", "stderr": ""}
322: 
323:         if not self.tools_dir.exists():
324:             status["message"] = (
325:                 f"‚ùå Tools directory not found: {self.tools_dir}\n"
326:                 f"üîß Run extract phase first"
327:             )
328:             return status
329: 
330:         if self.dry_run:
331:             status["success"] = True
332:             status["stdout"] = f"[DRY-RUN] Would run: uv sync in {self.tools_dir}"
333:             return status
334: 
335:         try:
336:             # Run uv sync
337:             result = subprocess.run(
338:                 ["uv", "sync"],
339:                 cwd=self.tools_dir,
340:                 capture_output=True,
341:                 text=True,
342:                 timeout=300  # 5 minutes for dependency installation
343:             )
344: 
345:             status["stdout"] = result.stdout
346:             status["stderr"] = result.stderr
347:             status["success"] = result.returncode == 0
348: 
349:             if not status["success"]:
350:                 status["message"] = (
351:                     f"‚ùå UV sync failed (exit code {result.returncode})\n"
352:                     f"üîß Check pyproject.toml and dependency versions:\n{result.stderr}"
353:                 )
354:             else:
355:                 status["message"] = "‚úÖ Python environment initialized"
356: 
357:         except subprocess.TimeoutExpired:
358:             status["message"] = "‚ùå UV sync timed out (300s limit)\nüîß Check network connection or package mirrors"
359:         except FileNotFoundError:
360:             status["message"] = (
361:                 "‚ùå uv not found in PATH\n"
362:                 "üîß Install UV: curl -LsSf https://astral.sh/uv/install.sh | sh"
363:             )
364:         except Exception as e:
365:             status["message"] = f"‚ùå Initialize phase failed: {str(e)}"
366: 
367:         return status
368: 
369:     def verify(self) -> Dict:
370:         """
371:         Verify installation.
372: 
373:         Checks:
374:         1. Critical files exist (.ce/tools/, .claude/, .serena/)
375:         2. settings.local.json is valid JSON
376:         3. pyproject.toml exists
377:         4. Reports summary
378: 
379:         Returns:
380:             Dict with verification results and warnings
381:         """
382:         status = {"success": True, "warnings": [], "checks": []}
383: 
384:         # Critical files to check
385:         critical_files = [
386:             self.ce_dir / "tools" / "pyproject.toml",
387:             self.target_project / ".claude" / "settings.local.json",
388:             self.target_project / ".serena" / "memories",
389:             self.ce_dir / "RULES.md"
390:         ]
391: 
392:         for file_path in critical_files:
393:             if file_path.exists():
394:                 status["checks"].append(f"‚úÖ {file_path.relative_to(self.target_project)}")
395:             else:
396:                 status["warnings"].append(f"‚ö†Ô∏è  Missing: {file_path.relative_to(self.target_project)}")
397:                 status["success"] = False
398: 
399:         # Validate settings.local.json
400:         settings_file = self.target_project / ".claude" / "settings.local.json"
401:         if settings_file.exists():
402:             try:
403:                 with open(settings_file) as f:
404:                     json.load(f)
405:                 status["checks"].append("‚úÖ settings.local.json is valid JSON")
406:             except json.JSONDecodeError as e:
407:                 status["warnings"].append(f"‚ö†Ô∏è  Invalid JSON in settings.local.json: {str(e)}")
408:                 status["success"] = False
409:         else:
410:             status["warnings"].append("‚ö†Ô∏è  settings.local.json not found")
411: 
412:         # Check Python installation
413:         venv_dir = self.tools_dir / ".venv"
414:         if venv_dir.exists():
415:             status["checks"].append("‚úÖ Python virtual environment created")
416:         else:
417:             status["warnings"].append("‚ö†Ô∏è  Virtual environment not found (run initialize phase)")
418: 
419:         # Summary message
420:         if status["success"]:
421:             status["message"] = f"‚úÖ Installation verified ({len(status['checks'])} checks passed)"
422:         else:
423:             status["message"] = (
424:                 f"‚ö†Ô∏è  Installation incomplete ({len(status['warnings'])} warnings)\n"
425:                 f"üîß Review warnings above and re-run failed phases"
426:             )
427: 
428:         return status
429: 
430: 
431: def main():
432:     """CLI entry point for testing."""
433:     if len(sys.argv) < 2:
434:         print("Usage: python init_project.py <target-project-path> [--dry-run]")
435:         sys.exit(1)
436: 
437:     target = Path(sys.argv[1])
438:     dry_run = "--dry-run" in sys.argv
439: 
440:     initializer = ProjectInitializer(target, dry_run=dry_run)
441:     results = initializer.run(phase="all")
442: 
443:     # Print results
444:     for phase, result in results.items():
445:         print(f"\n=== Phase: {phase} ===")
446:         print(result.get("message", "No message"))
447:         if not result.get("success", True):
448:             sys.exit(1)
449: 
450:     print("\n‚úÖ Initialization complete!")
451: 
452: 
453: if __name__ == "__main__":
454:     main()
</file>

<file path="tools/ce/blending/core.py">
  1: """Core blending framework: 4-phase pipeline orchestration."""
  2: 
  3: import shutil
  4: import json
  5: import logging
  6: from pathlib import Path
  7: from typing import Generator, Dict, List, Any, Optional, Union
  8: from contextlib import contextmanager
  9: 
 10: from ce.blending.llm_client import BlendingLLM
 11: from ce.config_loader import BlendConfig
 12: 
 13: logger = logging.getLogger(__name__)
 14: 
 15: 
 16: @contextmanager
 17: def backup_context(file_path: Path) -> Generator[Path, None, None]:
 18:     """
 19:     Context manager for backup-modify-restore pattern.
 20: 
 21:     Creates backup before modification, removes on success, restores on failure.
 22: 
 23:     Args:
 24:         file_path: Path to file to backup
 25: 
 26:     Yields:
 27:         backup_path: Path to backup file
 28: 
 29:     Raises:
 30:         OSError: If backup creation fails
 31: 
 32:     Usage:
 33:         >>> with backup_context(Path("CLAUDE.md")) as backup:
 34:         ...     # Modify file
 35:         ...     modify_file(Path("CLAUDE.md"))
 36:         ...     # If exception, auto-restore from backup
 37: 
 38:     Note: Atomic operation - either all changes succeed or all rolled back
 39:     """
 40:     backup_path = file_path.with_suffix(file_path.suffix + '.backup')
 41: 
 42:     # Create backup
 43:     if file_path.exists():
 44:         try:
 45:             shutil.copy2(file_path, backup_path)
 46:             logger.info(f"‚úì Backed up to {backup_path.name}")
 47:         except OSError as e:
 48:             raise OSError(
 49:                 f"Failed to create backup: {backup_path}\n"
 50:                 f"üîß Troubleshooting: Check file permissions and disk space"
 51:             ) from e
 52: 
 53:     try:
 54:         yield backup_path
 55: 
 56:         # Success - remove backup
 57:         if backup_path.exists():
 58:             backup_path.unlink()
 59:             logger.debug(f"Removed backup {backup_path.name}")
 60: 
 61:     except Exception as e:
 62:         # Failure - restore backup
 63:         logger.error(f"‚ùå Operation failed, restoring backup")
 64: 
 65:         if backup_path.exists():
 66:             try:
 67:                 shutil.copy2(backup_path, file_path)
 68:                 logger.info(f"‚úì Restored from backup {backup_path.name}")
 69:             except OSError as restore_error:
 70:                 logger.critical(
 71:                     f"‚ö†Ô∏è CRITICAL: Failed to restore backup!\n"
 72:                     f"Original error: {e}\n"
 73:                     f"Restore error: {restore_error}\n"
 74:                     f"üîß Manual recovery: cp {backup_path} {file_path}"
 75:                 )
 76:         raise
 77: 
 78: 
 79: class BlendingOrchestrator:
 80:     """
 81:     Orchestrates 4-phase migration pipeline.
 82: 
 83:     Phase A: DETECTION - Scan legacy locations
 84:     Phase B: CLASSIFICATION - Validate CE patterns
 85:     Phase C: BLENDING - Merge framework + target
 86:     Phase D: CLEANUP - Remove legacy directories
 87:     """
 88: 
 89:     def __init__(self, config: Union[BlendConfig, Dict[str, Any]], dry_run: bool = False):
 90:         """
 91:         Initialize orchestrator.
 92: 
 93:         Args:
 94:             config: BlendConfig instance or dict from blend-config.yml (for backward compatibility)
 95:             dry_run: If True, show what would be done without executing
 96:         """
 97:         # Store config - handle both BlendConfig and dict for backward compatibility
 98:         if isinstance(config, BlendConfig):
 99:             self.blend_config = config
100:             self.config = config._config  # Access underlying dict if needed
101:         else:
102:             # Backward compatibility: dict config provided
103:             self.blend_config = None
104:             self.config = config
105: 
106:         self.dry_run = dry_run
107:         self.strategies: Dict[str, Any] = {}  # domain -> strategy mapping
108:         self.detected_files: Dict[str, List[Path]] = {}  # Cached detection results
109:         self.classified_files: Dict[str, List[Path]] = {}  # Cached classification results
110: 
111:         # Auto-register default strategies
112:         self._register_default_strategies()
113: 
114:     def _register_default_strategies(self) -> None:
115:         """Register all 6 default domain strategies."""
116:         from ce.blending.strategies import (
117:             SettingsBlendStrategy,
118:             ClaudeMdBlendStrategy,
119:             MemoriesBlendStrategy,
120:             ExamplesBlendStrategy,
121:             PRPMoveStrategy,
122:             CommandOverwriteStrategy
123:         )
124:         from ce.blending.llm_client import BlendingLLM
125:         from unittest.mock import MagicMock
126: 
127:         # Create LLM client for strategies that need it
128:         # In dry-run mode or when API key unavailable, use mock
129:         if self.dry_run:
130:             # Dry-run mode - use mock LLM (no actual API calls)
131:             llm_client = MagicMock(spec=BlendingLLM)
132:             llm_client.get_token_usage.return_value = {
133:                 "input_tokens": 0,
134:                 "output_tokens": 0,
135:                 "total_tokens": 0
136:             }
137:             logger.debug("Using mock LLM client (dry-run mode)")
138:         else:
139:             try:
140:                 llm_client = BlendingLLM()
141:                 logger.debug("Using real LLM client")
142:             except ValueError:
143:                 # API key not available - use mock as fallback
144:                 logger.warning("Anthropic API key not found - using mock LLM client")
145:                 llm_client = MagicMock(spec=BlendingLLM)
146:                 llm_client.get_token_usage.return_value = {
147:                     "input_tokens": 0,
148:                     "output_tokens": 0,
149:                     "total_tokens": 0
150:                 }
151: 
152:         # Register strategy instances
153:         self.register_strategy("settings", SettingsBlendStrategy())
154:         self.register_strategy("claude_md", ClaudeMdBlendStrategy())  # No llm_client needed
155:         self.register_strategy("memories", MemoriesBlendStrategy(llm_client))
156:         self.register_strategy("examples", ExamplesBlendStrategy(llm_client))
157:         self.register_strategy("prps", PRPMoveStrategy())
158:         self.register_strategy("commands", CommandOverwriteStrategy())
159: 
160:         logger.debug(f"Registered {len(self.strategies)} domain strategies")
161: 
162:     def register_strategy(self, domain: str, strategy: Any) -> None:
163:         """
164:         Register a blending strategy for a specific domain.
165: 
166:         Args:
167:             domain: Domain name (settings, claude_md, memories, etc.)
168:             strategy: Instance of BlendStrategy subclass or domain-specific strategy
169:         """
170:         self.strategies[domain] = strategy
171:         logger.debug(f"Registered strategy for domain: {domain}")
172: 
173:     def run_phase(self, phase: str, target_dir: Path) -> Dict[str, Any]:
174:         """
175:         Run specific phase of pipeline.
176: 
177:         Args:
178:             phase: Phase name (detect, classify, blend, cleanup)
179:             target_dir: Target project directory
180: 
181:         Returns:
182:             Phase results dict
183: 
184:         Raises:
185:             ValueError: If phase unknown
186:             RuntimeError: If phase execution fails
187:         """
188:         if phase not in ['detect', 'classify', 'blend', 'cleanup']:
189:             raise ValueError(
190:                 f"Unknown phase: {phase}\n"
191:                 f"üîß Valid phases: detect, classify, blend, cleanup"
192:             )
193: 
194:         logger.info(f"üîÄ Running Phase: {phase.upper()}")
195: 
196:         # Placeholder - actual implementation in subsequent PRPs
197:         if phase == 'detect':
198:             return self._run_detection(target_dir)
199:         elif phase == 'classify':
200:             return self._run_classification(target_dir)
201:         elif phase == 'blend':
202:             return self._run_blending(target_dir)
203:         elif phase == 'cleanup':
204:             return self._run_cleanup(target_dir)
205: 
206:     def _run_detection(self, target_dir: Path) -> Dict[str, Any]:
207:         """
208:         Phase A: Scan legacy locations for CE framework files.
209: 
210:         Uses LegacyFileDetector to scan project for PRPs, memories, examples, etc.
211: 
212:         Args:
213:             target_dir: Target project directory
214: 
215:         Returns:
216:             Dict with detected files by domain
217:         """
218:         from ce.blending.detection import LegacyFileDetector
219: 
220:         logger.info("Phase A: DETECTION - Scanning legacy locations...")
221: 
222:         # Pass BlendConfig to detector if available for config-driven paths
223:         detector = LegacyFileDetector(target_dir, config=self.blend_config)
224:         inventory = detector.scan_all()
225: 
226:         # Cache results
227:         self.detected_files = inventory
228: 
229:         # Log summary
230:         total_files = sum(len(files) for files in inventory.values())
231:         logger.info(f"‚úì Detected {total_files} files across {len(inventory)} domains")
232: 
233:         for domain, files in inventory.items():
234:             if files:
235:                 logger.debug(f"  {domain}: {len(files)} files")
236: 
237:         return {
238:             "phase": "detect",
239:             "implemented": True,
240:             "inventory": inventory,
241:             "total_files": total_files
242:         }
243: 
244:     def _run_classification(self, target_dir: Path) -> Dict[str, Any]:
245:         """
246:         Phase B: Validate CE patterns using classification.
247: 
248:         Uses classification module to filter out garbage files and validate content.
249: 
250:         Args:
251:             target_dir: Target project directory
252: 
253:         Returns:
254:             Dict with classified files by domain
255:         """
256:         from ce.blending.classification import is_garbage
257: 
258:         logger.info("Phase B: CLASSIFICATION - Validating CE patterns...")
259: 
260:         if not self.detected_files:
261:             raise RuntimeError(
262:                 "No detected files - run detection phase first\n"
263:                 "üîß Troubleshooting: Call run_phase('detect', ...) before classify"
264:             )
265: 
266:         classified = {}
267:         total_valid = 0
268: 
269:         for domain, files in self.detected_files.items():
270:             valid_files = []
271: 
272:             for file_path in files:
273:                 # Simple garbage filter (can be enhanced with LLM classification later)
274:                 if not is_garbage(str(file_path)):
275:                     valid_files.append(file_path)
276:                 else:
277:                     logger.debug(f"  Filtered garbage: {file_path.name}")
278: 
279:             classified[domain] = valid_files
280:             total_valid += len(valid_files)
281: 
282:             if valid_files:
283:                 logger.debug(f"  {domain}: {len(valid_files)} valid files")
284: 
285:         # Cache results
286:         self.classified_files = classified
287: 
288:         logger.info(f"‚úì Classified {total_valid} valid files")
289: 
290:         return {
291:             "phase": "classify",
292:             "implemented": True,
293:             "classified": classified,
294:             "total_valid": total_valid
295:         }
296: 
297:     def _run_blending(self, target_dir: Path) -> Dict[str, Any]:
298:         """
299:         Phase C: Blend framework + target content using domain strategies.
300: 
301:         For each domain, execute the registered strategy to blend files.
302: 
303:         Args:
304:             target_dir: Target project directory
305: 
306:         Returns:
307:             Dict with blending results by domain
308:         """
309:         logger.info("Phase C: BLENDING - Merging framework + target...")
310: 
311:         if not self.classified_files:
312:             raise RuntimeError(
313:                 "No classified files - run classification phase first\n"
314:                 "üîß Troubleshooting: Call run_phase('classify', ...) before blend"
315:             )
316: 
317:         results = {}
318: 
319:         for domain, files in self.classified_files.items():
320:             # Special case: examples and memories domains should run even if no legacy files detected
321:             # (framework files need to be processed and properly located)
322:             if domain == "examples":
323:                 if self.blend_config:
324:                     framework_examples_dir = target_dir / ".ce" / self.blend_config.get_output_path("examples")
325:                 else:
326:                     framework_examples_dir = target_dir / ".ce" / "examples"
327:                 if framework_examples_dir.exists() and not files:
328:                     logger.info(f"  {domain}: No legacy files, but framework examples exist - processing...")
329:                     files = []  # Empty list signals blend mode (not migration mode)
330: 
331:             if domain == "memories":
332:                 if self.blend_config:
333:                     # Memories at project root: target/.serena/memories/
334:                     framework_memories_dir = target_dir / self.blend_config.get_framework_path("serena_memories")
335:                 else:
336:                     # Backward compatibility: hardcoded path at project root
337:                     framework_memories_dir = target_dir / ".serena" / "memories"
338:                 if framework_memories_dir.exists() and not files:
339:                     logger.info(f"  {domain}: No legacy files, but framework memories exist - processing...")
340:                     files = []  # Empty list signals blend mode (not migration mode)
341: 
342:             if not files and domain not in ["examples", "memories"]:
343:                 logger.debug(f"  {domain}: No files to blend")
344:                 continue
345: 
346:             strategy = self.strategies.get(domain)
347:             if not strategy:
348:                 logger.warning(f"  {domain}: No strategy registered (skipping)")
349:                 continue
350: 
351:             logger.info(f"  Blending {domain} ({len(files)} files)...")
352: 
353:             try:
354:                 # Execute strategy-specific blending
355:                 # Note: Each strategy has different interface (blend() vs execute())
356:                 # This is simplified - actual implementation may vary by strategy
357:                 if hasattr(strategy, 'blend'):
358:                     # BlendStrategy interface (settings, claude_md, memories, examples)
359:                     logger.debug(f"    Using blend() interface for {domain}")
360: 
361:                     # Domain-specific I/O and blending
362:                     if domain == 'settings':
363:                         # Read JSON files - use config if available, fallback to defaults
364:                         if self.blend_config:
365:                             framework_file = target_dir / self.blend_config.get_framework_path("settings")
366:                             target_file = target_dir / self.blend_config.get_output_path("claude_dir") / "settings.local.json"
367:                         else:
368:                             # Backward compatibility: hardcoded paths
369:                             framework_file = target_dir / ".ce" / ".claude" / "settings.local.json"
370:                             target_file = target_dir / ".claude" / "settings.local.json"
371: 
372:                         if not framework_file.exists():
373:                             logger.warning(f"  {domain}: Framework file not found: {framework_file}")
374:                             continue
375: 
376:                         with open(framework_file) as f:
377:                             framework_content = json.load(f)
378: 
379:                         target_content = None
380:                         if target_file.exists():
381:                             with open(target_file) as f:
382:                                 target_content = json.load(f)
383: 
384:                         # Call strategy
385:                         blended = strategy.blend(
386:                             framework_content=framework_content,
387:                             target_content=target_content,
388:                             context={"target_dir": target_dir, "llm_client": BlendingLLM()}
389:                         )
390: 
391:                         # Write result
392:                         target_file.parent.mkdir(parents=True, exist_ok=True)
393:                         with open(target_file, 'w') as f:
394:                             json.dump(blended, f, indent=2)
395: 
396:                         results[domain] = {
397:                             "success": True,
398:                             "files_processed": 1,
399:                             "message": f"‚úì {domain} blended successfully"
400:                         }
401: 
402:                     elif domain == 'claude_md':
403:                         # Read markdown files - use config if available, fallback to defaults
404:                         if self.blend_config:
405:                             framework_file = target_dir / ".ce" / self.blend_config.get_output_path("claude_md")
406:                             target_file = target_dir / self.blend_config.get_output_path("claude_md")
407:                         else:
408:                             # Backward compatibility: hardcoded paths
409:                             framework_file = target_dir / ".ce" / "CLAUDE.md"
410:                             target_file = target_dir / "CLAUDE.md"
411: 
412:                         if not framework_file.exists():
413:                             logger.warning(f"  {domain}: Framework file not found")
414:                             continue
415: 
416:                         framework_content = framework_file.read_text()
417:                         target_content = target_file.read_text() if target_file.exists() else None
418: 
419:                         # Call strategy (needs LLM client)
420:                         blended = strategy.blend(
421:                             framework_content=framework_content,
422:                             target_content=target_content,
423:                             context={"target_dir": target_dir, "llm_client": BlendingLLM()}
424:                         )
425: 
426:                         # Write result
427:                         target_file.write_text(blended)
428: 
429:                         results[domain] = {
430:                             "success": True,
431:                             "files_processed": 1,
432:                             "message": f"‚úì {domain} blended successfully"
433:                         }
434: 
435:                     elif domain in ['memories', 'examples']:
436:                         # Path-based strategies (handle their own I/O)
437:                         if domain == "memories":
438:                             # Read from framework memories location - use config if available
439:                             if self.blend_config:
440:                                 # Memories are at project root: target/.serena/memories/
441:                                 framework_dir = target_dir / self.blend_config.get_framework_path("serena_memories")
442:                             else:
443:                                 # Backward compatibility: hardcoded path at project root
444:                                 framework_dir = target_dir / ".serena" / "memories"
445:                         else:  # examples
446:                             if self.blend_config:
447:                                 framework_dir = target_dir / self.blend_config.get_framework_path("examples")
448:                             else:
449:                                 # Backward compatibility: hardcoded path
450:                                 framework_dir = target_dir / ".ce" / domain
451: 
452:                         # Pre-blend workflow for memories domain
453:                         if domain == "memories":
454:                             # Rename target's memories dir ‚Üí backup (preserve existing state)
455:                             if self.blend_config:
456:                                 output_memories = self.blend_config.get_output_path("serena_memories")
457:                                 target_serena = target_dir / output_memories.parent if output_memories.name == "memories" else target_dir / output_memories
458:                                 target_serena_old = target_dir / (output_memories.parent.name + ".old")
459:                             else:
460:                                 # Backward compatibility
461:                                 target_serena = target_dir / ".serena"
462:                                 target_serena_old = target_dir / ".serena.old"
463: 
464:                             if target_serena.exists():
465:                                 logger.info(f"    Renaming existing {target_serena.name}/ ‚Üí {target_serena_old.name}/")
466:                                 if target_serena_old.exists():
467:                                     logger.warning(f"    Removing old {target_serena_old.name}/ backup")
468:                                     shutil.rmtree(target_serena_old)
469:                                 shutil.move(str(target_serena), str(target_serena_old))
470: 
471:                             # Verify framework memories exist
472:                             if self.blend_config:
473:                                 # Memories at project root: target/.serena/memories/
474:                                 framework_serena = target_dir / self.blend_config.get_framework_path("serena_memories")
475:                             else:
476:                                 # Backward compatibility: hardcoded path at project root
477:                                 framework_serena = target_dir / ".serena" / "memories"
478: 
479:                             if not framework_serena.exists():
480:                                 raise RuntimeError(
481:                                     f"Framework memories not found at {framework_serena}\n"
482:                                     f"üîß Troubleshooting: Verify extraction completed successfully"
483:                                 )
484: 
485:                             # Update target_domain_dir to point to .serena.old/memories/ (for blending)
486:                             if target_serena_old.exists():
487:                                 target_domain_dir = target_serena_old / "memories"
488:                                 logger.info(f"    Blending from .serena.old/memories/ ‚Üí .serena/memories/")
489:                             else:
490:                                 target_domain_dir = None
491:                                 logger.info(f"    No existing memories to blend (fresh installation)")
492: 
493:                             # Note: .serena/memories/ output directory will be created by blend strategy
494: 
495:                         # Construct target directory path
496:                         if domain == "memories":
497:                             # target_domain_dir already set by pre-blend workflow above
498:                             # (either .serena.old/memories or None for fresh install)
499:                             pass
500:                         elif domain == "examples":
501:                             # Framework examples use config if available
502:                             if self.blend_config:
503:                                 target_domain_dir = target_dir / ".ce" / self.blend_config.get_output_path("examples")
504:                             else:
505:                                 # Backward compatibility
506:                                 target_domain_dir = target_dir / ".ce" / "examples"
507:                         else:
508:                             # Other domains
509:                             if self.blend_config:
510:                                 target_domain_dir = target_dir / ".ce" / self.blend_config.get_output_path(domain)
511:                             else:
512:                                 target_domain_dir = target_dir / ".ce" / domain
513: 
514:                         # Check framework dir exists (but allow examples to proceed for migration mode)
515:                         if not framework_dir.exists():
516:                             if domain == "memories":
517:                                 logger.warning(f"  {domain}: Framework directory not found: {framework_dir}")
518:                                 continue
519:                             else:  # examples - allow migration mode
520:                                 logger.info(f"  {domain}: Framework directory not found: {framework_dir}")
521: 
522:                         # Call strategy with paths (memories expects output_path in context + LLM client)
523:                         if domain == "memories":
524:                             result = strategy.blend(
525:                                 framework_content=framework_dir,  # .ce/.serena/memories/
526:                                 target_content=target_domain_dir if target_domain_dir and target_domain_dir.exists() else None,  # .serena.old/memories/
527:                                 context={
528:                                     "output_path": target_dir / ".serena" / "memories",  # Output to canonical location
529:                                     "target_dir": target_dir,
530:                                     "llm_client": BlendingLLM()
531:                                 }
532:                             )
533:                         else:  # examples
534:                             result = strategy.blend(
535:                                 framework_dir=framework_dir,
536:                                 target_dir=target_domain_dir,
537:                                 context={"target_dir": target_dir}
538:                             )
539: 
540:                         # Handle BlendResult object (memories) vs dict (examples)
541:                         if hasattr(result, 'success'):
542:                             # BlendResult object from memories strategy
543:                             results[domain] = {
544:                                 "success": result.success,
545:                                 "files_processed": result.files_processed,
546:                                 "message": f"‚úì {domain} blended successfully"
547:                             }
548:                         else:
549:                             # Dict from examples strategy
550:                             results[domain] = {
551:                                 "success": result.get("success", True),
552:                                 "files_processed": result.get("files_processed", 0),
553:                                 "message": f"‚úì {domain} blended successfully"
554:                             }
555: 
556:                     else:
557:                         logger.warning(f"  {domain}: Unknown blend() domain")
558:                         continue
559:                 elif hasattr(strategy, 'execute'):
560:                     # Simple strategy interface (prps, commands)
561:                     # Derive source_dir from classified files
562:                     if not files:
563:                         logger.debug(f"  {domain}: No files to blend")
564:                         continue
565: 
566:                     # Find common root directory for all files
567:                     # For PRPs: files could be in PRPs/, PRPs/executed/, PRPs/feature-requests/, etc.
568:                     # We need to find the common ancestor (PRPs/)
569:                     file_paths = [Path(f) for f in files]
570:                     source_dir = self._find_common_ancestor(file_paths)
571: 
572:                     # Domain-specific parameters
573:                     if domain == 'prps':
574:                         params = {
575:                             "source_dir": source_dir,
576:                             "target_dir": target_dir / ".ce" / "PRPs"
577:                         }
578:                     elif domain == 'commands':
579:                         params = {
580:                             "source_dir": source_dir,
581:                             "target_dir": target_dir / ".claude" / "commands",
582:                             "backup_dir": target_dir / ".claude" / "commands.backup"
583:                         }
584:                     else:
585:                         # Fallback for unknown strategies
586:                         params = {
587:                             "source_files": files,
588:                             "target_dir": target_dir,
589:                             "dry_run": self.dry_run
590:                         }
591: 
592:                     logger.debug(f"    Executing {domain} with params: {params}")
593:                     result = strategy.execute(params)
594:                     results[domain] = result
595:                 else:
596:                     logger.warning(f"    Strategy {domain} has no blend() or execute() method")
597: 
598:             except Exception as e:
599:                 error_msg = f"‚ùå {domain} blending failed: {e}"
600:                 logger.error(f"  {error_msg}")
601:                 results[domain] = {
602:                     "success": False,
603:                     "error": str(e),
604:                     "message": error_msg
605:                 }
606: 
607:                 # Fail fast for critical domains
608:                 if domain in ["settings", "claude_md"]:
609:                     raise RuntimeError(
610:                         f"Critical domain '{domain}' failed - cannot continue\n"
611:                         f"Error: {e}\n"
612:                         f"üîß Fix {domain} blending before proceeding"
613:                     )
614: 
615:         # Check for failures
616:         failed_domains = [d for d, r in results.items() if not r.get("success", False)]
617: 
618:         if failed_domains:
619:             logger.warning(f"‚ö†Ô∏è  Blending complete with failures ({len(results)} domains processed, {len(failed_domains)} failed)")
620:         else:
621:             logger.info(f"‚úì Blending complete ({len(results)} domains processed)")
622: 
623:         return {
624:             "phase": "blend",
625:             "implemented": True,
626:             "results": results,
627:             "success": len(failed_domains) == 0,
628:             "failed_domains": failed_domains,
629:             "message": self._format_blend_summary(results)
630:         }
631: 
632:     def _format_blend_summary(self, results: Dict[str, Any]) -> str:
633:         """
634:         Format a summary message for blend results.
635: 
636:         Args:
637:             results: Dictionary of blend results per domain
638: 
639:         Returns:
640:             Formatted summary string
641:         """
642:         success_count = sum(1 for r in results.values() if r.get("success", False))
643:         total = len(results)
644:         failed = [d for d, r in results.items() if not r.get("success", False)]
645: 
646:         summary_parts = []
647:         for domain, result in results.items():
648:             if result.get("success", False):
649:                 files = result.get("files_processed", 0)
650:                 summary_parts.append(f"  ‚úì {domain} ({files} file{'s' if files != 1 else ''})")
651:             else:
652:                 error = result.get("error", "Unknown error")
653:                 summary_parts.append(f"  ‚ùå {domain}: {error}")
654: 
655:         summary = "\n".join(summary_parts)
656: 
657:         if failed:
658:             header = f"‚ö†Ô∏è  Blend completed with failures: {success_count}/{total} domains succeeded\n"
659:         else:
660:             header = f"‚úÖ Blend completed successfully: {success_count}/{total} domains\n"
661: 
662:         return header + summary
663: 
664:     def _run_cleanup(self, target_dir: Path) -> Dict[str, Any]:
665:         """
666:         Phase D: Remove legacy directories after successful blending.
667: 
668:         Uses cleanup module to remove old CE 1.0 directories.
669: 
670:         Args:
671:             target_dir: Target project directory
672: 
673:         Returns:
674:             Dict with cleanup results
675:         """
676:         from ce.blending.cleanup import cleanup_legacy_dirs
677: 
678:         logger.info("Phase D: CLEANUP - Removing legacy directories...")
679: 
680:         status = cleanup_legacy_dirs(
681:             target_project=target_dir,
682:             dry_run=self.dry_run
683:         )
684: 
685:         success_count = sum(1 for v in status.values() if v)
686:         logger.info(f"‚úì Cleanup complete ({success_count}/{len(status)} directories removed)")
687: 
688:         return {
689:             "phase": "cleanup",
690:             "implemented": True,
691:             "status": status
692:         }
693: 
694:     def _find_common_ancestor(self, paths: List[Path]) -> Path:
695:         """
696:         Find common ancestor directory for a list of paths.
697: 
698:         For PRPs in subdirectories (PRPs/executed/, PRPs/feature-requests/),
699:         returns the common root (PRPs/).
700: 
701:         Args:
702:             paths: List of file paths
703: 
704:         Returns:
705:             Common ancestor directory
706: 
707:         Example:
708:             >>> paths = [Path("PRPs/executed/PRP-1.md"), Path("PRPs/feature-requests/PRP-2.md")]
709:             >>> _find_common_ancestor(paths)
710:             Path("PRPs")
711:         """
712:         if not paths:
713:             raise ValueError("Cannot find common ancestor of empty path list\nüîß Troubleshooting: Check inputs and system state")
714: 
715:         # Convert all paths to absolute for comparison
716:         abs_paths = [p.resolve() for p in paths]
717: 
718:         # Start with first path's parents
719:         common = abs_paths[0].parent
720: 
721:         # Find common ancestor by checking each parent
722:         while not all(common in p.parents or p.parent == common for p in abs_paths):
723:             common = common.parent
724: 
725:             # Safety check - don't go above project root
726:             if common.parent == common:
727:                 raise RuntimeError(f"Could not find common ancestor for paths: {paths}\nüîß Troubleshooting: Check inputs and system state")
728: 
729:         return common
</file>

</files>
